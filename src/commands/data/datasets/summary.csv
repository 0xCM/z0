Class               | Category      | Extension     | IsaSet        | BaseCode    | Mod       | Reg       | Pattern                                                                                             | Operands
 | AAA                  | DECIMAL        | BASE           | I86            | 37           |            |            | 0x37 not64                                                                                           | REG0=XED_REG_AL:rw:SUPP REG1=XED_REG_AH:rw:SUPP
 | AAD                  | DECIMAL        | BASE           | I86            | d5           |            |            | 0xD5 not64 UIMM8()                                                                                   | IMM0:r:b:i8 REG0=XED_REG_AL:rw:SUPP REG1=XED_REG_AH:rw:SUPP
 | AAM                  | DECIMAL        | BASE           | I86            | d4           |            |            | 0xD4 not64 UIMM8()                                                                                   | IMM0:r:b:i8 REG0=XED_REG_AL:rw:SUPP REG1=XED_REG_AH:w:SUPP
 | AAS                  | DECIMAL        | BASE           | I86            | 3f           |            |            | 0x3F not64                                                                                           | REG0=XED_REG_AL:rw:SUPP REG1=XED_REG_AH:rw:SUPP
 | ADC                  | BINARY         | BASE           | I86            | 80           | mm         | 0b010      | 0x80 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM() SIMM8() nolock_prefix                                 | MEM0:rw:b IMM0:r:b:i8
 | ADC                  | BINARY         | BASE           | I86            | 80           | 0b11       | 0b010      | 0x80 MOD[0b11] MOD=3 REG[0b010] RM[nnn] SIMM8()                                                      | REG0=GPR8_B():rw IMM0:r:b:i8
 | ADC                  | BINARY         | BASE           | I86            | 81           | mm         | 0b010      | 0x81 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM() SIMMz() nolock_prefix                                 | MEM0:rw:v IMM0:r:z
 | ADC                  | BINARY         | BASE           | I86            | 81           | 0b11       | 0b010      | 0x81 MOD[0b11] MOD=3 REG[0b010] RM[nnn] SIMMz()                                                      | REG0=GPRv_B():rw IMM0:r:z
 | ADC                  | BINARY         | BASE           | I86            | 82           | mm         | 0b010      | 0x82 MOD[mm] MOD!=3 REG[0b010] RM[nnn] not64 MODRM() SIMM8() nolock_prefix                           | MEM0:rw:b IMM0:r:b:i8
 | ADC                  | BINARY         | BASE           | I86            | 82           | 0b11       | 0b010      | 0x82 MOD[0b11] MOD=3 REG[0b010] RM[nnn] not64 SIMM8()                                                | REG0=GPR8_B():rw IMM0:r:b:i8
 | ADC                  | BINARY         | BASE           | I86            | 83           | mm         | 0b010      | 0x83 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM() SIMM8() nolock_prefix                                 | MEM0:rw:v IMM0:r:b:i8
 | ADC                  | BINARY         | BASE           | I86            | 83           | 0b11       | 0b010      | 0x83 MOD[0b11] MOD=3 REG[0b010] RM[nnn] SIMM8()                                                      | REG0=GPRv_B():rw IMM0:r:b:i8
 | ADC                  | BINARY         | BASE           | I86            | 10           | mm         | rrr        | 0x10 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                                           | MEM0:rw:b REG0=GPR8_R():r
 | ADC                  | BINARY         | BASE           | I86            | 10           | 0b11       | rrr        | 0x10 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPR8_B():rw REG1=GPR8_R():r
 | ADC                  | BINARY         | BASE           | I86            | 11           | mm         | rrr        | 0x11 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                                           | MEM0:rw:v REG0=GPRv_R():r
 | ADC                  | BINARY         | BASE           | I86            | 11           | 0b11       | rrr        | 0x11 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPRv_B():rw REG1=GPRv_R():r
 | ADC                  | BINARY         | BASE           | I86            | 12           | mm         | rrr        | 0x12 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                         | REG0=GPR8_R():rw MEM0:r:b
 | ADC                  | BINARY         | BASE           | I86            | 12           | 0b11       | rrr        | 0x12 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPR8_R():rw REG1=GPR8_B():r
 | ADC                  | BINARY         | BASE           | I86            | 13           | mm         | rrr        | 0x13 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                         | REG0=GPRv_R():rw MEM0:r:v
 | ADC                  | BINARY         | BASE           | I86            | 13           | 0b11       | rrr        | 0x13 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPRv_R():rw REG1=GPRv_B():r
 | ADC                  | BINARY         | BASE           | I86            | 14           |            |            | 0x14 SIMM8()                                                                                         | REG0=XED_REG_AL:rw:IMPL IMM0:r:b:i8
 | ADC                  | BINARY         | BASE           | I86            | 15           |            |            | 0x15 SIMMz()                                                                                         | REG0=OrAX():rw:IMPL IMM0:r:z
 | ADC_LOCK             | BINARY         | BASE           | I86            | 80           | mm         | 0b010      | 0x80 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM() SIMM8() lock_prefix                                   | MEM0:rw:b IMM0:r:b:i8
 | ADC_LOCK             | BINARY         | BASE           | I86            | 81           | mm         | 0b010      | 0x81 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM() SIMMz() lock_prefix                                   | MEM0:rw:v IMM0:r:z
 | ADC_LOCK             | BINARY         | BASE           | I86            | 82           | mm         | 0b010      | 0x82 MOD[mm] MOD!=3 REG[0b010] RM[nnn] not64 MODRM() SIMM8() lock_prefix                             | MEM0:rw:b IMM0:r:b:i8
 | ADC_LOCK             | BINARY         | BASE           | I86            | 83           | mm         | 0b010      | 0x83 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM() SIMM8() lock_prefix                                   | MEM0:rw:v IMM0:r:b:i8
 | ADC_LOCK             | BINARY         | BASE           | I86            | 10           | mm         | rrr        | 0x10 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                                             | MEM0:rw:b REG0=GPR8_R():r
 | ADC_LOCK             | BINARY         | BASE           | I86            | 11           | mm         | rrr        | 0x11 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                                             | MEM0:rw:v REG0=GPRv_R():r
 | ADCX                 | ADOX_ADCX      | ADOX_ADCX      | ADOX_ADCX      | 0f 38 f6     | 0b11       | rrr        | 0x0F 0x38 0xF6  MOD[0b11] MOD=3 REG[rrr] RM[nnn] osz_refining_prefix W0  IMMUNE66()                  | REG0=GPR32_R():rw:d REG1=GPR32_B():r:d
 | ADCX                 | ADOX_ADCX      | ADOX_ADCX      | ADOX_ADCX      | 0f 38 f6     | mm         | rrr        | 0x0F 0x38 0xF6   MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() osz_refining_prefix W0  IMMUNE66()          | REG0=GPR32_R():rw:d MEM0:r:d
 | ADCX                 | ADOX_ADCX      | ADOX_ADCX      | ADOX_ADCX      | 0f 38 f6     | 0b11       | rrr        | 0x0F 0x38 0xF6  MOD[0b11] MOD=3 REG[rrr] RM[nnn] osz_refining_prefix  W1 IMMUNE66() mode64           | REG0=GPR64_R():rw:q  REG1=GPR64_B():r:q
 | ADCX                 | ADOX_ADCX      | ADOX_ADCX      | ADOX_ADCX      | 0f 38 f6     | mm         | rrr        | 0x0F 0x38 0xF6  MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM() osz_refining_prefix  W1  IMMUNE66() mode64  | REG0=GPR64_R():rw:q  MEM0:r:q
 | ADD                  | BINARY         | BASE           | I86            | 80           | mm         | 0b000      | 0x80 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() SIMM8() nolock_prefix                                 | MEM0:rw:b IMM0:r:b:i8
 | ADD                  | BINARY         | BASE           | I86            | 80           | 0b11       | 0b000      | 0x80 MOD[0b11] MOD=3 REG[0b000] RM[nnn] SIMM8()                                                      | REG0=GPR8_B():rw IMM0:r:b
 | ADD                  | BINARY         | BASE           | I86            | 81           | mm         | 0b000      | 0x81 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() SIMMz() nolock_prefix                                 | MEM0:rw:v IMM0:r:z
 | ADD                  | BINARY         | BASE           | I86            | 81           | 0b11       | 0b000      | 0x81 MOD[0b11] MOD=3 REG[0b000] RM[nnn] SIMMz()                                                      | REG0=GPRv_B():rw IMM0:r:z
 | ADD                  | BINARY         | BASE           | I86            | 82           | mm         | 0b000      | 0x82 MOD[mm] MOD!=3 REG[0b000] RM[nnn] not64 MODRM() SIMM8() nolock_prefix                           | MEM0:rw:b IMM0:r:b:i8
 | ADD                  | BINARY         | BASE           | I86            | 82           | 0b11       | 0b000      | 0x82 MOD[0b11] MOD=3 REG[0b000] RM[nnn] not64 SIMM8()                                                | REG0=GPR8_B():rw IMM0:r:b:i8
 | ADD                  | BINARY         | BASE           | I86            | 83           | mm         | 0b000      | 0x83 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() SIMM8() nolock_prefix                                 | MEM0:rw:v IMM0:r:b:i8
 | ADD                  | BINARY         | BASE           | I86            | 83           | 0b11       | 0b000      | 0x83 MOD[0b11] MOD=3 REG[0b000] RM[nnn] SIMM8()                                                      | REG0=GPRv_B():rw IMM0:r:b:i8
 | ADD                  | BINARY         | BASE           | I86            | 00           | mm         | rrr        | 0x00 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                                           | MEM0:rw:b REG0=GPR8_R():r
 | ADD                  | BINARY         | BASE           | I86            | 00           | 0b11       | rrr        | 0x00 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPR8_B():rw REG1=GPR8_R():r
 | ADD                  | BINARY         | BASE           | I86            | 01           | mm         | rrr        | 0x01 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                                           | MEM0:rw:v REG0=GPRv_R():r
 | ADD                  | BINARY         | BASE           | I86            | 01           | 0b11       | rrr        | 0x01 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPRv_B():rw REG1=GPRv_R():r
 | ADD                  | BINARY         | BASE           | I86            | 02           | mm         | rrr        | 0x02 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                         | REG0=GPR8_R():rw MEM0:r:b
 | ADD                  | BINARY         | BASE           | I86            | 02           | 0b11       | rrr        | 0x02 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPR8_R():rw REG1=GPR8_B():r
 | ADD                  | BINARY         | BASE           | I86            | 03           | mm         | rrr        | 0x03 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                         | REG0=GPRv_R():rw MEM0:r:v
 | ADD                  | BINARY         | BASE           | I86            | 03           | 0b11       | rrr        | 0x03 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPRv_R():rw REG1=GPRv_B():r
 | ADD                  | BINARY         | BASE           | I86            | 04           |            |            | 0x04 SIMM8()                                                                                         | REG0=XED_REG_AL:rw:IMPL IMM0:r:b:i8
 | ADD                  | BINARY         | BASE           | I86            | 05           |            |            | 0x05 SIMMz()                                                                                         | REG0=OrAX():rw:IMPL IMM0:r:z
 | ADD_LOCK             | BINARY         | BASE           | I86            | 80           | mm         | 0b000      | 0x80 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() SIMM8() lock_prefix                                   | MEM0:rw:b IMM0:r:b:i8
 | ADD_LOCK             | BINARY         | BASE           | I86            | 81           | mm         | 0b000      | 0x81 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() SIMMz() lock_prefix                                   | MEM0:rw:v IMM0:r:z
 | ADD_LOCK             | BINARY         | BASE           | I86            | 82           | mm         | 0b000      | 0x82 MOD[mm] MOD!=3 REG[0b000] RM[nnn] not64 MODRM() SIMM8() lock_prefix                             | MEM0:rw:b IMM0:r:b:i8
 | ADD_LOCK             | BINARY         | BASE           | I86            | 83           | mm         | 0b000      | 0x83 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() SIMM8() lock_prefix                                   | MEM0:rw:v IMM0:r:b:i8
 | ADD_LOCK             | BINARY         | BASE           | I86            | 00           | mm         | rrr        | 0x00 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                                             | MEM0:rw:b REG0=GPR8_R():r
 | ADD_LOCK             | BINARY         | BASE           | I86            | 01           | mm         | rrr        | 0x01 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                                             | MEM0:rw:v REG0=GPRv_R():r
 | ADDPD                | SSE            | SSE2           |                | 0f 58        | mm         | rrr        | 0x0F 0x58 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:pd MEM0:r:pd
 | ADDPD                | SSE            | SSE2           |                | 0f 58        | 0b11       | rrr        | 0x0F 0x58 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:pd REG1=XMM_B():r:pd
 | ADDPS                | SSE            | SSE            |                | 0f 58        | mm         | rrr        | 0x0F 0x58 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=XMM_R():rw:ps MEM0:r:ps
 | ADDPS                | SSE            | SSE            |                | 0f 58        | 0b11       | rrr        | 0x0F 0x58 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=XMM_R():rw:ps REG1=XMM_B():r:ps
 | ADDSD                | SSE            | SSE2           |                | 0f 58        | mm         | rrr        | 0x0F 0x58 f2_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM()                     | REG0=XMM_R():rw:sd MEM0:r:sd
 | ADDSD                | SSE            | SSE2           |                | 0f 58        | 0b11       | rrr        | 0x0F 0x58 f2_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66()                            | REG0=XMM_R():rw:sd REG1=XMM_B():r:sd
 | ADDSS                | SSE            | SSE            |                | 0f 58        | mm         | rrr        | 0x0F 0x58 f3_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM()                     | REG0=XMM_R():rw:ss MEM0:r:ss
 | ADDSS                | SSE            | SSE            |                | 0f 58        | 0b11       | rrr        | 0x0F 0x58 f3_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66()                            | REG0=XMM_R():rw:ss REG1=XMM_B():r:ss
 | ADDSUBPD             | SSE            | SSE3           |                | 0f d0        | mm         | rrr        | 0x0F 0xD0 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:pd MEM0:r:pd
 | ADDSUBPD             | SSE            | SSE3           |                | 0f d0        | 0b11       | rrr        | 0x0F 0xD0 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:pd REG1=XMM_B():r:pd
 | ADDSUBPS             | SSE            | SSE3           |                | 0f d0        | mm         | rrr        | 0x0F 0xD0 f2_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM()                     | REG0=XMM_R():rw:ps MEM0:r:ps
 | ADDSUBPS             | SSE            | SSE3           |                | 0f d0        | 0b11       | rrr        | 0x0F 0xD0 f2_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66()                            | REG0=XMM_R():rw:ps REG1=XMM_B():r:ps
 | ADOX                 | ADOX_ADCX      | ADOX_ADCX      | ADOX_ADCX      | 0f 38 f6     | 0b11       | rrr        | 0x0F 0x38 0xF6  MOD[0b11] MOD=3 REG[rrr] RM[nnn] refining_f3  W0 IMMUNE66()                          | REG0=GPR32_R():rw:d  REG1=GPR32_B():r:d
 | ADOX                 | ADOX_ADCX      | ADOX_ADCX      | ADOX_ADCX      | 0f 38 f6     | mm         | rrr        | 0x0F 0x38 0xF6  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() refining_f3 W0 IMMUNE66()                    | REG0=GPR32_R():rw:d MEM0:r:d
 | ADOX                 | ADOX_ADCX      | ADOX_ADCX      | ADOX_ADCX      | 0f 38 f6     | 0b11       | rrr        | 0x0F 0x38 0xF6 MOD[0b11] MOD=3 REG[rrr] RM[nnn] refining_f3 W1 IMMUNE66()  mode64                    | REG0=GPR64_R():rw:q  REG1=GPR64_B():r:q
 | ADOX                 | ADOX_ADCX      | ADOX_ADCX      | ADOX_ADCX      | 0f 38 f6     | mm         | rrr        | 0x0F 0x38 0xF6 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM() refining_f3 W1   IMMUNE66() mode64           | REG0=GPR64_R():rw:q  MEM0:r:q
 | AESDEC               | AES            | AES            |                | 0f 38 de     | 0b11       | rrr        | 0x0F 0x38 0xDE osz_refining_prefix MOD[0b11] MOD=3  REG[rrr] RM[nnn]  REFINING66()                   | REG0=XMM_R():rw:dq  REG1=XMM_B():r:dq
 | AESDEC               | AES            | AES            |                | 0f 38 de     | mm         | rrr        | 0x0F 0x38 0xDE osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()             | REG0=XMM_R():rw:dq  MEM0:r:dq
 | AESDECLAST           | AES            | AES            |                | 0f 38 df     | 0b11       | rrr        | 0x0F 0x38 0xDF osz_refining_prefix MOD[0b11] MOD=3  REG[rrr] RM[nnn]  REFINING66()                   | REG0=XMM_R():rw:dq  REG1=XMM_B():r:dq
 | AESDECLAST           | AES            | AES            |                | 0f 38 df     | mm         | rrr        | 0x0F 0x38 0xDF osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()             | REG0=XMM_R():rw:dq  MEM0:r:dq
 | AESENC               | AES            | AES            |                | 0f 38 dc     | 0b11       | rrr        | 0x0F 0x38 0xDC osz_refining_prefix MOD[0b11] MOD=3  REG[rrr] RM[nnn]  REFINING66()                   | REG0=XMM_R():rw:dq  REG1=XMM_B():r:dq
 | AESENC               | AES            | AES            |                | 0f 38 dc     | mm         | rrr        | 0x0F 0x38 0xDC osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()             | REG0=XMM_R():rw:dq  MEM0:r:dq
 | AESENCLAST           | AES            | AES            |                | 0f 38 dd     | 0b11       | rrr        | 0x0F 0x38 0xDD osz_refining_prefix MOD[0b11] MOD=3  REG[rrr] RM[nnn]  REFINING66()                   | REG0=XMM_R():rw:dq  REG1=XMM_B():r:dq
 | AESENCLAST           | AES            | AES            |                | 0f 38 dd     | mm         | rrr        | 0x0F 0x38 0xDD osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()             | REG0=XMM_R():rw:dq  MEM0:r:dq
 | AESIMC               | AES            | AES            |                | 0f 38 db     | 0b11       | rrr        | 0x0F 0x38 0xDB osz_refining_prefix MOD[0b11] MOD=3  REG[rrr] RM[nnn]  REFINING66()                   | REG0=XMM_R():w:dq  REG1=XMM_B():r:dq
 | AESIMC               | AES            | AES            |                | 0f 38 db     | mm         | rrr        | 0x0F 0x38 0xDB osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()             | REG0=XMM_R():w:dq  MEM0:r:dq
 | AESKEYGENASSIST      | AES            | AES            |                | 0f 3a df     | 0b11       | rrr        | 0x0F 0x3A 0xDF osz_refining_prefix MOD[0b11] MOD=3  REG[rrr] RM[nnn]  REFINING66()  UIMM8()          | REG0=XMM_R():w:dq  REG1=XMM_B():r:dq IMM0:r:b
 | AESKEYGENASSIST      | AES            | AES            |                | 0f 3a df     | mm         | rrr        | 0x0F 0x3A 0xDF osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM() UIMM8()     | REG0=XMM_R():w:dq  MEM0:r:dq IMM0:r:b
 | AND                  | LOGICAL        | BASE           | I86            | 80           | mm         | 0b100      | 0x80 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM() UIMM8() nolock_prefix                                 | MEM0:rw:b IMM0:r:b
 | AND                  | LOGICAL        | BASE           | I86            | 80           | 0b11       | 0b100      | 0x80 MOD[0b11] MOD=3 REG[0b100] RM[nnn] UIMM8()                                                      | REG0=GPR8_B():rw IMM0:r:b
 | AND                  | LOGICAL        | BASE           | I86            | 81           | mm         | 0b100      | 0x81 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM() SIMMz() nolock_prefix                                 | MEM0:rw:v IMM0:r:z
 | AND                  | LOGICAL        | BASE           | I86            | 81           | 0b11       | 0b100      | 0x81 MOD[0b11] MOD=3 REG[0b100] RM[nnn] SIMMz()                                                      | REG0=GPRv_B():rw IMM0:r:z
 | AND                  | LOGICAL        | BASE           | I86            | 82           | mm         | 0b100      | 0x82 MOD[mm] MOD!=3 REG[0b100] RM[nnn] not64 MODRM() UIMM8() nolock_prefix                           | MEM0:rw:b IMM0:r:b
 | AND                  | LOGICAL        | BASE           | I86            | 82           | 0b11       | 0b100      | 0x82 MOD[0b11] MOD=3 REG[0b100] RM[nnn] not64 UIMM8()                                                | REG0=GPR8_B():rw IMM0:r:b
 | AND                  | LOGICAL        | BASE           | I86            | 83           | mm         | 0b100      | 0x83 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM() SIMM8() nolock_prefix                                 | MEM0:rw:v IMM0:r:b:i8
 | AND                  | LOGICAL        | BASE           | I86            | 83           | 0b11       | 0b100      | 0x83 MOD[0b11] MOD=3 REG[0b100] RM[nnn] SIMM8()                                                      | REG0=GPRv_B():rw IMM0:r:b:i8
 | AND                  | LOGICAL        | BASE           | I86            | 20           | mm         | rrr        | 0x20 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                                           | MEM0:rw:b REG0=GPR8_R():r
 | AND                  | LOGICAL        | BASE           | I86            | 20           | 0b11       | rrr        | 0x20 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPR8_B():rw REG1=GPR8_R():r
 | AND                  | LOGICAL        | BASE           | I86            | 21           | mm         | rrr        | 0x21 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                                           | MEM0:rw:v REG0=GPRv_R():r
 | AND                  | LOGICAL        | BASE           | I86            | 21           | 0b11       | rrr        | 0x21 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPRv_B():rw REG1=GPRv_R():r
 | AND                  | LOGICAL        | BASE           | I86            | 22           | 0b11       | rrr        | 0x22 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPR8_R():rw REG1=GPR8_B():r
 | AND                  | LOGICAL        | BASE           | I86            | 22           | mm         | rrr        | 0x22 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                         | REG0=GPR8_R():rw MEM0:r:b
 | AND                  | LOGICAL        | BASE           | I86            | 23           | 0b11       | rrr        | 0x23 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPRv_R():rw REG1=GPRv_B():r
 | AND                  | LOGICAL        | BASE           | I86            | 23           | mm         | rrr        | 0x23 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                         | REG0=GPRv_R():rw MEM0:r:v
 | AND                  | LOGICAL        | BASE           | I86            | 24           |            |            | 0x24 SIMM8()                                                                                         | REG0=XED_REG_AL:rw:IMPL IMM0:r:b:i8
 | AND                  | LOGICAL        | BASE           | I86            | 25           |            |            | 0x25 SIMMz()                                                                                         | REG0=OrAX():rw:IMPL IMM0:r:z
 | AND_LOCK             | LOGICAL        | BASE           | I86            | 80           | mm         | 0b100      | 0x80 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM() UIMM8() lock_prefix                                   | MEM0:rw:b IMM0:r:b
 | AND_LOCK             | LOGICAL        | BASE           | I86            | 81           | mm         | 0b100      | 0x81 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM() SIMMz() lock_prefix                                   | MEM0:rw:v IMM0:r:z
 | AND_LOCK             | LOGICAL        | BASE           | I86            | 82           | mm         | 0b100      | 0x82 MOD[mm] MOD!=3 REG[0b100] RM[nnn] not64 MODRM() UIMM8() lock_prefix                             | MEM0:rw:b IMM0:r:b
 | AND_LOCK             | LOGICAL        | BASE           | I86            | 83           | mm         | 0b100      | 0x83 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM() SIMM8() lock_prefix                                   | MEM0:rw:v IMM0:r:b:i8
 | AND_LOCK             | LOGICAL        | BASE           | I86            | 20           | mm         | rrr        | 0x20 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                                             | MEM0:rw:b REG0=GPR8_R():r
 | AND_LOCK             | LOGICAL        | BASE           | I86            | 21           | mm         | rrr        | 0x21 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                                             | MEM0:rw:v REG0=GPRv_R():r
 | ANDN                 | BMI1           | BMI1           |                | f2           | mm         | rrr        | VV1 0xF2 V0F38 VNP  not64 VL128 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                              | REG0=VGPR32_R():w:d REG1=VGPR32_N():r:d MEM0:r:d
 | ANDN                 | BMI1           | BMI1           |                | f2           | mm         | rrr        | VV1 0xF2 V0F38 VNP  W0 mode64 VL128 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                          | REG0=VGPR32_R():w:d REG1=VGPR32_N():r:d MEM0:r:d
 | ANDN                 | BMI1           | BMI1           |                | f2           | 0b11       | rrr        | VV1 0xF2 V0F38 VNP  not64 VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=VGPR32_R():w:d REG1=VGPR32_N():r:d REG2=VGPR32_B():r:d
 | ANDN                 | BMI1           | BMI1           |                | f2           | 0b11       | rrr        | VV1 0xF2 V0F38 VNP  W0 mode64 VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                 | REG0=VGPR32_R():w:d REG1=VGPR32_N():r:d REG2=VGPR32_B():r:d
 | ANDN                 | BMI1           | BMI1           |                | f2           | mm         | rrr        | VV1 0xF2 V0F38 VNP W1 VL128  mode64 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                          | REG0=VGPR64_R():w:q REG1=VGPR64_N():r:q MEM0:r:q
 | ANDN                 | BMI1           | BMI1           |                | f2           | 0b11       | rrr        | VV1 0xF2 V0F38 VNP W1 VL128  mode64 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                 | REG0=VGPR64_R():w:q REG1=VGPR64_N():r:q REG2=VGPR64_B():r:q
 | ANDNPD               | LOGICAL_FP     | SSE2           |                | 0f 55        | mm         | rrr        | 0x0F 0x55 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:xuq MEM0:r:xuq
 | ANDNPD               | LOGICAL_FP     | SSE2           |                | 0f 55        | 0b11       | rrr        | 0x0F 0x55 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:xuq REG1=XMM_B():r:xuq
 | ANDNPS               | LOGICAL_FP     | SSE            |                | 0f 55        | mm         | rrr        | 0x0F 0x55 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=XMM_R():rw:xud MEM0:r:xud
 | ANDNPS               | LOGICAL_FP     | SSE            |                | 0f 55        | 0b11       | rrr        | 0x0F 0x55 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=XMM_R():rw:xud REG1=XMM_B():r:xud
 | ANDPD                | LOGICAL_FP     | SSE2           |                | 0f 54        | mm         | rrr        | 0x0F 0x54 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:xuq MEM0:r:xuq
 | ANDPD                | LOGICAL_FP     | SSE2           |                | 0f 54        | 0b11       | rrr        | 0x0F 0x54 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:xuq REG1=XMM_B():r:xuq
 | ANDPS                | LOGICAL_FP     | SSE            |                | 0f 54        | mm         | rrr        | 0x0F 0x54 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=XMM_R():rw:xud MEM0:r:xud
 | ANDPS                | LOGICAL_FP     | SSE            |                | 0f 54        | 0b11       | rrr        | 0x0F 0x54 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=XMM_R():rw:xud REG1=XMM_B():r:xud
 | ARPL                 | SYSTEM         | BASE           | I286PROTECTED  | 63           | mm         | rrr        | 0x63 MOD[mm] MOD!=3 REG[rrr] RM[nnn] not64 MODRM()                                                   | MEM0:rw:w REG0=GPR16_R():r
 | ARPL                 | SYSTEM         | BASE           | I286PROTECTED  | 63           | 0b11       | rrr        | 0x63 MOD[0b11] MOD=3 REG[rrr] RM[nnn] not64                                                          | REG0=GPR16_B():rw REG1=GPR16_R():r
 | BEXTR                | BMI1           | BMI1           |                | f7           | mm         | rrr        | VV1 0xF7 V0F38 VNP not64 VL128  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                              | REG0=VGPR32_R():w:d MEM0:r:d REG1=VGPR32_N():r:d
 | BEXTR                | BMI1           | BMI1           |                | f7           | mm         | rrr        | VV1 0xF7 V0F38 VNP W0 mode64 VL128  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                          | REG0=VGPR32_R():w:d MEM0:r:d REG1=VGPR32_N():r:d
 | BEXTR                | BMI1           | BMI1           |                | f7           | 0b11       | rrr        | VV1 0xF7 V0F38 VNP not64 VL128  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=VGPR32_R():w:d REG1=VGPR32_B():r:d REG2=VGPR32_N():r:d
 | BEXTR                | BMI1           | BMI1           |                | f7           | 0b11       | rrr        | VV1 0xF7 V0F38 VNP W0 mode64 VL128  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                 | REG0=VGPR32_R():w:d REG1=VGPR32_B():r:d REG2=VGPR32_N():r:d
 | BEXTR                | BMI1           | BMI1           |                | f7           | mm         | rrr        | VV1 0xF7 V0F38 VNP W1 VL128 mode64  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                          | REG0=VGPR64_R():w:q  MEM0:r:q REG1=VGPR64_N():r:q
 | BEXTR                | BMI1           | BMI1           |                | f7           | 0b11       | rrr        | VV1 0xF7 V0F38 VNP W1 VL128 mode64 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                  | REG0=VGPR64_R():w:q REG1=VGPR64_B():r:q REG2=VGPR64_N():r:q
 | BEXTR_XOP            | TBM            | TBM            | TBM            | 10           | mm         | rrr        | XOPV 0x10 VNP not64 VL128 NOVSR XMAPA MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM32()               | REG0=GPR32_R():w:d MEM0:r:d IMM0:r:d
 | BEXTR_XOP            | TBM            | TBM            | TBM            | 10           | mm         | rrr        | XOPV 0x10 VNP mode64  VL128 NOVSR XMAPA MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM32()             | REG0=GPRy_R():w:y MEM0:r:y IMM0:r:d
 | BEXTR_XOP            | TBM            | TBM            | TBM            | 10           | 0b11       | rrr        | XOPV 0x10 VNP not64 VL128 NOVSR XMAPA MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM32()                      | REG0=GPR32_R():w:d REG1=GPR32_B():r:d IMM0:r:d
 | BEXTR_XOP            | TBM            | TBM            | TBM            | 10           | 0b11       | rrr        | XOPV 0x10 VNP mode64 VL128 NOVSR XMAPA MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM32()                     | REG0=GPRy_R():w:y REG1=GPRy_B():r:y IMM0:r:d
 | BLCFILL              | TBM            | TBM            | TBM            | 01           | mm         | 0b001      | XOPV 0x01 VNP not64 VL128  XMAP9 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM()                           | REG0=VGPR32_N():w:d MEM0:r:d
 | BLCFILL              | TBM            | TBM            | TBM            | 01           | mm         | 0b001      | XOPV 0x01 VNP mode64 VL128  XMAP9 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM()                          | REG0=VGPRy_N():w:y MEM0:r:y
 | BLCFILL              | TBM            | TBM            | TBM            | 01           | 0b11       | 0b001      | XOPV 0x01 VNP not64 VL128  XMAP9 MOD[0b11] MOD=3 REG[0b001] RM[nnn]                                  | REG0=VGPR32_N():w:d REG1=GPR32_B():r:d
 | BLCFILL              | TBM            | TBM            | TBM            | 01           | 0b11       | 0b001      | XOPV 0x01 VNP mode64 VL128  XMAP9 MOD[0b11] MOD=3 REG[0b001] RM[nnn]                                 | REG0=VGPRy_N():w:y REG1=GPRy_B():r:y
 | BLCI                 | TBM            | TBM            | TBM            | 02           | mm         | 0b110      | XOPV 0x02 VNP not64 VL128  XMAP9 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM()                           | REG0=VGPR32_N():w:d MEM0:r:d
 | BLCI                 | TBM            | TBM            | TBM            | 02           | mm         | 0b110      | XOPV 0x02 VNP mode64 VL128  XMAP9 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM()                          | REG0=VGPRy_N():w:y MEM0:r:y
 | BLCI                 | TBM            | TBM            | TBM            | 02           | 0b11       | 0b110      | XOPV 0x02 VNP not64 VL128  XMAP9 MOD[0b11] MOD=3 REG[0b110] RM[nnn]                                  | REG0=VGPR32_N():w:d REG1=GPR32_B():r:d
 | BLCI                 | TBM            | TBM            | TBM            | 02           | 0b11       | 0b110      | XOPV 0x02 VNP mode64 VL128  XMAP9 MOD[0b11] MOD=3 REG[0b110] RM[nnn]                                 | REG0=VGPRy_N():w:y REG1=GPRy_B():r:y
 | BLCIC                | TBM            | TBM            | TBM            | 01           | mm         | 0b101      | XOPV 0x01 VNP not64 VL128  XMAP9 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM()                           | REG0=VGPR32_N():w:d MEM0:r:d
 | BLCIC                | TBM            | TBM            | TBM            | 01           | mm         | 0b101      | XOPV 0x01 VNP mode64 VL128  XMAP9 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM()                          | REG0=VGPRy_N():w:y MEM0:r:y
 | BLCIC                | TBM            | TBM            | TBM            | 01           | 0b11       | 0b101      | XOPV 0x01 VNP not64 VL128  XMAP9 MOD[0b11] MOD=3 REG[0b101] RM[nnn]                                  | REG0=VGPR32_N():w:d REG1=GPR32_B():r:d
 | BLCIC                | TBM            | TBM            | TBM            | 01           | 0b11       | 0b101      | XOPV 0x01 VNP mode64 VL128  XMAP9 MOD[0b11] MOD=3 REG[0b101] RM[nnn]                                 | REG0=VGPRy_N():w:y REG1=GPRy_B():r:y
 | BLCMSK               | TBM            | TBM            | TBM            | 02           | mm         | 0b001      | XOPV 0x02 VNP not64 VL128  XMAP9 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM()                           | REG0=VGPR32_N():w:d MEM0:r:d
 | BLCMSK               | TBM            | TBM            | TBM            | 02           | mm         | 0b001      | XOPV 0x02 VNP mode64 VL128  XMAP9 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM()                          | REG0=VGPRy_N():w:y MEM0:r:y
 | BLCMSK               | TBM            | TBM            | TBM            | 02           | 0b11       | 0b001      | XOPV 0x02 VNP not64 VL128  XMAP9 MOD[0b11] MOD=3 REG[0b001] RM[nnn]                                  | REG0=VGPR32_N():w:d REG1=GPR32_B():r:d
 | BLCMSK               | TBM            | TBM            | TBM            | 02           | 0b11       | 0b001      | XOPV 0x02 VNP mode64 VL128  XMAP9 MOD[0b11] MOD=3 REG[0b001] RM[nnn]                                 | REG0=VGPRy_N():w:y REG1=GPRy_B():r:y
 | BLCS                 | TBM            | TBM            | TBM            | 01           | mm         | 0b011      | XOPV 0x01 VNP not64 VL128  XMAP9 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM()                           | REG0=VGPR32_N():w:d MEM0:r:d
 | BLCS                 | TBM            | TBM            | TBM            | 01           | mm         | 0b011      | XOPV 0x01 VNP mode64 VL128  XMAP9 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM()                          | REG0=VGPRy_N():w:y MEM0:r:y
 | BLCS                 | TBM            | TBM            | TBM            | 01           | 0b11       | 0b011      | XOPV 0x01 VNP not64 VL128  XMAP9 MOD[0b11] MOD=3 REG[0b011] RM[nnn]                                  | REG0=VGPR32_N():w:d REG1=GPR32_B():r:d
 | BLCS                 | TBM            | TBM            | TBM            | 01           | 0b11       | 0b011      | XOPV 0x01 VNP mode64 VL128  XMAP9 MOD[0b11] MOD=3 REG[0b011] RM[nnn]                                 | REG0=VGPRy_N():w:y REG1=GPRy_B():r:y
 | BLENDPD              | SSE            | SSE4           |                | 0f 3a 0d     | mm         | rrr        | 0x0F 0x3A 0x0D osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()      | REG0=XMM_R():rw:dq:f64 MEM0:r:dq:f64 IMM0:r:b
 | BLENDPD              | SSE            | SSE4           |                | 0f 3a 0d     | 0b11       | rrr        | 0x0F 0x3A 0x0D osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()             | REG0=XMM_R():rw:dq:f64 REG1=XMM_B():r:dq:f64 IMM0:r:b
 | BLENDPS              | SSE            | SSE4           |                | 0f 3a 0c     | mm         | rrr        | 0x0F 0x3A 0x0C osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()      | REG0=XMM_R():rw:dq:f32 MEM0:r:dq:f32 IMM0:r:b
 | BLENDPS              | SSE            | SSE4           |                | 0f 3a 0c     | 0b11       | rrr        | 0x0F 0x3A 0x0C osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()             | REG0=XMM_R():rw:dq:f32 REG1=XMM_B():r:dq:f32 IMM0:r:b
 | BLENDVPD             | SSE            | SSE4           |                | 0f 38 15     | mm         | rrr        | 0x0F 0x38 0x15 osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()              | REG0=XMM_R():rw:dq:f64 MEM0:r:dq:f64 REG1=XED_REG_XMM0:r:SUPP:dq:u64
 | BLENDVPD             | SSE            | SSE4           |                | 0f 38 15     | 0b11       | rrr        | 0x0F 0x38 0x15 osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn]                     | REG0=XMM_R():rw:dq:f64 REG1=XMM_B():r:dq:f64 REG2=XED_REG_XMM0:r:SUPP:dq:u64
 | BLENDVPS             | SSE            | SSE4           |                | 0f 38 14     | mm         | rrr        | 0x0F 0x38 0x14 osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()              | REG0=XMM_R():rw:dq:f32 MEM0:r:dq:f32 REG1=XED_REG_XMM0:r:SUPP:dq:u32
 | BLENDVPS             | SSE            | SSE4           |                | 0f 38 14     | 0b11       | rrr        | 0x0F 0x38 0x14 osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn]                     | REG0=XMM_R():rw:dq:f32 REG1=XMM_B():r:dq:f32 REG2=XED_REG_XMM0:r:SUPP:dq:u32
 | BLSFILL              | TBM            | TBM            | TBM            | 01           | mm         | 0b010      | XOPV 0x01 VNP not64 VL128  XMAP9 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM()                           | REG0=VGPR32_N():w:d MEM0:r:d
 | BLSFILL              | TBM            | TBM            | TBM            | 01           | mm         | 0b010      | XOPV 0x01 VNP mode64 VL128  XMAP9 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM()                          | REG0=VGPRy_N():w:y MEM0:r:y
 | BLSFILL              | TBM            | TBM            | TBM            | 01           | 0b11       | 0b010      | XOPV 0x01 VNP not64 VL128  XMAP9 MOD[0b11] MOD=3 REG[0b010] RM[nnn]                                  | REG0=VGPR32_N():w:d REG1=GPR32_B():r:d
 | BLSFILL              | TBM            | TBM            | TBM            | 01           | 0b11       | 0b010      | XOPV 0x01 VNP mode64 VL128  XMAP9 MOD[0b11] MOD=3 REG[0b010] RM[nnn]                                 | REG0=VGPRy_N():w:y REG1=GPRy_B():r:y
 | BLSI                 | BMI1           | BMI1           |                | f3           | mm         | 0b011      | VV1 0xF3 V0F38 VNP not64 VL128 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM()                             | REG0=VGPR32_N():w:d MEM0:r:d
 | BLSI                 | BMI1           | BMI1           |                | f3           | mm         | 0b011      | VV1 0xF3 V0F38 VNP W0 mode64 VL128 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM()                         | REG0=VGPR32_N():w:d MEM0:r:d
 | BLSI                 | BMI1           | BMI1           |                | f3           | 0b11       | 0b011      | VV1 0xF3 V0F38 VNP not64 VL128 MOD[0b11] MOD=3 REG[0b011] RM[nnn]                                    | REG0=VGPR32_N():w:d  REG1=VGPR32_B():r:d
 | BLSI                 | BMI1           | BMI1           |                | f3           | 0b11       | 0b011      | VV1 0xF3 V0F38 VNP W0 mode64 VL128 MOD[0b11] MOD=3 REG[0b011] RM[nnn]                                | REG0=VGPR32_N():w:d  REG1=VGPR32_B():r:d
 | BLSI                 | BMI1           | BMI1           |                | f3           | mm         | 0b011      | VV1 0xF3 V0F38 VNP W1 VL128 mode64 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM()                         | REG0=VGPR64_N():w:q MEM0:r:q
 | BLSI                 | BMI1           | BMI1           |                | f3           | 0b11       | 0b011      | VV1 0xF3 V0F38 VNP W1 VL128 mode64 MOD[0b11] MOD=3 REG[0b011] RM[nnn]                                | REG0=VGPR64_N():w:q  REG1=VGPR64_B():r:q
 | BLSIC                | TBM            | TBM            | TBM            | 01           | mm         | 0b110      | XOPV 0x01 VNP not64 VL128  XMAP9 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM()                           | REG0=VGPR32_N():w:d MEM0:r:d
 | BLSIC                | TBM            | TBM            | TBM            | 01           | mm         | 0b110      | XOPV 0x01 VNP mode64 VL128  XMAP9 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM()                          | REG0=VGPRy_N():w:y MEM0:r:y
 | BLSIC                | TBM            | TBM            | TBM            | 01           | 0b11       | 0b110      | XOPV 0x01 VNP not64 VL128  XMAP9 MOD[0b11] MOD=3 REG[0b110] RM[nnn]                                  | REG0=VGPR32_N():w:d REG1=GPR32_B():r:d
 | BLSIC                | TBM            | TBM            | TBM            | 01           | 0b11       | 0b110      | XOPV 0x01 VNP mode64 VL128  XMAP9 MOD[0b11] MOD=3 REG[0b110] RM[nnn]                                 | REG0=VGPRy_N():w:y REG1=GPRy_B():r:y
 | BLSMSK               | BMI1           | BMI1           |                | f3           | mm         | 0b010      | VV1 0xF3 V0F38 VNP not64 VL128 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM()                             | REG0=VGPR32_N():w:d MEM0:r:d
 | BLSMSK               | BMI1           | BMI1           |                | f3           | mm         | 0b010      | VV1 0xF3 V0F38 VNP W0 mode64 VL128 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM()                         | REG0=VGPR32_N():w:d MEM0:r:d
 | BLSMSK               | BMI1           | BMI1           |                | f3           | 0b11       | 0b010      | VV1 0xF3 V0F38 VNP not64 VL128 MOD[0b11] MOD=3 REG[0b010] RM[nnn]                                    | REG0=VGPR32_N():w:d  REG1=VGPR32_B():r:d
 | BLSMSK               | BMI1           | BMI1           |                | f3           | 0b11       | 0b010      | VV1 0xF3 V0F38 VNP W0 mode64 VL128 MOD[0b11] MOD=3 REG[0b010] RM[nnn]                                | REG0=VGPR32_N():w:d  REG1=VGPR32_B():r:d
 | BLSMSK               | BMI1           | BMI1           |                | f3           | mm         | 0b010      | VV1 0xF3 V0F38 VNP W1 VL128 mode64 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM()                         | REG0=VGPR64_N():w:q MEM0:r:q
 | BLSMSK               | BMI1           | BMI1           |                | f3           | 0b11       | 0b010      | VV1 0xF3 V0F38 VNP W1 VL128 mode64 MOD[0b11] MOD=3 REG[0b010] RM[nnn]                                | REG0=VGPR64_N():w:q  REG1=VGPR64_B():r:q
 | BLSR                 | BMI1           | BMI1           |                | f3           | mm         | 0b001      | VV1 0xF3 V0F38 VNP not64 VL128  MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM()                            | REG0=VGPR32_N():w:d MEM0:r:d
 | BLSR                 | BMI1           | BMI1           |                | f3           | mm         | 0b001      | VV1 0xF3 V0F38 VNP W0 mode64 VL128  MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM()                        | REG0=VGPR32_N():w:d MEM0:r:d
 | BLSR                 | BMI1           | BMI1           |                | f3           | 0b11       | 0b001      | VV1 0xF3 V0F38 VNP not64 VL128  MOD[0b11] MOD=3 REG[0b001] RM[nnn]                                   | REG0=VGPR32_N():w:d  REG1=VGPR32_B():r:d
 | BLSR                 | BMI1           | BMI1           |                | f3           | 0b11       | 0b001      | VV1 0xF3 V0F38 VNP W0 mode64 VL128  MOD[0b11] MOD=3 REG[0b001] RM[nnn]                               | REG0=VGPR32_N():w:d  REG1=VGPR32_B():r:d
 | BLSR                 | BMI1           | BMI1           |                | f3           | mm         | 0b001      | VV1 0xF3 V0F38 VNP W1 VL128 mode64 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM()                         | REG0=VGPR64_N():w:q MEM0:r:q
 | BLSR                 | BMI1           | BMI1           |                | f3           | 0b11       | 0b001      | VV1 0xF3 V0F38 VNP W1 VL128 mode64 MOD[0b11] MOD=3 REG[0b001] RM[nnn]                                | REG0=VGPR64_N():w:q  REG1=VGPR64_B():r:q
 | BNDCL                | MPX            | MPX            | MPX            | 0f 1a        | mm         | rrr        | 0x0F 0x1A MPXMODE=1  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()  f3_refining_prefix                     | REG0=BND_R():r AGEN:r
 | BNDCL                | MPX            | MPX            | MPX            | 0f 1a        | mm         | rrr        | 0x0F 0x1A MPXMODE=1 MOD[mm] MOD=3 REG[rrr] RM[nnn]   f3_refining_prefix  mode64                      | REG0=BND_R():r REG1=GPR64_B():r
 | BNDCL                | MPX            | MPX            | MPX            | 0f 1a        | mm         | rrr        | 0x0F 0x1A MPXMODE=1 MOD[mm] MOD=3 REG[rrr] RM[nnn]   f3_refining_prefix  not64                       | REG0=BND_R():r REG1=GPR32_B():r
 | BNDCN                | MPX            | MPX            | MPX            | 0f 1b        | mm         | rrr        | 0x0F 0x1B MPXMODE=1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() f2_refining_prefix                       | REG0=BND_R():r AGEN:r
 | BNDCN                | MPX            | MPX            | MPX            | 0f 1b        | mm         | rrr        | 0x0F 0x1B MPXMODE=1 MOD[mm] MOD=3 REG[rrr] RM[nnn]  f2_refining_prefix  mode64                       | REG0=BND_R():r REG1=GPR64_B():r
 | BNDCN                | MPX            | MPX            | MPX            | 0f 1b        | mm         | rrr        | 0x0F 0x1B MPXMODE=1 MOD[mm] MOD=3 REG[rrr] RM[nnn]  f2_refining_prefix  not64                        | REG0=BND_R():r REG1=GPR32_B():r
 | BNDCU                | MPX            | MPX            | MPX            | 0f 1a        | mm         | rrr        | 0x0F 0x1A MPXMODE=1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()  f2_refining_prefix                      | REG0=BND_R():r AGEN:r
 | BNDCU                | MPX            | MPX            | MPX            | 0f 1a        | mm         | rrr        | 0x0F 0x1A MPXMODE=1 MOD[mm] MOD=3 REG[rrr] RM[nnn]   f2_refining_prefix  mode64                      | REG0=BND_R():r REG1=GPR64_B():r
 | BNDCU                | MPX            | MPX            | MPX            | 0f 1a        | mm         | rrr        | 0x0F 0x1A MPXMODE=1 MOD[mm] MOD=3 REG[rrr] RM[nnn]   f2_refining_prefix  not64                       | REG0=BND_R():r REG1=GPR32_B():r
 | BNDLDX               | MPX            | MPX            | MPX            | 0f 1a        | mm         | rrr        | 0x0F 0x1A MPXMODE=1 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  no_refining_prefix not64 eamode32      | REG0=BND_R():w MEM0:r:bnd32
 | BNDLDX               | MPX            | MPX            | MPX            | 0f 1a        | mm         | rrr        | 0x0F 0x1A MPXMODE=1 MOD[mm] MOD=0 REG[rrr] RM[nnn]   MODRM()  no_refining_prefix mode64  # RM!=5     | REG0=BND_R():w MEM0:r:bnd64
 | BNDLDX               | MPX            | MPX            | MPX            | 0f 1a        | mm         | rrr        | 0x0F 0x1A MPXMODE=1 MOD[mm] MOD=1 REG[rrr] RM[nnn]   MODRM()  no_refining_prefix mode64              | REG0=BND_R():w MEM0:r:bnd64
 | BNDLDX               | MPX            | MPX            | MPX            | 0f 1a        | mm         | rrr        | 0x0F 0x1A MPXMODE=1 MOD[mm] MOD=2 REG[rrr] RM[nnn]   MODRM()  no_refining_prefix mode64              | REG0=BND_R():w MEM0:r:bnd64
 | BNDMK                | MPX            | MPX            | MPX            | 0f 1b        | mm         | rrr        | 0x0F 0x1B MPXMODE=1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()  f3_refining_prefix                      | REG0=BND_R():w  AGEN:r
 | BNDMOV               | MPX            | MPX            | MPX            | 0f 1a        | 0b11       | rrr        | 0x0F 0x1A MPXMODE=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]  osz_refining_prefix REFINING66()               | REG0=BND_R():w REG1=BND_B():r
 | BNDMOV               | MPX            | MPX            | MPX            | 0f 1a        | mm         | rrr        | 0x0F 0x1A MPXMODE=1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()  osz_refining_prefix REFINING66() mode16 eamode32 | REG0=BND_R():w MEM0:r:q:u32
 | BNDMOV               | MPX            | MPX            | MPX            | 0f 1a        | mm         | rrr        | 0x0F 0x1A MPXMODE=1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()  osz_refining_prefix REFINING66() mode32 eamode32 | REG0=BND_R():w MEM0:r:q:u32
 | BNDMOV               | MPX            | MPX            | MPX            | 0f 1a        | mm         | rrr        | 0x0F 0x1A MPXMODE=1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()  osz_refining_prefix REFINING66() mode64 | REG0=BND_R():w MEM0:r:dq:u64
 | BNDMOV               | MPX            | MPX            | MPX            | 0f 1b        | 0b11       | rrr        | 0x0F 0x1B MPXMODE=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] osz_refining_prefix REFINING66()                | REG0=BND_B():w   REG1=BND_R():r
 | BNDMOV               | MPX            | MPX            | MPX            | 0f 1b        | mm         | rrr        | 0x0F 0x1B MPXMODE=1 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  osz_refining_prefix REFINING66() mode16 eamode32 | MEM0:w:q:u32 REG0=BND_R():r
 | BNDMOV               | MPX            | MPX            | MPX            | 0f 1b        | mm         | rrr        | 0x0F 0x1B MPXMODE=1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()  osz_refining_prefix REFINING66() mode32 eamode32 | MEM0:w:q:u32 REG0=BND_R():r
 | BNDMOV               | MPX            | MPX            | MPX            | 0f 1b        | mm         | rrr        | 0x0F 0x1B MPXMODE=1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()  osz_refining_prefix REFINING66() mode64 | MEM0:w:dq:u64 REG0=BND_R():r
 | BNDSTX               | MPX            | MPX            | MPX            | 0f 1b        | mm         | rrr        | 0x0F 0x1B MPXMODE=1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()  no_refining_prefix not64 eamode32       | MEM0:w:bnd32 REG0=BND_R():r
 | BNDSTX               | MPX            | MPX            | MPX            | 0f 1b        | mm         | rrr        | 0x0F 0x1B MPXMODE=1 MOD[mm] MOD=0 REG[rrr] RM[nnn]  MODRM()  no_refining_prefix mode64 # RM!=5       | MEM0:w:bnd64 REG0=BND_R():r
 | BNDSTX               | MPX            | MPX            | MPX            | 0f 1b        | mm         | rrr        | 0x0F 0x1B MPXMODE=1 MOD[mm] MOD=1 REG[rrr] RM[nnn]  MODRM()  no_refining_prefix mode64               | MEM0:w:bnd64 REG0=BND_R():r
 | BNDSTX               | MPX            | MPX            | MPX            | 0f 1b        | mm         | rrr        | 0x0F 0x1B MPXMODE=1 MOD[mm] MOD=2 REG[rrr] RM[nnn]  MODRM()  no_refining_prefix mode64               | MEM0:w:bnd64 REG0=BND_R():r
 | BOUND                | INTERRUPT      | BASE           | I186           | 62           | mm         | rrr        | 0x62 mode16 no66_prefix  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=GPRv_R():r MEM0:r:a16
 | BOUND                | INTERRUPT      | BASE           | I186           | 62           | mm         | rrr        | 0x62 mode32 66_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                        | REG0=GPRv_R():r MEM0:r:a16
 | BOUND                | INTERRUPT      | BASE           | I186           | 62           | mm         | rrr        | 0x62 mode16 66_prefix  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                       | REG0=GPRv_R():r MEM0:r:a32
 | BOUND                | INTERRUPT      | BASE           | I186           | 62           | mm         | rrr        | 0x62 mode32 no66_prefix  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=GPRv_R():r MEM0:r:a32
 | BSF                  | BITBYTE        | BASE           | I386           | 0f bc        | mm         | rrr        | 0x0F 0xBC MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | REG0=GPRv_R():cw MEM0:r:v
 | BSF                  | BITBYTE        | BASE           | I386           | 0f bc        | 0b11       | rrr        | 0x0F 0xBC MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_R():cw REG1=GPRv_B():r
 | BSF                  | BITBYTE        | BASE           | I386           | 0f bc        | mm         | rrr        | 0x0F 0xBC not_refining_f3 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=GPRv_R():cw MEM0:r:v
 | BSF                  | BITBYTE        | BASE           | I386           | 0f bc        | 0b11       | rrr        | 0x0F 0xBC not_refining_f3 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=GPRv_R():cw REG1=GPRv_B():r
 | BSF                  | BITBYTE        | BASE           | I386           | 0f bc        | mm         | rrr        | 0x0F 0xBC refining_f3 TZCNT=0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=GPRv_R():cw MEM0:r:v
 | BSF                  | BITBYTE        | BASE           | I386           | 0f bc        | 0b11       | rrr        | 0x0F 0xBC refining_f3 TZCNT=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_R():cw REG1=GPRv_B():r
 | BSR                  | BITBYTE        | BASE           | I386           | 0f bd        | mm         | rrr        | 0x0F 0xBD  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                   | REG0=GPRv_R():cw MEM0:r:v
 | BSR                  | BITBYTE        | BASE           | I386           | 0f bd        | 0b11       | rrr        | 0x0F 0xBD  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                          | REG0=GPRv_R():cw REG1=GPRv_B():r
 | BSR                  | BITBYTE        | BASE           | I386           | 0f bd        | mm         | rrr        | 0x0F 0xBD not_refining_f3 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=GPRv_R():cw MEM0:r:v
 | BSR                  | BITBYTE        | BASE           | I386           | 0f bd        | 0b11       | rrr        | 0x0F 0xBD not_refining_f3 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=GPRv_R():cw REG1=GPRv_B():r
 | BSR                  | BITBYTE        | BASE           | I386           | 0f bd        | mm         | rrr        | 0x0F 0xBD  refining_f3 LZCNT=0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=GPRv_R():cw MEM0:r:v
 | BSR                  | BITBYTE        | BASE           | I386           | 0f bd        | 0b11       | rrr        | 0x0F 0xBD  refining_f3 LZCNT=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=GPRv_R():cw REG1=GPRv_B():r
 | BSWAP                | DATAXFER       | BASE           | I486REAL       | 0f           |            |            | 0x0F 0b1100_1 SRM[rrr]                                                                               | REG0=GPRv_SB():rw
 | BT                   | BITBYTE        | BASE           | I386           | 0f ba        | mm         | 0b100      | 0x0F 0xBA MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM() UIMM8()                                          | MEM0:r:v IMM0:r:b
 | BT                   | BITBYTE        | BASE           | I386           | 0f ba        | 0b11       | 0b100      | 0x0F 0xBA MOD[0b11] MOD=3 REG[0b100] RM[nnn] UIMM8()                                                 | REG0=GPRv_B():r IMM0:r:b
 | BT                   | BITBYTE        | BASE           | I386           | 0f a3        | mm         | rrr        | 0x0F 0xA3 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | MEM0:r:v REG0=GPRv_R():r
 | BT                   | BITBYTE        | BASE           | I386           | 0f a3        | 0b11       | rrr        | 0x0F 0xA3 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_B():r REG1=GPRv_R():r
 | BTC                  | BITBYTE        | BASE           | I386           | 0f ba        | mm         | 0b111      | 0x0F 0xBA MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM() UIMM8() nolock_prefix                            | MEM0:rw:v IMM0:r:b
 | BTC                  | BITBYTE        | BASE           | I386           | 0f ba        | 0b11       | 0b111      | 0x0F 0xBA MOD[0b11] MOD=3 REG[0b111] RM[nnn] UIMM8()                                                 | REG0=GPRv_B():rw IMM0:r:b
 | BTC                  | BITBYTE        | BASE           | I386           | 0f bb        | mm         | rrr        | 0x0F 0xBB MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                                      | MEM0:rw:v REG0=GPRv_R():r
 | BTC                  | BITBYTE        | BASE           | I386           | 0f bb        | 0b11       | rrr        | 0x0F 0xBB MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_B():rw REG1=GPRv_R():r
 | BTC_LOCK             | BITBYTE        | BASE           | I386           | 0f ba        | mm         | 0b111      | 0x0F 0xBA MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM() UIMM8() lock_prefix                              | MEM0:rw:v IMM0:r:b
 | BTC_LOCK             | BITBYTE        | BASE           | I386           | 0f bb        | mm         | rrr        | 0x0F 0xBB MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                                        | MEM0:rw:v REG0=GPRv_R():r
 | BTR                  | BITBYTE        | BASE           | I386           | 0f ba        | mm         | 0b110      | 0x0F 0xBA MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM() UIMM8() nolock_prefix                            | MEM0:rw:v IMM0:r:b
 | BTR                  | BITBYTE        | BASE           | I386           | 0f ba        | 0b11       | 0b110      | 0x0F 0xBA MOD[0b11] MOD=3 REG[0b110] RM[nnn] UIMM8()                                                 | REG0=GPRv_B():rw IMM0:r:b
 | BTR                  | BITBYTE        | BASE           | I386           | 0f b3        | mm         | rrr        | 0x0F 0xB3 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                                      | MEM0:rw:v REG0=GPRv_R():r
 | BTR                  | BITBYTE        | BASE           | I386           | 0f b3        | 0b11       | rrr        | 0x0F 0xB3 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_B():rw REG1=GPRv_R():r
 | BTR_LOCK             | BITBYTE        | BASE           | I386           | 0f ba        | mm         | 0b110      | 0x0F 0xBA MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM() UIMM8() lock_prefix                              | MEM0:rw:v IMM0:r:b
 | BTR_LOCK             | BITBYTE        | BASE           | I386           | 0f b3        | mm         | rrr        | 0x0F 0xB3 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                                        | MEM0:rw:v REG0=GPRv_R():r
 | BTS                  | BITBYTE        | BASE           | I386           | 0f ba        | mm         | 0b101      | 0x0F 0xBA MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM() UIMM8() nolock_prefix                            | MEM0:rw:v IMM0:r:b
 | BTS                  | BITBYTE        | BASE           | I386           | 0f ba        | 0b11       | 0b101      | 0x0F 0xBA MOD[0b11] MOD=3 REG[0b101] RM[nnn] UIMM8()                                                 | REG0=GPRv_B():rw IMM0:r:b
 | BTS                  | BITBYTE        | BASE           | I386           | 0f ab        | mm         | rrr        | 0x0F 0xAB MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                                      | MEM0:rw:v REG0=GPRv_R():r
 | BTS                  | BITBYTE        | BASE           | I386           | 0f ab        | 0b11       | rrr        | 0x0F 0xAB MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_B():rw REG1=GPRv_R():r
 | BTS_LOCK             | BITBYTE        | BASE           | I386           | 0f ba        | mm         | 0b101      | 0x0F 0xBA MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM() UIMM8() lock_prefix                              | MEM0:rw:v IMM0:r:b
 | BTS_LOCK             | BITBYTE        | BASE           | I386           | 0f ab        | mm         | rrr        | 0x0F 0xAB MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                                        | MEM0:rw:v REG0=GPRv_R():r
 | BZHI                 | BMI2           | BMI2           |                | f5           | mm         | rrr        | VV1 0xF5 V0F38 VNP not64 VL128  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                              | REG0=VGPR32_R():w:d MEM0:r:d REG1=VGPR32_N():r:d
 | BZHI                 | BMI2           | BMI2           |                | f5           | mm         | rrr        | VV1 0xF5 V0F38 VNP W0 mode64 VL128  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                          | REG0=VGPR32_R():w:d MEM0:r:d REG1=VGPR32_N():r:d
 | BZHI                 | BMI2           | BMI2           |                | f5           | 0b11       | rrr        | VV1 0xF5 V0F38 VNP not64 VL128  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=VGPR32_R():w:d REG1=VGPR32_B():r:d REG2=VGPR32_N():r:d
 | BZHI                 | BMI2           | BMI2           |                | f5           | 0b11       | rrr        | VV1 0xF5 V0F38 VNP W0 mode64 VL128  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                 | REG0=VGPR32_R():w:d REG1=VGPR32_B():r:d REG2=VGPR32_N():r:d
 | BZHI                 | BMI2           | BMI2           |                | f5           | mm         | rrr        | VV1 0xF5 V0F38 VNP W1 VL128 mode64  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                          | REG0=VGPR64_R():w:q  MEM0:r:q REG1=VGPR64_N():r:q
 | BZHI                 | BMI2           | BMI2           |                | f5           | 0b11       | rrr        | VV1 0xF5 V0F38 VNP W1 VL128 mode64 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                  | REG0=VGPR64_R():w:q REG1=VGPR64_B():r:q REG2=VGPR64_N():r:q
 | CALL_FAR             | CALL           | BASE           | I86            | ff           | mm         | 0b011      | 0xFF MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM()                                                       | MEM0:r:p2 REG0=XED_REG_STACKPUSH:w:spw2:SUPP REG1=rIP():w:SUPP
 | CALL_FAR             | CALL           | BASE           | I86            | 9a           |            |            | 0x9A not64 BRDISPz() UIMM16()                                                                        | PTR:r:p IMM0:r:w REG0=XED_REG_STACKPUSH:w:spw2:SUPP REG1=XED_REG_EIP:w:SUPP
 | CALL_NEAR            | CALL           | BASE           | I86            | ff           | mm         | 0b010      | 0xFF MOD[mm] MOD!=3 REG[0b010] RM[nnn]  DF64() IMMUNE66_LOOP64() MODRM()                             | MEM0:r:v REG0=XED_REG_STACKPUSH:w:spw:SUPP REG1=rIP():rw:SUPP
 | CALL_NEAR            | CALL           | BASE           | I86            | ff           | 0b11       | 0b010      | 0xFF MOD[0b11] MOD=3 REG[0b010] RM[nnn]  DF64() IMMUNE66_LOOP64()                                    | REG0=GPRv_B():r REG1=XED_REG_STACKPUSH:w:spw:SUPP REG2=rIP():rw:SUPP
 | CALL_NEAR            | CALL           | BASE           | I86            | e8           |            |            | 0xE8 not64 BRDISPz()                                                                                 | RELBR:r:z REG0=XED_REG_STACKPUSH:w:spw:SUPP REG1=XED_REG_EIP:rw:SUPP
 | CALL_NEAR            | CALL           | BASE           | I86            | e8           |            |            | 0xE8 mode64  BRDISP32() DF64() FORCE64()                                                             | RELBR:r:d REG0=XED_REG_STACKPUSH:w:spw:SUPP REG1=XED_REG_RIP:rw:SUPP
 | CBW                  | CONVERT        | BASE           | I86            | 98           |            |            | 0x98 mode16 no66_prefix                                                                              | REG0=XED_REG_AX:w:SUPP REG1=XED_REG_AL:r:SUPP
 | CBW                  | CONVERT        | BASE           | I86            | 98           |            |            | 0x98 mode32 66_prefix                                                                                | REG0=XED_REG_AX:w:SUPP REG1=XED_REG_AL:r:SUPP
 | CBW                  | CONVERT        | BASE           | I86            | 98           |            |            | 0x98 mode64 norexw_prefix 66_prefix                                                                  | REG0=XED_REG_AX:w:SUPP REG1=XED_REG_AL:r:SUPP
 | CDQ                  | CONVERT        | BASE           | I386           | 99           |            |            | 0x99 mode16 66_prefix                                                                                | REG0=XED_REG_EDX:w:SUPP REG1=XED_REG_EAX:r:SUPP
 | CDQ                  | CONVERT        | BASE           | I386           | 99           |            |            | 0x99 mode32 no66_prefix                                                                              | REG0=XED_REG_EDX:w:SUPP REG1=XED_REG_EAX:r:SUPP
 | CDQ                  | CONVERT        | BASE           | I386           | 99           |            |            | 0x99 mode64 norexw_prefix no66_prefix                                                                | REG0=XED_REG_EDX:w:SUPP REG1=XED_REG_EAX:r:SUPP
 | CDQE                 | CONVERT        | LONGMODE       |                | 98           |            |            | 0x98 mode64 rexw_prefix                                                                              | REG0=XED_REG_RAX:w:SUPP REG1=XED_REG_EAX:r:SUPP
 | CLAC                 | SMAP           | SMAP           |                | 0f 01        | 0b11       | 0b001      | 0x0F 0x01  MOD[0b11] MOD=3 REG[0b001] RM[0b010] no_refining_prefix                                   | 
 | CLC                  | FLAGOP         | BASE           | I86            | f8           |            |            | 0xF8                                                                                                 | 
 | CLD                  | FLAGOP         | BASE           | I86            | fc           |            |            | 0xFC                                                                                                 | 
 | CLDEMOTE             | CLDEMOTE       | CLDEMOTE       | CLDEMOTE       | 0f 1c        | mm         | 0b000      | 0x0F 0x1C MOD[mm] MOD!=3 REG[0b000] RM[nnn]  MODRM()  no_refining_prefix     CLDEMOTE=1              | MEM0:r:b:u8
 | CLEVICT0             | PREFETCH       | KNC            | KNCV           | ae           | mm         | 0b111      | VV1 0xAE  VL128 VF2 V0F  NOVSR MOD[mm] MOD!=3 REG[0b111] RM[nnn]  MODRM()                            | MEM0:r:mprefetch  #FIXME: rw or r???
 | CLEVICT0_EVEX        | PREFETCH       | KNCE           | KNCE           | ae           | mm         | 0b111      | KVV 0xAE  VL512 VF2 V0F  NOEVSR MOD[mm] MOD!=3 REG[0b111] RM[nnn]  MODRM() NOSWIZD()                 | MEM0:r:mprefetch
 | CLEVICT1             | PREFETCH       | KNC            | KNCV           | ae           | mm         | 0b111      | VV1 0xAE  VL128 VF3 V0F  NOVSR MOD[mm] MOD!=3 REG[0b111] RM[nnn]  MODRM()                            | MEM0:r:mprefetch  #FIXME: rw or r???
 | CLEVICT1_EVEX        | PREFETCH       | KNCE           | KNCE           | ae           | mm         | 0b111      | KVV 0xAE  VL512 VF3 V0F  NOEVSR MOD[mm] MOD!=3 REG[0b111] RM[nnn]  MODRM() NOSWIZD()                 | MEM0:r:mprefetch
 | CLFLUSH              | MISC           | CLFSH          | CLFSH          | 0f ae        | mm         | 0b111      | 0x0F 0xAE  MOD[mm] MOD!=3 REG[0b111] RM[nnn]  no_refining_prefix MODRM()                             | MEM0:r:mprefetch
 | CLFLUSHOPT           | CLFLUSHOPT     | CLFLUSHOPT     | CLFLUSHOPT     | 0f ae        | mm         | 0b111      | 0x0F 0xAE  MOD[mm] MOD!=3 REG[0b111] RM[nnn]  osz_refining_prefix REFINING66() MODRM()               | MEM0:r:mprefetch
 | CLGI                 | SYSTEM         | SVM            |                | 0f 01        | 0b11       | 0b011      | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b011] RM[0b101]                                                       | 
 | CLI                  | FLAGOP         | BASE           | I86            | fa           |            |            | 0xFA                                                                                                 | 
 | CLRSSBSY             | CET            | CET            | CET            | 0f ae        | mm         | 0b110      | 0x0F 0xAE MOD[mm] MOD!=3 REG[0b110] RM[nnn]  f3_refining_prefix     MODRM()                          | MEM0:rw:q:u64
 | CLTS                 | SYSTEM         | BASE           | I286REAL       | 0f 06        |            |            | 0x0F 0x06                                                                                            | 
 | CLWB                 | CLWB           | CLWB           | CLWB           | 0f ae        | mm         | 0b110      | 0x0F 0xAE  MOD[mm] MOD!=3 REG[0b110] RM[nnn]  osz_refining_prefix REFINING66() MODRM()               | MEM0:r:mprefetch
 | CLZERO               | CLZERO         | CLZERO         |                | 0f 01        | 0b11       | 0b111      | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b111] RM[0b100]                                                       | REG0=ArAX():r:SUPP
 | CMC                  | FLAGOP         | BASE           | I86            | f5           |            |            | 0xF5                                                                                                 | 
 | CMOVB                | CMOV           | BASE           | CMOV           | 0f 42        | mm         | rrr        | 0x0F 0x42 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | REG0=GPRv_R():cw MEM0:r:v
 | CMOVB                | CMOV           | BASE           | CMOV           | 0f 42        | 0b11       | rrr        | 0x0F 0x42 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_R():cw REG1=GPRv_B():r
 | CMOVBE               | CMOV           | BASE           | CMOV           | 0f 46        | mm         | rrr        | 0x0F 0x46 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | REG0=GPRv_R():cw MEM0:r:v
 | CMOVBE               | CMOV           | BASE           | CMOV           | 0f 46        | 0b11       | rrr        | 0x0F 0x46 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_R():cw REG1=GPRv_B():r
 | CMOVL                | CMOV           | BASE           | CMOV           | 0f 4c        | mm         | rrr        | 0x0F 0x4C MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | REG0=GPRv_R():cw MEM0:r:v
 | CMOVL                | CMOV           | BASE           | CMOV           | 0f 4c        | 0b11       | rrr        | 0x0F 0x4C MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_R():cw REG1=GPRv_B():r
 | CMOVLE               | CMOV           | BASE           | CMOV           | 0f 4e        | mm         | rrr        | 0x0F 0x4E MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | REG0=GPRv_R():cw MEM0:r:v
 | CMOVLE               | CMOV           | BASE           | CMOV           | 0f 4e        | 0b11       | rrr        | 0x0F 0x4E MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_R():cw REG1=GPRv_B():r
 | CMOVNB               | CMOV           | BASE           | CMOV           | 0f 43        | mm         | rrr        | 0x0F 0x43 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | REG0=GPRv_R():cw MEM0:r:v
 | CMOVNB               | CMOV           | BASE           | CMOV           | 0f 43        | 0b11       | rrr        | 0x0F 0x43 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_R():cw REG1=GPRv_B():r
 | CMOVNBE              | CMOV           | BASE           | CMOV           | 0f 47        | mm         | rrr        | 0x0F 0x47 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | REG0=GPRv_R():cw MEM0:r:v
 | CMOVNBE              | CMOV           | BASE           | CMOV           | 0f 47        | 0b11       | rrr        | 0x0F 0x47 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_R():cw REG1=GPRv_B():r
 | CMOVNL               | CMOV           | BASE           | CMOV           | 0f 4d        | mm         | rrr        | 0x0F 0x4D MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | REG0=GPRv_R():cw MEM0:r:v
 | CMOVNL               | CMOV           | BASE           | CMOV           | 0f 4d        | 0b11       | rrr        | 0x0F 0x4D MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_R():cw REG1=GPRv_B():r
 | CMOVNLE              | CMOV           | BASE           | CMOV           | 0f 4f        | mm         | rrr        | 0x0F 0x4F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | REG0=GPRv_R():cw MEM0:r:v
 | CMOVNLE              | CMOV           | BASE           | CMOV           | 0f 4f        | 0b11       | rrr        | 0x0F 0x4F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_R():cw REG1=GPRv_B():r
 | CMOVNO               | CMOV           | BASE           | CMOV           | 0f 41        | mm         | rrr        | 0x0F 0x41 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | REG0=GPRv_R():cw MEM0:r:v
 | CMOVNO               | CMOV           | BASE           | CMOV           | 0f 41        | 0b11       | rrr        | 0x0F 0x41 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_R():cw REG1=GPRv_B():r
 | CMOVNP               | CMOV           | BASE           | CMOV           | 0f 4b        | mm         | rrr        | 0x0F 0x4B MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | REG0=GPRv_R():cw MEM0:r:v
 | CMOVNP               | CMOV           | BASE           | CMOV           | 0f 4b        | 0b11       | rrr        | 0x0F 0x4B MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_R():cw REG1=GPRv_B():r
 | CMOVNS               | CMOV           | BASE           | CMOV           | 0f 49        | mm         | rrr        | 0x0F 0x49 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | REG0=GPRv_R():cw MEM0:r:v
 | CMOVNS               | CMOV           | BASE           | CMOV           | 0f 49        | 0b11       | rrr        | 0x0F 0x49 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_R():cw REG1=GPRv_B():r
 | CMOVNZ               | CMOV           | BASE           | CMOV           | 0f 45        | mm         | rrr        | 0x0F 0x45 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | REG0=GPRv_R():cw MEM0:r:v
 | CMOVNZ               | CMOV           | BASE           | CMOV           | 0f 45        | 0b11       | rrr        | 0x0F 0x45 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_R():cw REG1=GPRv_B():r
 | CMOVO                | CMOV           | BASE           | CMOV           | 0f 40        | mm         | rrr        | 0x0F 0x40 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | REG0=GPRv_R():cw MEM0:r:v
 | CMOVO                | CMOV           | BASE           | CMOV           | 0f 40        | 0b11       | rrr        | 0x0F 0x40 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_R():cw REG1=GPRv_B():r
 | CMOVP                | CMOV           | BASE           | CMOV           | 0f 4a        | mm         | rrr        | 0x0F 0x4A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | REG0=GPRv_R():cw MEM0:r:v
 | CMOVP                | CMOV           | BASE           | CMOV           | 0f 4a        | 0b11       | rrr        | 0x0F 0x4A MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_R():cw REG1=GPRv_B():r
 | CMOVS                | CMOV           | BASE           | CMOV           | 0f 48        | mm         | rrr        | 0x0F 0x48 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | REG0=GPRv_R():cw MEM0:r:v
 | CMOVS                | CMOV           | BASE           | CMOV           | 0f 48        | 0b11       | rrr        | 0x0F 0x48 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_R():cw REG1=GPRv_B():r
 | CMOVZ                | CMOV           | BASE           | CMOV           | 0f 44        | mm         | rrr        | 0x0F 0x44 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | REG0=GPRv_R():cw MEM0:r:v
 | CMOVZ                | CMOV           | BASE           | CMOV           | 0f 44        | 0b11       | rrr        | 0x0F 0x44 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_R():cw REG1=GPRv_B():r
 | CMP                  | BINARY         | BASE           | I86            | 80           | mm         | 0b111      | 0x80 MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM() SIMM8()                                               | MEM0:r:b IMM0:r:b:i8
 | CMP                  | BINARY         | BASE           | I86            | 80           | 0b11       | 0b111      | 0x80 MOD[0b11] MOD=3 REG[0b111] RM[nnn] SIMM8()                                                      | REG0=GPR8_B():r IMM0:r:b:i8
 | CMP                  | BINARY         | BASE           | I86            | 81           | mm         | 0b111      | 0x81 MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM() SIMMz()                                               | MEM0:r:v IMM0:r:z
 | CMP                  | BINARY         | BASE           | I86            | 81           | 0b11       | 0b111      | 0x81 MOD[0b11] MOD=3 REG[0b111] RM[nnn] SIMMz()                                                      | REG0=GPRv_B():r IMM0:r:z
 | CMP                  | BINARY         | BASE           | I86            | 82           | mm         | 0b111      | 0x82 MOD[mm] MOD!=3 REG[0b111] RM[nnn] not64 MODRM() SIMM8()                                         | MEM0:r:b IMM0:r:b:i8
 | CMP                  | BINARY         | BASE           | I86            | 82           | 0b11       | 0b111      | 0x82 MOD[0b11] MOD=3 REG[0b111] RM[nnn] not64 SIMM8()                                                | REG0=GPR8_B():r IMM0:r:b:i8
 | CMP                  | BINARY         | BASE           | I86            | 83           | mm         | 0b111      | 0x83 MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM() SIMM8()                                               | MEM0:r:v IMM0:r:b:i8
 | CMP                  | BINARY         | BASE           | I86            | 83           | 0b11       | 0b111      | 0x83 MOD[0b11] MOD=3 REG[0b111] RM[nnn] SIMM8()                                                      | REG0=GPRv_B():r IMM0:r:b:i8
 | CMP                  | BINARY         | BASE           | I86            | 38           | mm         | rrr        | 0x38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                         | MEM0:r:b REG0=GPR8_R():r
 | CMP                  | BINARY         | BASE           | I86            | 38           | 0b11       | rrr        | 0x38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPR8_B():r REG1=GPR8_R():r
 | CMP                  | BINARY         | BASE           | I86            | 39           | mm         | rrr        | 0x39 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                         | MEM0:r:v REG0=GPRv_R():r
 | CMP                  | BINARY         | BASE           | I86            | 39           | 0b11       | rrr        | 0x39 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPRv_B():r REG1=GPRv_R():r
 | CMP                  | BINARY         | BASE           | I86            | 3a           | mm         | rrr        | 0x3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                         | REG0=GPR8_R():r MEM0:r:b
 | CMP                  | BINARY         | BASE           | I86            | 3a           | 0b11       | rrr        | 0x3A MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPR8_R():r REG1=GPR8_B():r
 | CMP                  | BINARY         | BASE           | I86            | 3b           | mm         | rrr        | 0x3B MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                         | REG0=GPRv_R():r MEM0:r:v
 | CMP                  | BINARY         | BASE           | I86            | 3b           | 0b11       | rrr        | 0x3B MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPRv_R():r REG1=GPRv_B():r
 | CMP                  | BINARY         | BASE           | I86            | 3c           |            |            | 0x3C SIMM8()                                                                                         | REG0=XED_REG_AL:r:IMPL IMM0:r:b:i8
 | CMP                  | BINARY         | BASE           | I86            | 3d           |            |            | 0x3D SIMMz()                                                                                         | REG0=OrAX():r:IMPL IMM0:r:z
 | CMPPD                | SSE            | SSE2           |                | 0f c2        | mm         | rrr        | 0x0F 0xC2 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM() UIMM8()          | REG0=XMM_R():rw:pd MEM0:r:pd IMM0:r:b
 | CMPPD                | SSE            | SSE2           |                | 0f c2        | 0b11       | rrr        | 0x0F 0xC2 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66() UIMM8()                 | REG0=XMM_R():rw:pd REG1=XMM_B():r:pd IMM0:r:b
 | CMPPS                | SSE            | SSE            |                | 0f c2        | mm         | rrr        | 0x0F 0xC2 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM() UIMM8()                        | REG0=XMM_R():rw:ps MEM0:r:ps IMM0:r:b
 | CMPPS                | SSE            | SSE            |                | 0f c2        | 0b11       | rrr        | 0x0F 0xC2 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  UIMM8()                               | REG0=XMM_R():rw:ps REG1=XMM_B():r:ps IMM0:r:b
 | CMPSB                | STRINGOP       | BASE           | I86            | a6           |            |            | 0xA6 norep OVERRIDE_SEG0()                                                                           | MEM0:r:SUPP:b BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:r:SUPP:b BASE1=ArDI():rw:SUPP SEG1=FINAL_ESEG1():r:SUPP
 | CMPSD                | STRINGOP       | BASE           | I386           | a7           |            |            | 0xA7 mode16 66_prefix  norep OVERRIDE_SEG0()                                                         | MEM0:r:SUPP:d BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:r:SUPP:d BASE1=ArDI():rw:SUPP SEG1=FINAL_ESEG1():r:SUPP
 | CMPSD                | STRINGOP       | BASE           | I386           | a7           |            |            | 0xA7 mode32 no66_prefix  norep OVERRIDE_SEG0()                                                       | MEM0:r:SUPP:d BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:r:SUPP:d BASE1=ArDI():rw:SUPP SEG1=FINAL_ESEG1():r:SUPP
 | CMPSD                | STRINGOP       | BASE           | I386           | a7           |            |            | 0xA7 mode64 norexw_prefix no66_prefix norep OVERRIDE_SEG0()                                          | MEM0:r:SUPP:d BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:r:SUPP:d BASE1=ArDI():rw:SUPP SEG1=FINAL_ESEG1():r:SUPP
 | CMPSD_XMM            | SSE            | SSE2           |                | 0f c2        | mm         | rrr        | 0x0F 0xC2 f2_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM() UIMM8()             | REG0=XMM_R():rw:sd MEM0:r:sd IMM0:r:b
 | CMPSD_XMM            | SSE            | SSE2           |                | 0f c2        | 0b11       | rrr        | 0x0F 0xC2 f2_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66() UIMM8()                    | REG0=XMM_R():rw:sd REG1=XMM_B():r:sd IMM0:r:b
 | CMPSQ                | STRINGOP       | LONGMODE       |                | a7           |            |            | 0xA7 mode64 rexw_prefix norep OVERRIDE_SEG0()                                                        | MEM0:r:SUPP:q BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:r:SUPP:q BASE1=ArDI():rw:SUPP SEG1=FINAL_ESEG1():r:SUPP
 | CMPSS                | SSE            | SSE            |                | 0f c2        | mm         | rrr        | 0x0F 0xC2 f3_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM() UIMM8()             | REG0=XMM_R():rw:ss MEM0:r:ss IMM0:r:b
 | CMPSS                | SSE            | SSE            |                | 0f c2        | 0b11       | rrr        | 0x0F 0xC2 f3_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66() UIMM8()                    | REG0=XMM_R():rw:ss REG1=XMM_B():r:ss IMM0:r:b
 | CMPSW                | STRINGOP       | BASE           | I86            | a7           |            |            | 0xA7 mode16 no66_prefix   norep OVERRIDE_SEG0()                                                      | MEM0:r:SUPP:w BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:r:SUPP:w BASE1=ArDI():rw:SUPP SEG1=FINAL_ESEG1():r:SUPP
 | CMPSW                | STRINGOP       | BASE           | I86            | a7           |            |            | 0xA7 mode32 66_prefix  norep OVERRIDE_SEG0()                                                         | MEM0:r:SUPP:w BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:r:SUPP:w BASE1=ArDI():rw:SUPP SEG1=FINAL_ESEG1():r:SUPP
 | CMPSW                | STRINGOP       | BASE           | I86            | a7           |            |            | 0xA7 mode64 norexw_prefix 66_prefix  norep OVERRIDE_SEG0()                                           | MEM0:r:SUPP:w BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:r:SUPP:w BASE1=ArDI():rw:SUPP SEG1=FINAL_ESEG1():r:SUPP
 | CMPXCHG              | SEMAPHORE      | BASE           | I486REAL       | 0f b0        | mm         | rrr        | 0x0F 0xB0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                                      | MEM0:rcw:b REG0=GPR8_R():r REG1=XED_REG_AL:rcw:SUPP
 | CMPXCHG              | SEMAPHORE      | BASE           | I486REAL       | 0f b0        | 0b11       | rrr        | 0x0F 0xB0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPR8_B():rcw REG1=GPR8_R():r REG2=XED_REG_AL:rcw:SUPP
 | CMPXCHG              | SEMAPHORE      | BASE           | I486REAL       | 0f b1        | mm         | rrr        | 0x0F 0xB1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                                      | MEM0:rcw:v REG0=GPRv_R():r REG1=OrAX():rcw:SUPP
 | CMPXCHG              | SEMAPHORE      | BASE           | I486REAL       | 0f b1        | 0b11       | rrr        | 0x0F 0xB1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_B():rcw REG1=GPRv_R():r REG2=OrAX():rcw:SUPP
 | CMPXCHG_LOCK         | SEMAPHORE      | BASE           | I486REAL       | 0f b0        | mm         | rrr        | 0x0F 0xB0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                                        | MEM0:rcw:b REG0=GPR8_R():r REG1=XED_REG_AL:rcw:SUPP
 | CMPXCHG_LOCK         | SEMAPHORE      | BASE           | I486REAL       | 0f b1        | mm         | rrr        | 0x0F 0xB1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                                        | MEM0:rcw:v REG0=GPRv_R():r REG1=OrAX():rcw:SUPP
 | CMPXCHG16B           | SEMAPHORE      | LONGMODE       | CMPXCHG16B     | 0f c7        | mm         | 0b001      | 0x0F 0xC7 MOD[mm] MOD!=3 REG[0b001] RM[nnn] mode64  rexw_prefix IMMUNE66() MODRM() nolock_prefix     | MEM0:rcw:dq REG0=XED_REG_RDX:rcw:SUPP REG1=XED_REG_RAX:rcw:SUPP REG2=XED_REG_RCX:r:SUPP REG3=XED_REG_RBX:r:SUPP
 | CMPXCHG16B_LOCK      | SEMAPHORE      | LONGMODE       | CMPXCHG16B     | 0f c7        | mm         | 0b001      | 0x0F 0xC7 MOD[mm] MOD!=3 REG[0b001] RM[nnn] mode64  rexw_prefix IMMUNE66() MODRM() lock_prefix       | MEM0:rcw:dq REG0=XED_REG_RDX:rcw:SUPP REG1=XED_REG_RAX:rcw:SUPP REG2=XED_REG_RCX:r:SUPP REG3=XED_REG_RBX:r:SUPP
 | CMPXCHG8B            | SEMAPHORE      | BASE           | PENTIUMREAL    | 0f c7        | mm         | 0b001      | 0x0F 0xC7 MOD[mm] MOD!=3 REG[0b001] RM[nnn] not64 IMMUNE66() MODRM() nolock_prefix                   | MEM0:rcw:q REG0=XED_REG_EDX:rcw:SUPP REG1=XED_REG_EAX:rcw:SUPP REG2=XED_REG_ECX:r:SUPP REG3=XED_REG_EBX:r:SUPP
 | CMPXCHG8B            | SEMAPHORE      | BASE           | PENTIUMREAL    | 0f c7        | mm         | 0b001      | 0x0F 0xC7 MOD[mm] MOD!=3 REG[0b001] RM[nnn] mode64 norexw_prefix IMMUNE66() MODRM() nolock_prefix    | MEM0:rcw:q REG0=XED_REG_EDX:rcw:SUPP REG1=XED_REG_EAX:rcw:SUPP REG2=XED_REG_ECX:r:SUPP REG3=XED_REG_EBX:r:SUPP
 | CMPXCHG8B_LOCK       | SEMAPHORE      | BASE           | PENTIUMREAL    | 0f c7        | mm         | 0b001      | 0x0F 0xC7 MOD[mm] MOD!=3 REG[0b001] RM[nnn] not64 IMMUNE66() MODRM() lock_prefix                     | MEM0:rcw:q REG0=XED_REG_EDX:rcw:SUPP REG1=XED_REG_EAX:rcw:SUPP REG2=XED_REG_ECX:r:SUPP REG3=XED_REG_EBX:r:SUPP
 | CMPXCHG8B_LOCK       | SEMAPHORE      | BASE           | PENTIUMREAL    | 0f c7        | mm         | 0b001      | 0x0F 0xC7 MOD[mm] MOD!=3 REG[0b001] RM[nnn] mode64 norexw_prefix IMMUNE66() MODRM() lock_prefix      | MEM0:rcw:q REG0=XED_REG_EDX:rcw:SUPP REG1=XED_REG_EAX:rcw:SUPP REG2=XED_REG_ECX:r:SUPP REG3=XED_REG_EBX:r:SUPP
 | COMISD               | SSE            | SSE2           |                | 0f 2f        | mm         | rrr        | 0x0F 0x2F osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():r:sd MEM0:r:sd
 | COMISD               | SSE            | SSE2           |                | 0f 2f        | 0b11       | rrr        | 0x0F 0x2F osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():r:sd REG1=XMM_B():r:sd
 | COMISS               | SSE            | SSE            |                | 0f 2f        | mm         | rrr        | 0x0F 0x2F no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=XMM_R():r:ss MEM0:r:ss
 | COMISS               | SSE            | SSE            |                | 0f 2f        | 0b11       | rrr        | 0x0F 0x2F no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=XMM_R():r:ss REG1=XMM_B():r:ss
 | CPUID                | MISC           | BASE           | I486REAL       | 0f a2        |            |            | 0x0F 0xA2                                                                                            | REG0=XED_REG_EAX:rw:SUPP REG1=XED_REG_EBX:w:SUPP REG2=XED_REG_ECX:crw:SUPP REG3=XED_REG_EDX:w:SUPP
 | CQO                  | CONVERT        | LONGMODE       |                | 99           |            |            | 0x99 mode64 rexw_prefix                                                                              | REG0=XED_REG_RDX:w:SUPP REG1=XED_REG_RAX:r:SUPP
 | CRC32                | SSE            | SSE4           | SSE42          | 0f 38 f0     | mm         | rrr        | 0x0F 0x38 0xF0  f2_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]    MODRM()                        | REG0=GPRy_R():rw:y     MEM0:r:b
 | CRC32                | SSE            | SSE4           | SSE42          | 0f 38 f0     | 0b11       | rrr        | 0x0F 0x38 0xF0  f2_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                  | REG0=GPRy_R():rw:y     REG1=GPR8_B():r:b
 | CRC32                | SSE            | SSE4           | SSE42          | 0f 38 f1     | mm         | rrr        | 0x0F 0x38 0xF1  f2_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]   MODRM()                         | REG0=GPRy_R():rw:y     MEM0:r:v
 | CRC32                | SSE            | SSE4           | SSE42          | 0f 38 f1     | 0b11       | rrr        | 0x0F 0x38 0xF1  f2_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                  | REG0=GPRy_R():rw:y     REG1=GPRv_B():r:v
 | CVTDQ2PD             | CONVERT        | SSE2           |                | 0f e6        | mm         | rrr        | 0x0F 0xE6 f3_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM()                     | REG0=XMM_R():w:pd:f64 MEM0:r:q:i32
 | CVTDQ2PD             | CONVERT        | SSE2           |                | 0f e6        | 0b11       | rrr        | 0x0F 0xE6 f3_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66()                            | REG0=XMM_R():w:pd:f64 REG1=XMM_B():r:q:i32
 | CVTDQ2PS             | CONVERT        | SSE2           |                | 0f 5b        | mm         | rrr        | 0x0F 0x5B no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=XMM_R():w:ps:f32 MEM0:r:dq:i32
 | CVTDQ2PS             | CONVERT        | SSE2           |                | 0f 5b        | 0b11       | rrr        | 0x0F 0x5B no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=XMM_R():w:ps:f32 REG1=XMM_B():r:dq:i32
 | CVTPD2DQ             | CONVERT        | SSE2           |                | 0f e6        | mm         | rrr        | 0x0F 0xE6 f2_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM()                     | REG0=XMM_R():w:dq:i32 MEM0:r:pd:f64
 | CVTPD2DQ             | CONVERT        | SSE2           |                | 0f e6        | 0b11       | rrr        | 0x0F 0xE6 f2_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66()                            | REG0=XMM_R():w:dq:i32 REG1=XMM_B():r:pd:f64
 | CVTPD2PI             | CONVERT        | SSE2           |                | 0f 2d        | mm         | rrr        | 0x0F 0x2D osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=MMX_R():w:q:i32 MEM0:r:pd:f64
 | CVTPD2PI             | CONVERT        | SSE2           |                | 0f 2d        | 0b11       | rrr        | 0x0F 0x2D osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=MMX_R():w:q:i32 REG1=XMM_B():r:pd:f64
 | CVTPD2PS             | CONVERT        | SSE2           |                | 0f 5a        | mm         | rrr        | 0x0F 0x5A osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():w:ps:f32 MEM0:r:pd:f64
 | CVTPD2PS             | CONVERT        | SSE2           |                | 0f 5a        | 0b11       | rrr        | 0x0F 0x5A osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():w:ps:f32 REG1=XMM_B():r:pd:f64
 | CVTPI2PD             | CONVERT        | SSE2           |                | 0f 2a        | mm         | rrr        | 0x0F 0x2A osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():w:pd:f64 MEM0:r:q:i32
 | CVTPI2PD             | CONVERT        | SSE2           |                | 0f 2a        | 0b11       | rrr        | 0x0F 0x2A osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():w:pd:f64 REG1=MMX_B():r:q:i32
 | CVTPI2PS             | CONVERT        | SSE            |                | 0f 2a        | mm         | rrr        | 0x0F 0x2A no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=XMM_R():w:q:f32 MEM0:r:q:i32
 | CVTPI2PS             | CONVERT        | SSE            |                | 0f 2a        | 0b11       | rrr        | 0x0F 0x2A no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=XMM_R():w:q:f32 REG1=MMX_B():r:q:i32
 | CVTPS2DQ             | CONVERT        | SSE2           |                | 0f 5b        | mm         | rrr        | 0x0F 0x5B osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():w:dq:i32 MEM0:r:ps:f32
 | CVTPS2DQ             | CONVERT        | SSE2           |                | 0f 5b        | 0b11       | rrr        | 0x0F 0x5B osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():w:dq:i32 REG1=XMM_B():r:ps:f32
 | CVTPS2PD             | CONVERT        | SSE2           |                | 0f 5a        | mm         | rrr        | 0x0F 0x5A no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=XMM_R():w:pd:f64 MEM0:r:q:f32
 | CVTPS2PD             | CONVERT        | SSE2           |                | 0f 5a        | 0b11       | rrr        | 0x0F 0x5A no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=XMM_R():w:pd:f64 REG1=XMM_B():r:q:f32
 | CVTPS2PI             | CONVERT        | SSE            |                | 0f 2d        | mm         | rrr        | 0x0F 0x2D no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():w:q:i32 MEM0:r:q:f32
 | CVTPS2PI             | CONVERT        | SSE            |                | 0f 2d        | 0b11       | rrr        | 0x0F 0x2D no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():w:q:i32 REG1=XMM_B():r:q:f32
 | CVTSD2SI             | CONVERT        | SSE2           |                | 0f 2d        | mm         | rrr        | 0x0F 0x2D f2_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() norexw_prefix MODRM()       | REG0=GPR32_R():w:d:i32 MEM0:r:sd:f64
 | CVTSD2SI             | CONVERT        | SSE2           |                | 0f 2d        | 0b11       | rrr        | 0x0F 0x2D f2_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66() norexw_prefix              | REG0=GPR32_R():w:d:i32 REG1=XMM_B():r:sd:f64
 | CVTSD2SI             | CONVERT        | SSE2           |                | 0f 2d        | mm         | rrr        | 0x0F 0x2D f2_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() rexw_prefix MODRM()         | REG0=GPR64_R():w:q:i64 MEM0:r:sd:f64
 | CVTSD2SI             | CONVERT        | SSE2           |                | 0f 2d        | 0b11       | rrr        | 0x0F 0x2D f2_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66() rexw_prefix                | REG0=GPR64_R():w:q:i64 REG1=XMM_B():r:sd:f64
 | CVTSD2SS             | CONVERT        | SSE2           |                | 0f 5a        | mm         | rrr        | 0x0F 0x5A f2_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM()                     | REG0=XMM_R():w:ss:f32 MEM0:r:sd:f64
 | CVTSD2SS             | CONVERT        | SSE2           |                | 0f 5a        | 0b11       | rrr        | 0x0F 0x5A f2_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66()                            | REG0=XMM_R():w:ss:f32 REG1=XMM_B():r:sd:f64
 | CVTSI2SD             | CONVERT        | SSE2           |                | 0f 2a        | mm         | rrr        | 0x0F 0x2A f2_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() norexw_prefix MODRM()       | REG0=XMM_R():w:sd:f64 MEM0:r:d:i32
 | CVTSI2SD             | CONVERT        | SSE2           |                | 0f 2a        | 0b11       | rrr        | 0x0F 0x2A f2_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66() norexw_prefix              | REG0=XMM_R():w:sd:f64 REG1=GPR32_B():r:d:i32
 | CVTSI2SD             | CONVERT        | SSE2           |                | 0f 2a        | mm         | rrr        | 0x0F 0x2A f2_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() rexw_prefix MODRM()         | REG0=XMM_R():w:sd:f64 MEM0:r:q:i64
 | CVTSI2SD             | CONVERT        | SSE2           |                | 0f 2a        | 0b11       | rrr        | 0x0F 0x2A f2_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66() rexw_prefix                | REG0=XMM_R():w:sd:f64 REG1=GPR64_B():r:q:i64
 | CVTSI2SS             | CONVERT        | SSE            |                | 0f 2a        | mm         | rrr        | 0x0F 0x2A f3_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() norexw_prefix MODRM()       | REG0=XMM_R():w:ss:f32 MEM0:r:d:i32
 | CVTSI2SS             | CONVERT        | SSE            |                | 0f 2a        | 0b11       | rrr        | 0x0F 0x2A f3_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66() norexw_prefix              | REG0=XMM_R():w:ss:f32 REG1=GPR32_B():r:d:i32
 | CVTSI2SS             | CONVERT        | SSE            |                | 0f 2a        | mm         | rrr        | 0x0F 0x2A f3_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() rexw_prefix MODRM()         | REG0=XMM_R():w:ss:f32 MEM0:r:q:i32
 | CVTSI2SS             | CONVERT        | SSE            |                | 0f 2a        | 0b11       | rrr        | 0x0F 0x2A f3_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66() rexw_prefix                | REG0=XMM_R():w:ss:f32 REG1=GPR64_B():r:q:i32
 | CVTSS2SD             | CONVERT        | SSE2           |                | 0f 5a        | mm         | rrr        | 0x0F 0x5A f3_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM()                     | REG0=XMM_R():w:sd:f64 MEM0:r:ss:f32
 | CVTSS2SD             | CONVERT        | SSE2           |                | 0f 5a        | 0b11       | rrr        | 0x0F 0x5A f3_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66()                            | REG0=XMM_R():w:sd:f64 REG1=XMM_B():r:ss:f32
 | CVTSS2SI             | CONVERT        | SSE            |                | 0f 2d        | mm         | rrr        | 0x0F 0x2D f3_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() norexw_prefix MODRM()       | REG0=GPR32_R():w:d:i32 MEM0:r:ss:f32
 | CVTSS2SI             | CONVERT        | SSE            |                | 0f 2d        | 0b11       | rrr        | 0x0F 0x2D f3_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66() norexw_prefix              | REG0=GPR32_R():w:d:i32 REG1=XMM_B():r:ss:f32
 | CVTSS2SI             | CONVERT        | SSE            |                | 0f 2d        | mm         | rrr        | 0x0F 0x2D f3_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() rexw_prefix MODRM()         | REG0=GPR64_R():w:q:i64 MEM0:r:ss:f32
 | CVTSS2SI             | CONVERT        | SSE            |                | 0f 2d        | 0b11       | rrr        | 0x0F 0x2D f3_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66() rexw_prefix                | REG0=GPR64_R():w:q:i64 REG1=XMM_B():r:ss:f32
 | CVTTPD2DQ            | CONVERT        | SSE2           |                | 0f e6        | mm         | rrr        | 0x0F 0xE6 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():w:dq:i32 MEM0:r:pd:f64
 | CVTTPD2DQ            | CONVERT        | SSE2           |                | 0f e6        | 0b11       | rrr        | 0x0F 0xE6 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():w:dq:i32 REG1=XMM_B():r:pd:f64
 | CVTTPD2PI            | CONVERT        | SSE2           |                | 0f 2c        | mm         | rrr        | 0x0F 0x2C osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=MMX_R():w:q:i32 MEM0:r:pd:f64
 | CVTTPD2PI            | CONVERT        | SSE2           |                | 0f 2c        | 0b11       | rrr        | 0x0F 0x2C osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=MMX_R():w:q:i32 REG1=XMM_B():r:pd:f64
 | CVTTPS2DQ            | CONVERT        | SSE2           |                | 0f 5b        | mm         | rrr        | 0x0F 0x5B f3_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM()                     | REG0=XMM_R():w:dq:i32 MEM0:r:ps:f32
 | CVTTPS2DQ            | CONVERT        | SSE2           |                | 0f 5b        | 0b11       | rrr        | 0x0F 0x5B f3_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66()                            | REG0=XMM_R():w:dq:i32 REG1=XMM_B():r:ps:f32
 | CVTTPS2PI            | CONVERT        | SSE            |                | 0f 2c        | mm         | rrr        | 0x0F 0x2C no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():w:q:i32 MEM0:r:q:f32
 | CVTTPS2PI            | CONVERT        | SSE            |                | 0f 2c        | 0b11       | rrr        | 0x0F 0x2C no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():w:q:i32 REG1=XMM_B():r:q:f32
 | CVTTSD2SI            | CONVERT        | SSE2           |                | 0f 2c        | mm         | rrr        | 0x0F 0x2C f2_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() norexw_prefix MODRM()       | REG0=GPR32_R():w:d:i32 MEM0:r:sd:f64
 | CVTTSD2SI            | CONVERT        | SSE2           |                | 0f 2c        | 0b11       | rrr        | 0x0F 0x2C f2_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66() norexw_prefix              | REG0=GPR32_R():w:d:i32 REG1=XMM_B():r:sd:f64
 | CVTTSD2SI            | CONVERT        | SSE2           |                | 0f 2c        | mm         | rrr        | 0x0F 0x2C f2_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() rexw_prefix MODRM()         | REG0=GPR64_R():w:q:i64 MEM0:r:sd:f64
 | CVTTSD2SI            | CONVERT        | SSE2           |                | 0f 2c        | 0b11       | rrr        | 0x0F 0x2C f2_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66() rexw_prefix                | REG0=GPR64_R():w:q:i64 REG1=XMM_B():r:sd:f64
 | CVTTSS2SI            | CONVERT        | SSE            |                | 0f 2c        | mm         | rrr        | 0x0F 0x2C f3_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() norexw_prefix MODRM()       | REG0=GPR32_R():w:d:i32 MEM0:r:ss:f32
 | CVTTSS2SI            | CONVERT        | SSE            |                | 0f 2c        | 0b11       | rrr        | 0x0F 0x2C f3_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66() norexw_prefix              | REG0=GPR32_R():w:d:i32 REG1=XMM_B():r:ss:f32
 | CVTTSS2SI            | CONVERT        | SSE            |                | 0f 2c        | mm         | rrr        | 0x0F 0x2C f3_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() rexw_prefix MODRM()         | REG0=GPR64_R():w:q:i64 MEM0:r:ss:f32
 | CVTTSS2SI            | CONVERT        | SSE            |                | 0f 2c        | 0b11       | rrr        | 0x0F 0x2C f3_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66() rexw_prefix                | REG0=GPR64_R():w:q:i64 REG1=XMM_B():r:ss:f32
 | CWD                  | CONVERT        | BASE           | I86            | 99           |            |            | 0x99 mode16  no66_prefix                                                                             | REG0=XED_REG_DX:w:SUPP REG1=XED_REG_AX:r:SUPP
 | CWD                  | CONVERT        | BASE           | I86            | 99           |            |            | 0x99 mode32 66_prefix                                                                                | REG0=XED_REG_DX:w:SUPP REG1=XED_REG_AX:r:SUPP
 | CWD                  | CONVERT        | BASE           | I86            | 99           |            |            | 0x99 mode64 norexw_prefix 66_prefix                                                                  | REG0=XED_REG_DX:w:SUPP REG1=XED_REG_AX:r:SUPP
 | CWDE                 | CONVERT        | BASE           | I386           | 98           |            |            | 0x98 mode16 66_prefix                                                                                | REG0=XED_REG_EAX:w:SUPP REG1=XED_REG_AX:r:SUPP
 | CWDE                 | CONVERT        | BASE           | I386           | 98           |            |            | 0x98 mode32 no66_prefix                                                                              | REG0=XED_REG_EAX:w:SUPP REG1=XED_REG_AX:r:SUPP
 | CWDE                 | CONVERT        | BASE           | I386           | 98           |            |            | 0x98 mode64 norexw_prefix no66_prefix                                                                | REG0=XED_REG_EAX:w:SUPP REG1=XED_REG_AX:r:SUPP
 | DAA                  | DECIMAL        | BASE           | I86            | 27           |            |            | 0x27 not64                                                                                           | REG0=XED_REG_AL:rw:SUPP
 | DAS                  | DECIMAL        | BASE           | I86            | 2f           |            |            | 0x2F not64                                                                                           | REG0=XED_REG_AL:rw:SUPP
 | DEC                  | BINARY         | BASE           | I86            | fe           | mm         | 0b001      | 0xFE MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM() nolock_prefix                                         | MEM0:rw:b
 | DEC                  | BINARY         | BASE           | I86            | fe           | 0b11       | 0b001      | 0xFE MOD[0b11] MOD=3 REG[0b001] RM[nnn]                                                              | REG0=GPR8_B():rw
 | DEC                  | BINARY         | BASE           | I86            | ff           | mm         | 0b001      | 0xFF MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM() nolock_prefix                                         | MEM0:rw:v
 | DEC                  | BINARY         | BASE           | I86            | ff           | 0b11       | 0b001      | 0xFF MOD[0b11] MOD=3 REG[0b001] RM[nnn]                                                              | REG0=GPRv_B():rw
 | DEC                  | BINARY         | BASE           | I86            |              |            |            | 0b0100_1 SRM[rrr] not64                                                                              | REG0=GPRv_SB():rw
 | DEC_LOCK             | BINARY         | BASE           | I86            | fe           | mm         | 0b001      | 0xFE MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM() lock_prefix                                           | MEM0:rw:b
 | DEC_LOCK             | BINARY         | BASE           | I86            | ff           | mm         | 0b001      | 0xFF MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM() lock_prefix                                           | MEM0:rw:v
 | DELAY                | KNCSCALAR      | KNC            | KNCV           | ae           | 0b11       | 0b110      | VV1 0xAE  VL128 VF3 V0F NOVSR MOD[0b11] MOD=3 REG[0b110] RM[nnn]  W0                                 | REG0=GPR32_B():r:d
 | DELAY                | KNCSCALAR      | KNC            | KNCV           | ae           | 0b11       | 0b110      | VV1 0xAE  VL128 VF3 V0F NOVSR MOD[0b11] MOD=3 REG[0b110] RM[nnn]  W1                                 | REG0=GPR64_B():r:q
 | DIV                  | BINARY         | BASE           | I86            | f6           | mm         | 0b110      | 0xF6 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM()                                                       | MEM0:r:b REG0=XED_REG_AX:rw:SUPP
 | DIV                  | BINARY         | BASE           | I86            | f6           | 0b11       | 0b110      | 0xF6 MOD[0b11] MOD=3 REG[0b110] RM[nnn]                                                              | REG0=GPR8_B():r REG1=XED_REG_AX:rw:SUPP
 | DIV                  | BINARY         | BASE           | I86            | f7           | mm         | 0b110      | 0xF7 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM()                                                       | MEM0:r:v REG0=OrAX():rw:SUPP REG1=OrDX():rw:SUPP
 | DIV                  | BINARY         | BASE           | I86            | f7           | 0b11       | 0b110      | 0xF7 MOD[0b11] MOD=3 REG[0b110] RM[nnn]                                                              | REG0=GPRv_B():r REG1=OrAX():rw:SUPP REG2=OrDX():rw:SUPP
 | DIVPD                | SSE            | SSE2           |                | 0f 5e        | mm         | rrr        | 0x0F 0x5E osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:pd MEM0:r:pd
 | DIVPD                | SSE            | SSE2           |                | 0f 5e        | 0b11       | rrr        | 0x0F 0x5E osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:pd REG1=XMM_B():r:pd
 | DIVPS                | SSE            | SSE            |                | 0f 5e        | mm         | rrr        | 0x0F 0x5E no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=XMM_R():rw:ps MEM0:r:ps
 | DIVPS                | SSE            | SSE            |                | 0f 5e        | 0b11       | rrr        | 0x0F 0x5E no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=XMM_R():rw:ps REG1=XMM_B():r:ps
 | DIVSD                | SSE            | SSE2           |                | 0f 5e        | mm         | rrr        | 0x0F 0x5E f2_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM()                     | REG0=XMM_R():rw:sd MEM0:r:sd
 | DIVSD                | SSE            | SSE2           |                | 0f 5e        | 0b11       | rrr        | 0x0F 0x5E f2_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66()                            | REG0=XMM_R():rw:sd REG1=XMM_B():r:sd
 | DIVSS                | SSE            | SSE            |                | 0f 5e        | mm         | rrr        | 0x0F 0x5E f3_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM()                     | REG0=XMM_R():rw:ss MEM0:r:ss
 | DIVSS                | SSE            | SSE            |                | 0f 5e        | 0b11       | rrr        | 0x0F 0x5E f3_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66()                            | REG0=XMM_R():rw:ss REG1=XMM_B():r:ss
 | DPPD                 | SSE            | SSE4           |                | 0f 3a 41     | mm         | rrr        | 0x0F 0x3A 0x41 osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()      | REG0=XMM_R():rw:dq:f64 MEM0:r:dq:f64  IMM0:r:b
 | DPPD                 | SSE            | SSE4           |                | 0f 3a 41     | 0b11       | rrr        | 0x0F 0x3A 0x41 osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()             | REG0=XMM_R():rw:dq:f64 REG1=XMM_B():r:dq:f64  IMM0:r:b
 | DPPS                 | SSE            | SSE4           |                | 0f 3a 40     | mm         | rrr        | 0x0F 0x3A 0x40 osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()      | REG0=XMM_R():rw:dq:f32 MEM0:r:dq:f32  IMM0:r:b
 | DPPS                 | SSE            | SSE4           |                | 0f 3a 40     | 0b11       | rrr        | 0x0F 0x3A 0x40 osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()             | REG0=XMM_R():rw:dq:f32 REG1=XMM_B():r:dq:f32  IMM0:r:b
 | EMMS                 | MMX            | MMX            | PENTIUMMMX     | 0f 77        |            |            | 0x0F 0x77 no_refining_prefix                                                                         | 
 | ENCLS                | SGX            | SGX            | SGX            | 0f 01        | 0b11       | 0b001      | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b001] RM[0b111] no_refining_prefix                                    | REG0=XED_REG_EAX:r:SUPP    \
 | ENCLU                | SGX            | SGX            | SGX            | 0f 01        | 0b11       | 0b010      | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b010] RM[0b111] no_refining_prefix                                    | REG0=XED_REG_EAX:r:SUPP    \
 | ENCLV                | SGX            | SGX_ENCLV      | SGX_ENCLV      | 0f 01        | 0b11       | 0b000      | 0x0F 0x01 MOD[0b11] MOD=3  REG[0b000] RM[0b000]  no_refining_prefix                                  | REG0=XED_REG_EAX:r:SUPP:d:u32 REG1=XED_REG_RBX:crw:SUPP:q:u64 REG2=XED_REG_RCX:crw:SUPP:q:u64 REG3=XED_REG_RDX:crw:SUPP:q:u64
 | ENDBR32              | CET            | CET            | CET            | 0f 1e        | 0b11       | 0b111      | 0x0F 0x1E MOD[0b11] MOD=3  REG[0b111] RM[0b011]  f3_refining_prefix     CET=1                        | 
 | ENDBR64              | CET            | CET            | CET            | 0f 1e        | 0b11       | 0b111      | 0x0F 0x1E MOD[0b11] MOD=3  REG[0b111] RM[0b010]  f3_refining_prefix     CET=1                        | 
 | ENQCMD               | ENQCMD         | ENQCMD         | ENQCMD         | 0f 38 f8     | mm         | rrr        | 0x0F 0x38 0xF8 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  f2_refining_prefix                          | REG0=A_GPR_R():r MEM0:r:zd:u32
 | ENQCMDS              | ENQCMD         | ENQCMD         | ENQCMD         | 0f 38 f8     | mm         | rrr        | 0x0F 0x38 0xF8 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  f3_refining_prefix                          | REG0=A_GPR_R():r MEM0:r:zd:u32
 | ENTER                | MISC           | BASE           | I186           | c8           |            |            | 0xC8 DF64() UIMM16() UIMM8_1()                                                                       | IMM0:r:w IMM1:r:b REG0=XED_REG_STACKPUSH:w:spw:SUPP REG1=OrBP():rw:SUPP
 | EXTRACTPS            | SSE            | SSE4           |                | 0f 3a 17     | mm         | rrr        | 0x0F 0x3A 0x17 osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()      | MEM0:w:d REG0=XMM_R():r:ps  IMM0:r:b
 | EXTRACTPS            | SSE            | SSE4           |                | 0f 3a 17     | 0b11       | rrr        | 0x0F 0x3A 0x17 osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()             | REG0=GPR32_B():w:d REG1=XMM_R():r:dq  IMM0:r:b
 | F2XM1                | X87_ALU        | X87            |                | d9           | 0b11       | 0b110      | 0xD9 MOD[0b11] MOD=3 REG[0b110] RM[0b000]                                                            | REG0=XED_REG_ST0:rw:SUPP:f80 REG1=XED_REG_X87STATUS:w:SUPP
 | FABS                 | X87_ALU        | X87            |                | d9           | 0b11       | 0b100      | 0xD9 MOD[0b11] MOD=3 REG[0b100] RM[0b001]                                                            | REG0=XED_REG_ST0:rw:SUPP:f80 REG1=XED_REG_X87STATUS:w:SUPP
 | FADD                 | X87_ALU        | X87            |                | d8           | mm         | 0b000      | 0xD8 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:rw:IMPL:f80 MEM0:r:mem32real REG1=XED_REG_X87STATUS:w:SUPP
 | FADD                 | X87_ALU        | X87            |                | d8           | 0b11       | 0b000      | 0xD8 MOD[0b11] MOD=3 REG[0b000] RM[nnn]                                                              | REG0=XED_REG_ST0:rw:IMPL:f80 REG1=X87():r:f80 REG2=XED_REG_X87STATUS:w:SUPP
 | FADD                 | X87_ALU        | X87            |                | dc           | mm         | 0b000      | 0xDC MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:rw:IMPL:f80 MEM0:r:m64real REG1=XED_REG_X87STATUS:w:SUPP
 | FADD                 | X87_ALU        | X87            |                | dc           | 0b11       | 0b000      | 0xDC MOD[0b11] MOD=3 REG[0b000] RM[nnn]                                                              | REG0=X87():rw:f80 REG1=XED_REG_ST0:r:IMPL:f80 REG2=XED_REG_X87STATUS:w:SUPP
 | FADDP                | X87_ALU        | X87            |                | de           | 0b11       | 0b000      | 0xDE MOD[0b11] MOD=3 REG[0b000] RM[nnn]                                                              | REG0=X87():rw:f80 REG1=XED_REG_ST0:r:IMPL:f80 REG2=XED_REG_X87POP:r:SUPP REG3=XED_REG_X87STATUS:w:SUPP
 | FBLD                 | X87_ALU        | X87            |                | df           | mm         | 0b100      | 0xDF MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:w:IMPL:f80 MEM0:r:mem80dec  REG1=XED_REG_X87PUSH:r:SUPP REG2=XED_REG_X87STATUS:w:SUPP
 | FBSTP                | X87_ALU        | X87            |                | df           | mm         | 0b110      | 0xDF MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM()                                                       | MEM0:w:mem80dec REG0=XED_REG_ST0:r:IMPL:f80  REG1=XED_REG_X87POP:r:SUPP REG2=XED_REG_X87STATUS:w:SUPP
 | FCHS                 | X87_ALU        | X87            |                | d9           | 0b11       | 0b100      | 0xD9 MOD[0b11] MOD=3 REG[0b100] RM[0b000]                                                            | REG0=XED_REG_ST0:rw:SUPP:f80 REG1=XED_REG_X87STATUS:w:SUPP
 | FCMOVB               | FCMOV          | X87            | FCMOV          | da           | 0b11       | 0b000      | 0xDA MOD[0b11] MOD=3 REG[0b000] RM[nnn]                                                              | REG0=XED_REG_ST0:cw:IMPL:f80 REG1=X87():r:f80 REG2=XED_REG_X87STATUS:w:SUPP
 | FCMOVBE              | FCMOV          | X87            | FCMOV          | da           | 0b11       | 0b010      | 0xDA MOD[0b11] MOD=3 REG[0b010] RM[nnn]                                                              | REG0=XED_REG_ST0:cw:IMPL:f80 REG1=X87():r:f80 REG2=XED_REG_X87STATUS:w:SUPP
 | FCMOVE               | FCMOV          | X87            | FCMOV          | da           | 0b11       | 0b001      | 0xDA MOD[0b11] MOD=3 REG[0b001] RM[nnn]                                                              | REG0=XED_REG_ST0:cw:IMPL:f80 REG1=X87():r:f80 REG2=XED_REG_X87STATUS:w:SUPP
 | FCMOVNB              | FCMOV          | X87            | FCMOV          | db           | 0b11       | 0b000      | 0xDB MOD[0b11] MOD=3 REG[0b000] RM[nnn]                                                              | REG0=XED_REG_ST0:cw:IMPL:f80 REG1=X87():r:f80 REG2=XED_REG_X87STATUS:w:SUPP
 | FCMOVNBE             | FCMOV          | X87            | FCMOV          | db           | 0b11       | 0b010      | 0xDB MOD[0b11] MOD=3 REG[0b010] RM[nnn]                                                              | REG0=XED_REG_ST0:cw:IMPL:f80 REG1=X87():r:f80 REG2=XED_REG_X87STATUS:w:SUPP
 | FCMOVNE              | FCMOV          | X87            | FCMOV          | db           | 0b11       | 0b001      | 0xDB MOD[0b11] MOD=3 REG[0b001] RM[nnn]                                                              | REG0=XED_REG_ST0:cw:IMPL:f80 REG1=X87():r:f80 REG2=XED_REG_X87STATUS:w:SUPP
 | FCMOVNU              | FCMOV          | X87            | FCMOV          | db           | 0b11       | 0b011      | 0xDB MOD[0b11] MOD=3 REG[0b011] RM[nnn]                                                              | REG0=XED_REG_ST0:cw:IMPL:f80 REG1=X87():r:f80 REG2=XED_REG_X87STATUS:w:SUPP
 | FCMOVU               | FCMOV          | X87            | FCMOV          | da           | 0b11       | 0b011      | 0xDA MOD[0b11] MOD=3 REG[0b011] RM[nnn]                                                              | REG0=XED_REG_ST0:cw:IMPL:f80 REG1=X87():r:f80 REG2=XED_REG_X87STATUS:w:SUPP
 | FCOM                 | X87_ALU        | X87            |                | d8           | mm         | 0b010      | 0xD8 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:r:IMPL:f80 MEM0:r:mem32real REG1=XED_REG_X87STATUS:w:SUPP
 | FCOM                 | X87_ALU        | X87            |                | dc           | mm         | 0b010      | 0xDC MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:r:IMPL:f80 MEM0:r:m64real REG1=XED_REG_X87STATUS:w:SUPP
 | FCOM                 | X87_ALU        | X87            |                | d8           | 0b11       | 0b010      | 0xD8 MOD[0b11] MOD=3 REG[0b010] RM[nnn]                                                              | REG0=XED_REG_ST0:r:IMPL:f80 REG1=X87():r:f80   REG2=XED_REG_X87STATUS:w:SUPP
 | FCOM                 | X87_ALU        | X87            |                | dc           | 0b11       | 0b010      | 0xDC MOD[0b11] MOD=3 REG[0b010] RM[nnn]                                                              | REG0=XED_REG_ST0:r:IMPL:f80 REG1=X87():r:f80   REG2=XED_REG_X87STATUS:w:SUPP
 | FCOMI                | X87_ALU        | X87            | PPRO           | db           | 0b11       | 0b110      | 0xDB MOD[0b11] MOD=3 REG[0b110] RM[nnn]                                                              | REG0=XED_REG_ST0:r:IMPL:f80 REG1=X87():r:f80  REG2=XED_REG_X87STATUS:w:SUPP
 | FCOMIP               | X87_ALU        | X87            | PPRO           | df           | 0b11       | 0b110      | 0xDF MOD[0b11] MOD=3 REG[0b110] RM[nnn]                                                              | REG0=XED_REG_ST0:r:IMPL:f80 REG1=X87():r:f80  REG2=XED_REG_X87POP:r:SUPP REG3=XED_REG_X87STATUS:w:SUPP
 | FCOMP                | X87_ALU        | X87            |                | d8           | mm         | 0b011      | 0xD8 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:r:IMPL:f80 MEM0:r:mem32real REG1=XED_REG_X87POP:r:SUPP REG2=XED_REG_X87STATUS:w:SUPP
 | FCOMP                | X87_ALU        | X87            |                | d8           | 0b11       | 0b011      | 0xD8 MOD[0b11] MOD=3 REG[0b011] RM[nnn]                                                              | REG0=XED_REG_ST0:r:IMPL:f80 REG1=X87():r:f80   REG2=XED_REG_X87POP:r:SUPP REG3=XED_REG_X87STATUS:w:SUPP
 | FCOMP                | X87_ALU        | X87            |                | dc           | 0b11       | 0b011      | 0xDC MOD[0b11] MOD=3 REG[0b011] RM[nnn]                                                              | REG0=XED_REG_ST0:r:IMPL:f80 REG1=X87():r:f80   REG2=XED_REG_X87POP:r:SUPP REG3=XED_REG_X87STATUS:w:SUPP
 | FCOMP                | X87_ALU        | X87            |                | de           | 0b11       | 0b010      | 0xDE MOD[0b11] MOD=3 REG[0b010] RM[nnn]                                                              | REG0=XED_REG_ST0:r:IMPL:f80 REG1=X87():r:f80   REG2=XED_REG_X87POP:r:SUPP REG3=XED_REG_X87STATUS:w:SUPP
 | FCOMP                | X87_ALU        | X87            |                | dc           | mm         | 0b011      | 0xDC MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:r:IMPL:f80 MEM0:r:m64real REG1=XED_REG_X87POP:r:SUPP REG2=XED_REG_X87STATUS:w:SUPP
 | FCOMPP               | X87_ALU        | X87            |                | de           | 0b11       | 0b011      | 0xDE MOD[0b11] MOD=3 REG[0b011] RM[0b001]                                                            | REG0=XED_REG_ST0:r:SUPP:f80 REG1=XED_REG_ST1:r:SUPP:f80 REG2=XED_REG_X87POP2:r:SUPP REG3=XED_REG_X87STATUS:rw:SUPP
 | FCOS                 | X87_ALU        | X87            |                | d9           | 0b11       | 0b111      | 0xD9 MOD[0b11] MOD=3 REG[0b111] RM[0b111]                                                            | REG0=XED_REG_ST0:rw:SUPP:f80 REG1=XED_REG_X87STATUS:w:SUPP
 | FDECSTP              | X87_ALU        | X87            |                | d9           | 0b11       | 0b110      | 0xD9 MOD[0b11] MOD=3 REG[0b110] RM[0b110]                                                            | REG0=XED_REG_X87STATUS:rw:SUPP
 | FDIV                 | X87_ALU        | X87            |                | d8           | mm         | 0b110      | 0xD8 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:rw:IMPL:f80 MEM0:r:mem32real REG1=XED_REG_X87STATUS:w:SUPP
 | FDIV                 | X87_ALU        | X87            |                | d8           | 0b11       | 0b110      | 0xD8 MOD[0b11] MOD=3 REG[0b110] RM[nnn]                                                              | REG0=XED_REG_ST0:rw:IMPL:f80 REG1=X87():r:f80 REG2=XED_REG_X87STATUS:w:SUPP
 | FDIV                 | X87_ALU        | X87            |                | dc           | mm         | 0b110      | 0xDC MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:rw:IMPL:f80 MEM0:r:m64real REG1=XED_REG_X87STATUS:w:SUPP
 | FDIV                 | X87_ALU        | X87            |                | dc           | 0b11       | 0b111      | 0xDC MOD[0b11] MOD=3 REG[0b111] RM[nnn]                                                              | REG0=X87():rw:f80 REG1=XED_REG_ST0:r:IMPL:f80 REG2=XED_REG_X87STATUS:w:SUPP
 | FDIVP                | X87_ALU        | X87            |                | de           | 0b11       | 0b111      | 0xDE MOD[0b11] MOD=3 REG[0b111] RM[nnn]                                                              | REG0=X87():rw:f80 REG1=XED_REG_ST0:r:IMPL:f80 REG2=XED_REG_X87POP:r:SUPP REG3=XED_REG_X87STATUS:w:SUPP
 | FDIVR                | X87_ALU        | X87            |                | d8           | mm         | 0b111      | 0xD8 MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:rw:IMPL:f80 MEM0:r:mem32real REG1=XED_REG_X87STATUS:w:SUPP
 | FDIVR                | X87_ALU        | X87            |                | d8           | 0b11       | 0b111      | 0xD8 MOD[0b11] MOD=3 REG[0b111] RM[nnn]                                                              | REG0=XED_REG_ST0:rw:IMPL:f80 REG1=X87():r:f80 REG2=XED_REG_X87STATUS:w:SUPP
 | FDIVR                | X87_ALU        | X87            |                | dc           | mm         | 0b111      | 0xDC MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:rw:IMPL:f80 MEM0:r:m64real REG1=XED_REG_X87STATUS:w:SUPP
 | FDIVR                | X87_ALU        | X87            |                | dc           | 0b11       | 0b110      | 0xDC MOD[0b11] MOD=3 REG[0b110] RM[nnn]                                                              | REG0=X87():rw:f80 REG1=XED_REG_ST0:r:IMPL:f80 REG2=XED_REG_X87STATUS:w:SUPP
 | FDIVRP               | X87_ALU        | X87            |                | de           | 0b11       | 0b110      | 0xDE MOD[0b11] MOD=3 REG[0b110] RM[nnn]                                                              | REG0=X87():rw:f80 REG1=XED_REG_ST0:r:IMPL:f80 REG2=XED_REG_X87POP:r:SUPP REG3=XED_REG_X87STATUS:w:SUPP
 | FEMMS                | MMX            | 3DNOW          |                | 0f 0e        |            |            | 0x0F 0x0E                                                                                            | 
 | FENI8087_NOP         | X87_ALU        | X87            |                | db           | 0b11       | 0b100      | 0xDB MOD[0b11] MOD=3 REG[0b100] RM[0b000]                                                            | 
 | FFREE                | X87_ALU        | X87            |                | dd           | 0b11       | 0b000      | 0xDD MOD[0b11] MOD=3 REG[0b000] RM[nnn]                                                              | REG0=X87():r:f80 REG1=XED_REG_X87TAG:w:SUPP
 | FFREEP               | X87_ALU        | X87            |                | df           | 0b11       | 0b000      | 0xDF MOD[0b11] MOD=3 REG[0b000] RM[nnn]                                                              | REG0=X87():r:f80 REG1=XED_REG_X87TAG:w:SUPP REG2=XED_REG_X87POP:r:SUPP
 | FIADD                | X87_ALU        | X87            |                | da           | mm         | 0b000      | 0xDA MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:rw:IMPL:f80 MEM0:r:mem32int REG1=XED_REG_X87STATUS:w:SUPP
 | FIADD                | X87_ALU        | X87            |                | de           | mm         | 0b000      | 0xDE MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:rw:IMPL:f80 MEM0:r:mem16int REG1=XED_REG_X87STATUS:w:SUPP
 | FICOM                | X87_ALU        | X87            |                | da           | mm         | 0b010      | 0xDA MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:r:IMPL:f80 MEM0:r:mem32int REG1=XED_REG_X87STATUS:w:SUPP
 | FICOM                | X87_ALU        | X87            |                | de           | mm         | 0b010      | 0xDE MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:r:IMPL:f80 MEM0:r:mem16int REG1=XED_REG_X87STATUS:w:SUPP
 | FICOMP               | X87_ALU        | X87            |                | da           | mm         | 0b011      | 0xDA MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:r:IMPL:f80 MEM0:r:mem32int REG1=XED_REG_X87POP:r:SUPP REG2=XED_REG_X87STATUS:w:SUPP
 | FICOMP               | X87_ALU        | X87            |                | de           | mm         | 0b011      | 0xDE MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:r:IMPL:f80 MEM0:r:mem16int REG1=XED_REG_X87POP:r:SUPP REG2=XED_REG_X87STATUS:w:SUPP
 | FIDIV                | X87_ALU        | X87            |                | da           | mm         | 0b110      | 0xDA MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:rw:IMPL:f80 MEM0:r:mem32int REG1=XED_REG_X87STATUS:w:SUPP
 | FIDIV                | X87_ALU        | X87            |                | de           | mm         | 0b110      | 0xDE MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:rw:IMPL:f80 MEM0:r:mem16int REG1=XED_REG_X87STATUS:w:SUPP
 | FIDIVR               | X87_ALU        | X87            |                | da           | mm         | 0b111      | 0xDA MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:rw:IMPL:f80 MEM0:r:mem32int REG1=XED_REG_X87STATUS:w:SUPP
 | FIDIVR               | X87_ALU        | X87            |                | de           | mm         | 0b111      | 0xDE MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:rw:IMPL:f80 MEM0:r:mem16int REG1=XED_REG_X87STATUS:w:SUPP
 | FILD                 | X87_ALU        | X87            |                | db           | mm         | 0b000      | 0xDB MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:w:IMPL:f80 MEM0:r:mem32int  REG1=XED_REG_X87PUSH:r:SUPP REG2=XED_REG_X87STATUS:w:SUPP
 | FILD                 | X87_ALU        | X87            |                | df           | mm         | 0b000      | 0xDF MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:w:IMPL:f80 MEM0:r:mem16int REG1=XED_REG_X87PUSH:r:SUPP REG2=XED_REG_X87STATUS:w:SUPP
 | FILD                 | X87_ALU        | X87            |                | df           | mm         | 0b101      | 0xDF MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:w:IMPL:f80 MEM0:r:m64int  REG1=XED_REG_X87PUSH:r:SUPP REG2=XED_REG_X87STATUS:w:SUPP
 | FIMUL                | X87_ALU        | X87            |                | da           | mm         | 0b001      | 0xDA MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:rw:IMPL:f80 MEM0:r:mem32int REG1=XED_REG_X87STATUS:w:SUPP
 | FIMUL                | X87_ALU        | X87            |                | de           | mm         | 0b001      | 0xDE MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:rw:IMPL:f80 MEM0:r:mem16int REG1=XED_REG_X87STATUS:w:SUPP
 | FINCSTP              | X87_ALU        | X87            |                | d9           | 0b11       | 0b110      | 0xD9 MOD[0b11] MOD=3 REG[0b110] RM[0b111]                                                            | REG0=XED_REG_X87STATUS:rw:SUPP
 | FIST                 | X87_ALU        | X87            |                | db           | mm         | 0b010      | 0xDB MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM()                                                       | MEM0:w:mem32int REG0=XED_REG_ST0:r:IMPL:f80 REG1=XED_REG_X87STATUS:w:SUPP
 | FIST                 | X87_ALU        | X87            |                | df           | mm         | 0b010      | 0xDF MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM()                                                       | MEM0:w:mem16int REG0=XED_REG_ST0:r:IMPL:f80 REG1=XED_REG_X87STATUS:w:SUPP
 | FISTP                | X87_ALU        | X87            |                | db           | mm         | 0b011      | 0xDB MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM()                                                       | MEM0:w:mem32int REG0=XED_REG_ST0:r:IMPL:f80 REG1=XED_REG_X87POP:r:SUPP REG2=XED_REG_X87STATUS:w:SUPP
 | FISTP                | X87_ALU        | X87            |                | df           | mm         | 0b011      | 0xDF MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM()                                                       | MEM0:w:mem16int REG0=XED_REG_ST0:r:IMPL:f80 REG1=XED_REG_X87POP:r:SUPP REG2=XED_REG_X87STATUS:w:SUPP
 | FISTP                | X87_ALU        | X87            |                | df           | mm         | 0b111      | 0xDF MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM()                                                       | MEM0:w:m64int REG0=XED_REG_ST0:r:IMPL:f80 REG1=XED_REG_X87POP:r:SUPP REG2=XED_REG_X87STATUS:w:SUPP
 | FISTTP               | X87_ALU        | SSE3           |                | db           | mm         | 0b001      | 0xDB MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM()                                                       | MEM0:w:mem32int REG0=XED_REG_ST0:r:IMPL:f80  REG1=XED_REG_X87POP:r:SUPP REG2=XED_REG_X87STATUS:w:SUPP
 | FISTTP               | X87_ALU        | SSE3           | SSE3X87        | dd           | mm         | 0b001      | 0xDD MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM()                                                       | MEM0:w:m64int REG0=XED_REG_ST0:r:IMPL:f80 REG1=XED_REG_X87POP:r:SUPP REG2=XED_REG_X87STATUS:w:SUPP
 | FISTTP               | X87_ALU        | SSE3           | SSE3X87        | df           | mm         | 0b001      | 0xDF MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM()                                                       | MEM0:w:mem16int REG0=XED_REG_ST0:r:IMPL:f80 REG1=XED_REG_X87POP:r:SUPP REG2=XED_REG_X87STATUS:w:SUPP
 | FISUB                | X87_ALU        | X87            |                | da           | mm         | 0b100      | 0xDA MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:rw:IMPL:f80 MEM0:r:mem32int REG1=XED_REG_X87STATUS:w:SUPP
 | FISUB                | X87_ALU        | X87            |                | de           | mm         | 0b100      | 0xDE MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:rw:IMPL:f80 MEM0:r:mem16int REG1=XED_REG_X87STATUS:w:SUPP
 | FISUBR               | X87_ALU        | X87            |                | da           | mm         | 0b101      | 0xDA MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:rw:IMPL:f80 MEM0:r:mem32int REG1=XED_REG_X87STATUS:w:SUPP
 | FISUBR               | X87_ALU        | X87            |                | de           | mm         | 0b101      | 0xDE MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:rw:IMPL:f80 MEM0:r:mem16int REG1=XED_REG_X87STATUS:w:SUPP
 | FLD                  | X87_ALU        | X87            |                | d9           | mm         | 0b000      | 0xD9 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:w:IMPL:f80 MEM0:r:mem32real  REG1=XED_REG_X87PUSH:r:SUPP REG2=XED_REG_X87STATUS:w:SUPP
 | FLD                  | X87_ALU        | X87            |                | d9           | 0b11       | 0b000      | 0xD9 MOD[0b11] MOD=3 REG[0b000] RM[nnn]                                                              | REG0=XED_REG_ST0:w:IMPL:f80 REG1=X87():r:f80  REG2=XED_REG_X87PUSH:r:SUPP REG3=XED_REG_X87STATUS:w:SUPP
 | FLD                  | X87_ALU        | X87            |                | db           | mm         | 0b101      | 0xDB MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:w:IMPL:f80 MEM0:r:mem80real  REG1=XED_REG_X87PUSH:r:SUPP REG2=XED_REG_X87STATUS:w:SUPP
 | FLD                  | X87_ALU        | X87            |                | dd           | mm         | 0b000      | 0xDD MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:w:IMPL:f80 MEM0:r:m64real  REG1=XED_REG_X87PUSH:r:SUPP REG2=XED_REG_X87STATUS:w:SUPP
 | FLD1                 | X87_ALU        | X87            |                | d9           | 0b11       | 0b101      | 0xD9 MOD[0b11] MOD=3 REG[0b101] RM[0b000]                                                            | REG0=XED_REG_ST0:w:SUPP:f80 REG1=XED_REG_X87PUSH:r:SUPP REG2=XED_REG_X87STATUS:w:SUPP
 | FLDCW                | X87_ALU        | X87            |                | d9           | mm         | 0b101      | 0xD9 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM()                                                       | MEM0:r:mem16 REG0=XED_REG_X87CONTROL:w:SUPP REG1=XED_REG_X87STATUS:w:SUPP
 | FLDENV               | X87_ALU        | X87            |                | d9           | mm         | 0b100      | 0xD9 MOD[mm] MOD!=3 REG[0b100] RM[nnn] mode16 no66_prefix MODRM()                                    | MEM0:r:mem14 REG0=XED_REG_X87STATUS:w:SUPP
 | FLDENV               | X87_ALU        | X87            |                | d9           | mm         | 0b100      | 0xD9 MOD[mm] MOD!=3 REG[0b100] RM[nnn] mode32 66_prefix MODRM()                                      | MEM0:r:mem14 REG0=XED_REG_X87STATUS:w:SUPP
 | FLDENV               | X87_ALU        | X87            |                | d9           | mm         | 0b100      | 0xD9 MOD[mm] MOD!=3 REG[0b100] RM[nnn] mode64 norexw_prefix 66_prefix  MODRM()                       | MEM0:r:mem14 REG0=XED_REG_X87STATUS:w:SUPP
 | FLDENV               | X87_ALU        | X87            |                | d9           | mm         | 0b100      | 0xD9 MOD[mm] MOD!=3 REG[0b100] RM[nnn] mode16 66_prefix MODRM()                                      | MEM0:r:mem28 REG0=XED_REG_X87STATUS:w:SUPP
 | FLDENV               | X87_ALU        | X87            |                | d9           | mm         | 0b100      | 0xD9 MOD[mm] MOD!=3 REG[0b100] RM[nnn] mode32 no66_prefix MODRM()                                    | MEM0:r:mem28 REG0=XED_REG_X87STATUS:w:SUPP
 | FLDENV               | X87_ALU        | X87            |                | d9           | mm         | 0b100      | 0xD9 MOD[mm] MOD!=3 REG[0b100] RM[nnn] mode64 rexw_prefix MODRM()                                    | MEM0:r:mem28 REG0=XED_REG_X87STATUS:w:SUPP
 | FLDENV               | X87_ALU        | X87            |                | d9           | mm         | 0b100      | 0xD9 MOD[mm] MOD!=3 REG[0b100] RM[nnn] mode64 norexw_prefix no66_prefix MODRM()                      | MEM0:r:mem28 REG0=XED_REG_X87STATUS:w:SUPP
 | FLDL2E               | X87_ALU        | X87            |                | d9           | 0b11       | 0b101      | 0xD9 MOD[0b11] MOD=3 REG[0b101] RM[0b010]                                                            | REG0=XED_REG_ST0:w:SUPP:f80 REG1=XED_REG_X87PUSH:r:SUPP REG2=XED_REG_X87STATUS:w:SUPP
 | FLDL2T               | X87_ALU        | X87            |                | d9           | 0b11       | 0b101      | 0xD9 MOD[0b11] MOD=3 REG[0b101] RM[0b001]                                                            | REG0=XED_REG_ST0:w:SUPP:f80 REG1=XED_REG_X87PUSH:r:SUPP REG2=XED_REG_X87STATUS:w:SUPP
 | FLDLG2               | X87_ALU        | X87            |                | d9           | 0b11       | 0b101      | 0xD9 MOD[0b11] MOD=3 REG[0b101] RM[0b100]                                                            | REG0=XED_REG_ST0:w:SUPP:f80 REG1=XED_REG_X87PUSH:r:SUPP REG2=XED_REG_X87STATUS:w:SUPP
 | FLDLN2               | X87_ALU        | X87            |                | d9           | 0b11       | 0b101      | 0xD9 MOD[0b11] MOD=3 REG[0b101] RM[0b101]                                                            | REG0=XED_REG_ST0:w:SUPP:f80 REG1=XED_REG_X87PUSH:r:SUPP REG2=XED_REG_X87STATUS:w:SUPP
 | FLDPI                | X87_ALU        | X87            |                | d9           | 0b11       | 0b101      | 0xD9 MOD[0b11] MOD=3 REG[0b101] RM[0b011]                                                            | REG0=XED_REG_ST0:w:SUPP:f80 REG1=XED_REG_X87PUSH:r:SUPP REG2=XED_REG_X87STATUS:w:SUPP
 | FLDZ                 | X87_ALU        | X87            |                | d9           | 0b11       | 0b101      | 0xD9 MOD[0b11] MOD=3 REG[0b101] RM[0b110]                                                            | REG0=XED_REG_ST0:w:SUPP:f80 REG1=XED_REG_X87PUSH:r:SUPP REG2=XED_REG_X87STATUS:w:SUPP
 | FMUL                 | X87_ALU        | X87            |                | d8           | mm         | 0b001      | 0xD8 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:rw:IMPL:f80 MEM0:r:mem32real REG1=XED_REG_X87STATUS:w:SUPP
 | FMUL                 | X87_ALU        | X87            |                | d8           | 0b11       | 0b001      | 0xD8 MOD[0b11] MOD=3 REG[0b001] RM[nnn]                                                              | REG0=XED_REG_ST0:rw:IMPL:f80 REG1=X87():r:f80 REG2=XED_REG_X87STATUS:w:SUPP
 | FMUL                 | X87_ALU        | X87            |                | dc           | mm         | 0b001      | 0xDC MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:rw:IMPL:f80 MEM0:r:m64real REG1=XED_REG_X87STATUS:w:SUPP
 | FMUL                 | X87_ALU        | X87            |                | dc           | 0b11       | 0b001      | 0xDC MOD[0b11] MOD=3 REG[0b001] RM[nnn]                                                              | REG0=X87():rw:f80 REG1=XED_REG_ST0:r:IMPL:f80 REG2=XED_REG_X87STATUS:w:SUPP
 | FMULP                | X87_ALU        | X87            |                | de           | 0b11       | 0b001      | 0xDE MOD[0b11] MOD=3 REG[0b001] RM[nnn]                                                              | REG0=X87():rw:f80 REG1=XED_REG_ST0:r:IMPL:f80 REG2=XED_REG_X87POP:r:SUPP REG3=XED_REG_X87STATUS:w:SUPP
 | FNCLEX               | X87_ALU        | X87            |                | db           | 0b11       | 0b100      | 0xDB MOD[0b11] MOD=3 REG[0b100] RM[0b010]                                                            | REG0=XED_REG_X87STATUS:w:SUPP
 | FNINIT               | X87_ALU        | X87            |                | db           | 0b11       | 0b100      | 0xDB MOD[0b11] MOD=3 REG[0b100] RM[0b011]                                                            | REG0=XED_REG_X87CONTROL:w:SUPP REG1=XED_REG_X87TAG:w:SUPP  REG2=XED_REG_X87STATUS:w:SUPP
 | FNOP                 | X87_ALU        | X87            |                | d9           | 0b11       | 0b010      | 0xD9 MOD[0b11] MOD=3 REG[0b010] RM[0b000]                                                            | 
 | FNSAVE               | X87_ALU        | X87            |                | dd           | mm         | 0b110      | 0xDD MOD[mm] MOD!=3 REG[0b110] RM[nnn] mode16 no66_prefix  MODRM()                                   | MEM0:w:mem94 \
 | FNSAVE               | X87_ALU        | X87            |                | dd           | mm         | 0b110      | 0xDD MOD[mm] MOD!=3 REG[0b110] RM[nnn] mode32 66_prefix  MODRM()                                     | MEM0:w:mem94 \
 | FNSAVE               | X87_ALU        | X87            |                | dd           | mm         | 0b110      | 0xDD MOD[mm] MOD!=3 REG[0b110] RM[nnn] mode64 norexw_prefix 66_prefix MODRM()                        | MEM0:w:mem94 \
 | FNSAVE               | X87_ALU        | X87            |                | dd           | mm         | 0b110      | 0xDD MOD[mm] MOD!=3 REG[0b110] RM[nnn] mode16 66_prefix MODRM()                                      | MEM0:w:mem108  \
 | FNSAVE               | X87_ALU        | X87            |                | dd           | mm         | 0b110      | 0xDD MOD[mm] MOD!=3 REG[0b110] RM[nnn] mode32 no66_prefix  MODRM()                                   | MEM0:w:mem108  \
 | FNSAVE               | X87_ALU        | X87            |                | dd           | mm         | 0b110      | 0xDD MOD[mm] MOD!=3 REG[0b110] RM[nnn] mode64 rexw_prefix  MODRM()                                   | MEM0:w:mem108  \
 | FNSAVE               | X87_ALU        | X87            |                | dd           | mm         | 0b110      | 0xDD MOD[mm] MOD!=3 REG[0b110] RM[nnn] mode64 norexw_prefix no66_prefix  MODRM()                     | MEM0:w:mem108  \
 | FNSTCW               | X87_ALU        | X87            |                | d9           | mm         | 0b111      | 0xD9 MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM()                                                       | MEM0:w:mem16 REG0=XED_REG_X87CONTROL:r:SUPP REG1=XED_REG_X87STATUS:w:SUPP
 | FNSTENV              | X87_ALU        | X87            |                | d9           | mm         | 0b110      | 0xD9 MOD[mm] MOD!=3 REG[0b110] RM[nnn] mode16 no66_prefix MODRM()                                    | MEM0:w:mem14 REG0=XED_REG_X87STATUS:w:SUPP
 | FNSTENV              | X87_ALU        | X87            |                | d9           | mm         | 0b110      | 0xD9 MOD[mm] MOD!=3 REG[0b110] RM[nnn] mode32 66_prefix  MODRM()                                     | MEM0:w:mem14 REG0=XED_REG_X87STATUS:w:SUPP
 | FNSTENV              | X87_ALU        | X87            |                | d9           | mm         | 0b110      | 0xD9 MOD[mm] MOD!=3 REG[0b110] RM[nnn] mode64 norexw_prefix 66_prefix MODRM()                        | MEM0:w:mem14 REG0=XED_REG_X87STATUS:w:SUPP
 | FNSTENV              | X87_ALU        | X87            |                | d9           | mm         | 0b110      | 0xD9 MOD[mm] MOD!=3 REG[0b110] RM[nnn] mode16 66_prefix MODRM()                                      | MEM0:w:mem28 REG0=XED_REG_X87STATUS:w:SUPP
 | FNSTENV              | X87_ALU        | X87            |                | d9           | mm         | 0b110      | 0xD9 MOD[mm] MOD!=3 REG[0b110] RM[nnn] mode32 no66_prefix MODRM()                                    | MEM0:w:mem28 REG0=XED_REG_X87STATUS:w:SUPP
 | FNSTENV              | X87_ALU        | X87            |                | d9           | mm         | 0b110      | 0xD9 MOD[mm] MOD!=3 REG[0b110] RM[nnn] mode64 rexw_prefix MODRM()                                    | MEM0:w:mem28 REG0=XED_REG_X87STATUS:w:SUPP
 | FNSTENV              | X87_ALU        | X87            |                | d9           | mm         | 0b110      | 0xD9 MOD[mm] MOD!=3 REG[0b110] RM[nnn] mode64 norexw_prefix no66_prefix MODRM()                      | MEM0:w:mem28 REG0=XED_REG_X87STATUS:w:SUPP
 | FNSTSW               | X87_ALU        | X87            |                | dd           | mm         | 0b111      | 0xDD MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM()                                                       | MEM0:w:mem16 REG0=XED_REG_X87STATUS:rw:SUPP
 | FNSTSW               | X87_ALU        | X87            |                | df           | 0b11       | 0b100      | 0xDF MOD[0b11] MOD=3 REG[0b100] RM[0b000]                                                            | REG0=XED_REG_AX:w:IMPL REG1=XED_REG_X87STATUS:rw:SUPP
 | FPATAN               | X87_ALU        | X87            |                | d9           | 0b11       | 0b110      | 0xD9 MOD[0b11] MOD=3 REG[0b110] RM[0b011]                                                            | REG0=XED_REG_ST0:r:SUPP:f80 REG1=XED_REG_ST1:rw:SUPP:f80 REG2=XED_REG_X87POP:r:SUPP REG3=XED_REG_X87STATUS:w:SUPP
 | FPREM                | X87_ALU        | X87            |                | d9           | 0b11       | 0b111      | 0xD9 MOD[0b11] MOD=3 REG[0b111] RM[0b000]                                                            | REG0=XED_REG_ST0:rw:SUPP:f80 REG1=XED_REG_ST1:r:SUPP:f80 REG2=XED_REG_X87STATUS:w:SUPP
 | FPREM1               | X87_ALU        | X87            |                | d9           | 0b11       | 0b110      | 0xD9 MOD[0b11] MOD=3 REG[0b110] RM[0b101]                                                            | REG0=XED_REG_ST0:rw:SUPP:f80 REG1=XED_REG_ST1:r:SUPP:f80 REG2=XED_REG_X87STATUS:w:SUPP
 | FPTAN                | X87_ALU        | X87            |                | d9           | 0b11       | 0b110      | 0xD9 MOD[0b11] MOD=3 REG[0b110] RM[0b010]                                                            | REG0=XED_REG_ST0:rw:SUPP:f80 REG1=XED_REG_ST1:w:SUPP:f80 REG2=XED_REG_X87STATUS:w:SUPP
 | FRNDINT              | X87_ALU        | X87            |                | d9           | 0b11       | 0b111      | 0xD9 MOD[0b11] MOD=3 REG[0b111] RM[0b100]                                                            | REG0=XED_REG_ST0:rw:SUPP:f80 REG1=XED_REG_X87STATUS:w:SUPP
 | FRSTOR               | X87_ALU        | X87            |                | dd           | mm         | 0b100      | 0xDD MOD[mm] MOD!=3 REG[0b100] RM[nnn] mode16 no66_prefix  MODRM()                                   | MEM0:r:mem94 REG0=XED_REG_X87CONTROL:w:SUPP
 | FRSTOR               | X87_ALU        | X87            |                | dd           | mm         | 0b100      | 0xDD MOD[mm] MOD!=3 REG[0b100] RM[nnn] mode32 66_prefix MODRM()                                      | MEM0:r:mem94 REG0=XED_REG_X87CONTROL:w:SUPP
 | FRSTOR               | X87_ALU        | X87            |                | dd           | mm         | 0b100      | 0xDD MOD[mm] MOD!=3 REG[0b100] RM[nnn] mode64 norexw_prefix 66_prefix  MODRM()                       | MEM0:r:mem94 REG0=XED_REG_X87CONTROL:w:SUPP
 | FRSTOR               | X87_ALU        | X87            |                | dd           | mm         | 0b100      | 0xDD MOD[mm] MOD!=3 REG[0b100] RM[nnn] mode16 66_prefix  MODRM()                                     | MEM0:r:mem108 REG0=XED_REG_X87CONTROL:w:SUPP
 | FRSTOR               | X87_ALU        | X87            |                | dd           | mm         | 0b100      | 0xDD MOD[mm] MOD!=3 REG[0b100] RM[nnn] mode32 no66_prefix MODRM()                                    | MEM0:r:mem108 REG0=XED_REG_X87CONTROL:w:SUPP
 | FRSTOR               | X87_ALU        | X87            |                | dd           | mm         | 0b100      | 0xDD MOD[mm] MOD!=3 REG[0b100] RM[nnn] mode64 rexw_prefix MODRM()                                    | MEM0:r:mem108 REG0=XED_REG_X87CONTROL:w:SUPP
 | FRSTOR               | X87_ALU        | X87            |                | dd           | mm         | 0b100      | 0xDD MOD[mm] MOD!=3 REG[0b100] RM[nnn] mode64 norexw_prefix no66_prefix MODRM()                      | MEM0:r:mem108 REG0=XED_REG_X87CONTROL:w:SUPP
 | FSCALE               | X87_ALU        | X87            |                | d9           | 0b11       | 0b111      | 0xD9 MOD[0b11] MOD=3 REG[0b111] RM[0b101]                                                            | REG0=XED_REG_ST0:rw:SUPP:f80 REG1=XED_REG_ST1:r:SUPP:f80 REG2=XED_REG_X87STATUS:w:SUPP
 | FSETPM287_NOP        | X87_ALU        | X87            |                | db           | 0b11       | 0b100      | 0xDB MOD[0b11] MOD=3 REG[0b100] RM[0b100]                                                            | 
 | FSIN                 | X87_ALU        | X87            |                | d9           | 0b11       | 0b111      | 0xD9 MOD[0b11] MOD=3 REG[0b111] RM[0b110]                                                            | REG0=XED_REG_ST0:rw:SUPP:f80 REG1=XED_REG_X87STATUS:w:SUPP
 | FSINCOS              | X87_ALU        | X87            |                | d9           | 0b11       | 0b111      | 0xD9 MOD[0b11] MOD=3 REG[0b111] RM[0b011]                                                            | REG0=XED_REG_ST0:rw:SUPP:f80 REG1=XED_REG_ST1:w:SUPP:f80 REG2=XED_REG_X87STATUS:w:SUPP
 | FSQRT                | X87_ALU        | X87            |                | d9           | 0b11       | 0b111      | 0xD9 MOD[0b11] MOD=3 REG[0b111] RM[0b010]                                                            | REG0=XED_REG_ST0:rw:SUPP:f80 REG1=XED_REG_X87STATUS:w:SUPP
 | FST                  | X87_ALU        | X87            |                | d9           | mm         | 0b010      | 0xD9 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM()                                                       | MEM0:w:mem32real REG0=XED_REG_ST0:r:IMPL:f80  REG1=XED_REG_X87STATUS:w:SUPP
 | FST                  | X87_ALU        | X87            |                | dd           | mm         | 0b010      | 0xDD MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM()                                                       | MEM0:w:m64real REG0=XED_REG_ST0:r:IMPL:f80  REG1=XED_REG_X87STATUS:w:SUPP
 | FST                  | X87_ALU        | X87            |                | dd           | 0b11       | 0b010      | 0xDD MOD[0b11] MOD=3 REG[0b010] RM[nnn]                                                              | REG0=X87():w:f80 REG1=XED_REG_ST0:r:IMPL:f80  REG2=XED_REG_X87STATUS:w:SUPP
 | FSTP                 | X87_ALU        | X87            |                | d9           | mm         | 0b011      | 0xD9 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM()                                                       | MEM0:w:mem32real REG0=XED_REG_ST0:r:IMPL:f80  REG1=XED_REG_X87POP:r:SUPP REG2=XED_REG_X87STATUS:w:SUPP
 | FSTP                 | X87_ALU        | X87            |                | db           | mm         | 0b111      | 0xDB MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM()                                                       | MEM0:w:mem80real   REG0=XED_REG_ST0:r:IMPL:f80 REG1=XED_REG_X87POP:r:SUPP REG2=XED_REG_X87STATUS:w:SUPP
 | FSTP                 | X87_ALU        | X87            |                | dd           | mm         | 0b011      | 0xDD MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM()                                                       | MEM0:w:m64real REG0=XED_REG_ST0:r:IMPL:f80 REG1=XED_REG_X87POP:r:SUPP REG2=XED_REG_X87STATUS:w:SUPP
 | FSTP                 | X87_ALU        | X87            |                | dd           | 0b11       | 0b011      | 0xDD MOD[0b11] MOD=3 REG[0b011] RM[nnn]                                                              | REG0=X87():w:f80 REG1=XED_REG_ST0:r:IMPL:f80  REG2=XED_REG_X87POP:r:SUPP REG3=XED_REG_X87STATUS:w:SUPP
 | FSTP                 | X87_ALU        | X87            |                | df           | 0b11       | 0b010      | 0xDF MOD[0b11] MOD=3 REG[0b010] RM[nnn]                                                              | REG0=X87():w:f80 REG1=XED_REG_ST0:r:IMPL:f80  REG2=XED_REG_X87POP:r:SUPP REG3=XED_REG_X87STATUS:w:SUPP
 | FSTP                 | X87_ALU        | X87            |                | df           | 0b11       | 0b011      | 0xDF MOD[0b11] MOD=3 REG[0b011] RM[nnn]                                                              | REG0=X87():w:f80 REG1=XED_REG_ST0:r:IMPL:f80  REG2=XED_REG_X87POP:r:SUPP REG3=XED_REG_X87STATUS:w:SUPP
 | FSTPNCE              | X87_ALU        | X87            |                | d9           | 0b11       | 0b011      | 0xD9 MOD[0b11] MOD=3 REG[0b011] RM[nnn]                                                              | REG0=X87():w:f80 REG1=XED_REG_ST0:r:IMPL:f80  REG2=XED_REG_X87POP:r:SUPP REG3=XED_REG_X87STATUS:w:SUPP
 | FSUB                 | X87_ALU        | X87            |                | d8           | mm         | 0b100      | 0xD8 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:rw:IMPL:f80 MEM0:r:mem32real REG1=XED_REG_X87STATUS:w:SUPP
 | FSUB                 | X87_ALU        | X87            |                | d8           | 0b11       | 0b100      | 0xD8 MOD[0b11] MOD=3 REG[0b100] RM[nnn]                                                              | REG0=XED_REG_ST0:rw:IMPL:f80 REG1=X87():r:f80 REG2=XED_REG_X87STATUS:w:SUPP
 | FSUB                 | X87_ALU        | X87            |                | dc           | mm         | 0b100      | 0xDC MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:rw:IMPL:f80 MEM0:r:m64real REG1=XED_REG_X87STATUS:w:SUPP
 | FSUB                 | X87_ALU        | X87            |                | dc           | 0b11       | 0b101      | 0xDC MOD[0b11] MOD=3 REG[0b101] RM[nnn]                                                              | REG0=X87():rw:f80 REG1=XED_REG_ST0:r:IMPL:f80 REG2=XED_REG_X87STATUS:w:SUPP
 | FSUBP                | X87_ALU        | X87            |                | de           | 0b11       | 0b101      | 0xDE MOD[0b11] MOD=3 REG[0b101] RM[nnn]                                                              | REG0=X87():rw:f80 REG1=XED_REG_ST0:r:IMPL:f80 REG2=XED_REG_X87POP:r:SUPP REG3=XED_REG_X87STATUS:w:SUPP
 | FSUBR                | X87_ALU        | X87            |                | d8           | mm         | 0b101      | 0xD8 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:rw:IMPL:f80 MEM0:r:mem32real REG1=XED_REG_X87STATUS:w:SUPP
 | FSUBR                | X87_ALU        | X87            |                | d8           | 0b11       | 0b101      | 0xD8 MOD[0b11] MOD=3 REG[0b101] RM[nnn]                                                              | REG0=XED_REG_ST0:rw:IMPL:f80 REG1=X87():r:f80 REG2=XED_REG_X87STATUS:w:SUPP
 | FSUBR                | X87_ALU        | X87            |                | dc           | mm         | 0b101      | 0xDC MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM()                                                       | REG0=XED_REG_ST0:rw:IMPL:f80 MEM0:r:m64real REG1=XED_REG_X87STATUS:w:SUPP
 | FSUBR                | X87_ALU        | X87            |                | dc           | 0b11       | 0b100      | 0xDC MOD[0b11] MOD=3 REG[0b100] RM[nnn]                                                              | REG0=X87():rw:f80 REG1=XED_REG_ST0:r:IMPL:f80 REG2=XED_REG_X87STATUS:w:SUPP
 | FSUBRP               | X87_ALU        | X87            |                | de           | 0b11       | 0b100      | 0xDE MOD[0b11] MOD=3 REG[0b100] RM[nnn]                                                              | REG0=X87():rw:f80 REG1=XED_REG_ST0:r:IMPL:f80 REG2=XED_REG_X87POP:r:SUPP REG3=XED_REG_X87STATUS:w:SUPP
 | FTST                 | X87_ALU        | X87            |                | d9           | 0b11       | 0b100      | 0xD9 MOD[0b11] MOD=3 REG[0b100] RM[0b100]                                                            | REG0=XED_REG_ST0:rw:SUPP:f80 REG1=XED_REG_X87STATUS:w:SUPP
 | FUCOM                | X87_ALU        | X87            |                | dd           | 0b11       | 0b100      | 0xDD MOD[0b11] MOD=3 REG[0b100] RM[nnn]                                                              | REG0=XED_REG_ST0:r:IMPL:f80 REG1=X87():r:f80  REG2=XED_REG_X87STATUS:w:SUPP
 | FUCOMI               | X87_ALU        | X87            | PPRO           | db           | 0b11       | 0b101      | 0xDB MOD[0b11] MOD=3 REG[0b101] RM[nnn]                                                              | REG0=XED_REG_ST0:r:IMPL:f80 REG1=X87():r:f80   REG2=XED_REG_X87STATUS:w:SUPP
 | FUCOMIP              | X87_ALU        | X87            | PPRO           | df           | 0b11       | 0b101      | 0xDF MOD[0b11] MOD=3 REG[0b101] RM[nnn]                                                              | REG0=XED_REG_ST0:r:IMPL:f80 REG1=X87():r:f80   REG2=XED_REG_X87POP:r:SUPP REG3=XED_REG_X87STATUS:w:SUPP
 | FUCOMP               | X87_ALU        | X87            |                | dd           | 0b11       | 0b101      | 0xDD MOD[0b11] MOD=3 REG[0b101] RM[nnn]                                                              | REG0=XED_REG_ST0:r:IMPL:f80 REG1=X87():r:f80  REG2=XED_REG_X87POP:r:SUPP REG3=XED_REG_X87STATUS:rw:SUPP
 | FUCOMPP              | X87_ALU        | X87            |                | da           | 0b11       | 0b101      | 0xDA MOD[0b11] MOD=3 REG[0b101] RM[0b001]                                                            | REG0=XED_REG_ST0:r:SUPP:f80  REG1=XED_REG_ST1:r:SUPP:f80 REG2=XED_REG_X87POP2:rw:SUPP REG3=XED_REG_X87STATUS:rw:SUPP
 | FWAIT                | X87_ALU        | X87            |                | 9b           |            |            | 0x9B                                                                                                 | 
 | FXAM                 | X87_ALU        | X87            |                | d9           | 0b11       | 0b100      | 0xD9 MOD[0b11] MOD=3 REG[0b100] RM[0b101]                                                            | REG0=XED_REG_ST0:rw:SUPP:f80 REG1=XED_REG_X87STATUS:w:SUPP
 | FXCH                 | X87_ALU        | X87            |                | d9           | 0b11       | 0b001      | 0xD9 MOD[0b11] MOD=3 REG[0b001] RM[nnn]                                                              | REG0=XED_REG_ST0:rw:IMPL:f80 REG1=X87():rw:f80   REG2=XED_REG_X87STATUS:w:SUPP
 | FXCH                 | X87_ALU        | X87            |                | df           | 0b11       | 0b001      | 0xDF MOD[0b11] MOD=3 REG[0b001] RM[nnn]                                                              | REG0=XED_REG_ST0:rw:IMPL:f80 REG1=X87():rw:f80   REG2=XED_REG_X87STATUS:w:SUPP
 | FXCH                 | X87_ALU        | X87            |                | dd           | 0b11       | 0b001      | 0xDD MOD[0b11] MOD=3 REG[0b001] RM[nnn]                                                              | REG0=XED_REG_ST0:rw:IMPL:f80 REG1=X87():rw:f80   REG2=XED_REG_X87STATUS:w:SUPP
 | FXRSTOR              | SSE            | SSE            | FXSAVE         | 0f ae        | mm         | 0b001      | 0x0F 0xAE MOD[mm] MOD!=3 REG[0b001] RM[nnn]   no_refining_prefix norexw_prefix MODRM()               | MEM0:r:mfpxenv REG0=XED_REG_X87CONTROL:w:SUPP
 | FXRSTOR64            | SSE            | SSE            | FXSAVE64       | 0f ae        | mm         | 0b001      | 0x0F 0xAE MOD[mm] MOD!=3 REG[0b001] RM[nnn]   no_refining_prefix rexw_prefix MODRM()                 | MEM0:r:mfpxenv REG0=XED_REG_X87CONTROL:w:SUPP
 | FXSAVE               | SSE            | SSE            | FXSAVE         | 0f ae        | mm         | 0b000      | 0x0F 0xAE  MOD[mm] MOD!=3 REG[0b000] RM[nnn]  no_refining_prefix norexw_prefix MODRM()               | MEM0:w:mfpxenv REG0=XED_REG_X87CONTROL:r:SUPP
 | FXSAVE64             | SSE            | SSE            | FXSAVE64       | 0f ae        | mm         | 0b000      | 0x0F 0xAE  MOD[mm] MOD!=3 REG[0b000] RM[nnn]  no_refining_prefix rexw_prefix MODRM()                 | MEM0:w:mfpxenv REG0=XED_REG_X87CONTROL:r:SUPP
 | FXTRACT              | X87_ALU        | X87            |                | d9           | 0b11       | 0b110      | 0xD9 MOD[0b11] MOD=3 REG[0b110] RM[0b100]                                                            | REG0=XED_REG_ST0:rw:SUPP:f80 REG1=XED_REG_ST1:w:SUPP:f80 REG2=XED_REG_X87STATUS:w:SUPP
 | FYL2X                | X87_ALU        | X87            |                | d9           | 0b11       | 0b110      | 0xD9 MOD[0b11] MOD=3 REG[0b110] RM[0b001]                                                            | REG0=XED_REG_ST0:r:SUPP:f80 REG1=XED_REG_ST1:rw:SUPP:f80 REG2=XED_REG_X87POP:r:SUPP REG3=XED_REG_X87STATUS:w:SUPP
 | FYL2XP1              | X87_ALU        | X87            |                | d9           | 0b11       | 0b111      | 0xD9 MOD[0b11] MOD=3 REG[0b111] RM[0b001]                                                            | REG0=XED_REG_ST0:r:SUPP:f80 REG1=XED_REG_ST1:rw:SUPP:f80 REG2=XED_REG_X87POP:r:SUPP REG3=XED_REG_X87STATUS:w:SUPP
 | GETSEC               | SYSTEM         | SMX            |                | 0f 37        |            |            | 0x0F 0x37 no_refining_prefix                                                                         | REG0=XED_REG_EAX:rcw:SUPP  REG1=XED_REG_EBX:r:SUPP
 | GF2P8AFFINEINVQB     | GFNI           | GFNI           | GFNI           | 0f 3a cf     | 0b11       | rrr        | 0x0F 0x3A 0xCF MOD[0b11] MOD=3  REG[rrr] RM[nnn]  osz_refining_prefix     UIMM8()                    | REG0=XMM_R():rw:dq:u8 REG1=XMM_B():r:dq:u64 IMM0:r:b
 | GF2P8AFFINEINVQB     | GFNI           | GFNI           | GFNI           | 0f 3a cf     | mm         | rrr        | 0x0F 0x3A 0xCF MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  osz_refining_prefix     UIMM8()             | REG0=XMM_R():rw:dq:u8 MEM0:r:dq:u64 IMM0:r:b
 | GF2P8AFFINEQB        | GFNI           | GFNI           | GFNI           | 0f 3a ce     | 0b11       | rrr        | 0x0F 0x3A 0xCE MOD[0b11] MOD=3  REG[rrr] RM[nnn]  osz_refining_prefix     UIMM8()                    | REG0=XMM_R():rw:dq:u8 REG1=XMM_B():r:dq:u64 IMM0:r:b
 | GF2P8AFFINEQB        | GFNI           | GFNI           | GFNI           | 0f 3a ce     | mm         | rrr        | 0x0F 0x3A 0xCE MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  osz_refining_prefix     UIMM8()             | REG0=XMM_R():rw:dq:u8 MEM0:r:dq:u64 IMM0:r:b
 | GF2P8MULB            | GFNI           | GFNI           | GFNI           | 0f 38 cf     | 0b11       | rrr        | 0x0F 0x38 0xCF MOD[0b11] MOD=3  REG[rrr] RM[nnn]  osz_refining_prefix                                | REG0=XMM_R():rw:dq:u8 REG1=XMM_B():r:dq:u8
 | GF2P8MULB            | GFNI           | GFNI           | GFNI           | 0f 38 cf     | mm         | rrr        | 0x0F 0x38 0xCF MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  osz_refining_prefix                         | REG0=XMM_R():rw:dq:u8 MEM0:r:dq:u8
 | HADDPD               | SSE            | SSE3           |                | 0f 7c        | mm         | rrr        | 0x0F 0x7C osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:pd MEM0:r:pd
 | HADDPD               | SSE            | SSE3           |                | 0f 7c        | 0b11       | rrr        | 0x0F 0x7C osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:pd REG1=XMM_B():r:pd
 | HADDPS               | SSE            | SSE3           |                | 0f 7c        | mm         | rrr        | 0x0F 0x7C f2_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM()                     | REG0=XMM_R():rw:ps MEM0:r:ps
 | HADDPS               | SSE            | SSE3           |                | 0f 7c        | 0b11       | rrr        | 0x0F 0x7C f2_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66()                            | REG0=XMM_R():rw:ps REG1=XMM_B():r:ps
 | HLT                  | SYSTEM         | BASE           | I86            | f4           |            |            | 0xF4                                                                                                 | 
 | HSUBPD               | SSE            | SSE3           |                | 0f 7d        | mm         | rrr        | 0x0F 0x7D osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:pd MEM0:r:pd
 | HSUBPD               | SSE            | SSE3           |                | 0f 7d        | 0b11       | rrr        | 0x0F 0x7D osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:pd REG1=XMM_B():r:pd
 | HSUBPS               | SSE            | SSE3           |                | 0f 7d        | mm         | rrr        | 0x0F 0x7D f2_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM()                     | REG0=XMM_R():rw:ps MEM0:r:ps
 | HSUBPS               | SSE            | SSE3           |                | 0f 7d        | 0b11       | rrr        | 0x0F 0x7D f2_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66()                            | REG0=XMM_R():rw:ps REG1=XMM_B():r:ps
 | IDIV                 | BINARY         | BASE           | I86            | f6           | mm         | 0b111      | 0xF6 MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM()                                                       | MEM0:r:b REG0=XED_REG_AX:rw:SUPP
 | IDIV                 | BINARY         | BASE           | I86            | f6           | 0b11       | 0b111      | 0xF6 MOD[0b11] MOD=3 REG[0b111] RM[nnn]                                                              | REG0=GPR8_B():r REG1=XED_REG_AX:rw:SUPP
 | IDIV                 | BINARY         | BASE           | I86            | f7           | mm         | 0b111      | 0xF7 MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM()                                                       | MEM0:r:v REG0=OrAX():rw:SUPP REG1=OrDX():rw:SUPP
 | IDIV                 | BINARY         | BASE           | I86            | f7           | 0b11       | 0b111      | 0xF7 MOD[0b11] MOD=3 REG[0b111] RM[nnn]                                                              | REG0=GPRv_B():r REG1=OrAX():rw:SUPP REG2=OrDX():rw:SUPP
 | IMUL                 | BINARY         | BASE           | I186           | 69           | mm         | rrr        | 0x69 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SIMMz()                                                 | REG0=GPRv_R():w MEM0:r:v IMM0:r:z
 | IMUL                 | BINARY         | BASE           | I186           | 69           | 0b11       | rrr        | 0x69 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SIMMz()                                                        | REG0=GPRv_R():w REG1=GPRv_B():r IMM0:r:z
 | IMUL                 | BINARY         | BASE           | I186           | 6b           | mm         | rrr        | 0x6B MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SIMM8()                                                 | REG0=GPRv_R():w MEM0:r:v IMM0:r:b:i8
 | IMUL                 | BINARY         | BASE           | I186           | 6b           | 0b11       | rrr        | 0x6B MOD[0b11] MOD=3 REG[rrr] RM[nnn] SIMM8()                                                        | REG0=GPRv_R():w REG1=GPRv_B():r IMM0:r:b:i8
 | IMUL                 | BINARY         | BASE           | I86            | f6           | mm         | 0b101      | 0xF6 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM()                                                       | MEM0:r:b REG0=XED_REG_AL:r:SUPP REG1=XED_REG_AX:w:SUPP
 | IMUL                 | BINARY         | BASE           | I86            | f6           | 0b11       | 0b101      | 0xF6 MOD[0b11] MOD=3 REG[0b101] RM[nnn]                                                              | REG0=GPR8_B():r REG1=XED_REG_AL:r:SUPP REG2=XED_REG_AX:w:SUPP
 | IMUL                 | BINARY         | BASE           | I86            | f7           | mm         | 0b101      | 0xF7 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM()                                                       | MEM0:r:v REG0=OrAX():rw:SUPP REG1=OrDX():w:SUPP
 | IMUL                 | BINARY         | BASE           | I86            | f7           | 0b11       | 0b101      | 0xF7 MOD[0b11] MOD=3 REG[0b101] RM[nnn]                                                              | REG0=GPRv_B():r REG1=OrAX():rw:SUPP REG2=OrDX():w:SUPP
 | IMUL                 | BINARY         | BASE           | I86            | 0f af        | mm         | rrr        | 0x0F 0xAF MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | REG0=GPRv_R():rw MEM0:r:v
 | IMUL                 | BINARY         | BASE           | I86            | 0f af        | 0b11       | rrr        | 0x0F 0xAF MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_R():rw REG1=GPRv_B():r
 | IN                   | IO             | BASE           | I86            | e4           |            |            | 0xE4 UIMM8()                                                                                         | REG0=XED_REG_AL:w:IMPL IMM0:r:b
 | IN                   | IO             | BASE           | I86            | e5           |            |            | 0xE5 UIMM8() IMMUNE_REXW()                                                                           | REG0=OeAX():w:IMPL IMM0:r:b
 | IN                   | IO             | BASE           | I86            | ec           |            |            | 0xEC                                                                                                 | REG0=XED_REG_AL:w:IMPL REG1=XED_REG_DX:r:IMPL
 | IN                   | IO             | BASE           | I86            | ed           |            |            | 0xED IMMUNE_REXW()                                                                                   | REG0=OeAX():w:IMPL REG1=XED_REG_DX:r:IMPL
 | INC                  | BINARY         | BASE           | I86            | fe           | mm         | 0b000      | 0xFE MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() nolock_prefix                                         | MEM0:rw:b
 | INC                  | BINARY         | BASE           | I86            | fe           | 0b11       | 0b000      | 0xFE MOD[0b11] MOD=3 REG[0b000] RM[nnn]                                                              | REG0=GPR8_B():rw
 | INC                  | BINARY         | BASE           | I86            | ff           | mm         | 0b000      | 0xFF MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() nolock_prefix                                         | MEM0:rw:v
 | INC                  | BINARY         | BASE           | I86            | ff           | 0b11       | 0b000      | 0xFF MOD[0b11] MOD=3 REG[0b000] RM[nnn]                                                              | REG0=GPRv_B():rw
 | INC                  | BINARY         | BASE           | I86            |              |            |            | 0b0100_0 SRM[rrr] not64                                                                              | REG0=GPRv_SB():rw
 | INC_LOCK             | BINARY         | BASE           | I86            | fe           | mm         | 0b000      | 0xFE MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() lock_prefix                                           | MEM0:rw:b
 | INC_LOCK             | BINARY         | BASE           | I86            | ff           | mm         | 0b000      | 0xFF MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() lock_prefix                                           | MEM0:rw:v
 | INCSSPD              | CET            | CET            | CET            | 0f ae        | 0b11       | 0b101      | 0x0F 0xAE MOD[0b11] MOD=3  REG[0b101] RM[nnn]  f3_refining_prefix    W0                              | REG0=GPR32_B():r:d:u8 REG1=XED_REG_SSP:rw:SUPP:u64
 | INCSSPQ              | CET            | CET            | CET            | 0f ae        | 0b11       | 0b101      | 0x0F 0xAE MOD[0b11] MOD=3  REG[0b101] RM[nnn]  f3_refining_prefix    W1  mode64                      | REG0=GPR64_B():r:q:u8 REG1=XED_REG_SSP:rw:SUPP:u64
 | INSB                 | IOSTRINGOP     | BASE           | I186           | 6c           |            |            | 0x6C norep                                                                                           | MEM0:w:SUPP:b BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP
 | INSD                 | IOSTRINGOP     | BASE           | I386           | 6d           |            |            | 0x6D mode16 66_prefix  norep                                                                         | MEM0:w:SUPP:d BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP
 | INSD                 | IOSTRINGOP     | BASE           | I386           | 6d           |            |            | 0x6D mode32 no66_prefix  norep                                                                       | MEM0:w:SUPP:d BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP
 | INSD                 | IOSTRINGOP     | BASE           | I386           | 6d           |            |            | 0x6D mode64 norexw_prefix no66_prefix  norep                                                         | MEM0:w:SUPP:d BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP
 | INSD                 | IOSTRINGOP     | BASE           | I386           | 6d           |            |            | 0x6D mode64 rexw_prefix norep                                                                        | MEM0:w:SUPP:d BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP
 | INSERTPS             | SSE            | SSE4           |                | 0f 3a 21     | mm         | rrr        | 0x0F 0x3A 0x21 osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()      | REG0=XMM_R():rw:ps MEM0:r:d IMM0:r:b
 | INSERTPS             | SSE            | SSE4           |                | 0f 3a 21     | 0b11       | rrr        | 0x0F 0x3A 0x21 osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()             | REG0=XMM_R():rw:ps REG1=XMM_B():r:ps  IMM0:r:b
 | INSW                 | IOSTRINGOP     | BASE           | I186           | 6d           |            |            | 0x6D mode16 no66_prefix norep                                                                        | MEM0:w:SUPP:w BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP
 | INSW                 | IOSTRINGOP     | BASE           | I186           | 6d           |            |            | 0x6D mode32 66_prefix  norep                                                                         | MEM0:w:SUPP:w BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP
 | INSW                 | IOSTRINGOP     | BASE           | I186           | 6d           |            |            | 0x6D mode64 norexw_prefix 66_prefix  norep                                                           | MEM0:w:SUPP:w BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP
 | INT                  | INTERRUPT      | BASE           | I86            | cd           |            |            | 0xCD UIMM8()                                                                                         | IMM0:r:b REG0=rIP():w:SUPP
 | INT1                 | INTERRUPT      | BASE           | I86            | f1           |            |            | 0xF1                                                                                                 | REG0=rIP():w:SUPP
 | INT3                 | INTERRUPT      | BASE           | I86            | cc           |            |            | 0xCC                                                                                                 | REG0=rIP():w:SUPP
 | INTO                 | INTERRUPT      | BASE           | I86            | ce           |            |            | 0xCE not64                                                                                           | REG0=XED_REG_EIP:w:SUPP
 | INVD                 | SYSTEM         | BASE           | I486REAL       | 0f 08        |            |            | 0x0F 0x08                                                                                            | 
 | INVEPT               | VTX            | VTX            |                | 0f 38 80     | mm         | rrr        | 0x0F 0x38 0x80 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() mode64 MODRM() CR_WIDTH() | REG0=GPR64_R():r MEM0:r:dq
 | INVEPT               | VTX            | VTX            |                | 0f 38 80     | mm         | rrr        | 0x0F 0x38 0x80 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() not64  MODRM() CR_WIDTH() | REG0=GPR32_R():r MEM0:r:dq
 | INVLPG               | SYSTEM         | BASE           | I486REAL       | 0f 01        | mm         | 0b111      | 0x0F 0x01 MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM()                                                  | MEM0:r:b
 | INVLPGA              | SYSTEM         | SVM            |                | 0f 01        | 0b11       | 0b011      | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b011] RM[0b111]                                                       | REG0=ArAX():r:IMPL REG1=XED_REG_ECX:r:IMPL
 | INVPCID              | MISC           | INVPCID        | INVPCID        | 0f 38 82     | mm         | rrr        | 0x0F 0x38 0x82 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() mode64 MODRM() CR_WIDTH() | REG0=GPR64_R():r MEM0:r:dq
 | INVPCID              | MISC           | INVPCID        | INVPCID        | 0f 38 82     | mm         | rrr        | 0x0F 0x38 0x82 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() not64 MODRM() CR_WIDTH() | REG0=GPR32_R():r MEM0:r:dq
 | INVVPID              | VTX            | VTX            |                | 0f 38 81     | mm         | rrr        | 0x0F 0x38 0x81 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() mode64 MODRM() CR_WIDTH() | REG0=GPR64_R():r MEM0:r:dq
 | INVVPID              | VTX            | VTX            |                | 0f 38 81     | mm         | rrr        | 0x0F 0x38 0x81 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() not64  MODRM() CR_WIDTH() | REG0=GPR32_R():r MEM0:r:dq
 | IRET                 | RET            | BASE           | I86            | cf           |            |            | 0xCF mode16 no66_prefix                                                                              | REG0=XED_REG_STACKPOP:r:spw5:SUPP REG1=rIP():w:SUPP
 | IRET                 | RET            | BASE           | I86            | cf           |            |            | 0xCF mode32 66_prefix                                                                                | REG0=XED_REG_STACKPOP:r:spw5:SUPP REG1=rIP():w:SUPP
 | IRET                 | RET            | BASE           | I86            | cf           |            |            | 0xCF mode64 norexw_prefix 66_prefix                                                                  | REG0=XED_REG_STACKPOP:r:spw5:SUPP REG1=rIP():w:SUPP
 | IRETD                | RET            | BASE           | I386           | cf           |            |            | 0xCF mode16 66_prefix                                                                                | REG0=XED_REG_STACKPOP:r:spw5:SUPP REG1=rIP():w:SUPP
 | IRETD                | RET            | BASE           | I386           | cf           |            |            | 0xCF mode32 no66_prefix                                                                              | REG0=XED_REG_STACKPOP:r:spw5:SUPP REG1=rIP():w:SUPP
 | IRETD                | RET            | BASE           | I386           | cf           |            |            | 0xCF mode64 norexw_prefix no66_prefix                                                                | REG0=XED_REG_STACKPOP:r:spw5:SUPP REG1=rIP():w:SUPP
 | IRETQ                | RET            | LONGMODE       |                | cf           |            |            | 0xCF mode64 rexw_prefix                                                                              | REG0=XED_REG_STACKPOP:r:spw5:SUPP REG1=XED_REG_RIP:w:SUPP
 | JB                   | COND_BR        | BASE           | I86            | 72           |            |            | 0x72 mode64 FORCE64() BRANCH_HINT() BRDISP8()                                                        | RELBR:r:b:i8 REG0=XED_REG_RIP:rw:SUPP
 | JB                   | COND_BR        | BASE           | I86            | 72           |            |            | 0x72 not64 BRANCH_HINT() BRDISP8()                                                                   | RELBR:r:b:i8 REG0=XED_REG_EIP:rw:SUPP
 | JB                   | COND_BR        | BASE           | I86            | 0f 82        |            |            | 0x0F 0x82 not64 BRANCH_HINT() BRDISPz()                                                              | RELBR:r:z REG0=XED_REG_EIP:rw:SUPP
 | JB                   | COND_BR        | BASE           | I86            | 0f 82        |            |            | 0x0F 0x82 mode64 FORCE64() BRANCH_HINT() BRDISP32()                                                  | RELBR:r:d REG0=XED_REG_RIP:rw:SUPP
 | JBE                  | COND_BR        | BASE           | I86            | 76           |            |            | 0x76 mode64 FORCE64() BRANCH_HINT() BRDISP8()                                                        | RELBR:r:b:i8  REG0=XED_REG_RIP:rw:SUPP
 | JBE                  | COND_BR        | BASE           | I86            | 76           |            |            | 0x76 not64  BRANCH_HINT() BRDISP8()                                                                  | RELBR:r:b:i8 REG0=XED_REG_EIP:rw:SUPP
 | JBE                  | COND_BR        | BASE           | I86            | 0f 86        |            |            | 0x0F 0x86 not64 BRANCH_HINT() BRDISPz()                                                              | RELBR:r:z REG0=XED_REG_EIP:rw:SUPP
 | JBE                  | COND_BR        | BASE           | I86            | 0f 86        |            |            | 0x0F 0x86 mode64 FORCE64() BRANCH_HINT() BRDISP32()                                                  | RELBR:r:d REG0=XED_REG_RIP:rw:SUPP
 | JCXZ                 | COND_BR        | BASE           | I386           | e3           |            |            | 0xE3 eamode16 BRDISP8()                                                                              | RELBR:r:b:i8 REG0=XED_REG_CX:r:SUPP REG1=XED_REG_IP:rw:SUPP
 | JECXZ                | COND_BR        | BASE           | I386           | e3           |            |            | 0xE3 eamode32 not64 BRDISP8()                                                                        | RELBR:r:b:i8 REG0=XED_REG_ECX:r:SUPP REG1=XED_REG_EIP:rw:SUPP
 | JECXZ                | COND_BR        | BASE           | I386           | e3           |            |            | 0xE3 eamode32 mode64 BRDISP8() FORCE64()                                                             | RELBR:r:b:i8 REG0=XED_REG_ECX:r:SUPP REG1=XED_REG_RIP:rw:SUPP
 | JKNZD                | COND_BR        | KNCV           | KNCJKBR        | 75           |            |            | VV1 0x75 VNP VMAP0   W0 BRDISP8()                                                                    | REG0=MASK_N():w:mskw RELBR:r:b
 | JKNZD                | COND_BR        | KNCV           | KNCJKBR        | 85           |            |            | VV1 0x85 VNP not64 V0F   W0 BRDISPz()                                                                | REG0=MASK_N():w:mskw RELBR:r:z
 | JKNZD                | COND_BR        | KNCV           | KNCJKBR        | 85           |            |            | VV1 0x85 VNP mode64 V0F   W0 BRDISP32()                                                              | REG0=MASK_N():w:mskw RELBR:r:z
 | JKZD                 | COND_BR        | KNCV           | KNCJKBR        | 74           |            |            | VV1 0x74 VNP VMAP0   W0 BRDISP8()                                                                    | REG0=MASK_N():w:mskw RELBR:r:b
 | JKZD                 | COND_BR        | KNCV           | KNCJKBR        | 84           |            |            | VV1 0x84 not64 VNP V0F   W0 BRDISPz()                                                                | REG0=MASK_N():w:mskw RELBR:r:z
 | JKZD                 | COND_BR        | KNCV           | KNCJKBR        | 84           |            |            | VV1 0x84 mode64 VNP V0F   W0 BRDISP32()                                                              | REG0=MASK_N():w:mskw RELBR:r:z
 | JL                   | COND_BR        | BASE           | I86            | 7c           |            |            | 0x7C mode64 FORCE64() BRANCH_HINT() BRDISP8()                                                        | RELBR:r:b:i8 REG0=XED_REG_RIP:rw:SUPP
 | JL                   | COND_BR        | BASE           | I86            | 7c           |            |            | 0x7C not64 BRANCH_HINT() BRDISP8()                                                                   | RELBR:r:b:i8 REG0=XED_REG_EIP:rw:SUPP
 | JL                   | COND_BR        | BASE           | I86            | 0f 8c        |            |            | 0x0F 0x8C not64 BRANCH_HINT() BRDISPz()                                                              | RELBR:r:z REG0=XED_REG_EIP:rw:SUPP
 | JL                   | COND_BR        | BASE           | I86            | 0f 8c        |            |            | 0x0F 0x8C mode64 FORCE64() BRANCH_HINT() BRDISP32()                                                  | RELBR:r:d REG0=XED_REG_RIP:rw:SUPP
 | JLE                  | COND_BR        | BASE           | I86            | 7e           |            |            | 0x7E mode64 FORCE64() BRANCH_HINT() BRDISP8()                                                        | RELBR:r:b:i8 REG0=XED_REG_RIP:rw:SUPP
 | JLE                  | COND_BR        | BASE           | I86            | 7e           |            |            | 0x7E not64 BRANCH_HINT() BRDISP8()                                                                   | RELBR:r:b:i8 REG0=XED_REG_EIP:rw:SUPP
 | JLE                  | COND_BR        | BASE           | I86            | 0f 8e        |            |            | 0x0F 0x8E not64 BRANCH_HINT() BRDISPz()                                                              | RELBR:r:z REG0=XED_REG_EIP:rw:SUPP
 | JLE                  | COND_BR        | BASE           | I86            | 0f 8e        |            |            | 0x0F 0x8E mode64 FORCE64() BRANCH_HINT() BRDISP32()                                                  | RELBR:r:d REG0=XED_REG_RIP:rw:SUPP
 | JMP                  | UNCOND_BR      | BASE           | I86            | ff           | mm         | 0b100      | 0xFF MOD[mm] MOD!=3 REG[0b100] RM[nnn] DF64() IMMUNE66_LOOP64() MODRM()                              | MEM0:r:v REG0=rIP():w:SUPP
 | JMP                  | UNCOND_BR      | BASE           | I86            | ff           | 0b11       | 0b100      | 0xFF MOD[0b11] MOD=3 REG[0b100] RM[nnn] DF64() IMMUNE66_LOOP64()                                     | REG0=GPRv_B():r REG1=rIP():w:SUPP
 | JMP                  | UNCOND_BR      | BASE           | I86            | e9           |            |            | 0xE9 not64 BRDISPz()                                                                                 | RELBR:r:z REG0=XED_REG_EIP:rw:SUPP
 | JMP                  | UNCOND_BR      | BASE           | I86            | e9           |            |            | 0xE9 mode64 FORCE64() BRDISP32()                                                                     | RELBR:r:d REG0=XED_REG_RIP:rw:SUPP
 | JMP                  | UNCOND_BR      | BASE           | I86            | eb           |            |            | 0xEB not64 BRDISP8()                                                                                 | RELBR:r:b:i8 REG0=XED_REG_EIP:rw:SUPP
 | JMP                  | UNCOND_BR      | BASE           | I86            | eb           |            |            | 0xEB mode64 FORCE64() BRDISP8()                                                                      | RELBR:r:b:i8 REG0=XED_REG_RIP:rw:SUPP
 | JMP_FAR              | UNCOND_BR      | BASE           | I86            | ff           | mm         | 0b101      | 0xFF MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM()                                                       | MEM0:r:p2 REG0=rIP():w:SUPP
 | JMP_FAR              | UNCOND_BR      | BASE           | I86            | ea           |            |            | 0xEA not64 BRDISPz() UIMM16()                                                                        | PTR:r:p IMM0:r:w REG0=XED_REG_EIP:w:SUPP
 | JNB                  | COND_BR        | BASE           | I86            | 73           |            |            | 0x73 mode64 FORCE64() BRANCH_HINT() BRDISP8()                                                        | RELBR:r:b:i8 REG0=XED_REG_RIP:rw:SUPP
 | JNB                  | COND_BR        | BASE           | I86            | 73           |            |            | 0x73 not64 BRANCH_HINT() BRDISP8()                                                                   | RELBR:r:b:i8 REG0=XED_REG_EIP:rw:SUPP
 | JNB                  | COND_BR        | BASE           | I86            | 0f 83        |            |            | 0x0F 0x83 not64 BRANCH_HINT() BRDISPz()                                                              | RELBR:r:z REG0=XED_REG_EIP:rw:SUPP
 | JNB                  | COND_BR        | BASE           | I86            | 0f 83        |            |            | 0x0F 0x83 mode64 FORCE64() BRANCH_HINT() BRDISP32()                                                  | RELBR:r:d REG0=XED_REG_RIP:rw:SUPP
 | JNBE                 | COND_BR        | BASE           | I86            | 77           |            |            | 0x77 mode64 FORCE64() BRANCH_HINT() BRDISP8()                                                        | RELBR:r:b:i8 REG0=XED_REG_RIP:rw:SUPP
 | JNBE                 | COND_BR        | BASE           | I86            | 77           |            |            | 0x77 not64  BRANCH_HINT() BRDISP8()                                                                  | RELBR:r:b:i8 REG0=XED_REG_EIP:rw:SUPP
 | JNBE                 | COND_BR        | BASE           | I86            | 0f 87        |            |            | 0x0F 0x87 not64 BRANCH_HINT() BRDISPz()                                                              | RELBR:r:z REG0=XED_REG_EIP:rw:SUPP
 | JNBE                 | COND_BR        | BASE           | I86            | 0f 87        |            |            | 0x0F 0x87 mode64 FORCE64() BRANCH_HINT() BRDISP32()                                                  | RELBR:r:d REG0=XED_REG_RIP:rw:SUPP
 | JNL                  | COND_BR        | BASE           | I86            | 7d           |            |            | 0x7D mode64 FORCE64() BRANCH_HINT() BRDISP8()                                                        | RELBR:r:b:i8 REG0=XED_REG_RIP:rw:SUPP
 | JNL                  | COND_BR        | BASE           | I86            | 7d           |            |            | 0x7D not64 BRANCH_HINT() BRDISP8()                                                                   | RELBR:r:b:i8 REG0=XED_REG_EIP:rw:SUPP
 | JNL                  | COND_BR        | BASE           | I86            | 0f 8d        |            |            | 0x0F 0x8D not64 BRANCH_HINT() BRDISPz()                                                              | RELBR:r:z REG0=XED_REG_EIP:rw:SUPP
 | JNL                  | COND_BR        | BASE           | I86            | 0f 8d        |            |            | 0x0F 0x8D mode64 FORCE64() BRANCH_HINT() BRDISP32()                                                  | RELBR:r:d REG0=XED_REG_RIP:rw:SUPP
 | JNLE                 | COND_BR        | BASE           | I86            | 7f           |            |            | 0x7F mode64 FORCE64() BRANCH_HINT() BRDISP8()                                                        | RELBR:r:b:i8 REG0=XED_REG_RIP:rw:SUPP
 | JNLE                 | COND_BR        | BASE           | I86            | 7f           |            |            | 0x7F not64 BRANCH_HINT() BRDISP8()                                                                   | RELBR:r:b:i8 REG0=XED_REG_EIP:rw:SUPP
 | JNLE                 | COND_BR        | BASE           | I86            | 0f 8f        |            |            | 0x0F 0x8F not64 BRANCH_HINT() BRDISPz()                                                              | RELBR:r:z REG0=XED_REG_EIP:rw:SUPP
 | JNLE                 | COND_BR        | BASE           | I86            | 0f 8f        |            |            | 0x0F 0x8F mode64 FORCE64() BRANCH_HINT() BRDISP32()                                                  | RELBR:r:d REG0=XED_REG_RIP:rw:SUPP
 | JNO                  | COND_BR        | BASE           | I86            | 71           |            |            | 0x71 mode64 FORCE64() BRANCH_HINT() BRDISP8()                                                        | RELBR:r:b:i8 REG0=XED_REG_RIP:rw:SUPP
 | JNO                  | COND_BR        | BASE           | I86            | 71           |            |            | 0x71 not64 BRANCH_HINT() BRDISP8()                                                                   | RELBR:r:b:i8 REG0=XED_REG_EIP:rw:SUPP
 | JNO                  | COND_BR        | BASE           | I86            | 0f 81        |            |            | 0x0F 0x81 not64 BRANCH_HINT() BRDISPz()                                                              | RELBR:r:z REG0=XED_REG_EIP:rw:SUPP
 | JNO                  | COND_BR        | BASE           | I86            | 0f 81        |            |            | 0x0F 0x81 mode64 FORCE64() BRANCH_HINT() BRDISP32()                                                  | RELBR:r:d REG0=XED_REG_RIP:rw:SUPP
 | JNP                  | COND_BR        | BASE           | I86            | 7b           |            |            | 0x7B mode64 FORCE64() BRANCH_HINT() BRDISP8()                                                        | RELBR:r:b:i8 REG0=XED_REG_RIP:rw:SUPP
 | JNP                  | COND_BR        | BASE           | I86            | 7b           |            |            | 0x7B not64  BRANCH_HINT() BRDISP8()                                                                  | RELBR:r:b:i8 REG0=XED_REG_EIP:rw:SUPP
 | JNP                  | COND_BR        | BASE           | I86            | 0f 8b        |            |            | 0x0F 0x8B not64 BRANCH_HINT() BRDISPz()                                                              | RELBR:r:z REG0=XED_REG_EIP:rw:SUPP
 | JNP                  | COND_BR        | BASE           | I86            | 0f 8b        |            |            | 0x0F 0x8B mode64 FORCE64() BRANCH_HINT() BRDISP32()                                                  | RELBR:r:d REG0=XED_REG_RIP:rw:SUPP
 | JNS                  | COND_BR        | BASE           | I86            | 79           |            |            | 0x79 mode64 FORCE64() BRANCH_HINT() BRDISP8()                                                        | RELBR:r:b:i8 REG0=XED_REG_RIP:rw:SUPP
 | JNS                  | COND_BR        | BASE           | I86            | 79           |            |            | 0x79 not64  BRANCH_HINT() BRDISP8()                                                                  | RELBR:r:b:i8 REG0=XED_REG_EIP:rw:SUPP
 | JNS                  | COND_BR        | BASE           | I86            | 0f 89        |            |            | 0x0F 0x89 not64 BRANCH_HINT() BRDISPz()                                                              | RELBR:r:z REG0=XED_REG_EIP:rw:SUPP
 | JNS                  | COND_BR        | BASE           | I86            | 0f 89        |            |            | 0x0F 0x89 mode64 FORCE64() BRANCH_HINT() BRDISP32()                                                  | RELBR:r:d REG0=XED_REG_RIP:rw:SUPP
 | JNZ                  | COND_BR        | BASE           | I86            | 75           |            |            | 0x75 mode64 FORCE64() BRANCH_HINT() BRDISP8()                                                        | RELBR:r:b:i8 REG0=XED_REG_RIP:rw:SUPP
 | JNZ                  | COND_BR        | BASE           | I86            | 75           |            |            | 0x75 not64  BRANCH_HINT() BRDISP8()                                                                  | RELBR:r:b:i8 REG0=XED_REG_EIP:rw:SUPP
 | JNZ                  | COND_BR        | BASE           | I86            | 0f 85        |            |            | 0x0F 0x85 not64 BRANCH_HINT() BRDISPz()                                                              | RELBR:r:z REG0=XED_REG_EIP:rw:SUPP
 | JNZ                  | COND_BR        | BASE           | I86            | 0f 85        |            |            | 0x0F 0x85 mode64 FORCE64() BRANCH_HINT() BRDISP32()                                                  | RELBR:r:d REG0=XED_REG_RIP:rw:SUPP
 | JO                   | COND_BR        | BASE           | I86            | 70           |            |            | 0x70 mode64 FORCE64() BRANCH_HINT() BRDISP8()                                                        | RELBR:r:b:i8 REG0=XED_REG_RIP:rw:SUPP
 | JO                   | COND_BR        | BASE           | I86            | 70           |            |            | 0x70 not64 BRANCH_HINT() BRDISP8()                                                                   | RELBR:r:b:i8 REG0=XED_REG_EIP:rw:SUPP
 | JO                   | COND_BR        | BASE           | I86            | 0f 80        |            |            | 0x0F 0x80 mode64 FORCE64() BRANCH_HINT() BRDISP32()                                                  | RELBR:r:d REG0=XED_REG_RIP:rw:SUPP
 | JO                   | COND_BR        | BASE           | I86            | 0f 80        |            |            | 0x0F 0x80 not64 BRANCH_HINT() BRDISPz()                                                              | RELBR:r:z REG0=XED_REG_EIP:rw:SUPP
 | JP                   | COND_BR        | BASE           | I86            | 7a           |            |            | 0x7A mode64 FORCE64() BRANCH_HINT() BRDISP8()                                                        | RELBR:r:b:i8 REG0=XED_REG_RIP:rw:SUPP
 | JP                   | COND_BR        | BASE           | I86            | 7a           |            |            | 0x7A not64  BRANCH_HINT() BRDISP8()                                                                  | RELBR:r:b:i8 REG0=XED_REG_EIP:rw:SUPP
 | JP                   | COND_BR        | BASE           | I86            | 0f 8a        |            |            | 0x0F 0x8A not64 BRANCH_HINT() BRDISPz()                                                              | RELBR:r:z REG0=XED_REG_EIP:rw:SUPP
 | JP                   | COND_BR        | BASE           | I86            | 0f 8a        |            |            | 0x0F 0x8A mode64 FORCE64() BRANCH_HINT() BRDISP32()                                                  | RELBR:r:d REG0=XED_REG_RIP:rw:SUPP
 | JRCXZ                | COND_BR        | BASE           | LONGMODE       | e3           |            |            | 0xE3 eamode64 BRDISP8() FORCE64()                                                                    | RELBR:r:b:i8 REG0=XED_REG_RCX:r:SUPP REG1=XED_REG_RIP:rw:SUPP
 | JS                   | COND_BR        | BASE           | I86            | 78           |            |            | 0x78 mode64 FORCE64() BRANCH_HINT() BRDISP8()                                                        | RELBR:r:b:i8 REG0=XED_REG_RIP:rw:SUPP
 | JS                   | COND_BR        | BASE           | I86            | 78           |            |            | 0x78 not64  BRANCH_HINT() BRDISP8()                                                                  | RELBR:r:b:i8 REG0=XED_REG_EIP:rw:SUPP
 | JS                   | COND_BR        | BASE           | I86            | 0f 88        |            |            | 0x0F 0x88 not64 BRANCH_HINT() BRDISPz()                                                              | RELBR:r:z REG0=XED_REG_EIP:rw:SUPP
 | JS                   | COND_BR        | BASE           | I86            | 0f 88        |            |            | 0x0F 0x88 mode64 FORCE64() BRANCH_HINT() BRDISP32()                                                  | RELBR:r:d REG0=XED_REG_RIP:rw:SUPP
 | JZ                   | COND_BR        | BASE           | I86            | 74           |            |            | 0x74 mode64 FORCE64() BRANCH_HINT() BRDISP8()                                                        | RELBR:r:b:i8 REG0=XED_REG_RIP:rw:SUPP
 | JZ                   | COND_BR        | BASE           | I86            | 74           |            |            | 0x74 not64 BRANCH_HINT() BRDISP8()                                                                   | RELBR:r:b:i8 REG0=XED_REG_EIP:rw:SUPP
 | JZ                   | COND_BR        | BASE           | I86            | 0f 84        |            |            | 0x0F 0x84 not64 BRANCH_HINT() BRDISPz()                                                              | RELBR:r:z REG0=XED_REG_EIP:rw:SUPP
 | JZ                   | COND_BR        | BASE           | I86            | 0f 84        |            |            | 0x0F 0x84 mode64 FORCE64() BRANCH_HINT() BRDISP32()                                                  | RELBR:r:d REG0=XED_REG_RIP:rw:SUPP
 | KADDB                | KMASK          | AVX512VEX      | AVX512DQ_KOP   | 4a           | 0b11       | rrr        | VV1 0x4A V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W0                                        | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
 | KADDD                | KMASK          | AVX512VEX      | AVX512BW_KOP   | 4a           | 0b11       | rrr        | VV1 0x4A V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W1                                        | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
 | KADDQ                | KMASK          | AVX512VEX      | AVX512BW_KOP   | 4a           | 0b11       | rrr        | VV1 0x4A VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W1                                        | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
 | KADDW                | KMASK          | AVX512VEX      | AVX512DQ_KOP   | 4a           | 0b11       | rrr        | VV1 0x4A VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W0                                        | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
 | KAND                 | KNCMASK        | KNC            | KNCV           | 41           | 0b11       | rrr        | VV1 0x41  VL128 VNP V0F REXW=0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                | REG0=MASK_R():w:mskw    REG1=MASK_B():r:mskw
 | KANDB                | KMASK          | AVX512VEX      | AVX512DQ_KOP   | 41           | 0b11       | rrr        | VV1 0x41 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W0                                        | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
 | KANDD                | KMASK          | AVX512VEX      | AVX512BW_KOP   | 41           | 0b11       | rrr        | VV1 0x41 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W1                                        | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
 | KANDN                | KNCMASK        | KNC            | KNCV           | 42           | 0b11       | rrr        | VV1 0x42  VL128 VNP V0F REXW=0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                | REG0=MASK_R():w:mskw    REG1=MASK_B():r:mskw
 | KANDNB               | KMASK          | AVX512VEX      | AVX512DQ_KOP   | 42           | 0b11       | rrr        | VV1 0x42 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W0                                        | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
 | KANDND               | KMASK          | AVX512VEX      | AVX512BW_KOP   | 42           | 0b11       | rrr        | VV1 0x42 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W1                                        | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
 | KANDNQ               | KMASK          | AVX512VEX      | AVX512BW_KOP   | 42           | 0b11       | rrr        | VV1 0x42 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W1                                        | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
 | KANDNR               | KNCMASK        | KNC            | KNCV           | 43           | 0b11       | rrr        | VV1 0x43  VL128 VNP V0F REXW=0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                | REG0=MASK_R():w:mskw    REG1=MASK_B():r:mskw
 | KANDNW               | KMASK          | AVX512VEX      | AVX512F_KOP    | 42           | 0b11       | rrr        | VV1 0x42 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W0                                        | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
 | KANDQ                | KMASK          | AVX512VEX      | AVX512BW_KOP   | 41           | 0b11       | rrr        | VV1 0x41 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W1                                        | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
 | KANDW                | KMASK          | AVX512VEX      | AVX512F_KOP    | 41           | 0b11       | rrr        | VV1 0x41 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W0                                        | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
 | KCONCATH             | KNCMASK        | KNC            | KNCV           | 95           | 0b11       | rrr        | VV1 0x95  VL128 VNP V0F REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=GPR64_R():w:q    REG1=MASK_N():r:mskw    REG2=MASK_B():r:mskw
 | KCONCATL             | KNCMASK        | KNC            | KNCV           | 97           | 0b11       | rrr        | VV1 0x97  VL128 VNP V0F REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=GPR64_R():w:q    REG1=MASK_N():r:mskw    REG2=MASK_B():r:mskw
 | KEXTRACT             | KNCMASK        | KNC            | KNCV           | 3e           | 0b11       | rrr        | VV1 0x3E  VL128 V66 V0F3A REXW=0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]  UIMM8()                     | REG0=MASK_R():w:mskw REG1=GPR64_B():r:q IMM0:r:b
 | KMERGE2L1H           | KNCMASK        | KNC            | KNCV           | 48           | 0b11       | rrr        | VV1 0x48  VL128 VNP V0F REXW=0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                | REG0=MASK_R():w:mskw    REG1=MASK_B():r:mskw
 | KMERGE2L1L           | KNCMASK        | KNC            | KNCV           | 49           | 0b11       | rrr        | VV1 0x49  VL128 VNP V0F REXW=0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                | REG0=MASK_R():w:mskw    REG1=MASK_B():r:mskw
 | KMOV                 | KNCMASK        | KNC            | KNCV           | 90           | 0b11       | rrr        | VV1 0x90  VL128 VNP V0F REXW=0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                | REG0=MASK_R():w:mskw    REG1=MASK_B():r:mskw
 | KMOV                 | KNCMASK        | KNC            | KNCV           | 92           | 0b11       | rrr        | VV1 0x92  VL128 VNP V0F REXW=0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] mode64                         | REG0=MASK_R():w:mskw    REG1=GPR32_B():r:mskw
 | KMOV                 | KNCMASK        | KNC            | KNCV           | 93           | 0b11       | rrr        | VV1 0x93  VL128 VNP V0F REXW=0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] mode64                         | REG0=GPR32_R():w:mskw   REG1=MASK_B():r:mskw
 | KMOVB                | KMASK          | AVX512VEX      | AVX512DQ_KOP   | 90           | 0b11       | rrr        | VV1 0x90 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0  NOVSR                                 | REG0=MASK_R():w:mskw REG1=MASK_B():r:mskw:u8
 | KMOVB                | KMASK          | AVX512VEX      | AVX512DQ_KOP   | 90           | mm         | rrr        | VV1 0x90 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOVSR                          | REG0=MASK_R():w:mskw MEM0:r:b:u8
 | KMOVB                | KMASK          | AVX512VEX      | AVX512DQ_KOP   | 91           | mm         | rrr        | VV1 0x91 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOVSR                          | MEM0:w:b:u8 REG0=MASK_R():r:mskw
 | KMOVB                | KMASK          | AVX512VEX      | AVX512DQ_KOP   | 92           | 0b11       | rrr        | VV1 0x92 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0  NOVSR                                 | REG0=MASK_R():w:mskw REG1=GPR32_B():r:d:u32
 | KMOVB                | KMASK          | AVX512VEX      | AVX512DQ_KOP   | 93           | 0b11       | rrr        | VV1 0x93 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0  NOVSR                                 | REG0=GPR32_R():w:d:u32 REG1=MASK_B():r:mskw
 | KMOVD                | KMASK          | AVX512VEX      | AVX512BW_KOP   | 90           | 0b11       | rrr        | VV1 0x90 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W1  NOVSR                                 | REG0=MASK_R():w:mskw REG1=MASK_B():r:mskw:u32
 | KMOVD                | KMASK          | AVX512VEX      | AVX512BW_KOP   | 90           | mm         | rrr        | VV1 0x90 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOVSR                          | REG0=MASK_R():w:mskw MEM0:r:d:u32
 | KMOVD                | KMASK          | AVX512VEX      | AVX512BW_KOP   | 91           | mm         | rrr        | VV1 0x91 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOVSR                          | MEM0:w:d:u32 REG0=MASK_R():r:mskw
 | KMOVD                | KMASK          | AVX512VEX      | AVX512BW_KOP   | 92           | 0b11       | rrr        | VV1 0x92 VF2 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0 mode64  NOVSR                          | REG0=MASK_R():w:mskw REG1=GPR32_B():r:d:u32
 | KMOVD                | KMASK          | AVX512VEX      | AVX512BW_KOP   | 92           | 0b11       | rrr        | VV1 0x92 VF2 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  not64 NOVSR                               | REG0=MASK_R():w:mskw REG1=GPR32_B():r:d:u32
 | KMOVD                | KMASK          | AVX512VEX      | AVX512BW_KOP   | 93           | 0b11       | rrr        | VV1 0x93 VF2 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0  mode64 NOVSR                          | REG0=GPR32_R():w:d:u32 REG1=MASK_B():r:mskw
 | KMOVD                | KMASK          | AVX512VEX      | AVX512BW_KOP   | 93           | 0b11       | rrr        | VV1 0x93 VF2 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  not64  NOVSR                              | REG0=GPR32_R():w:d:u32 REG1=MASK_B():r:mskw
 | KMOVQ                | KMASK          | AVX512VEX      | AVX512BW_KOP   | 90           | 0b11       | rrr        | VV1 0x90 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W1  NOVSR                                 | REG0=MASK_R():w:mskw REG1=MASK_B():r:mskw:u64
 | KMOVQ                | KMASK          | AVX512VEX      | AVX512BW_KOP   | 90           | mm         | rrr        | VV1 0x90 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOVSR                          | REG0=MASK_R():w:mskw MEM0:r:q:u64
 | KMOVQ                | KMASK          | AVX512VEX      | AVX512BW_KOP   | 91           | mm         | rrr        | VV1 0x91 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOVSR                          | MEM0:w:q:u64 REG0=MASK_R():r:mskw
 | KMOVQ                | KMASK          | AVX512VEX      | AVX512BW_KOP   | 92           | 0b11       | rrr        | VV1 0x92 VF2 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W1  mode64  NOVSR                         | REG0=MASK_R():w:mskw REG1=GPR64_B():r:q:u64
 | KMOVQ                | KMASK          | AVX512VEX      | AVX512BW_KOP   | 93           | 0b11       | rrr        | VV1 0x93 VF2 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W1  mode64  NOVSR                         | REG0=GPR64_R():w:q:u64 REG1=MASK_B():r:mskw
 | KMOVW                | KMASK          | AVX512VEX      | AVX512F_KOP    | 90           | 0b11       | rrr        | VV1 0x90 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0  NOVSR                                 | REG0=MASK_R():w:mskw REG1=MASK_B():r:mskw:u16
 | KMOVW                | KMASK          | AVX512VEX      | AVX512F_KOP    | 90           | mm         | rrr        | VV1 0x90 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOVSR                          | REG0=MASK_R():w:mskw MEM0:r:wrd:u16
 | KMOVW                | KMASK          | AVX512VEX      | AVX512F_KOP    | 91           | mm         | rrr        | VV1 0x91 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOVSR                          | MEM0:w:wrd:u16 REG0=MASK_R():r:mskw
 | KMOVW                | KMASK          | AVX512VEX      | AVX512F_KOP    | 92           | 0b11       | rrr        | VV1 0x92 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0  NOVSR                                 | REG0=MASK_R():w:mskw REG1=GPR32_B():r:d:u32
 | KMOVW                | KMASK          | AVX512VEX      | AVX512F_KOP    | 93           | 0b11       | rrr        | VV1 0x93 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0  NOVSR                                 | REG0=GPR32_R():w:d:u32 REG1=MASK_B():r:mskw
 | KNOT                 | KNCMASK        | KNC            | KNCV           | 44           | 0b11       | rrr        | VV1 0x44  VL128 VNP V0F REXW=0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                | REG0=MASK_R():w:mskw    REG1=MASK_B():r:mskw
 | KNOTB                | KMASK          | AVX512VEX      | AVX512DQ_KOP   | 44           | 0b11       | rrr        | VV1 0x44 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0  NOVSR                                 | REG0=MASK_R():w:mskw REG1=MASK_B():r:mskw
 | KNOTD                | KMASK          | AVX512VEX      | AVX512BW_KOP   | 44           | 0b11       | rrr        | VV1 0x44 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W1  NOVSR                                 | REG0=MASK_R():w:mskw REG1=MASK_B():r:mskw
 | KNOTQ                | KMASK          | AVX512VEX      | AVX512BW_KOP   | 44           | 0b11       | rrr        | VV1 0x44 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W1  NOVSR                                 | REG0=MASK_R():w:mskw REG1=MASK_B():r:mskw
 | KNOTW                | KMASK          | AVX512VEX      | AVX512F_KOP    | 44           | 0b11       | rrr        | VV1 0x44 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0  NOVSR                                 | REG0=MASK_R():w:mskw REG1=MASK_B():r:mskw
 | KOR                  | KNCMASK        | KNC            | KNCV           | 45           | 0b11       | rrr        | VV1 0x45  VL128 VNP V0F REXW=0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                | REG0=MASK_R():w:mskw    REG1=MASK_B():r:mskw
 | KORB                 | KMASK          | AVX512VEX      | AVX512DQ_KOP   | 45           | 0b11       | rrr        | VV1 0x45 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W0                                        | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
 | KORD                 | KMASK          | AVX512VEX      | AVX512BW_KOP   | 45           | 0b11       | rrr        | VV1 0x45 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W1                                        | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
 | KORQ                 | KMASK          | AVX512VEX      | AVX512BW_KOP   | 45           | 0b11       | rrr        | VV1 0x45 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W1                                        | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
 | KORTESTB             | KMASK          | AVX512VEX      | AVX512DQ_KOP   | 98           | 0b11       | rrr        | VV1 0x98 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0  NOVSR                                 | REG0=MASK_R():r:mskw REG1=MASK_B():r:mskw
 | KORTESTD             | KMASK          | AVX512VEX      | AVX512BW_KOP   | 98           | 0b11       | rrr        | VV1 0x98 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W1  NOVSR                                 | REG0=MASK_R():r:mskw REG1=MASK_B():r:mskw
 | KORTESTD             | KNCMASK        | KNC            | KNCV           | 98           | 0b11       | rrr        | VV1 0x98  VL128 VNP V0F REXW=0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                | REG0=MASK_R():r:mskw    REG1=MASK_B():r:mskw
 | KORTESTQ             | KMASK          | AVX512VEX      | AVX512BW_KOP   | 98           | 0b11       | rrr        | VV1 0x98 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W1  NOVSR                                 | REG0=MASK_R():r:mskw REG1=MASK_B():r:mskw
 | KORTESTW             | KMASK          | AVX512VEX      | AVX512F_KOP    | 98           | 0b11       | rrr        | VV1 0x98 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0  NOVSR                                 | REG0=MASK_R():r:mskw REG1=MASK_B():r:mskw
 | KORW                 | KMASK          | AVX512VEX      | AVX512F_KOP    | 45           | 0b11       | rrr        | VV1 0x45 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W0                                        | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
 | KSHIFTLB             | KMASK          | AVX512VEX      | AVX512DQ_KOP   | 32           | 0b11       | rrr        | VV1 0x32 V66 V0F3A MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0  NOVSR UIMM8()                       | REG0=MASK_R():w:mskw REG1=MASK_B():r:mskw IMM0:r:b
 | KSHIFTLD             | KMASK          | AVX512VEX      | AVX512BW_KOP   | 33           | 0b11       | rrr        | VV1 0x33 V66 V0F3A MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0  NOVSR UIMM8()                       | REG0=MASK_R():w:mskw REG1=MASK_B():r:mskw IMM0:r:b
 | KSHIFTLQ             | KMASK          | AVX512VEX      | AVX512BW_KOP   | 33           | 0b11       | rrr        | VV1 0x33 V66 V0F3A MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W1  NOVSR UIMM8()                       | REG0=MASK_R():w:mskw REG1=MASK_B():r:mskw IMM0:r:b
 | KSHIFTLW             | KMASK          | AVX512VEX      | AVX512F_KOP    | 32           | 0b11       | rrr        | VV1 0x32 V66 V0F3A MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W1  NOVSR UIMM8()                       | REG0=MASK_R():w:mskw REG1=MASK_B():r:mskw IMM0:r:b
 | KSHIFTRB             | KMASK          | AVX512VEX      | AVX512DQ_KOP   | 30           | 0b11       | rrr        | VV1 0x30 V66 V0F3A MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0  NOVSR UIMM8()                       | REG0=MASK_R():w:mskw REG1=MASK_B():r:mskw IMM0:r:b
 | KSHIFTRD             | KMASK          | AVX512VEX      | AVX512BW_KOP   | 31           | 0b11       | rrr        | VV1 0x31 V66 V0F3A MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0  NOVSR UIMM8()                       | REG0=MASK_R():w:mskw REG1=MASK_B():r:mskw IMM0:r:b
 | KSHIFTRQ             | KMASK          | AVX512VEX      | AVX512BW_KOP   | 31           | 0b11       | rrr        | VV1 0x31 V66 V0F3A MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W1  NOVSR UIMM8()                       | REG0=MASK_R():w:mskw REG1=MASK_B():r:mskw IMM0:r:b
 | KSHIFTRW             | KMASK          | AVX512VEX      | AVX512F_KOP    | 30           | 0b11       | rrr        | VV1 0x30 V66 V0F3A MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W1  NOVSR UIMM8()                       | REG0=MASK_R():w:mskw REG1=MASK_B():r:mskw IMM0:r:b
 | KTESTB               | KMASK          | AVX512VEX      | AVX512DQ_KOP   | 99           | 0b11       | rrr        | VV1 0x99 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0  NOVSR                                 | REG0=MASK_R():r:mskw REG1=MASK_B():r:mskw
 | KTESTD               | KMASK          | AVX512VEX      | AVX512BW_KOP   | 99           | 0b11       | rrr        | VV1 0x99 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W1  NOVSR                                 | REG0=MASK_R():r:mskw REG1=MASK_B():r:mskw
 | KTESTQ               | KMASK          | AVX512VEX      | AVX512BW_KOP   | 99           | 0b11       | rrr        | VV1 0x99 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W1  NOVSR                                 | REG0=MASK_R():r:mskw REG1=MASK_B():r:mskw
 | KTESTW               | KMASK          | AVX512VEX      | AVX512DQ_KOP   | 99           | 0b11       | rrr        | VV1 0x99 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0  NOVSR                                 | REG0=MASK_R():r:mskw REG1=MASK_B():r:mskw
 | KUNPCKBW             | KMASK          | AVX512VEX      | AVX512F_KOP    | 4b           | 0b11       | rrr        | VV1 0x4B V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W0                                        | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
 | KUNPCKDQ             | KMASK          | AVX512VEX      | AVX512BW_KOP   | 4b           | 0b11       | rrr        | VV1 0x4B VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W1                                        | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
 | KUNPCKWD             | KMASK          | AVX512VEX      | AVX512BW_KOP   | 4b           | 0b11       | rrr        | VV1 0x4B VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W0                                        | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
 | KXNOR                | KNCMASK        | KNC            | KNCV           | 46           | 0b11       | rrr        | VV1 0x46  VL128 VNP V0F REXW=0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                | REG0=MASK_R():w:mskw    REG1=MASK_B():r:mskw
 | KXNORB               | KMASK          | AVX512VEX      | AVX512DQ_KOP   | 46           | 0b11       | rrr        | VV1 0x46 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W0                                        | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
 | KXNORD               | KMASK          | AVX512VEX      | AVX512BW_KOP   | 46           | 0b11       | rrr        | VV1 0x46 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W1                                        | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
 | KXNORQ               | KMASK          | AVX512VEX      | AVX512BW_KOP   | 46           | 0b11       | rrr        | VV1 0x46 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W1                                        | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
 | KXNORW               | KMASK          | AVX512VEX      | AVX512F_KOP    | 46           | 0b11       | rrr        | VV1 0x46 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W0                                        | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
 | KXOR                 | KNCMASK        | KNC            | KNCV           | 47           | 0b11       | rrr        | VV1 0x47  VL128 VNP V0F REXW=0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                | REG0=MASK_R():w:mskw    REG1=MASK_B():r:mskw
 | KXORB                | KMASK          | AVX512VEX      | AVX512DQ_KOP   | 47           | 0b11       | rrr        | VV1 0x47 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W0                                        | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
 | KXORD                | KMASK          | AVX512VEX      | AVX512BW_KOP   | 47           | 0b11       | rrr        | VV1 0x47 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W1                                        | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
 | KXORQ                | KMASK          | AVX512VEX      | AVX512BW_KOP   | 47           | 0b11       | rrr        | VV1 0x47 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W1                                        | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
 | KXORW                | KMASK          | AVX512VEX      | AVX512F_KOP    | 47           | 0b11       | rrr        | VV1 0x47 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W0                                        | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
 | LAHF                 | FLAGOP         | BASE           | LAHF           | 9f           |            |            | 0x9F                                                                                                 | REG0=XED_REG_AH:w:SUPP
 | LAR                  | SYSTEM         | BASE           | I286PROTECTED  | 0f 02        | mm         | rrr        | 0x0F 0x02 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | REG0=GPRv_R():cw MEM0:r:w
 | LAR                  | SYSTEM         | BASE           | I286PROTECTED  | 0f 02        | 0b11       | rrr        | 0x0F 0x02 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_R():cw REG1=GPRv_B():r
 | LDDQU                | SSE            | SSE3           |                | 0f f0        | mm         | rrr        | 0x0F 0xF0  MOD[mm] MOD!=3 REG[rrr] RM[nnn] IGNORE66() MODRM() f2_refining_prefix                     | REG0=XMM_R():w:pd MEM0:r:dq
 | LDMXCSR              | SSE            | SSE            | SSEMXCSR       | 0f ae        | mm         | 0b010      | 0x0F 0xAE MOD[mm] MOD!=3 REG[0b010] RM[nnn]  no_refining_prefix MODRM()                              | MEM0:r:d REG0=XED_REG_MXCSR:w:SUPP
 | LDS                  | SEGOP          | BASE           | I86            | c5           | mm         | rrr        | 0xC5 MOD[mm] MOD!=3 REG[rrr] RM[nnn] not64 MODRM()                                                   | REG0=GPRz_R():w MEM0:r:p REG1=XED_REG_DS:w:SUPP
 | LEA                  | MISC           | BASE           | I86            | 8d           | mm         | rrr        | 0x8D MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() REMOVE_SEGMENT()                                        | REG0=GPRv_R():w AGEN:r
 | LEAVE                | MISC           | BASE           | I186           | c9           |            |            | 0xC9 DF64()                                                                                          | MEM0:r:SUPP:v BASE0=ArBP():r:SUPP SEG0=FINAL_SSEG0():r:SUPP REG0=OrBP():rw:SUPP REG1=OrSP():rw:SUPP
 | LES                  | SEGOP          | BASE           | I86            | c4           | mm         | rrr        | 0xC4 MOD[mm] MOD!=3 REG[rrr] RM[nnn] not64 MODRM()                                                   | REG0=GPRz_R():w MEM0:r:p REG1=XED_REG_ES:w:SUPP
 | LFENCE               | MISC           | SSE2           | SSE2           | 0f ae        | 0b11       | 0b101      | 0x0F 0xAE  MOD[0b11] MOD=3 REG[0b101] RM[nnn] no_refining_prefix                                     | 
 | LFS                  | SEGOP          | BASE           | I386           | 0f b4        | mm         | rrr        | 0x0F 0xB4 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | REG0=GPRv_R():w MEM0:r:p2 REG1=XED_REG_FS:w:SUPP
 | LGDT                 | SYSTEM         | BASE           | I286REAL       | 0f 01        | mm         | 0b010      | 0x0F 0x01 MOD[mm] MOD!=3 REG[0b010]  RM[nnn] mode64 FORCE64() MODRM()                                | MEM0:r:s64 REG0=XED_REG_GDTR:w:SUPP
 | LGDT                 | SYSTEM         | BASE           | I286REAL       | 0f 01        | mm         | 0b010      | 0x0F 0x01 MOD[mm] MOD!=3 REG[0b010]  RM[nnn] not64 MODRM()                                           | MEM0:r:s REG0=XED_REG_GDTR:w:SUPP
 | LGS                  | SEGOP          | BASE           | I386           | 0f b5        | mm         | rrr        | 0x0F 0xB5 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | REG0=GPRv_R():w MEM0:r:p2 REG1=XED_REG_GS:w:SUPP
 | LIDT                 | SYSTEM         | BASE           | I286REAL       | 0f 01        | mm         | 0b011      | 0x0F 0x01 MOD[mm] MOD!=3 REG[0b011] RM[nnn] mode64 FORCE64() MODRM()                                 | MEM0:r:s64 REG0=XED_REG_IDTR:w:SUPP
 | LIDT                 | SYSTEM         | BASE           | I286REAL       | 0f 01        | mm         | 0b011      | 0x0F 0x01 MOD[mm] MOD!=3 REG[0b011] RM[nnn] not64 MODRM()                                            | MEM0:r:s REG0=XED_REG_IDTR:w:SUPP
 | LLDT                 | SYSTEM         | BASE           | I286PROTECTED  | 0f 00        | mm         | 0b010      | 0x0F 0x00 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM()                                                  | MEM0:r:w REG0=XED_REG_LDTR:w:SUPP
 | LLDT                 | SYSTEM         | BASE           | I286PROTECTED  | 0f 00        | 0b11       | 0b010      | 0x0F 0x00 MOD[0b11] MOD=3 REG[0b010] RM[nnn]                                                         | REG0=GPR16_B():r REG1=XED_REG_LDTR:w:SUPP
 | LLWPCB               | XOP            | XOP            | XOP            | 12           | 0b11       | 0b000      | XOPV 0x12 VNP VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[0b000] RM[nnn]                                   | REG0=GPRy_B():w:y
 | LMSW                 | SYSTEM         | BASE           | I286REAL       | 0f 01        | mm         | 0b110      | 0x0F 0x01 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM()                                                  | MEM0:r:w REG0=XED_REG_CR0:w:SUPP
 | LMSW                 | SYSTEM         | BASE           | I286REAL       | 0f 01        | 0b11       | 0b110      | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b110] RM[nnn]                                                         | REG0=GPR16_B():r REG1=XED_REG_CR0:w:SUPP
 | LODSB                | STRINGOP       | BASE           | I86            | ac           |            |            | 0xAC norep OVERRIDE_SEG0()                                                                           | REG0=XED_REG_AL:w:SUPP MEM0:r:SUPP:b BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP
 | LODSD                | STRINGOP       | BASE           | I386           | ad           |            |            | 0xAD mode16 66_prefix  norep OVERRIDE_SEG0()                                                         | REG0=XED_REG_EAX:w:SUPP MEM0:r:SUPP:d BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP
 | LODSD                | STRINGOP       | BASE           | I386           | ad           |            |            | 0xAD mode32 no66_prefix  norep OVERRIDE_SEG0()                                                       | REG0=XED_REG_EAX:w:SUPP MEM0:r:SUPP:d BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP
 | LODSD                | STRINGOP       | BASE           | I386           | ad           |            |            | 0xAD mode64 norexw_prefix no66_prefix  norep OVERRIDE_SEG0()                                         | REG0=XED_REG_EAX:w:SUPP MEM0:r:SUPP:d BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP
 | LODSQ                | STRINGOP       | LONGMODE       |                | ad           |            |            | 0xAD mode64 rexw_prefix  norep OVERRIDE_SEG0()                                                       | REG0=XED_REG_RAX:w:SUPP MEM0:r:SUPP:q BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP
 | LODSW                | STRINGOP       | BASE           | I86            | ad           |            |            | 0xAD mode16 no66_prefix  norep OVERRIDE_SEG0()                                                       | REG0=XED_REG_AX:w:SUPP MEM0:r:SUPP:w BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP
 | LODSW                | STRINGOP       | BASE           | I86            | ad           |            |            | 0xAD mode32 66_prefix  norep OVERRIDE_SEG0()                                                         | REG0=XED_REG_AX:w:SUPP MEM0:r:SUPP:w BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP
 | LODSW                | STRINGOP       | BASE           | I86            | ad           |            |            | 0xAD mode64 norexw_prefix 66_prefix  norep OVERRIDE_SEG0()                                           | REG0=XED_REG_AX:w:SUPP MEM0:r:SUPP:w BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP
 | LOOP                 | COND_BR        | BASE           | I86            | e2           |            |            | 0xE2 DF64() BRDISP8() IMMUNE66_LOOP64()                                                              | RELBR:r:b:i8 REG0=ArCX():rw:SUPP REG1=rIP():rw:SUPP
 | LOOPE                | COND_BR        | BASE           | I86            | e1           |            |            | 0xE1 MODEP5=1 REP=0 DF64() BRDISP8() IMMUNE66_LOOP64()                                               | RELBR:r:b:i8 REG0=ArCX():rw:SUPP REG1=rIP():rw:SUPP
 | LOOPE                | COND_BR        | BASE           | I86            | e1           |            |            | 0xE1 MODEP5=1 REP=3 DF64() BRDISP8() IMMUNE66_LOOP64()                                               | RELBR:r:b:i8 REG0=ArCX():rw:SUPP REG1=rIP():rw:SUPP
 | LOOPE                | COND_BR        | BASE           | I86            | e1           |            |            | 0xE1 MODEP5=0       DF64() BRDISP8() IMMUNE66_LOOP64()                                               | RELBR:r:b:i8 REG0=ArCX():rw:SUPP REG1=rIP():rw:SUPP
 | LOOPE                | COND_BR        | BASE           | I86            | e0           |            |            | 0xE0 MODEP5=1 REP=3  DF64() BRDISP8() IMMUNE66_LOOP64()                                              | RELBR:r:b:i8 REG0=ArCX():rw:SUPP REG1=rIP():rw:SUPP
 | LOOPNE               | COND_BR        | BASE           | I86            | e0           |            |            | 0xE0 MODEP5=1  REP=0  DF64() BRDISP8() IMMUNE66_LOOP64()                                             | RELBR:r:b:i8 REG0=ArCX():rw:SUPP REG1=rIP():rw:SUPP
 | LOOPNE               | COND_BR        | BASE           | I86            | e0           |            |            | 0xE0 MODEP5=1  REP=2  DF64() BRDISP8() IMMUNE66_LOOP64()                                             | RELBR:r:b:i8 REG0=ArCX():rw:SUPP REG1=rIP():rw:SUPP
 | LOOPNE               | COND_BR        | BASE           | I86            | e0           |            |            | 0xE0 MODEP5=0         DF64() BRDISP8() IMMUNE66_LOOP64()                                             | RELBR:r:b:i8 REG0=ArCX():rw:SUPP REG1=rIP():rw:SUPP
 | LOOPNE               | COND_BR        | BASE           | I86            | e1           |            |            | 0xE1 MODEP5=1 REP=2  DF64() BRDISP8() IMMUNE66_LOOP64()                                              | RELBR:r:b:i8 REG0=ArCX():rw:SUPP REG1=rIP():rw:SUPP
 | LSL                  | SYSTEM         | BASE           | I286PROTECTED  | 0f 03        | mm         | rrr        | 0x0F 0x03 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | REG0=GPRv_R():rw MEM0:r:w
 | LSL                  | SYSTEM         | BASE           | I286PROTECTED  | 0f 03        | 0b11       | rrr        | 0x0F 0x03 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_R():rw REG1=GPRz_B():r
 | LSS                  | SEGOP          | BASE           | I386           | 0f b2        | mm         | rrr        | 0x0F 0xB2 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | REG0=GPRv_R():w MEM0:r:p2 REG1=XED_REG_SS:w:SUPP
 | LTR                  | SYSTEM         | BASE           | I286PROTECTED  | 0f 00        | mm         | 0b011      | 0x0F 0x00 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM()                                                  | MEM0:r:w REG0=XED_REG_TR:w:SUPP
 | LTR                  | SYSTEM         | BASE           | I286PROTECTED  | 0f 00        | 0b11       | 0b011      | 0x0F 0x00 MOD[0b11] MOD=3 REG[0b011] RM[nnn]                                                         | REG0=GPR16_B():r REG1=XED_REG_TR:w:SUPP
 | LWPINS               | XOP            | XOP            | XOP            | 12           | mm         | 0b000      | XOPV 0x12 VNP  VL128  XMAPA MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() UIMM32()                       | REG0=VGPRy_N():w:y MEM0:r:d IMM0:r:d
 | LWPINS               | XOP            | XOP            | XOP            | 12           | 0b11       | 0b000      | XOPV 0x12 VNP  VL128  XMAPA MOD[0b11] MOD=3 REG[0b000] RM[nnn] UIMM32()                              | REG0=VGPRy_N():w:y REG1=GPR32_B():r:y IMM0:r:d
 | LWPVAL               | XOP            | XOP            | XOP            | 12           | mm         | 0b001      | XOPV 0x12 VNP VL128  XMAPA MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM() UIMM32()                        | REG0=VGPRy_N():w:y MEM0:r:d IMM0:r:d
 | LWPVAL               | XOP            | XOP            | XOP            | 12           | 0b11       | 0b001      | XOPV 0x12 VNP VL128  XMAPA MOD[0b11] MOD=3 REG[0b001] RM[nnn] UIMM32()                               | REG0=VGPRy_N():w:y REG1=GPR32_B():r:y IMM0:r:d
 | LZCNT                | LZCNT          | LZCNT          |                | 0f bd        | mm         | rrr        | 0x0F 0xBD f3_refining_prefix LZCNT=1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                         | REG0=GPRv_R():w:v     MEM0:r:v
 | LZCNT                | LZCNT          | LZCNT          |                | 0f bd        | 0b11       | rrr        | 0x0F 0xBD f3_refining_prefix LZCNT=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                | REG0=GPRv_R():w:v     REG1=GPRv_B():r:v
 | LZCNT_VEX            | KNCSCALAR      | KNC            | KNCV           | bd           | 0b11       | rrr        | VV1 0xBD  VL128 VF3 V0F W0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                    | REG0=GPR32_R():w:d    REG1=GPR32_B():r:d
 | LZCNT_VEX            | KNCSCALAR      | KNC            | KNCV           | bd           | 0b11       | rrr        | VV1 0xBD  VL128 VF3 V0F W1 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                    | REG0=GPR64_R():w:q    REG1=GPR64_B():r:q
 | MASKMOVDQU           | DATAXFER       | SSE2           |                | 0f f7        | 0b11       | rrr        | 0x0F 0xF7 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66() OVERRIDE_SEG0()         | REG0=XMM_R():r:dq REG1=XMM_B():r:dq MEM0:w:dq:SUPP BASE0=ArDI():r:SUPP SEG0=FINAL_DSEG():r:SUPP
 | MASKMOVQ             | DATAXFER       | MMX            | PENTIUMMMX     | 0f f7        | 0b11       | rrr        | 0x0F 0xF7 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  OVERRIDE_SEG0()                       | REG0=MMX_R():r:q:u8 REG1=MMX_B():r:q:i8 MEM0:w:q:SUPP BASE0=ArDI():r:SUPP SEG0=FINAL_DSEG():r:SUPP
 | MAXPD                | SSE            | SSE2           |                | 0f 5f        | mm         | rrr        | 0x0F 0x5F osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:pd MEM0:r:pd
 | MAXPD                | SSE            | SSE2           |                | 0f 5f        | 0b11       | rrr        | 0x0F 0x5F osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:pd REG1=XMM_B():r:pd
 | MAXPS                | SSE            | SSE            |                | 0f 5f        | mm         | rrr        | 0x0F 0x5F no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=XMM_R():rw:ps MEM0:r:ps
 | MAXPS                | SSE            | SSE            |                | 0f 5f        | 0b11       | rrr        | 0x0F 0x5F no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=XMM_R():rw:ps REG1=XMM_B():r:ps
 | MAXSD                | SSE            | SSE2           |                | 0f 5f        | mm         | rrr        | 0x0F 0x5F f2_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM()                     | REG0=XMM_R():rw:sd MEM0:r:sd
 | MAXSD                | SSE            | SSE2           |                | 0f 5f        | 0b11       | rrr        | 0x0F 0x5F f2_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66()                            | REG0=XMM_R():rw:sd REG1=XMM_B():r:sd
 | MAXSS                | SSE            | SSE            |                | 0f 5f        | mm         | rrr        | 0x0F 0x5F f3_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM()                     | REG0=XMM_R():rw:ss MEM0:r:ss
 | MAXSS                | SSE            | SSE            |                | 0f 5f        | 0b11       | rrr        | 0x0F 0x5F f3_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66()                            | REG0=XMM_R():rw:ss REG1=XMM_B():r:ss
 | MFENCE               | MISC           | SSE2           | SSE2           | 0f ae        | 0b11       | 0b110      | 0x0F 0xAE  MOD[0b11] MOD=3 REG[0b110] RM[nnn] no_refining_prefix                                     | 
 | MINPD                | SSE            | SSE2           |                | 0f 5d        | mm         | rrr        | 0x0F 0x5D osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:pd MEM0:r:pd
 | MINPD                | SSE            | SSE2           |                | 0f 5d        | 0b11       | rrr        | 0x0F 0x5D osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:pd REG1=XMM_B():r:pd
 | MINPS                | SSE            | SSE            |                | 0f 5d        | mm         | rrr        | 0x0F 0x5D no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=XMM_R():rw:ps MEM0:r:ps
 | MINPS                | SSE            | SSE            |                | 0f 5d        | 0b11       | rrr        | 0x0F 0x5D no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=XMM_R():rw:ps REG1=XMM_B():r:ps
 | MINSD                | SSE            | SSE2           |                | 0f 5d        | mm         | rrr        | 0x0F 0x5D f2_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM()                     | REG0=XMM_R():rw:sd MEM0:r:sd
 | MINSD                | SSE            | SSE2           |                | 0f 5d        | 0b11       | rrr        | 0x0F 0x5D f2_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66()                            | REG0=XMM_R():rw:sd REG1=XMM_B():r:sd
 | MINSS                | SSE            | SSE            |                | 0f 5d        | mm         | rrr        | 0x0F 0x5D f3_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM()                     | REG0=XMM_R():rw:ss MEM0:r:ss
 | MINSS                | SSE            | SSE            |                | 0f 5d        | 0b11       | rrr        | 0x0F 0x5D f3_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66()                            | REG0=XMM_R():rw:ss REG1=XMM_B():r:ss
 | MONITOR              | MISC           | MONITOR        | MONITOR        | 0f 01        | 0b11       | 0b001      | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b001] RM[0b000] no_refining_prefix not64 eamode32                     | REG0=XED_REG_EAX:r:SUPP REG1=XED_REG_ECX:r:SUPP REG2=XED_REG_EDX:r:SUPP
 | MONITOR              | MISC           | MONITOR        | MONITOR        | 0f 01        | 0b11       | 0b001      | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b001] RM[0b000] no_refining_prefix not64 eamode16                     | REG0=XED_REG_AX:r:SUPP REG1=XED_REG_ECX:r:SUPP REG2=XED_REG_EDX:r:SUPP
 | MONITOR              | MISC           | MONITOR        | MONITOR        | 0f 01        | 0b11       | 0b001      | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b001] RM[0b000] no_refining_prefix mode64 eamode64                    | REG0=XED_REG_RAX:r:SUPP REG1=XED_REG_ECX:r:SUPP REG2=XED_REG_EDX:r:SUPP
 | MONITOR              | MISC           | MONITOR        | MONITOR        | 0f 01        | 0b11       | 0b001      | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b001] RM[0b000] no_refining_prefix mode64 eamode32                    | REG0=XED_REG_RAX:r:SUPP REG1=XED_REG_ECX:r:SUPP REG2=XED_REG_EDX:r:SUPP
 | MOV                  | DATAXFER       | BASE           | I86            | c6           | 0b11       | 0b000      | 0xC6 MOD[0b11] MOD=3 REG[0b000] RM[nnn] UIMM8()                                                      | REG0=GPR8_B():w IMM0:r:b
 | MOV                  | DATAXFER       | BASE           | I86            | c6           | mm         | 0b000      | 0xC6 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() UIMM8()                                               | MEM0:w:b IMM0:r:b
 | MOV                  | DATAXFER       | BASE           | I86            | c7           | 0b11       | 0b000      | 0xC7 MOD[0b11] MOD=3 REG[0b000] RM[nnn] SIMMz()                                                      | REG0=GPRv_B():w IMM0:r:z
 | MOV                  | DATAXFER       | BASE           | I86            | c7           | mm         | 0b000      | 0xC7 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() SIMMz()                                               | MEM0:w:v IMM0:r:z
 | MOV                  | DATAXFER       | BASE           | I86            | 88           | 0b11       | rrr        | 0x88 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPR8_B():w REG1=GPR8_R():r
 | MOV                  | DATAXFER       | BASE           | I86            | 88           | mm         | rrr        | 0x88 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                         | MEM0:w:b REG0=GPR8_R():r
 | MOV                  | DATAXFER       | BASE           | I86            | 89           | mm         | rrr        | 0x89 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                         | MEM0:w:v REG0=GPRv_R():r
 | MOV                  | DATAXFER       | BASE           | I86            | 89           | 0b11       | rrr        | 0x89 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPRv_B():w REG1=GPRv_R():r
 | MOV                  | DATAXFER       | BASE           | I86            | 8a           | mm         | rrr        | 0x8A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                         | REG0=GPR8_R():w MEM0:r:b
 | MOV                  | DATAXFER       | BASE           | I86            | 8a           | 0b11       | rrr        | 0x8A MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPR8_R():w REG1=GPR8_B():r
 | MOV                  | DATAXFER       | BASE           | I86            | 8b           | mm         | rrr        | 0x8B MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                         | REG0=GPRv_R():w MEM0:r:v
 | MOV                  | DATAXFER       | BASE           | I86            | 8b           | 0b11       | rrr        | 0x8B MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPRv_R():w REG1=GPRv_B():r
 | MOV                  | DATAXFER       | BASE           | I86            | 8c           | mm         | rrr        | 0x8C MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                         | MEM0:w:w REG0=SEG():r
 | MOV                  | DATAXFER       | BASE           | I86            | 8c           | 0b11       | rrr        | 0x8C MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPRv_B():w REG1=SEG():r
 | MOV                  | DATAXFER       | BASE           | I86            | 8e           | mm         | rrr        | 0x8E MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                         | REG0=SEG_MOV():w MEM0:r:w
 | MOV                  | DATAXFER       | BASE           | I86            | 8e           | 0b11       | rrr        | 0x8E MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=SEG_MOV():w REG1=GPR16_B():r
 | MOV                  | DATAXFER       | BASE           | I86            | a0           |            |            | 0xA0 MEMDISPv()   OVERRIDE_SEG0()                                                                    | REG0=XED_REG_AL:w:IMPL MEM0:r:b SEG0=FINAL_DSEG():r:SUPP BASE0=XED_REG_INVALID:r:ECOND INDEX=XED_REG_INVALID:r:ECOND
 | MOV                  | DATAXFER       | BASE           | I86            | a1           |            |            | 0xA1 MEMDISPv() OVERRIDE_SEG0()                                                                      | REG0=OrAX():w:IMPL MEM0:r:v SEG0=FINAL_DSEG():r:SUPP BASE0=XED_REG_INVALID:r:ECOND INDEX=XED_REG_INVALID:r:ECOND
 | MOV                  | DATAXFER       | BASE           | I86            | a2           |            |            | 0xA2 MEMDISPv()  OVERRIDE_SEG0()                                                                     | MEM0:w:b REG0=XED_REG_AL:r:IMPL SEG0=FINAL_DSEG():r:SUPP BASE0=XED_REG_INVALID:r:ECOND INDEX=XED_REG_INVALID:r:ECOND
 | MOV                  | DATAXFER       | BASE           | I86            | a3           |            |            | 0xA3 MEMDISPv() OVERRIDE_SEG0()                                                                      | MEM0:w:v REG0=OrAX():r:IMPL  SEG0=FINAL_DSEG():r:SUPP BASE0=XED_REG_INVALID:r:ECOND INDEX=XED_REG_INVALID:r:ECOND
 | MOV                  | DATAXFER       | BASE           | I86            |              |            |            | 0b1011_0 SRM[rrr] UIMM8()                                                                            | REG0=GPR8_SB():w IMM0:r:b
 | MOV                  | DATAXFER       | BASE           | I86            |              |            |            | 0b1011_1 SRM[rrr] UIMMv()                                                                            | REG0=GPRv_SB():w IMM0:r:v
 | MOV_CR               | DATAXFER       | BASE           | I86            | 0f 22        | mm         | rrr        | 0x0F 0x22 MOD[mm] REG[rrr] RM[nnn] CR_WIDTH() not64                                                  | REG0=CR_R():w REG1=GPR32_B():r
 | MOV_CR               | DATAXFER       | BASE           | I86            | 0f 22        | mm         | rrr        | 0x0F 0x22 MOD[mm] REG[rrr] RM[nnn] CR_WIDTH() mode64                                                 | REG0=CR_R():w REG1=GPR64_B():r
 | MOV_CR               | DATAXFER       | BASE           | I86            | 0f 20        | mm         | rrr        | 0x0F 0x20 MOD[mm] REG[rrr] RM[nnn] CR_WIDTH() not64                                                  | REG0=GPR32_B():w REG1=CR_R():r
 | MOV_CR               | DATAXFER       | BASE           | I86            | 0f 20        | mm         | rrr        | 0x0F 0x20 MOD[mm] REG[rrr] RM[nnn] CR_WIDTH() mode64                                                 | REG0=GPR64_B():w REG1=CR_R():r
 | MOV_DR               | DATAXFER       | BASE           | I86            | 0f 23        | mm         | rrr        | 0x0F 0x23 MOD[mm] REG[rrr] RM[nnn] CR_WIDTH() not64                                                  | REG0=DR_R():w REG1=GPR32_B():r
 | MOV_DR               | DATAXFER       | BASE           | I86            | 0f 23        | mm         | rrr        | 0x0F 0x23 MOD[mm] REG[rrr] RM[nnn] CR_WIDTH() mode64                                                 | REG0=DR_R():w REG1=GPR64_B():r
 | MOV_DR               | DATAXFER       | BASE           | I86            | 0f 21        | mm         | rrr        | 0x0F 0x21 MOD[mm] REG[rrr] RM[nnn] CR_WIDTH() not64                                                  | REG0=GPR32_B():w REG1=DR_R():r
 | MOV_DR               | DATAXFER       | BASE           | I86            | 0f 21        | mm         | rrr        | 0x0F 0x21 MOD[mm] REG[rrr] RM[nnn] CR_WIDTH() mode64                                                 | REG0=GPR64_B():w REG1=DR_R():r
 | MOVAPD               | DATAXFER       | SSE2           |                | 0f 28        | mm         | rrr        | 0x0F 0x28 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():w:pd MEM0:r:pd
 | MOVAPD               | DATAXFER       | SSE2           |                | 0f 28        | 0b11       | rrr        | 0x0F 0x28 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():w:pd REG1=XMM_B():r:pd
 | MOVAPD               | DATAXFER       | SSE2           |                | 0f 29        | mm         | rrr        | 0x0F 0x29 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | MEM0:w:pd REG0=XMM_R():r:pd
 | MOVAPD               | DATAXFER       | SSE2           |                | 0f 29        | 0b11       | rrr        | 0x0F 0x29 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_B():w:pd REG1=XMM_R():r:pd
 | MOVAPS               | DATAXFER       | SSE            |                | 0f 28        | mm         | rrr        | 0x0F 0x28 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=XMM_R():w:ps MEM0:r:ps
 | MOVAPS               | DATAXFER       | SSE            |                | 0f 28        | 0b11       | rrr        | 0x0F 0x28 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=XMM_R():w:ps REG1=XMM_B():r:ps
 | MOVAPS               | DATAXFER       | SSE            |                | 0f 29        | mm         | rrr        | 0x0F 0x29 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | MEM0:w:ps REG0=XMM_R():r:ps
 | MOVAPS               | DATAXFER       | SSE            |                | 0f 29        | 0b11       | rrr        | 0x0F 0x29 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=XMM_B():w:ps REG1=XMM_R():r:ps
 | MOVBE                | DATAXFER       | MOVBE          |                | 0f 38 f0     | mm         | rrr        | 0x0F 0x38 0xF0   not_refining MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                               | REG0=GPRv_R():w MEM0:r:v
 | MOVBE                | DATAXFER       | MOVBE          |                | 0f 38 f1     | mm         | rrr        | 0x0F 0x38 0xF1   not_refining MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                               | MEM0:w:v  REG0=GPRv_R():r
 | MOVD                 | DATAXFER       | MMX            | PENTIUMMMX     | 0f 6e        | mm         | rrr        | 0x0F 0x6E no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  mode64 norexw_prefix MODRM()           | REG0=MMX_R():w:q MEM0:r:d
 | MOVD                 | DATAXFER       | MMX            | PENTIUMMMX     | 0f 6e        | 0b11       | rrr        | 0x0F 0x6E no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  mode64 norexw_prefix                  | REG0=MMX_R():w:q REG1=GPR32_B():r
 | MOVD                 | DATAXFER       | MMX            | PENTIUMMMX     | 0f 6e        | mm         | rrr        | 0x0F 0x6E no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  not64  MODRM()                         | REG0=MMX_R():w:q MEM0:r:d
 | MOVD                 | DATAXFER       | MMX            | PENTIUMMMX     | 0f 6e        | 0b11       | rrr        | 0x0F 0x6E no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  not64                                 | REG0=MMX_R():w:q REG1=GPR32_B():r
 | MOVD                 | DATAXFER       | MMX            | PENTIUMMMX     | 0f 7e        | mm         | rrr        | 0x0F 0x7E no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  mode64 norexw_prefix MODRM()           | MEM0:w:d REG0=MMX_R():r:d
 | MOVD                 | DATAXFER       | MMX            | PENTIUMMMX     | 0f 7e        | 0b11       | rrr        | 0x0F 0x7E no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  mode64 norexw_prefix                  | REG0=GPR32_B():w REG1=MMX_R():r:d
 | MOVD                 | DATAXFER       | MMX            | PENTIUMMMX     | 0f 7e        | mm         | rrr        | 0x0F 0x7E no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  not64 MODRM()                          | MEM0:w:d REG0=MMX_R():r:d
 | MOVD                 | DATAXFER       | MMX            | PENTIUMMMX     | 0f 7e        | 0b11       | rrr        | 0x0F 0x7E no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  not64                                 | REG0=GPR32_B():w REG1=MMX_R():r:d
 | MOVD                 | DATAXFER       | SSE2           |                | 0f 6e        | mm         | rrr        | 0x0F 0x6E osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() mode64 norexw_prefix MODRM() | REG0=XMM_R():w:dq MEM0:r:d
 | MOVD                 | DATAXFER       | SSE2           |                | 0f 6e        | 0b11       | rrr        | 0x0F 0x6E osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66() mode64 norexw_prefix    | REG0=XMM_R():w:dq REG1=GPR32_B():r
 | MOVD                 | DATAXFER       | SSE2           |                | 0f 6e        | mm         | rrr        | 0x0F 0x6E osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() not64 MODRM()            | REG0=XMM_R():w:dq MEM0:r:d
 | MOVD                 | DATAXFER       | SSE2           |                | 0f 6e        | 0b11       | rrr        | 0x0F 0x6E osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66() not64                   | REG0=XMM_R():w:dq REG1=GPR32_B():r
 | MOVD                 | DATAXFER       | SSE2           |                | 0f 7e        | mm         | rrr        | 0x0F 0x7E osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() mode64 norexw_prefix  MODRM() | MEM0:w:d REG0=XMM_R():r:d
 | MOVD                 | DATAXFER       | SSE2           |                | 0f 7e        | 0b11       | rrr        | 0x0F 0x7E osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66() mode64 norexw_prefix    | REG0=GPR32_B():w REG1=XMM_R():r:d
 | MOVD                 | DATAXFER       | SSE2           |                | 0f 7e        | mm         | rrr        | 0x0F 0x7E osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() not64 MODRM()            | MEM0:w:d REG0=XMM_R():r:d
 | MOVD                 | DATAXFER       | SSE2           |                | 0f 7e        | 0b11       | rrr        | 0x0F 0x7E osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66() not64                   | REG0=GPR32_B():w REG1=XMM_R():r:d
 | MOVDDUP              | DATAXFER       | SSE3           |                | 0f 12        | mm         | rrr        | 0x0F 0x12 f2_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM()                     | REG0=XMM_R():w:dq MEM0:r:q
 | MOVDDUP              | DATAXFER       | SSE3           |                | 0f 12        | 0b11       | rrr        | 0x0F 0x12 f2_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66()                            | REG0=XMM_R():w:dq REG1=XMM_B():r:q
 | MOVDIR64B            | MOVDIR         | MOVDIR         | MOVDIR         | 0f 38 f8     | mm         | rrr        | 0x0F 0x38 0xF8 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  osz_refining_prefix   not64                 | REG0=A_GPR_R():r MEM0:r:zd:u32 MEM1:w:zd:SUPP BASE1=A_GPR_R():r:SUPP  SEG1=XED_REG_ES:r:SUPP
 | MOVDIR64B            | MOVDIR         | MOVDIR         | MOVDIR         | 0f 38 f8     | mm         | rrr        | 0x0F 0x38 0xF8 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  osz_refining_prefix   mode64                | REG0=A_GPR_R():r MEM0:r:zd:u32 MEM1:w:zd:SUPP BASE1=A_GPR_R():r:SUPP
 | MOVDIRI              | MOVDIR         | MOVDIR         | MOVDIR         | 0f 38 f9     | mm         | rrr        | 0x0F 0x38 0xF9 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  no_refining_prefix      norexw_prefix       | MEM0:w:d:u32 REG0=GPR32_R():r:d:u32
 | MOVDIRI              | MOVDIR         | MOVDIR         | MOVDIR         | 0f 38 f9     | mm         | rrr        | 0x0F 0x38 0xF9 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  no_refining_prefix      mode64 rexw_prefix  | MEM0:w:q:u64 REG0=GPR64_R():r:q:u64
 | MOVDQ2Q              | DATAXFER       | SSE2           |                | 0f d6        | 0b11       | rrr        | 0x0F 0xD6 f2_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66()                            | REG0=MMX_R():w:q:u64 REG1=XMM_B():r:q:u64
 | MOVDQA               | DATAXFER       | SSE2           |                | 0f 7f        | mm         | rrr        | 0x0F 0x7F osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | MEM0:w:dq REG0=XMM_R():r:dq
 | MOVDQA               | DATAXFER       | SSE2           |                | 0f 7f        | 0b11       | rrr        | 0x0F 0x7F osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_B():w:dq REG1=XMM_R():r:dq
 | MOVDQA               | DATAXFER       | SSE2           |                | 0f 6f        | mm         | rrr        | 0x0F 0x6F osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():w:dq MEM0:r:dq
 | MOVDQA               | DATAXFER       | SSE2           |                | 0f 6f        | 0b11       | rrr        | 0x0F 0x6F osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():w:dq REG1=XMM_B():r:dq
 | MOVDQU               | DATAXFER       | SSE2           |                | 0f 6f        | mm         | rrr        | 0x0F 0x6F f3_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM()                     | REG0=XMM_R():w:dq MEM0:r:dq
 | MOVDQU               | DATAXFER       | SSE2           |                | 0f 6f        | 0b11       | rrr        | 0x0F 0x6F f3_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66()                            | REG0=XMM_R():w:dq REG1=XMM_B():r:dq
 | MOVDQU               | DATAXFER       | SSE2           |                | 0f 7f        | mm         | rrr        | 0x0F 0x7F f3_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM()                     | MEM0:w:dq REG0=XMM_R():r:dq
 | MOVDQU               | DATAXFER       | SSE2           |                | 0f 7f        | 0b11       | rrr        | 0x0F 0x7F f3_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66()                            | REG0=XMM_B():w:dq REG1=XMM_R():r:dq
 | MOVHLPS              | DATAXFER       | SSE            |                | 0f 12        | 0b11       | rrr        | 0x0F 0x12 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=XMM_R():w:q:f32 REG1=XMM_B():r:q:f32
 | MOVHPD               | DATAXFER       | SSE2           |                | 0f 16        | mm         | rrr        | 0x0F 0x16 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():w:sd MEM0:r:q
 | MOVHPD               | DATAXFER       | SSE2           |                | 0f 17        | mm         | rrr        | 0x0F 0x17 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | MEM0:w:q REG0=XMM_R():r:sd
 | MOVHPS               | DATAXFER       | SSE            |                | 0f 16        | mm         | rrr        | 0x0F 0x16 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=XMM_R():w:q:f32 MEM0:r:q:f32
 | MOVHPS               | DATAXFER       | SSE            |                | 0f 17        | mm         | rrr        | 0x0F 0x17 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                 | MEM0:w:q:f32 REG0=XMM_R():r:ps:f32
 | MOVLHPS              | DATAXFER       | SSE            |                | 0f 16        | 0b11       | rrr        | 0x0F 0x16 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=XMM_R():w:q:f32 REG1=XMM_B():r:q:f32
 | MOVLPD               | DATAXFER       | SSE2           |                | 0f 12        | mm         | rrr        | 0x0F 0x12 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():w:sd MEM0:r:q
 | MOVLPD               | DATAXFER       | SSE2           |                | 0f 13        | mm         | rrr        | 0x0F 0x13 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | MEM0:w:q REG0=XMM_R():r:sd
 | MOVLPS               | DATAXFER       | SSE            |                | 0f 12        | mm         | rrr        | 0x0F 0x12 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=XMM_R():w:q:f32 MEM0:r:q:f32
 | MOVLPS               | DATAXFER       | SSE            |                | 0f 13        | mm         | rrr        | 0x0F 0x13 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                 | MEM0:w:q:f32 REG0=XMM_R():r:ps:f32
 | MOVMSKPD             | DATAXFER       | SSE2           |                | 0f 50        | 0b11       | rrr        | 0x0F 0x50 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=GPR32_R():w REG1=XMM_B():r:pd
 | MOVMSKPS             | DATAXFER       | SSE            |                | 0f 50        | 0b11       | rrr        | 0x0F 0x50 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=GPR32_R():w REG1=XMM_B():r:ps
 | MOVNTDQ              | DATAXFER       | SSE2           |                | 0f e7        | mm         | rrr        | 0x0F 0xE7 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | MEM0:w:dq REG0=XMM_R():r:dq
 | MOVNTDQA             | SSE            | SSE4           |                | 0f 38 2a     | mm         | rrr        | 0x0F 0x38 0x2A osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()              | REG0=XMM_R():w:dq MEM0:r:dq
 | MOVNTI               | DATAXFER       | SSE2           |                | 0f c3        | mm         | rrr        | 0x0F 0xC3 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  not64 MODRM()                          | MEM0:w:d REG0=GPR32_R():r
 | MOVNTI               | DATAXFER       | SSE2           |                | 0f c3        | mm         | rrr        | 0x0F 0xC3 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  mode64 norexw_prefix MODRM()           | MEM0:w:d REG0=GPR32_R():r
 | MOVNTI               | DATAXFER       | SSE2           |                | 0f c3        | mm         | rrr        | 0x0F 0xC3 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  mode64 rexw_prefix MODRM()             | MEM0:w:q REG0=GPR64_R():r
 | MOVNTPD              | DATAXFER       | SSE2           |                | 0f 2b        | mm         | rrr        | 0x0F 0x2B osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                   | MEM0:w:dq REG0=XMM_R():r:pd
 | MOVNTPS              | DATAXFER       | SSE            |                | 0f 2b        | mm         | rrr        | 0x0F 0x2B no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | MEM0:w:dq REG0=XMM_R():r:ps
 | MOVNTQ               | DATAXFER       | MMX            | PENTIUMMMX     | 0f e7        | mm         | rrr        | 0x0F 0xE7 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | MEM0:w:q REG0=MMX_R():r:q
 | MOVQ                 | DATAXFER       | MMX            | PENTIUMMMX     | 0f 6e        | mm         | rrr        | 0x0F 0x6E no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  mode64 rexw_prefix MODRM()             | REG0=MMX_R():w:q MEM0:r:q
 | MOVQ                 | DATAXFER       | MMX            | PENTIUMMMX     | 0f 6e        | 0b11       | rrr        | 0x0F 0x6E no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  mode64 rexw_prefix                    | REG0=MMX_R():w:q REG1=GPR64_B():r
 | MOVQ                 | DATAXFER       | MMX            | PENTIUMMMX     | 0f 7e        | mm         | rrr        | 0x0F 0x7E no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  mode64 rexw_prefix MODRM()             | MEM0:w:q REG0=MMX_R():r:q
 | MOVQ                 | DATAXFER       | MMX            | PENTIUMMMX     | 0f 7e        | 0b11       | rrr        | 0x0F 0x7E no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  mode64 rexw_prefix                    | REG0=GPR64_B():w REG1=MMX_R():r:q
 | MOVQ                 | DATAXFER       | MMX            | PENTIUMMMX     | 0f 6f        | mm         | rrr        | 0x0F 0x6F no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():w:q MEM0:r:q
 | MOVQ                 | DATAXFER       | MMX            | PENTIUMMMX     | 0f 6f        | 0b11       | rrr        | 0x0F 0x6F no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():w:q REG1=MMX_B():r:q
 | MOVQ                 | DATAXFER       | MMX            | PENTIUMMMX     | 0f 7f        | mm         | rrr        | 0x0F 0x7F no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | MEM0:w:q REG0=MMX_R():r:q
 | MOVQ                 | DATAXFER       | MMX            | PENTIUMMMX     | 0f 7f        | 0b11       | rrr        | 0x0F 0x7F no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_B():w:q REG1=MMX_R():r:q
 | MOVQ                 | DATAXFER       | SSE2           |                | 0f 6e        | mm         | rrr        | 0x0F 0x6E osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() mode64 rexw_prefix MODRM() | REG0=XMM_R():w:dq MEM0:r:q
 | MOVQ                 | DATAXFER       | SSE2           |                | 0f 6e        | 0b11       | rrr        | 0x0F 0x6E osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66() mode64 rexw_prefix      | REG0=XMM_R():w:dq REG1=GPR64_B():r
 | MOVQ                 | DATAXFER       | SSE2           |                | 0f 7e        | mm         | rrr        | 0x0F 0x7E osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() mode64 rexw_prefix MODRM() | MEM0:w:q REG0=XMM_R():r:q
 | MOVQ                 | DATAXFER       | SSE2           |                | 0f 7e        | 0b11       | rrr        | 0x0F 0x7E osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66() mode64 rexw_prefix      | REG0=GPR64_B():w REG1=XMM_R():r:q
 | MOVQ                 | DATAXFER       | SSE2           |                | 0f d6        | mm         | rrr        | 0x0F 0xD6 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | MEM0:w:q REG0=XMM_R():r:q
 | MOVQ                 | DATAXFER       | SSE2           |                | 0f d6        | 0b11       | rrr        | 0x0F 0xD6 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_B():w:dq REG1=XMM_R():r:q
 | MOVQ                 | DATAXFER       | SSE2           |                | 0f 7e        | mm         | rrr        | 0x0F 0x7E f3_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM()                     | REG0=XMM_R():w:dq MEM0:r:q
 | MOVQ                 | DATAXFER       | SSE2           |                | 0f 7e        | 0b11       | rrr        | 0x0F 0x7E f3_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66()                            | REG0=XMM_R():w:dq REG1=XMM_B():r:q
 | MOVQ2DQ              | DATAXFER       | SSE2           |                | 0f d6        | 0b11       | rrr        | 0x0F 0xD6 f3_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66()                            | REG0=XMM_R():w:dq:u64 REG1=MMX_B():r:q:u64
 | MOVSB                | STRINGOP       | BASE           | I86            | a4           |            |            | 0xA4 norep OVERRIDE_SEG1()                                                                           | MEM0:w:SUPP:b BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP  MEM1:r:SUPP:b BASE1=ArSI():rw:SUPP SEG1=FINAL_DSEG1():r:SUPP
 | MOVSD                | STRINGOP       | BASE           | I386           | a5           |            |            | 0xA5 mode16 66_prefix  norep OVERRIDE_SEG1()                                                         | MEM0:w:SUPP:d BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:r:SUPP:d BASE1=ArSI():rw:SUPP SEG1=FINAL_DSEG1():r:SUPP
 | MOVSD                | STRINGOP       | BASE           | I386           | a5           |            |            | 0xA5 mode32 no66_prefix  norep OVERRIDE_SEG1()                                                       | MEM0:w:SUPP:d BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:r:SUPP:d BASE1=ArSI():rw:SUPP SEG1=FINAL_DSEG1():r:SUPP
 | MOVSD                | STRINGOP       | BASE           | I386           | a5           |            |            | 0xA5 mode64 norexw_prefix no66_prefix  norep OVERRIDE_SEG1()                                         | MEM0:w:SUPP:d BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:r:SUPP:d BASE1=ArSI():rw:SUPP SEG1=FINAL_DSEG1():r:SUPP
 | MOVSD_XMM            | DATAXFER       | SSE2           |                | 0f 10        | mm         | rrr        | 0x0F 0x10 f2_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM()                     | REG0=XMM_R():w:dq:f64 MEM0:r:sd
 | MOVSD_XMM            | DATAXFER       | SSE2           |                | 0f 10        | 0b11       | rrr        | 0x0F 0x10 f2_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66()                            | REG0=XMM_R():w:sd REG1=XMM_B():r:sd
 | MOVSD_XMM            | DATAXFER       | SSE2           |                | 0f 11        | mm         | rrr        | 0x0F 0x11 f2_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM()                     | MEM0:w:sd REG0=XMM_R():r:sd
 | MOVSD_XMM            | DATAXFER       | SSE2           |                | 0f 11        | 0b11       | rrr        | 0x0F 0x11 f2_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66()                            | REG0=XMM_B():w:sd REG1=XMM_R():r:sd
 | MOVSHDUP             | DATAXFER       | SSE3           |                | 0f 16        | mm         | rrr        | 0x0F 0x16 f3_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM()                     | REG0=XMM_R():w:ps MEM0:r:ps
 | MOVSHDUP             | DATAXFER       | SSE3           |                | 0f 16        | 0b11       | rrr        | 0x0F 0x16 f3_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66()                            | REG0=XMM_R():w:ps REG1=XMM_B():r:ps
 | MOVSLDUP             | DATAXFER       | SSE3           |                | 0f 12        | mm         | rrr        | 0x0F 0x12 f3_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM()                     | REG0=XMM_R():w:ps MEM0:r:ps
 | MOVSLDUP             | DATAXFER       | SSE3           |                | 0f 12        | 0b11       | rrr        | 0x0F 0x12 f3_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66()                            | REG0=XMM_R():w:ps REG1=XMM_B():r:ps
 | MOVSQ                | STRINGOP       | LONGMODE       |                | a5           |            |            | 0xA5 mode64 rexw_prefix norep OVERRIDE_SEG1()                                                        | MEM0:w:SUPP:q BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:r:SUPP:q BASE1=ArSI():rw:SUPP SEG1=FINAL_DSEG1():r:SUPP
 | MOVSS                | DATAXFER       | SSE            |                | 0f 10        | mm         | rrr        | 0x0F 0x10 f3_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM()                     | REG0=XMM_R():w:dq:f32 MEM0:r:ss
 | MOVSS                | DATAXFER       | SSE            |                | 0f 10        | 0b11       | rrr        | 0x0F 0x10 f3_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66()                            | REG0=XMM_R():w:ss REG1=XMM_B():r:ss
 | MOVSS                | DATAXFER       | SSE            |                | 0f 11        | mm         | rrr        | 0x0F 0x11 f3_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM()                     | MEM0:w:ss REG0=XMM_R():r:ss
 | MOVSS                | DATAXFER       | SSE            |                | 0f 11        | 0b11       | rrr        | 0x0F 0x11 f3_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66()                            | REG0=XMM_B():w:ss REG1=XMM_R():r:ss
 | MOVSW                | STRINGOP       | BASE           | I86            | a5           |            |            | 0xA5 mode16 no66_prefix  norep  OVERRIDE_SEG1()                                                      | MEM0:w:SUPP:w BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:r:SUPP:w BASE1=ArSI():rw:SUPP SEG1=FINAL_DSEG1():r:SUPP
 | MOVSW                | STRINGOP       | BASE           | I86            | a5           |            |            | 0xA5 mode32 66_prefix  norep  OVERRIDE_SEG1()                                                        | MEM0:w:SUPP:w BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:r:SUPP:w BASE1=ArSI():rw:SUPP SEG1=FINAL_DSEG1():r:SUPP
 | MOVSW                | STRINGOP       | BASE           | I86            | a5           |            |            | 0xA5 mode64 norexw_prefix 66_prefix  norep  OVERRIDE_SEG1()                                          | MEM0:w:SUPP:w BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:r:SUPP:w BASE1=ArSI():rw:SUPP SEG1=FINAL_DSEG1():r:SUPP
 | MOVSX                | DATAXFER       | BASE           | I386           | 0f be        | mm         | rrr        | 0x0F 0xBE MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | REG0=GPRv_R():w MEM0:r:b
 | MOVSX                | DATAXFER       | BASE           | I386           | 0f be        | 0b11       | rrr        | 0x0F 0xBE MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_R():w REG1=GPR8_B():r
 | MOVSX                | DATAXFER       | BASE           | I386           | 0f bf        | mm         | rrr        | 0x0F 0xBF MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | REG0=GPRv_R():w MEM0:r:w
 | MOVSX                | DATAXFER       | BASE           | I386           | 0f bf        | 0b11       | rrr        | 0x0F 0xBF MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_R():w REG1=GPR16_B():r
 | MOVSXD               | DATAXFER       | LONGMODE       |                | 63           | mm         | rrr        | 0x63 MOD[mm] MOD!=3 REG[rrr] RM[nnn] mode64 MODRM()                                                  | REG0=GPRv_R():w MEM0:r:z
 | MOVSXD               | DATAXFER       | LONGMODE       |                | 63           | 0b11       | rrr        | 0x63 MOD[0b11] MOD=3 REG[rrr] RM[nnn] mode64                                                         | REG0=GPRv_R():w REG1=GPRz_B():r
 | MOVUPD               | DATAXFER       | SSE2           |                | 0f 10        | mm         | rrr        | 0x0F 0x10 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():w:pd MEM0:r:pd
 | MOVUPD               | DATAXFER       | SSE2           |                | 0f 10        | 0b11       | rrr        | 0x0F 0x10 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():w:pd REG1=XMM_B():r:pd
 | MOVUPD               | DATAXFER       | SSE2           |                | 0f 11        | mm         | rrr        | 0x0F 0x11 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | MEM0:w:pd REG0=XMM_R():r:pd
 | MOVUPD               | DATAXFER       | SSE2           |                | 0f 11        | 0b11       | rrr        | 0x0F 0x11 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_B():w:pd REG1=XMM_R():r:pd
 | MOVUPS               | DATAXFER       | SSE            |                | 0f 10        | mm         | rrr        | 0x0F 0x10 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=XMM_R():w:ps MEM0:r:ps
 | MOVUPS               | DATAXFER       | SSE            |                | 0f 10        | 0b11       | rrr        | 0x0F 0x10 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=XMM_R():w:ps REG1=XMM_B():r:ps
 | MOVUPS               | DATAXFER       | SSE            |                | 0f 11        | mm         | rrr        | 0x0F 0x11 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | MEM0:w:ps REG0=XMM_R():r:ps
 | MOVUPS               | DATAXFER       | SSE            |                | 0f 11        | 0b11       | rrr        | 0x0F 0x11 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=XMM_B():w:ps REG1=XMM_R():r:ps
 | MOVZX                | DATAXFER       | BASE           | I386           | 0f b6        | mm         | rrr        | 0x0F 0xB6 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | REG0=GPRv_R():w MEM0:r:b
 | MOVZX                | DATAXFER       | BASE           | I386           | 0f b6        | 0b11       | rrr        | 0x0F 0xB6 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_R():w REG1=GPR8_B():r
 | MOVZX                | DATAXFER       | BASE           | I386           | 0f b7        | mm         | rrr        | 0x0F 0xB7 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | REG0=GPRv_R():w MEM0:r:w
 | MOVZX                | DATAXFER       | BASE           | I386           | 0f b7        | 0b11       | rrr        | 0x0F 0xB7 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_R():w REG1=GPR16_B():r
 | MPSADBW              | SSE            | SSE4           |                | 0f 3a 42     | mm         | rrr        | 0x0F 0x3A 0x42 osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()      | REG0=XMM_R():rw:dq:u8 MEM0:r:dq:u8 IMM0:r:b
 | MPSADBW              | SSE            | SSE4           |                | 0f 3a 42     | 0b11       | rrr        | 0x0F 0x3A 0x42 osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()             | REG0=XMM_R():rw:dq:u8 REG1=XMM_B():r:dq:u8 IMM0:r:b
 | MUL                  | BINARY         | BASE           | I86            | f6           | mm         | 0b100      | 0xF6 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM()                                                       | MEM0:r:b REG0=XED_REG_AL:r:SUPP REG1=XED_REG_AX:w:SUPP
 | MUL                  | BINARY         | BASE           | I86            | f6           | 0b11       | 0b100      | 0xF6 MOD[0b11] MOD=3 REG[0b100] RM[nnn]                                                              | REG0=GPR8_B():r REG1=XED_REG_AL:r:SUPP REG2=XED_REG_AX:w:SUPP
 | MUL                  | BINARY         | BASE           | I86            | f7           | mm         | 0b100      | 0xF7 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM()                                                       | MEM0:r:v REG0=OrAX():rw:SUPP REG1=OrDX():w:SUPP
 | MUL                  | BINARY         | BASE           | I86            | f7           | 0b11       | 0b100      | 0xF7 MOD[0b11] MOD=3 REG[0b100] RM[nnn]                                                              | REG0=GPRv_B():r REG1=OrAX():rw:SUPP REG2=OrDX():w:SUPP
 | MULPD                | SSE            | SSE2           |                | 0f 59        | mm         | rrr        | 0x0F 0x59 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:pd MEM0:r:pd
 | MULPD                | SSE            | SSE2           |                | 0f 59        | 0b11       | rrr        | 0x0F 0x59 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:pd REG1=XMM_B():r:pd
 | MULPS                | SSE            | SSE            |                | 0f 59        | mm         | rrr        | 0x0F 0x59 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=XMM_R():rw:ps MEM0:r:ps
 | MULPS                | SSE            | SSE            |                | 0f 59        | 0b11       | rrr        | 0x0F 0x59 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=XMM_R():rw:ps REG1=XMM_B():r:ps
 | MULSD                | SSE            | SSE2           |                | 0f 59        | mm         | rrr        | 0x0F 0x59 f2_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM()                     | REG0=XMM_R():rw:sd MEM0:r:sd
 | MULSD                | SSE            | SSE2           |                | 0f 59        | 0b11       | rrr        | 0x0F 0x59 f2_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66()                            | REG0=XMM_R():rw:sd REG1=XMM_B():r:sd
 | MULSS                | SSE            | SSE            |                | 0f 59        | mm         | rrr        | 0x0F 0x59 f3_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM()                     | REG0=XMM_R():rw:ss MEM0:r:ss
 | MULSS                | SSE            | SSE            |                | 0f 59        | 0b11       | rrr        | 0x0F 0x59 f3_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66()                            | REG0=XMM_R():rw:ss REG1=XMM_B():r:ss
 | MULX                 | BMI2           | BMI2           |                | f6           | 0b11       | rrr        | VV1 0xF6 VF2 V0F38 not64 VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=VGPR32_R():w:d REG1=VGPR32_N():w:d REG2=VGPR32_B():r:d REG3=XED_REG_EDX:r:SUPP
 | MULX                 | BMI2           | BMI2           |                | f6           | 0b11       | rrr        | VV1 0xF6 VF2 V0F38 W0 mode64 VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                  | REG0=VGPR32_R():w:d REG1=VGPR32_N():w:d REG2=VGPR32_B():r:d REG3=XED_REG_EDX:r:SUPP
 | MULX                 | BMI2           | BMI2           |                | f6           | mm         | rrr        | VV1 0xF6 VF2 V0F38 not64 VL128 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=VGPR32_R():w:d REG1=VGPR32_N():w:d MEM0:r:d  REG2=XED_REG_EDX:r:SUPP
 | MULX                 | BMI2           | BMI2           |                | f6           | mm         | rrr        | VV1 0xF6 VF2 V0F38 W0 mode64 VL128 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                           | REG0=VGPR32_R():w:d REG1=VGPR32_N():w:d MEM0:r:d  REG2=XED_REG_EDX:r:SUPP
 | MULX                 | BMI2           | BMI2           |                | f6           | 0b11       | rrr        | VV1 0xF6 VF2 V0F38 W1 VL128 mode64 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                  | REG0=VGPR64_R():w:q REG1=VGPR64_N():w:q REG2=VGPR64_B():r:q REG3=XED_REG_RDX:r:SUPP
 | MULX                 | BMI2           | BMI2           |                | f6           | mm         | rrr        | VV1 0xF6 VF2 V0F38 W1 VL128 mode64  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                          | REG0=VGPR64_R():w:q REG1=VGPR64_N():w:q MEM0:r:q REG2=XED_REG_RDX:r:SUPP
 | MWAIT                | MISC           | MONITOR        | MONITOR        | 0f 01        | 0b11       | 0b001      | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b001] RM[0b001] no_refining_prefix                                    | REG0=XED_REG_EAX:r:SUPP REG1=XED_REG_ECX:r:SUPP
 | NEG                  | BINARY         | BASE           | I86            | f6           | mm         | 0b011      | 0xF6 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM() nolock_prefix                                         | MEM0:rw:b
 | NEG                  | BINARY         | BASE           | I86            | f6           | 0b11       | 0b011      | 0xF6 MOD[0b11] MOD=3 REG[0b011] RM[nnn]                                                              | REG0=GPR8_B():rw
 | NEG                  | BINARY         | BASE           | I86            | f7           | mm         | 0b011      | 0xF7 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM() nolock_prefix                                         | MEM0:rw:v
 | NEG                  | BINARY         | BASE           | I86            | f7           | 0b11       | 0b011      | 0xF7 MOD[0b11] MOD=3 REG[0b011] RM[nnn]                                                              | REG0=GPRv_B():rw
 | NEG_LOCK             | BINARY         | BASE           | I86            | f6           | mm         | 0b011      | 0xF6 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM() lock_prefix                                           | MEM0:rw:b
 | NEG_LOCK             | BINARY         | BASE           | I86            | f7           | mm         | 0b011      | 0xF7 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM() lock_prefix                                           | MEM0:rw:v
 | NOP                  | NOP            | BASE           | I86            |              |            |            | 0b1001_0 SRM[0b000] SRM=0  not_refining_f3 norexb_prefix                                             | 
 | NOP                  | NOP            | BASE           | I86            |              |            |            | 0b1001_0 SRM[0b000] SRM=0  refining_f3 P4=0                                                          | 
 | NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0f 18        | 0b11       | 0b000      | 0x0F 0x18 MOD[0b11] MOD=3 REG[0b000] RM[nnn]                                                         | REG0=GPRv_B():r
 | NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0f 18        | 0b11       | 0b001      | 0x0F 0x18 MOD[0b11] MOD=3 REG[0b001] RM[nnn]                                                         | REG0=GPRv_B():r
 | NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0f 18        | 0b11       | 0b010      | 0x0F 0x18 MOD[0b11] MOD=3 REG[0b010] RM[nnn]                                                         | REG0=GPRv_B():r
 | NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0f 18        | 0b11       | 0b011      | 0x0F 0x18 MOD[0b11] MOD=3 REG[0b011] RM[nnn]                                                         | REG0=GPRv_B():r
 | NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0f 18        | mm         | 0b100      | 0x0F 0x18 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM()                                                  | MEM0:r:v
 | NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0f 18        | 0b11       | 0b100      | 0x0F 0x18 MOD[0b11] MOD=3 REG[0b100] RM[nnn]                                                         | REG0=GPRv_B():r
 | NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0f 18        | mm         | 0b101      | 0x0F 0x18 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM()                                                  | MEM0:r:v
 | NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0f 18        | 0b11       | 0b101      | 0x0F 0x18 MOD[0b11] MOD=3 REG[0b101] RM[nnn]                                                         | REG0=GPRv_B():r
 | NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0f 18        | mm         | 0b110      | 0x0F 0x18 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM()                                                  | MEM0:r:v
 | NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0f 18        | 0b11       | 0b110      | 0x0F 0x18 MOD[0b11] MOD=3 REG[0b110] RM[nnn]                                                         | REG0=GPRv_B():r
 | NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0f 18        | mm         | 0b111      | 0x0F 0x18 MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM()                                                  | MEM0:r:v
 | NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0f 18        | 0b11       | 0b111      | 0x0F 0x18 MOD[0b11] MOD=3 REG[0b111] RM[nnn]                                                         | REG0=GPRv_B():r
 | NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0f 19        | mm         | rrr        | 0x0F 0x19 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | MEM0:r:v REG0=GPRv_R():r
 | NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0f 19        | 0b11       | rrr        | 0x0F 0x19 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_B():r REG1=GPRv_R():r
 | NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0f 1a        | mm         | rrr        | 0x0F 0x1A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | MEM0:r:v REG0=GPRv_R():r
 | NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0f 1a        | 0b11       | rrr        | 0x0F 0x1A MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_B():r REG1=GPRv_R():r
 | NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0f 1b        | mm         | rrr        | 0x0F 0x1B MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | MEM0:r:v REG0=GPRv_R():r
 | NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0f 1b        | 0b11       | rrr        | 0x0F 0x1B MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_B():r REG1=GPRv_R():r
 | NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0f 1c        | mm         | rrr        | 0x0F 0x1C MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | MEM0:r:v REG0=GPRv_R():r
 | NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0f 1c        | 0b11       | rrr        | 0x0F 0x1C MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_B():r REG1=GPRv_R():r
 | NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0f 1d        | mm         | rrr        | 0x0F 0x1D MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | MEM0:r:v REG0=GPRv_R():r
 | NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0f 1d        | 0b11       | rrr        | 0x0F 0x1D MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_B():r REG1=GPRv_R():r
 | NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0f 1e        | mm         | rrr        | 0x0F 0x1E MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | MEM0:r:v REG0=GPRv_R():r
 | NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0f 1e        | 0b11       | rrr        | 0x0F 0x1E MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_B():r REG1=GPRv_R():r
 | NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0f 1f        | mm         | rrr        | 0x0F 0x1F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | MEM0:r:v REG0=GPRv_R():r
 | NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0f 1f        | 0b11       | rrr        | 0x0F 0x1F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_B():r REG1=GPRv_R():r
 | NOP                  | WIDENOP        | BASE           | KNC_MISC       | 0f 1f        | mm         | 0b000      | 0x0F 0x1F MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM()                                                  | MEM0:r:v
 | NOP                  | WIDENOP        | BASE           | KNC_MISC       | 0f 1f        | 0b11       | 0b000      | 0x0F 0x1F MOD[0b11] MOD=3 REG[0b000] RM[nnn]                                                         | REG0=GPRv_B():r
 | NOP                  | WIDENOP        | BASE           | PPRO           | 0f 1e        | 0b11       | 0b111      | 0x0F 0x1E MOD[0b11] MOD=3  REG[0b111] RM[0b010]  f3_refining_prefix CET=0                            | REG0=GPRv_B():r REG1=GPRv_R():r
 | NOP                  | WIDENOP        | BASE           | PPRO           | 0f 1e        | 0b11       | 0b111      | 0x0F 0x1E MOD[0b11] MOD=3  REG[0b111] RM[0b011]  f3_refining_prefix CET=0                            | REG0=GPRv_B():r REG1=GPRv_R():r
 | NOP                  | WIDENOP        | BASE           | PPRO           | 0f 1e        | 0b11       | 0b001      | 0x0F 0x1E MOD[0b11] MOD=3  REG[0b001] RM[nnn]  f3_refining_prefix W0 CET=0                           | REG0=GPRv_B():r REG1=GPRv_R():r
 | NOP                  | WIDENOP        | BASE           | PPRO           | 0f 1e        | 0b11       | 0b001      | 0x0F 0x1E MOD[0b11] MOD=3  REG[0b001] RM[nnn]  f3_refining_prefix W1 mode64  CET=0                   | REG0=GPRv_B():r REG1=GPRv_R():r
 | NOP                  | WIDENOP        | BASE           | PPRO           | 0f 1c        | mm         | 0b000      | 0x0F 0x1C MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() f2_refining_prefix                               | MEM0:r:v REG0=GPRv_R():r
 | NOP                  | WIDENOP        | BASE           | PPRO           | 0f 1c        | mm         | 0b000      | 0x0F 0x1C MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() f3_refining_prefix                               | MEM0:r:v REG0=GPRv_R():r
 | NOP                  | WIDENOP        | BASE           | PPRO           | 0f 1c        | mm         | 0b000      | 0x0F 0x1C MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() osz_refining_prefix                              | MEM0:r:v REG0=GPRv_R():r
 | NOP                  | WIDENOP        | BASE           | PPRO           | 0f 1c        | mm         | 0b001      | 0x0F 0x1C MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM()                                                  | MEM0:r:v REG0=GPRv_R():r
 | NOP                  | WIDENOP        | BASE           | PPRO           | 0f 1c        | mm         | 0b010      | 0x0F 0x1C MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM()                                                  | MEM0:r:v REG0=GPRv_R():r
 | NOP                  | WIDENOP        | BASE           | PPRO           | 0f 1c        | mm         | 0b011      | 0x0F 0x1C MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM()                                                  | MEM0:r:v REG0=GPRv_R():r
 | NOP                  | WIDENOP        | BASE           | PPRO           | 0f 1c        | mm         | 0b100      | 0x0F 0x1C MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM()                                                  | MEM0:r:v REG0=GPRv_R():r
 | NOP                  | WIDENOP        | BASE           | PPRO           | 0f 1c        | mm         | 0b101      | 0x0F 0x1C MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM()                                                  | MEM0:r:v REG0=GPRv_R():r
 | NOP                  | WIDENOP        | BASE           | PPRO           | 0f 1c        | mm         | 0b110      | 0x0F 0x1C MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM()                                                  | MEM0:r:v REG0=GPRv_R():r
 | NOP                  | WIDENOP        | BASE           | PPRO           | 0f 1c        | mm         | 0b111      | 0x0F 0x1C MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM()                                                  | MEM0:r:v REG0=GPRv_R():r
 | NOP                  | WIDENOP        | BASE           | PPRO           | 0f 1c        | 0b11       | rrr        | 0x0F 0x1C MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_B():r REG1=GPRv_R():r
 | NOP                  | WIDENOP        | BASE           | PPRO           | 0f 1c        | mm         | 0b000      | 0x0F 0x1C MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() no_refining_prefix CLDEMOTE=0                    | MEM0:r:v REG0=GPRv_R():r
 | NOP                  | WIDENOP        | BASE           | PPRO           | 0f 1a        | 0b11       | rrr        | 0x0F 0x1A MPXMODE=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] no_refining_prefix                              | REG0=GPRv_B():r REG1=GPRv_R():r
 | NOP                  | WIDENOP        | BASE           | PPRO           | 0f 1b        | 0b11       | rrr        | 0x0F 0x1B MPXMODE=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] no_refining_prefix                              | REG0=GPRv_B():r REG1=GPRv_R():r
 | NOP                  | WIDENOP        | BASE           | PPRO           | 0f 1b        | 0b11       | rrr        | 0x0F 0x1B MPXMODE=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] f3_refining_prefix                              | REG0=GPRv_B():r REG1=GPRv_R():r
 | NOP                  | WIDENOP        | BASE           | PPRO           | 0f 1a        | 0b11       | rrr        | 0x0F 0x1A MPXMODE=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                 | REG0=GPRv_B():r REG1=GPRv_R():r
 | NOP                  | WIDENOP        | BASE           | PPRO           | 0f 1b        | 0b11       | rrr        | 0x0F 0x1B MPXMODE=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                 | REG0=GPRv_B():r REG1=GPRv_R():r
 | NOP                  | WIDENOP        | BASE           | PPRO           | 0f 1a        | mm         | rrr        | 0x0F 0x1A MPXMODE=0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                          | REG0=GPRv_B():r MEM0:r:v
 | NOP                  | WIDENOP        | BASE           | PPRO           | 0f 1b        | mm         | rrr        | 0x0F 0x1B MPXMODE=0 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                         | REG0=GPRv_B():r MEM0:r:v
 | NOP                  | WIDENOP        | BASE           | PREFETCH_NOP   | 0f 0d        | 0b11       | rrr        | 0x0F 0x0D MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_B():r REG1=GPRv_R():r
 | NOT                  | LOGICAL        | BASE           | I86            | f6           | mm         | 0b010      | 0xF6 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM() nolock_prefix                                         | MEM0:rw:b
 | NOT                  | LOGICAL        | BASE           | I86            | f6           | 0b11       | 0b010      | 0xF6 MOD[0b11] MOD=3 REG[0b010] RM[nnn]                                                              | REG0=GPR8_B():rw
 | NOT                  | LOGICAL        | BASE           | I86            | f7           | mm         | 0b010      | 0xF7 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM() nolock_prefix                                         | MEM0:rw:v
 | NOT                  | LOGICAL        | BASE           | I86            | f7           | 0b11       | 0b010      | 0xF7 MOD[0b11] MOD=3 REG[0b010] RM[nnn]                                                              | REG0=GPRv_B():rw
 | NOT_LOCK             | LOGICAL        | BASE           | I86            | f6           | mm         | 0b010      | 0xF6 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM() lock_prefix                                           | MEM0:rw:b
 | NOT_LOCK             | LOGICAL        | BASE           | I86            | f7           | mm         | 0b010      | 0xF7 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM() lock_prefix                                           | MEM0:rw:v
 | OR                   | LOGICAL        | BASE           | I86            | 80           | mm         | 0b001      | 0x80 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM() SIMM8() nolock_prefix                                 | MEM0:rw:b IMM0:r:b:i8
 | OR                   | LOGICAL        | BASE           | I86            | 80           | 0b11       | 0b001      | 0x80 MOD[0b11] MOD=3 REG[0b001] RM[nnn] SIMM8()                                                      | REG0=GPR8_B():rw IMM0:r:b:i8
 | OR                   | LOGICAL        | BASE           | I86            | 81           | mm         | 0b001      | 0x81 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM() SIMMz() nolock_prefix                                 | MEM0:rw:v IMM0:r:z
 | OR                   | LOGICAL        | BASE           | I86            | 81           | 0b11       | 0b001      | 0x81 MOD[0b11] MOD=3 REG[0b001] RM[nnn] SIMMz()                                                      | REG0=GPRv_B():rw IMM0:r:z
 | OR                   | LOGICAL        | BASE           | I86            | 82           | mm         | 0b001      | 0x82 MOD[mm] MOD!=3 REG[0b001] RM[nnn] not64 MODRM() SIMM8() nolock_prefix                           | MEM0:rw:b IMM0:r:b:i8
 | OR                   | LOGICAL        | BASE           | I86            | 82           | 0b11       | 0b001      | 0x82 MOD[0b11] MOD=3 REG[0b001] RM[nnn] not64 SIMM8()                                                | REG0=GPR8_B():rw IMM0:r:b:i8
 | OR                   | LOGICAL        | BASE           | I86            | 83           | mm         | 0b001      | 0x83 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM() SIMM8() nolock_prefix                                 | MEM0:rw:v IMM0:r:b:i8
 | OR                   | LOGICAL        | BASE           | I86            | 83           | 0b11       | 0b001      | 0x83 MOD[0b11] MOD=3 REG[0b001] RM[nnn] SIMM8()                                                      | REG0=GPRv_B():rw IMM0:r:b:i8
 | OR                   | LOGICAL        | BASE           | I86            | 08           | mm         | rrr        | 0x08 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                                           | MEM0:rw:b REG0=GPR8_R():r
 | OR                   | LOGICAL        | BASE           | I86            | 08           | 0b11       | rrr        | 0x08 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPR8_B():rw REG1=GPR8_R():r
 | OR                   | LOGICAL        | BASE           | I86            | 09           | mm         | rrr        | 0x09 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                                           | MEM0:rw:v REG0=GPRv_R():r
 | OR                   | LOGICAL        | BASE           | I86            | 09           | 0b11       | rrr        | 0x09 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPRv_B():rw REG1=GPRv_R():r
 | OR                   | LOGICAL        | BASE           | I86            | 0a           | mm         | rrr        | 0x0A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                         | REG0=GPR8_R():rw MEM0:r:b
 | OR                   | LOGICAL        | BASE           | I86            | 0a           | 0b11       | rrr        | 0x0A MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPR8_R():rw REG1=GPR8_B():r
 | OR                   | LOGICAL        | BASE           | I86            | 0b           | mm         | rrr        | 0x0B MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                         | REG0=GPRv_R():rw MEM0:r:v
 | OR                   | LOGICAL        | BASE           | I86            | 0b           | 0b11       | rrr        | 0x0B MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPRv_R():rw REG1=GPRv_B():r
 | OR                   | LOGICAL        | BASE           | I86            | 0c           |            |            | 0x0C UIMM8()                                                                                         | REG0=XED_REG_AL:rw:IMPL IMM0:r:b
 | OR                   | LOGICAL        | BASE           | I86            | 0d           |            |            | 0x0D SIMMz()                                                                                         | REG0=OrAX():rw:IMPL IMM0:r:z
 | OR_LOCK              | LOGICAL        | BASE           | I86            | 80           | mm         | 0b001      | 0x80 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM() SIMM8() lock_prefix                                   | MEM0:rw:b IMM0:r:b:i8
 | OR_LOCK              | LOGICAL        | BASE           | I86            | 81           | mm         | 0b001      | 0x81 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM() SIMMz() lock_prefix                                   | MEM0:rw:v IMM0:r:z
 | OR_LOCK              | LOGICAL        | BASE           | I86            | 82           | mm         | 0b001      | 0x82 MOD[mm] MOD!=3 REG[0b001] RM[nnn] not64 MODRM() SIMM8() lock_prefix                             | MEM0:rw:b IMM0:r:b:i8
 | OR_LOCK              | LOGICAL        | BASE           | I86            | 83           | mm         | 0b001      | 0x83 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM() SIMM8() lock_prefix                                   | MEM0:rw:v IMM0:r:b:i8
 | OR_LOCK              | LOGICAL        | BASE           | I86            | 08           | mm         | rrr        | 0x08 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                                             | MEM0:rw:b REG0=GPR8_R():r
 | OR_LOCK              | LOGICAL        | BASE           | I86            | 09           | mm         | rrr        | 0x09 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                                             | MEM0:rw:v REG0=GPRv_R():r
 | ORPD                 | LOGICAL_FP     | SSE2           |                | 0f 56        | mm         | rrr        | 0x0F 0x56 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:xuq MEM0:r:xuq
 | ORPD                 | LOGICAL_FP     | SSE2           |                | 0f 56        | 0b11       | rrr        | 0x0F 0x56 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:xuq REG1=XMM_B():r:xuq
 | ORPS                 | LOGICAL_FP     | SSE            |                | 0f 56        | mm         | rrr        | 0x0F 0x56 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=XMM_R():rw:xud MEM0:r:xud
 | ORPS                 | LOGICAL_FP     | SSE            |                | 0f 56        | 0b11       | rrr        | 0x0F 0x56 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=XMM_R():rw:xud REG1=XMM_B():r:xud
 | OUT                  | IO             | BASE           | I86            | e6           |            |            | 0xE6 UIMM8()                                                                                         | IMM0:r:b REG0=XED_REG_AL:r:IMPL
 | OUT                  | IO             | BASE           | I86            | e7           |            |            | 0xE7 UIMM8() IMMUNE_REXW()                                                                           | IMM0:r:b REG0=OeAX():r:IMPL
 | OUT                  | IO             | BASE           | I86            | ee           |            |            | 0xEE                                                                                                 | REG0=XED_REG_DX:r:IMPL REG1=XED_REG_AL:r:IMPL
 | OUT                  | IO             | BASE           | I86            | ef           |            |            | 0xEF IMMUNE_REXW()                                                                                   | REG0=XED_REG_DX:r:IMPL REG1=OeAX():r:IMPL
 | OUTSB                | IOSTRINGOP     | BASE           | I186           | 6e           |            |            | 0x6E norep OVERRIDE_SEG0()                                                                           | REG0=XED_REG_DX:r:SUPP MEM0:r:SUPP:b BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP
 | OUTSD                | IOSTRINGOP     | BASE           | I386           | 6f           |            |            | 0x6F mode16 66_prefix  norep OVERRIDE_SEG0()                                                         | REG0=XED_REG_DX:r:SUPP MEM0:r:SUPP:d BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP
 | OUTSD                | IOSTRINGOP     | BASE           | I386           | 6f           |            |            | 0x6F mode32 no66_prefix  norep OVERRIDE_SEG0()                                                       | REG0=XED_REG_DX:r:SUPP MEM0:r:SUPP:d BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP
 | OUTSD                | IOSTRINGOP     | BASE           | I386           | 6f           |            |            | 0x6F mode64 norexw_prefix no66_prefix  norep OVERRIDE_SEG0()                                         | REG0=XED_REG_DX:r:SUPP MEM0:r:SUPP:d BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP
 | OUTSD                | IOSTRINGOP     | BASE           | I386           | 6f           |            |            | 0x6F mode64 rexw_prefix  norep OVERRIDE_SEG0()                                                       | REG0=XED_REG_DX:r:SUPP MEM0:r:SUPP:d BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP
 | OUTSW                | IOSTRINGOP     | BASE           | I186           | 6f           |            |            | 0x6F mode16 no66_prefix norep OVERRIDE_SEG0()                                                        | REG0=XED_REG_DX:r:SUPP MEM0:r:SUPP:w BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP
 | OUTSW                | IOSTRINGOP     | BASE           | I186           | 6f           |            |            | 0x6F mode32 66_prefix norep OVERRIDE_SEG0()                                                          | REG0=XED_REG_DX:r:SUPP MEM0:r:SUPP:w BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP
 | OUTSW                | IOSTRINGOP     | BASE           | I186           | 6f           |            |            | 0x6F mode64 norexw_prefix 66_prefix  norep OVERRIDE_SEG0()                                           | REG0=XED_REG_DX:r:SUPP MEM0:r:SUPP:w BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP
 | PABSB                | MMX            | SSSE3          | SSSE3MMX       | 0f 38 1c     | mm         | rrr        | 0x0F 0x38 0x1C no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                           | REG0=MMX_R():w:q MEM0:r:q
 | PABSB                | MMX            | SSSE3          | SSSE3MMX       | 0f 38 1c     | 0b11       | rrr        | 0x0F 0x38 0x1C no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                   | REG0=MMX_R():w:q REG1=MMX_B():r:q
 | PABSB                | SSE            | SSSE3          |                | 0f 38 1c     | mm         | rrr        | 0x0F 0x38 0x1C osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()             | REG0=XMM_R():w:dq MEM0:r:dq
 | PABSB                | SSE            | SSSE3          |                | 0f 38 1c     | 0b11       | rrr        | 0x0F 0x38 0x1C osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                    | REG0=XMM_R():w:dq REG1=XMM_B():r:dq
 | PABSD                | MMX            | SSSE3          | SSSE3MMX       | 0f 38 1e     | mm         | rrr        | 0x0F 0x38 0x1E no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                           | REG0=MMX_R():w:q MEM0:r:q
 | PABSD                | MMX            | SSSE3          | SSSE3MMX       | 0f 38 1e     | 0b11       | rrr        | 0x0F 0x38 0x1E no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                   | REG0=MMX_R():w:q REG1=MMX_B():r:q
 | PABSD                | SSE            | SSSE3          |                | 0f 38 1e     | mm         | rrr        | 0x0F 0x38 0x1E osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()             | REG0=XMM_R():w:dq MEM0:r:dq
 | PABSD                | SSE            | SSSE3          |                | 0f 38 1e     | 0b11       | rrr        | 0x0F 0x38 0x1E osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                    | REG0=XMM_R():w:dq REG1=XMM_B():r:dq
 | PABSW                | MMX            | SSSE3          | SSSE3MMX       | 0f 38 1d     | mm         | rrr        | 0x0F 0x38 0x1D no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                           | REG0=MMX_R():w:q MEM0:r:q
 | PABSW                | MMX            | SSSE3          | SSSE3MMX       | 0f 38 1d     | 0b11       | rrr        | 0x0F 0x38 0x1D no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                   | REG0=MMX_R():w:q REG1=MMX_B():r:q
 | PABSW                | SSE            | SSSE3          |                | 0f 38 1d     | mm         | rrr        | 0x0F 0x38 0x1D osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()             | REG0=XMM_R():w:dq MEM0:r:dq
 | PABSW                | SSE            | SSSE3          |                | 0f 38 1d     | 0b11       | rrr        | 0x0F 0x38 0x1D osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                    | REG0=XMM_R():w:dq REG1=XMM_B():r:dq
 | PACKSSDW             | MMX            | MMX            | PENTIUMMMX     | 0f 6b        | mm         | rrr        | 0x0F 0x6B no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q:i32 MEM0:r:q:i32
 | PACKSSDW             | MMX            | MMX            | PENTIUMMMX     | 0f 6b        | 0b11       | rrr        | 0x0F 0x6B no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q:i32 REG1=MMX_B():r:q:i32
 | PACKSSDW             | SSE            | SSE2           |                | 0f 6b        | mm         | rrr        | 0x0F 0x6B osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq:i32 MEM0:r:dq:i32
 | PACKSSDW             | SSE            | SSE2           |                | 0f 6b        | 0b11       | rrr        | 0x0F 0x6B osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq:i32 REG1=XMM_B():r:dq:i32
 | PACKSSWB             | MMX            | MMX            | PENTIUMMMX     | 0f 63        | mm         | rrr        | 0x0F 0x63 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q:i16 MEM0:r:q:i16
 | PACKSSWB             | MMX            | MMX            | PENTIUMMMX     | 0f 63        | 0b11       | rrr        | 0x0F 0x63 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q:i16 REG1=MMX_B():r:q:i16
 | PACKSSWB             | SSE            | SSE2           |                | 0f 63        | mm         | rrr        | 0x0F 0x63 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq:i16 MEM0:r:dq:i16
 | PACKSSWB             | SSE            | SSE2           |                | 0f 63        | 0b11       | rrr        | 0x0F 0x63 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq:i16 REG1=XMM_B():r:dq:i16
 | PACKUSDW             | SSE            | SSE4           |                | 0f 38 2b     | mm         | rrr        | 0x0F 0x38 0x2B osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()              | REG0=XMM_R():rw:dq:i32 MEM0:r:dq:i32
 | PACKUSDW             | SSE            | SSE4           |                | 0f 38 2b     | 0b11       | rrr        | 0x0F 0x38 0x2B osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn]                     | REG0=XMM_R():rw:dq:i32 REG1=XMM_B():r:dq:i32
 | PACKUSWB             | MMX            | MMX            | PENTIUMMMX     | 0f 67        | mm         | rrr        | 0x0F 0x67 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q:i16 MEM0:r:q:i16
 | PACKUSWB             | MMX            | MMX            | PENTIUMMMX     | 0f 67        | 0b11       | rrr        | 0x0F 0x67 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q:i16 REG1=MMX_B():r:q:i16
 | PACKUSWB             | SSE            | SSE2           |                | 0f 67        | mm         | rrr        | 0x0F 0x67 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq:i16 MEM0:r:dq:i16
 | PACKUSWB             | SSE            | SSE2           |                | 0f 67        | 0b11       | rrr        | 0x0F 0x67 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq:i16 REG1=XMM_B():r:dq:i16
 | PADDB                | MMX            | MMX            | PENTIUMMMX     | 0f fc        | mm         | rrr        | 0x0F 0xFC no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q MEM0:r:q
 | PADDB                | MMX            | MMX            | PENTIUMMMX     | 0f fc        | 0b11       | rrr        | 0x0F 0xFC no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PADDB                | SSE            | SSE2           |                | 0f fc        | mm         | rrr        | 0x0F 0xFC osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PADDB                | SSE            | SSE2           |                | 0f fc        | 0b11       | rrr        | 0x0F 0xFC osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PADDD                | MMX            | MMX            | PENTIUMMMX     | 0f fe        | mm         | rrr        | 0x0F 0xFE no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q MEM0:r:q
 | PADDD                | MMX            | MMX            | PENTIUMMMX     | 0f fe        | 0b11       | rrr        | 0x0F 0xFE no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PADDD                | SSE            | SSE2           |                | 0f fe        | mm         | rrr        | 0x0F 0xFE osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PADDD                | SSE            | SSE2           |                | 0f fe        | 0b11       | rrr        | 0x0F 0xFE osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PADDQ                | MMX            | SSE2           | SSE2MMX        | 0f d4        | mm         | rrr        | 0x0F 0xD4 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q:u64 MEM0:r:q:u64
 | PADDQ                | MMX            | SSE2           | SSE2MMX        | 0f d4        | 0b11       | rrr        | 0x0F 0xD4 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q:u64 REG1=MMX_B():r:q:u64
 | PADDQ                | SSE            | SSE2           |                | 0f d4        | mm         | rrr        | 0x0F 0xD4 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PADDQ                | SSE            | SSE2           |                | 0f d4        | 0b11       | rrr        | 0x0F 0xD4 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PADDSB               | MMX            | MMX            | PENTIUMMMX     | 0f ec        | mm         | rrr        | 0x0F 0xEC no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q MEM0:r:q
 | PADDSB               | MMX            | MMX            | PENTIUMMMX     | 0f ec        | 0b11       | rrr        | 0x0F 0xEC no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PADDSB               | SSE            | SSE2           |                | 0f ec        | mm         | rrr        | 0x0F 0xEC osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PADDSB               | SSE            | SSE2           |                | 0f ec        | 0b11       | rrr        | 0x0F 0xEC osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PADDSW               | MMX            | MMX            | PENTIUMMMX     | 0f ed        | mm         | rrr        | 0x0F 0xED no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q MEM0:r:q
 | PADDSW               | MMX            | MMX            | PENTIUMMMX     | 0f ed        | 0b11       | rrr        | 0x0F 0xED no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PADDSW               | SSE            | SSE2           |                | 0f ed        | mm         | rrr        | 0x0F 0xED osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PADDSW               | SSE            | SSE2           |                | 0f ed        | 0b11       | rrr        | 0x0F 0xED osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PADDUSB              | MMX            | MMX            | PENTIUMMMX     | 0f dc        | mm         | rrr        | 0x0F 0xDC no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q MEM0:r:q
 | PADDUSB              | MMX            | MMX            | PENTIUMMMX     | 0f dc        | 0b11       | rrr        | 0x0F 0xDC no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PADDUSB              | SSE            | SSE2           |                | 0f dc        | mm         | rrr        | 0x0F 0xDC osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PADDUSB              | SSE            | SSE2           |                | 0f dc        | 0b11       | rrr        | 0x0F 0xDC osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PADDUSW              | MMX            | MMX            | PENTIUMMMX     | 0f dd        | mm         | rrr        | 0x0F 0xDD no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q MEM0:r:q
 | PADDUSW              | MMX            | MMX            | PENTIUMMMX     | 0f dd        | 0b11       | rrr        | 0x0F 0xDD no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PADDUSW              | SSE            | SSE2           |                | 0f dd        | mm         | rrr        | 0x0F 0xDD osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PADDUSW              | SSE            | SSE2           |                | 0f dd        | 0b11       | rrr        | 0x0F 0xDD osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PADDW                | MMX            | MMX            | PENTIUMMMX     | 0f fd        | mm         | rrr        | 0x0F 0xFD no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q MEM0:r:q
 | PADDW                | MMX            | MMX            | PENTIUMMMX     | 0f fd        | 0b11       | rrr        | 0x0F 0xFD no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PADDW                | SSE            | SSE2           |                | 0f fd        | mm         | rrr        | 0x0F 0xFD osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq:i16 MEM0:r:dq:i16
 | PADDW                | SSE            | SSE2           |                | 0f fd        | 0b11       | rrr        | 0x0F 0xFD osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq:i16 REG1=XMM_B():r:dq:i16
 | PALIGNR              | MMX            | SSSE3          | SSSE3MMX       | 0f 3a 0f     | mm         | rrr        | 0x0F 0x3A 0x0F no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM() UIMM8()                   | REG0=MMX_R():rw:q MEM0:r:q IMM0:r:b
 | PALIGNR              | MMX            | SSSE3          | SSSE3MMX       | 0f 3a 0f     | 0b11       | rrr        | 0x0F 0x3A 0x0F no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  UIMM8()                          | REG0=MMX_R():rw:q REG1=MMX_B():r:q IMM0:r:b
 | PALIGNR              | SSE            | SSSE3          |                | 0f 3a 0f     | mm         | rrr        | 0x0F 0x3A 0x0F osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM() UIMM8()     | REG0=XMM_R():rw:dq MEM0:r:dq IMM0:r:b
 | PALIGNR              | SSE            | SSSE3          |                | 0f 3a 0f     | 0b11       | rrr        | 0x0F 0x3A 0x0F osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66() UIMM8()            | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq IMM0:r:b
 | PAND                 | LOGICAL        | MMX            | PENTIUMMMX     | 0f db        | mm         | rrr        | 0x0F 0xDB no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q MEM0:r:q
 | PAND                 | LOGICAL        | MMX            | PENTIUMMMX     | 0f db        | 0b11       | rrr        | 0x0F 0xDB no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PAND                 | LOGICAL        | SSE2           |                | 0f db        | mm         | rrr        | 0x0F 0xDB osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PAND                 | LOGICAL        | SSE2           |                | 0f db        | 0b11       | rrr        | 0x0F 0xDB osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PANDN                | LOGICAL        | MMX            | PENTIUMMMX     | 0f df        | mm         | rrr        | 0x0F 0xDF no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q MEM0:r:q
 | PANDN                | LOGICAL        | MMX            | PENTIUMMMX     | 0f df        | 0b11       | rrr        | 0x0F 0xDF no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PANDN                | LOGICAL        | SSE2           |                | 0f df        | mm         | rrr        | 0x0F 0xDF osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PANDN                | LOGICAL        | SSE2           |                | 0f df        | 0b11       | rrr        | 0x0F 0xDF osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PAUSE                | MISC           | PAUSE          | PAUSE          |              |            |            | 0b1001_0 SRM[0b000] SRM=0  refining_f3 P4=1                                                          | 
 | PAVGB                | MMX            | MMX            | PENTIUMMMX     | 0f e0        | mm         | rrr        | 0x0F 0xE0 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q MEM0:r:q
 | PAVGB                | MMX            | MMX            | PENTIUMMMX     | 0f e0        | 0b11       | rrr        | 0x0F 0xE0 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q:i8 REG1=MMX_B():r:q:i8
 | PAVGB                | SSE            | SSE2           |                | 0f e0        | mm         | rrr        | 0x0F 0xE0 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq:u8 MEM0:r:dq:u8
 | PAVGB                | SSE            | SSE2           |                | 0f e0        | 0b11       | rrr        | 0x0F 0xE0 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq:u8 REG1=XMM_B():r:dq:u8
 | PAVGUSB              | 3DNOW          | 3DNOW          |                | 0f 0f bf     | mm         | rrr        | 0x0F 0x0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() 0xBF                                               | REG0=MMX_R():rw:q MEM0:r:q
 | PAVGUSB              | 3DNOW          | 3DNOW          |                | 0f 0f bf     | 0b11       | rrr        | 0x0F 0x0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] 0xBF                                                      | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PAVGW                | MMX            | MMX            | PENTIUMMMX     | 0f e3        | mm         | rrr        | 0x0F 0xE3 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q:i16 MEM0:r:q:i16
 | PAVGW                | MMX            | MMX            | PENTIUMMMX     | 0f e3        | 0b11       | rrr        | 0x0F 0xE3 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q:i16 REG1=MMX_B():r:q:i16
 | PAVGW                | SSE            | SSE2           |                | 0f e3        | mm         | rrr        | 0x0F 0xE3 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq:u16 MEM0:r:dq:u16
 | PAVGW                | SSE            | SSE2           |                | 0f e3        | 0b11       | rrr        | 0x0F 0xE3 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq:u16 REG1=XMM_B():r:dq:u16
 | PBLENDVB             | SSE            | SSE4           |                | 0f 38 10     | mm         | rrr        | 0x0F 0x38 0x10 osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()              | REG0=XMM_R():rw:dq MEM0:r:dq REG1=XED_REG_XMM0:r:dq:SUPP
 | PBLENDVB             | SSE            | SSE4           |                | 0f 38 10     | 0b11       | rrr        | 0x0F 0x38 0x10 osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn]                     | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq REG2=XED_REG_XMM0:r:dq:SUPP
 | PBLENDW              | SSE            | SSE4           |                | 0f 3a 0e     | mm         | rrr        | 0x0F 0x3A 0x0E osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()      | REG0=XMM_R():rw:dq MEM0:r:dq IMM0:r:b
 | PBLENDW              | SSE            | SSE4           |                | 0f 3a 0e     | 0b11       | rrr        | 0x0F 0x3A 0x0E osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()             | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq IMM0:r:b
 | PCLMULQDQ            | PCLMULQDQ      | PCLMULQDQ      |                | 0f 3a 44     | 0b11       | rrr        | 0x0F 0x3A 0x44 osz_refining_prefix MOD[0b11] MOD=3  REG[rrr] RM[nnn]  REFINING66()  UIMM8()          | REG0=XMM_R():rw:dq  REG1=XMM_B():r:dq IMM0:r:b
 | PCLMULQDQ            | PCLMULQDQ      | PCLMULQDQ      |                | 0f 3a 44     | mm         | rrr        | 0x0F 0x3A 0x44 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM() UIMM8()     | REG0=XMM_R():rw:dq  MEM0:r:dq IMM0:r:b
 | PCMPEQB              | MMX            | MMX            | PENTIUMMMX     | 0f 74        | mm         | rrr        | 0x0F 0x74 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q:i8 MEM0:r:q:i8
 | PCMPEQB              | MMX            | MMX            | PENTIUMMMX     | 0f 74        | 0b11       | rrr        | 0x0F 0x74 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q:i8 REG1=MMX_B():r:q:i8
 | PCMPEQB              | SSE            | SSE2           |                | 0f 74        | mm         | rrr        | 0x0F 0x74 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq:i8 MEM0:r:dq:i8
 | PCMPEQB              | SSE            | SSE2           |                | 0f 74        | 0b11       | rrr        | 0x0F 0x74 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq:i8 REG1=XMM_B():r:dq:i8
 | PCMPEQD              | MMX            | MMX            | PENTIUMMMX     | 0f 76        | mm         | rrr        | 0x0F 0x76 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q:i32 MEM0:r:q:i32
 | PCMPEQD              | MMX            | MMX            | PENTIUMMMX     | 0f 76        | 0b11       | rrr        | 0x0F 0x76 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q:i32 REG1=MMX_B():r:q:i32
 | PCMPEQD              | SSE            | SSE2           |                | 0f 76        | mm         | rrr        | 0x0F 0x76 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq:i32 MEM0:r:dq:i32
 | PCMPEQD              | SSE            | SSE2           |                | 0f 76        | 0b11       | rrr        | 0x0F 0x76 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq:i32 REG1=XMM_B():r:dq:i32
 | PCMPEQQ              | SSE            | SSE4           |                | 0f 38 29     | mm         | rrr        | 0x0F 0x38 0x29 osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()              | REG0=XMM_R():rw:dq MEM0:r:dq
 | PCMPEQQ              | SSE            | SSE4           |                | 0f 38 29     | 0b11       | rrr        | 0x0F 0x38 0x29 osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn]                     | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PCMPEQW              | MMX            | MMX            | PENTIUMMMX     | 0f 75        | mm         | rrr        | 0x0F 0x75 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q:i16 MEM0:r:q:i16
 | PCMPEQW              | MMX            | MMX            | PENTIUMMMX     | 0f 75        | 0b11       | rrr        | 0x0F 0x75 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q:i16 REG1=MMX_B():r:q:i16
 | PCMPEQW              | SSE            | SSE2           |                | 0f 75        | mm         | rrr        | 0x0F 0x75 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq:i16 MEM0:r:dq:i16
 | PCMPEQW              | SSE            | SSE2           |                | 0f 75        | 0b11       | rrr        | 0x0F 0x75 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq:i16 REG1=XMM_B():r:dq:i16
 | PCMPESTRI            | SSE            | SSE4           | SSE42          | 0f 3a 61     | mm         | rrr        | 0x0F 0x3A 0x61 osz_refining_prefix IMMUNE66() not64 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()  | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_EAX:r:SUPP REG2=XED_REG_EDX:r:SUPP REG3=XED_REG_ECX:w:SUPP
 | PCMPESTRI            | SSE            | SSE4           | SSE42          | 0f 3a 61     | 0b11       | rrr        | 0x0F 0x3A 0x61 osz_refining_prefix IMMUNE66() not64  MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()        | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_EAX:r:SUPP REG3=XED_REG_EDX:r:SUPP REG4=XED_REG_ECX:w:SUPP
 | PCMPESTRI            | SSE            | SSE4           | SSE42          | 0f 3a 61     | mm         | rrr        | 0x0F 0x3A 0x61 osz_refining_prefix IMMUNE66() mode64 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_EAX:r:SUPP REG2=XED_REG_EDX:r:SUPP REG3=XED_REG_ECX:w:SUPP
 | PCMPESTRI            | SSE            | SSE4           | SSE42          | 0f 3a 61     | 0b11       | rrr        | 0x0F 0x3A 0x61 osz_refining_prefix IMMUNE66() mode64 norexw_prefix  MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8() | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_EAX:r:SUPP REG3=XED_REG_EDX:r:SUPP REG4=XED_REG_ECX:w:SUPP
 | PCMPESTRI64          | SSE            | SSE4           | SSE42          | 0f 3a 61     | mm         | rrr        | 0x0F 0x3A 0x61 osz_refining_prefix IMMUNE66() mode64 rexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_RAX:r:SUPP REG2=XED_REG_RDX:r:SUPP REG3=XED_REG_RCX:w:SUPP
 | PCMPESTRI64          | SSE            | SSE4           | SSE42          | 0f 3a 61     | 0b11       | rrr        | 0x0F 0x3A 0x61 osz_refining_prefix IMMUNE66() mode64 rexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8() | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_RAX:r:SUPP REG3=XED_REG_RDX:r:SUPP REG4=XED_REG_RCX:w:SUPP
 | PCMPESTRM            | SSE            | SSE4           | SSE42          | 0f 3a 60     | mm         | rrr        | 0x0F 0x3A 0x60 osz_refining_prefix IMMUNE66() not64 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()  | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_EAX:r:SUPP REG2=XED_REG_EDX:r:SUPP REG3=XED_REG_XMM0:w:dq:SUPP
 | PCMPESTRM            | SSE            | SSE4           | SSE42          | 0f 3a 60     | 0b11       | rrr        | 0x0F 0x3A 0x60 osz_refining_prefix IMMUNE66() not64 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()         | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_EAX:r:SUPP REG3=XED_REG_EDX:r:SUPP REG4=XED_REG_XMM0:w:dq:SUPP
 | PCMPESTRM            | SSE            | SSE4           | SSE42          | 0f 3a 60     | mm         | rrr        | 0x0F 0x3A 0x60 osz_refining_prefix IMMUNE66() mode64 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_EAX:r:SUPP REG2=XED_REG_EDX:r:SUPP REG3=XED_REG_XMM0:w:dq:SUPP
 | PCMPESTRM            | SSE            | SSE4           | SSE42          | 0f 3a 60     | 0b11       | rrr        | 0x0F 0x3A 0x60 osz_refining_prefix IMMUNE66() mode64 norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8() | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_EAX:r:SUPP REG3=XED_REG_EDX:r:SUPP REG4=XED_REG_XMM0:w:dq:SUPP
 | PCMPESTRM64          | SSE            | SSE4           | SSE42          | 0f 3a 60     | mm         | rrr        | 0x0F 0x3A 0x60 osz_refining_prefix IMMUNE66() mode64 rexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_RAX:r:SUPP REG2=XED_REG_RDX:r:SUPP REG3=XED_REG_XMM0:w:dq:SUPP
 | PCMPESTRM64          | SSE            | SSE4           | SSE42          | 0f 3a 60     | 0b11       | rrr        | 0x0F 0x3A 0x60 osz_refining_prefix IMMUNE66() mode64 rexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8() | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_RAX:r:SUPP REG3=XED_REG_RDX:r:SUPP REG4=XED_REG_XMM0:w:dq:SUPP
 | PCMPGTB              | MMX            | MMX            | PENTIUMMMX     | 0f 64        | mm         | rrr        | 0x0F 0x64 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q:i8 MEM0:r:q:i8
 | PCMPGTB              | MMX            | MMX            | PENTIUMMMX     | 0f 64        | 0b11       | rrr        | 0x0F 0x64 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q:i8 REG1=MMX_B():r:q:i8
 | PCMPGTB              | SSE            | SSE2           |                | 0f 64        | mm         | rrr        | 0x0F 0x64 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq:i8 MEM0:r:dq:i8
 | PCMPGTB              | SSE            | SSE2           |                | 0f 64        | 0b11       | rrr        | 0x0F 0x64 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq:i8 REG1=XMM_B():r:dq:i8
 | PCMPGTD              | MMX            | MMX            | PENTIUMMMX     | 0f 66        | mm         | rrr        | 0x0F 0x66 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q:i32 MEM0:r:q:i32
 | PCMPGTD              | MMX            | MMX            | PENTIUMMMX     | 0f 66        | 0b11       | rrr        | 0x0F 0x66 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q:i32 REG1=MMX_B():r:q:i32
 | PCMPGTD              | SSE            | SSE2           |                | 0f 66        | mm         | rrr        | 0x0F 0x66 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq:i32 MEM0:r:dq:i32
 | PCMPGTD              | SSE            | SSE2           |                | 0f 66        | 0b11       | rrr        | 0x0F 0x66 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq:i32 REG1=XMM_B():r:dq:i32
 | PCMPGTQ              | SSE            | SSE4           | SSE42          | 0f 38 37     | mm         | rrr        | 0x0F 0x38 0x37  osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()            | REG0=XMM_R():rw:dq     MEM0:r:dq
 | PCMPGTQ              | SSE            | SSE4           | SSE42          | 0f 38 37     | 0b11       | rrr        | 0x0F 0x38 0x37  osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                   | REG0=XMM_R():rw:dq     REG1=XMM_B():r:dq
 | PCMPGTW              | MMX            | MMX            | PENTIUMMMX     | 0f 65        | mm         | rrr        | 0x0F 0x65 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q:i16 MEM0:r:q:i16
 | PCMPGTW              | MMX            | MMX            | PENTIUMMMX     | 0f 65        | 0b11       | rrr        | 0x0F 0x65 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q:i16 REG1=MMX_B():r:q:i16
 | PCMPGTW              | SSE            | SSE2           |                | 0f 65        | mm         | rrr        | 0x0F 0x65 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq:i16 MEM0:r:dq:i16
 | PCMPGTW              | SSE            | SSE2           |                | 0f 65        | 0b11       | rrr        | 0x0F 0x65 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq:i16 REG1=XMM_B():r:dq:i16
 | PCMPISTRI            | SSE            | SSE4           | SSE42          | 0f 3a 63     | mm         | rrr        | 0x0F 0x3A 0x63 osz_refining_prefix IMMUNE66() not64 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()  | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_ECX:w:SUPP
 | PCMPISTRI            | SSE            | SSE4           | SSE42          | 0f 3a 63     | 0b11       | rrr        | 0x0F 0x3A 0x63 osz_refining_prefix IMMUNE66() not64 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()         | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_ECX:w:SUPP
 | PCMPISTRI            | SSE            | SSE4           | SSE42          | 0f 3a 63     | mm         | rrr        | 0x0F 0x3A 0x63 osz_refining_prefix IMMUNE66() mode64 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_ECX:w:SUPP
 | PCMPISTRI            | SSE            | SSE4           | SSE42          | 0f 3a 63     | 0b11       | rrr        | 0x0F 0x3A 0x63 osz_refining_prefix IMMUNE66() mode64 norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8() | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_ECX:w:SUPP
 | PCMPISTRI64          | SSE            | SSE4           | SSE42          | 0f 3a 63     | mm         | rrr        | 0x0F 0x3A 0x63 osz_refining_prefix IMMUNE66() mode64 rexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_RCX:w:SUPP
 | PCMPISTRI64          | SSE            | SSE4           | SSE42          | 0f 3a 63     | 0b11       | rrr        | 0x0F 0x3A 0x63 osz_refining_prefix IMMUNE66() mode64 rexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8() | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_RCX:w:SUPP
 | PCMPISTRM            | SSE            | SSE4           | SSE42          | 0f 3a 62     | mm         | rrr        | 0x0F 0x3A 0x62 osz_refining_prefix IMMUNE66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()        | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_XMM0:w:dq:SUPP
 | PCMPISTRM            | SSE            | SSE4           | SSE42          | 0f 3a 62     | 0b11       | rrr        | 0x0F 0x3A 0x62 osz_refining_prefix IMMUNE66() MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()               | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_XMM0:w:dq:SUPP
 | PCONFIG              | PCONFIG        | PCONFIG        | PCONFIG        | 0f 01        | 0b11       | 0b000      | 0x0F 0x01 MOD[0b11] MOD=3  REG[0b000] RM[0b101]  no_refining_prefix                                  | REG0=XED_REG_EAX:rw:SUPP:d:u32 REG1=XED_REG_EBX:crw:SUPP:d:u32 REG2=XED_REG_ECX:crw:SUPP:d:u32 REG3=XED_REG_EDX:crw:SUPP:d:u32
 | PDEP                 | BMI2           | BMI2           |                | f5           | mm         | rrr        | VV1 0xF5 V0F38 VF2 not64 VL128 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=VGPR32_R():w:d REG1=VGPR32_N():r:d MEM0:r:d
 | PDEP                 | BMI2           | BMI2           |                | f5           | mm         | rrr        | VV1 0xF5 V0F38 VF2 W0 mode64 VL128 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                           | REG0=VGPR32_R():w:d REG1=VGPR32_N():r:d MEM0:r:d
 | PDEP                 | BMI2           | BMI2           |                | f5           | 0b11       | rrr        | VV1 0xF5 V0F38 VF2 not64 VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=VGPR32_R():w:d REG1=VGPR32_N():r:d REG2=VGPR32_B():r:d
 | PDEP                 | BMI2           | BMI2           |                | f5           | 0b11       | rrr        | VV1 0xF5 V0F38 VF2 W0 mode64 VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                  | REG0=VGPR32_R():w:d REG1=VGPR32_N():r:d REG2=VGPR32_B():r:d
 | PDEP                 | BMI2           | BMI2           |                | f5           | mm         | rrr        | VV1 0xF5 V0F38 VF2 W1 VL128 mode64 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                           | REG0=VGPR64_R():w:q REG1=VGPR64_N():r:q MEM0:r:q
 | PDEP                 | BMI2           | BMI2           |                | f5           | 0b11       | rrr        | VV1 0xF5 V0F38 VF2 W1 VL128 mode64 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                  | REG0=VGPR64_R():w:q REG1=VGPR64_N():r:q REG2=VGPR64_B():r:q
 | PEXT                 | BMI2           | BMI2           |                | f5           | mm         | rrr        | VV1 0xF5 V0F38 VF3 not64 VL128 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=VGPR32_R():w:d REG1=VGPR32_N():r:d MEM0:r:d
 | PEXT                 | BMI2           | BMI2           |                | f5           | mm         | rrr        | VV1 0xF5 V0F38 VF3 W0 mode64 VL128 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                           | REG0=VGPR32_R():w:d REG1=VGPR32_N():r:d MEM0:r:d
 | PEXT                 | BMI2           | BMI2           |                | f5           | 0b11       | rrr        | VV1 0xF5 V0F38 VF3 not64 VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=VGPR32_R():w:d REG1=VGPR32_N():r:d REG2=VGPR32_B():r:d
 | PEXT                 | BMI2           | BMI2           |                | f5           | 0b11       | rrr        | VV1 0xF5 V0F38 VF3 W0 mode64 VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                  | REG0=VGPR32_R():w:d REG1=VGPR32_N():r:d REG2=VGPR32_B():r:d
 | PEXT                 | BMI2           | BMI2           |                | f5           | mm         | rrr        | VV1 0xF5 V0F38 VF3 W1 VL128 mode64 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                           | REG0=VGPR64_R():w:q REG1=VGPR64_N():r:q MEM0:r:q
 | PEXT                 | BMI2           | BMI2           |                | f5           | 0b11       | rrr        | VV1 0xF5 V0F38 VF3 W1 VL128 mode64 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                  | REG0=VGPR64_R():w:q REG1=VGPR64_N():r:q REG2=VGPR64_B():r:q
 | PEXTRB               | SSE            | SSE4           |                | 0f 3a 14     | mm         | rrr        | 0x0F 0x3A 0x14 osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()      | MEM0:w:b           REG0=XMM_R():r:dq IMM0:r:b
 | PEXTRB               | SSE            | SSE4           |                | 0f 3a 14     | 0b11       | rrr        | 0x0F 0x3A 0x14 osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn]  UIMM8()            | REG0=GPR32_B():w:d REG1=XMM_R():r:dq IMM0:r:b
 | PEXTRD               | SSE            | SSE4           |                | 0f 3a 16     | mm         | rrr        | 0x0F 0x3A 0x16 osz_refining_prefix REFINING66() norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() | MEM0:w:d           REG0=XMM_R():r:dq IMM0:r:b
 | PEXTRD               | SSE            | SSE4           |                | 0f 3a 16     | 0b11       | rrr        | 0x0F 0x3A 0x16 osz_refining_prefix REFINING66() norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  UIMM8() | REG0=GPR32_B():w:d REG1=XMM_R():r:dq IMM0:r:b
 | PEXTRQ               | SSE            | SSE4           |                | 0f 3a 16     | mm         | rrr        | 0x0F 0x3A 0x16 osz_refining_prefix REFINING66() rexw_prefix mode64 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() | MEM0:w:q           REG0=XMM_R():r:dq IMM0:r:b
 | PEXTRQ               | SSE            | SSE4           |                | 0f 3a 16     | 0b11       | rrr        | 0x0F 0x3A 0x16 osz_refining_prefix REFINING66() rexw_prefix mode64 MOD[0b11] MOD=3 REG[rrr] RM[nnn]  UIMM8() | REG0=GPR64_B():w:q REG1=XMM_R():r:dq IMM0:r:b
 | PEXTRW               | MMX            | MMX            | PENTIUMMMX     | 0f c5        | 0b11       | rrr        | 0x0F 0xC5 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  UIMM8()                               | REG0=GPR32_R():w REG1=MMX_B():r:q:u16 IMM0:r:b
 | PEXTRW               | SSE            | SSE2           |                | 0f c5        | 0b11       | rrr        | 0x0F 0xC5 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66() UIMM8()                 | REG0=GPR32_R():w REG1=XMM_B():r:dq IMM0:r:b
 | PEXTRW_SSE4          | SSE            | SSE4           |                | 0f 3a 15     | mm         | rrr        | 0x0F 0x3A 0x15 osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()      | MEM0:w:w           REG0=XMM_R():r:dq IMM0:r:b
 | PEXTRW_SSE4          | SSE            | SSE4           |                | 0f 3a 15     | 0b11       | rrr        | 0x0F 0x3A 0x15 osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn]  UIMM8()            | REG0=GPR32_B():w REG1=XMM_R():r:dq IMM0:r:b
 | PF2ID                | 3DNOW          | 3DNOW          |                | 0f 0f 1d     | mm         | rrr        | 0x0F 0x0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() 0x1D                                               | REG0=MMX_R():rw:q MEM0:r:q
 | PF2ID                | 3DNOW          | 3DNOW          |                | 0f 0f 1d     | 0b11       | rrr        | 0x0F 0x0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] 0x1D                                                      | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PF2IW                | 3DNOW          | 3DNOW          |                | 0f 0f 1c     | mm         | rrr        | 0x0F 0x0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() 0x1C                                               | REG0=MMX_R():rw:q MEM0:r:q
 | PF2IW                | 3DNOW          | 3DNOW          |                | 0f 0f 1c     | 0b11       | rrr        | 0x0F 0x0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] 0x1C                                                      | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PFACC                | 3DNOW          | 3DNOW          |                | 0f 0f ae     | mm         | rrr        | 0x0F 0x0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() 0xAE                                               | REG0=MMX_R():rw:q MEM0:r:q
 | PFACC                | 3DNOW          | 3DNOW          |                | 0f 0f ae     | 0b11       | rrr        | 0x0F 0x0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] 0xAE                                                      | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PFADD                | 3DNOW          | 3DNOW          |                | 0f 0f 9e     | mm         | rrr        | 0x0F 0x0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() 0x9E                                               | REG0=MMX_R():rw:q MEM0:r:q
 | PFADD                | 3DNOW          | 3DNOW          |                | 0f 0f 9e     | 0b11       | rrr        | 0x0F 0x0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] 0x9E                                                      | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PFCMPEQ              | 3DNOW          | 3DNOW          |                | 0f 0f b0     | mm         | rrr        | 0x0F 0x0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() 0xB0                                               | REG0=MMX_R():rw:q MEM0:r:q
 | PFCMPEQ              | 3DNOW          | 3DNOW          |                | 0f 0f b0     | 0b11       | rrr        | 0x0F 0x0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] 0xB0                                                      | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PFCMPGE              | 3DNOW          | 3DNOW          |                | 0f 0f 90     | mm         | rrr        | 0x0F 0x0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() 0x90                                               | REG0=MMX_R():rw:q MEM0:r:q
 | PFCMPGE              | 3DNOW          | 3DNOW          |                | 0f 0f 90     | 0b11       | rrr        | 0x0F 0x0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] 0x90                                                      | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PFCMPGT              | 3DNOW          | 3DNOW          |                | 0f 0f a0     | mm         | rrr        | 0x0F 0x0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() 0xA0                                               | REG0=MMX_R():rw:q MEM0:r:q
 | PFCMPGT              | 3DNOW          | 3DNOW          |                | 0f 0f a0     | 0b11       | rrr        | 0x0F 0x0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] 0xA0                                                      | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PFMAX                | 3DNOW          | 3DNOW          |                | 0f 0f a4     | mm         | rrr        | 0x0F 0x0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() 0xA4                                               | REG0=MMX_R():rw:q MEM0:r:q
 | PFMAX                | 3DNOW          | 3DNOW          |                | 0f 0f a4     | 0b11       | rrr        | 0x0F 0x0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] 0xA4                                                      | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PFMIN                | 3DNOW          | 3DNOW          |                | 0f 0f 94     | mm         | rrr        | 0x0F 0x0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() 0x94                                               | REG0=MMX_R():rw:q MEM0:r:q
 | PFMIN                | 3DNOW          | 3DNOW          |                | 0f 0f 94     | 0b11       | rrr        | 0x0F 0x0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] 0x94                                                      | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PFMUL                | 3DNOW          | 3DNOW          |                | 0f 0f b4     | mm         | rrr        | 0x0F 0x0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() 0xB4                                               | REG0=MMX_R():rw:q MEM0:r:q
 | PFMUL                | 3DNOW          | 3DNOW          |                | 0f 0f b4     | 0b11       | rrr        | 0x0F 0x0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] 0xB4                                                      | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PFNACC               | 3DNOW          | 3DNOW          |                | 0f 0f 8a     | mm         | rrr        | 0x0F 0x0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() 0x8A                                               | REG0=MMX_R():rw:q MEM0:r:q
 | PFNACC               | 3DNOW          | 3DNOW          |                | 0f 0f 8a     | 0b11       | rrr        | 0x0F 0x0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] 0x8A                                                      | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PFPNACC              | 3DNOW          | 3DNOW          |                | 0f 0f 8e     | mm         | rrr        | 0x0F 0x0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() 0x8E                                               | REG0=MMX_R():rw:q MEM0:r:q
 | PFPNACC              | 3DNOW          | 3DNOW          |                | 0f 0f 8e     | 0b11       | rrr        | 0x0F 0x0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] 0x8E                                                      | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PFRCP                | 3DNOW          | 3DNOW          |                | 0f 0f 96     | mm         | rrr        | 0x0F 0x0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() 0x96                                               | REG0=MMX_R():rw:q MEM0:r:q
 | PFRCP                | 3DNOW          | 3DNOW          |                | 0f 0f 96     | 0b11       | rrr        | 0x0F 0x0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] 0x96                                                      | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PFRCPIT1             | 3DNOW          | 3DNOW          |                | 0f 0f a6     | mm         | rrr        | 0x0F 0x0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() 0xA6                                               | REG0=MMX_R():rw:q MEM0:r:q
 | PFRCPIT1             | 3DNOW          | 3DNOW          |                | 0f 0f a6     | 0b11       | rrr        | 0x0F 0x0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] 0xA6                                                      | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PFRCPIT2             | 3DNOW          | 3DNOW          |                | 0f 0f b6     | mm         | rrr        | 0x0F 0x0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() 0xB6                                               | REG0=MMX_R():rw:q MEM0:r:q
 | PFRCPIT2             | 3DNOW          | 3DNOW          |                | 0f 0f b6     | 0b11       | rrr        | 0x0F 0x0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] 0xB6                                                      | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PFRSQIT1             | 3DNOW          | 3DNOW          |                | 0f 0f a7     | mm         | rrr        | 0x0F 0x0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() 0xA7                                               | REG0=MMX_R():rw:q MEM0:r:q
 | PFRSQIT1             | 3DNOW          | 3DNOW          |                | 0f 0f a7     | 0b11       | rrr        | 0x0F 0x0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] 0xA7                                                      | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PFRSQRT              | 3DNOW          | 3DNOW          |                | 0f 0f 97     | mm         | rrr        | 0x0F 0x0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() 0x97                                               | REG0=MMX_R():rw:q MEM0:r:q
 | PFRSQRT              | 3DNOW          | 3DNOW          |                | 0f 0f 97     | 0b11       | rrr        | 0x0F 0x0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] 0x97                                                      | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PFSUB                | 3DNOW          | 3DNOW          |                | 0f 0f 9a     | mm         | rrr        | 0x0F 0x0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() 0x9A                                               | REG0=MMX_R():rw:q MEM0:r:q
 | PFSUB                | 3DNOW          | 3DNOW          |                | 0f 0f 9a     | 0b11       | rrr        | 0x0F 0x0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] 0x9A                                                      | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PFSUBR               | 3DNOW          | 3DNOW          |                | 0f 0f aa     | mm         | rrr        | 0x0F 0x0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() 0xAA                                               | REG0=MMX_R():rw:q MEM0:r:q
 | PFSUBR               | 3DNOW          | 3DNOW          |                | 0f 0f aa     | 0b11       | rrr        | 0x0F 0x0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] 0xAA                                                      | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PHADDD               | MMX            | SSSE3          | SSSE3MMX       | 0f 38 02     | mm         | rrr        | 0x0F 0x38 0x02 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                           | REG0=MMX_R():rw:q MEM0:r:q
 | PHADDD               | MMX            | SSSE3          | SSSE3MMX       | 0f 38 02     | 0b11       | rrr        | 0x0F 0x38 0x02 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                   | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PHADDD               | SSE            | SSSE3          |                | 0f 38 02     | mm         | rrr        | 0x0F 0x38 0x02 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()             | REG0=XMM_R():rw:dq MEM0:r:dq
 | PHADDD               | SSE            | SSSE3          |                | 0f 38 02     | 0b11       | rrr        | 0x0F 0x38 0x02 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                    | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PHADDSW              | MMX            | SSSE3          | SSSE3MMX       | 0f 38 03     | mm         | rrr        | 0x0F 0x38 0x03 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                           | REG0=MMX_R():rw:q MEM0:r:q
 | PHADDSW              | MMX            | SSSE3          | SSSE3MMX       | 0f 38 03     | 0b11       | rrr        | 0x0F 0x38 0x03 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                   | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PHADDSW              | SSE            | SSSE3          |                | 0f 38 03     | mm         | rrr        | 0x0F 0x38 0x03 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()             | REG0=XMM_R():rw:dq MEM0:r:dq
 | PHADDSW              | SSE            | SSSE3          |                | 0f 38 03     | 0b11       | rrr        | 0x0F 0x38 0x03 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                    | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PHADDW               | MMX            | SSSE3          | SSSE3MMX       | 0f 38 01     | mm         | rrr        | 0x0F 0x38 0x01 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                           | REG0=MMX_R():rw:q MEM0:r:q
 | PHADDW               | MMX            | SSSE3          | SSSE3MMX       | 0f 38 01     | 0b11       | rrr        | 0x0F 0x38 0x01 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                   | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PHADDW               | SSE            | SSSE3          |                | 0f 38 01     | mm         | rrr        | 0x0F 0x38 0x01 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()             | REG0=XMM_R():rw:dq MEM0:r:dq
 | PHADDW               | SSE            | SSSE3          |                | 0f 38 01     | 0b11       | rrr        | 0x0F 0x38 0x01 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                    | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PHMINPOSUW           | SSE            | SSE4           |                | 0f 38 41     | mm         | rrr        | 0x0F 0x38 0x41 osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()              | REG0=XMM_R():w:dq   MEM0:r:dq
 | PHMINPOSUW           | SSE            | SSE4           |                | 0f 38 41     | 0b11       | rrr        | 0x0F 0x38 0x41 osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn]                     | REG0=XMM_R():w:dq   REG1=XMM_B():r:dq
 | PHSUBD               | MMX            | SSSE3          | SSSE3MMX       | 0f 38 06     | mm         | rrr        | 0x0F 0x38 0x06 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                           | REG0=MMX_R():rw:q MEM0:r:q
 | PHSUBD               | MMX            | SSSE3          | SSSE3MMX       | 0f 38 06     | 0b11       | rrr        | 0x0F 0x38 0x06 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                   | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PHSUBD               | SSE            | SSSE3          |                | 0f 38 06     | mm         | rrr        | 0x0F 0x38 0x06 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()             | REG0=XMM_R():rw:dq MEM0:r:dq
 | PHSUBD               | SSE            | SSSE3          |                | 0f 38 06     | 0b11       | rrr        | 0x0F 0x38 0x06 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                    | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PHSUBSW              | MMX            | SSSE3          | SSSE3MMX       | 0f 38 07     | mm         | rrr        | 0x0F 0x38 0x07 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                           | REG0=MMX_R():rw:q MEM0:r:q
 | PHSUBSW              | MMX            | SSSE3          | SSSE3MMX       | 0f 38 07     | 0b11       | rrr        | 0x0F 0x38 0x07 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                   | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PHSUBSW              | SSE            | SSSE3          |                | 0f 38 07     | mm         | rrr        | 0x0F 0x38 0x07 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()             | REG0=XMM_R():rw:dq MEM0:r:dq
 | PHSUBSW              | SSE            | SSSE3          |                | 0f 38 07     | 0b11       | rrr        | 0x0F 0x38 0x07 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                    | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PHSUBW               | MMX            | SSSE3          | SSSE3MMX       | 0f 38 05     | mm         | rrr        | 0x0F 0x38 0x05 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                           | REG0=MMX_R():rw:q MEM0:r:q
 | PHSUBW               | MMX            | SSSE3          | SSSE3MMX       | 0f 38 05     | 0b11       | rrr        | 0x0F 0x38 0x05 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                   | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PHSUBW               | SSE            | SSSE3          |                | 0f 38 05     | mm         | rrr        | 0x0F 0x38 0x05 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()             | REG0=XMM_R():rw:dq MEM0:r:dq
 | PHSUBW               | SSE            | SSSE3          |                | 0f 38 05     | 0b11       | rrr        | 0x0F 0x38 0x05 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                    | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PI2FD                | 3DNOW          | 3DNOW          |                | 0f 0f 0d     | mm         | rrr        | 0x0F 0x0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() 0x0D                                               | REG0=MMX_R():rw:q MEM0:r:q
 | PI2FD                | 3DNOW          | 3DNOW          |                | 0f 0f 0d     | 0b11       | rrr        | 0x0F 0x0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] 0x0D                                                      | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PI2FW                | 3DNOW          | 3DNOW          |                | 0f 0f 0c     | mm         | rrr        | 0x0F 0x0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() 0x0C                                               | REG0=MMX_R():rw:q MEM0:r:q
 | PI2FW                | 3DNOW          | 3DNOW          |                | 0f 0f 0c     | 0b11       | rrr        | 0x0F 0x0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] 0x0C                                                      | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PINSRB               | SSE            | SSE4           |                | 0f 3a 20     | mm         | rrr        | 0x0F 0x3A 0x20 osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()      | REG0=XMM_R():rw:dq MEM0:r:b            IMM0:r:b
 | PINSRB               | SSE            | SSE4           |                | 0f 3a 20     | 0b11       | rrr        | 0x0F 0x3A 0x20 osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn]  UIMM8()            | REG0=XMM_R():rw:dq REG1=GPR32_B():r:d  IMM0:r:b
 | PINSRD               | SSE            | SSE4           |                | 0f 3a 22     | mm         | rrr        | 0x0F 0x3A 0x22 osz_refining_prefix REFINING66() norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() | REG0=XMM_R():rw:dq MEM0:r:d            IMM0:r:b
 | PINSRD               | SSE            | SSE4           |                | 0f 3a 22     | 0b11       | rrr        | 0x0F 0x3A 0x22 osz_refining_prefix REFINING66() norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  UIMM8() | REG0=XMM_R():rw:dq REG1=GPR32_B():r:d  IMM0:r:b
 | PINSRQ               | SSE            | SSE4           |                | 0f 3a 22     | mm         | rrr        | 0x0F 0x3A 0x22 osz_refining_prefix REFINING66() rexw_prefix mode64 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() | REG0=XMM_R():rw:dq MEM0:r:q            IMM0:r:b
 | PINSRQ               | SSE            | SSE4           |                | 0f 3a 22     | 0b11       | rrr        | 0x0F 0x3A 0x22 osz_refining_prefix REFINING66() rexw_prefix mode64 MOD[0b11] MOD=3 REG[rrr] RM[nnn]  UIMM8() | REG0=XMM_R():rw:dq REG1=GPR64_B():r:q  IMM0:r:b
 | PINSRW               | MMX            | MMX            | PENTIUMMMX     | 0f c4        | mm         | rrr        | 0x0F 0xC4 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM() UIMM8()                        | REG0=MMX_R():rw:q:u16 MEM0:r:w:u16 IMM0:r:b
 | PINSRW               | MMX            | MMX            | PENTIUMMMX     | 0f c4        | 0b11       | rrr        | 0x0F 0xC4 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  UIMM8()                               | REG0=MMX_R():rw:q:u16 REG1=GPR32_B():r IMM0:r:b
 | PINSRW               | SSE            | SSE2           |                | 0f c4        | mm         | rrr        | 0x0F 0xC4 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM() UIMM8()          | REG0=XMM_R():rw:dq MEM0:r:w IMM0:r:b
 | PINSRW               | SSE            | SSE2           |                | 0f c4        | 0b11       | rrr        | 0x0F 0xC4 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66() UIMM8()                 | REG0=XMM_R():rw:dq REG1=GPR32_B():r IMM0:r:b
 | PMADDUBSW            | MMX            | SSSE3          | SSSE3MMX       | 0f 38 04     | mm         | rrr        | 0x0F 0x38 0x04 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                           | REG0=MMX_R():rw:q:i8 MEM0:r:q:i8
 | PMADDUBSW            | MMX            | SSSE3          | SSSE3MMX       | 0f 38 04     | 0b11       | rrr        | 0x0F 0x38 0x04 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                   | REG0=MMX_R():rw:q:i8 REG1=MMX_B():r:q:i8
 | PMADDUBSW            | SSE            | SSSE3          |                | 0f 38 04     | mm         | rrr        | 0x0F 0x38 0x04 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()             | REG0=XMM_R():rw:dq:i8 MEM0:r:dq:i8
 | PMADDUBSW            | SSE            | SSSE3          |                | 0f 38 04     | 0b11       | rrr        | 0x0F 0x38 0x04 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                    | REG0=XMM_R():rw:dq:i8 REG1=XMM_B():r:dq:i8
 | PMADDWD              | MMX            | MMX            | PENTIUMMMX     | 0f f5        | mm         | rrr        | 0x0F 0xF5 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q:i16 MEM0:r:q:i16
 | PMADDWD              | MMX            | MMX            | PENTIUMMMX     | 0f f5        | 0b11       | rrr        | 0x0F 0xF5 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q:i16 REG1=MMX_B():r:q:i16
 | PMADDWD              | SSE            | SSE2           |                | 0f f5        | mm         | rrr        | 0x0F 0xF5 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq:i16 MEM0:r:dq:i16
 | PMADDWD              | SSE            | SSE2           |                | 0f f5        | 0b11       | rrr        | 0x0F 0xF5 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq:i16 REG1=XMM_B():r:dq:i16
 | PMAXSB               | SSE            | SSE4           |                | 0f 38 3c     | mm         | rrr        | 0x0F 0x38 0x3C osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()              | REG0=XMM_R():rw:dq    MEM0:r:dq
 | PMAXSB               | SSE            | SSE4           |                | 0f 38 3c     | 0b11       | rrr        | 0x0F 0x38 0x3C osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn]                     | REG0=XMM_R():rw:dq    REG1=XMM_B():r:dq
 | PMAXSD               | SSE            | SSE4           |                | 0f 38 3d     | mm         | rrr        | 0x0F 0x38 0x3D osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()              | REG0=XMM_R():rw:dq    MEM0:r:dq
 | PMAXSD               | SSE            | SSE4           |                | 0f 38 3d     | 0b11       | rrr        | 0x0F 0x38 0x3D osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn]                     | REG0=XMM_R():rw:dq    REG1=XMM_B():r:dq
 | PMAXSW               | MMX            | MMX            | PENTIUMMMX     | 0f ee        | mm         | rrr        | 0x0F 0xEE no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q MEM0:r:q
 | PMAXSW               | MMX            | MMX            | PENTIUMMMX     | 0f ee        | 0b11       | rrr        | 0x0F 0xEE no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PMAXSW               | SSE            | SSE2           |                | 0f ee        | mm         | rrr        | 0x0F 0xEE osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PMAXSW               | SSE            | SSE2           |                | 0f ee        | 0b11       | rrr        | 0x0F 0xEE osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PMAXUB               | MMX            | MMX            | PENTIUMMMX     | 0f de        | mm         | rrr        | 0x0F 0xDE no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q MEM0:r:q
 | PMAXUB               | MMX            | MMX            | PENTIUMMMX     | 0f de        | 0b11       | rrr        | 0x0F 0xDE no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PMAXUB               | SSE            | SSE2           |                | 0f de        | mm         | rrr        | 0x0F 0xDE osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PMAXUB               | SSE            | SSE2           |                | 0f de        | 0b11       | rrr        | 0x0F 0xDE osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PMAXUD               | SSE            | SSE4           |                | 0f 38 3f     | mm         | rrr        | 0x0F 0x38 0x3F osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()              | REG0=XMM_R():rw:dq    MEM0:r:dq
 | PMAXUD               | SSE            | SSE4           |                | 0f 38 3f     | 0b11       | rrr        | 0x0F 0x38 0x3F osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn]                     | REG0=XMM_R():rw:dq    REG1=XMM_B():r:dq
 | PMAXUW               | SSE            | SSE4           |                | 0f 38 3e     | mm         | rrr        | 0x0F 0x38 0x3E osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()              | REG0=XMM_R():rw:dq    MEM0:r:dq
 | PMAXUW               | SSE            | SSE4           |                | 0f 38 3e     | 0b11       | rrr        | 0x0F 0x38 0x3E osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn]                     | REG0=XMM_R():rw:dq    REG1=XMM_B():r:dq
 | PMINSB               | SSE            | SSE4           |                | 0f 38 38     | mm         | rrr        | 0x0F 0x38 0x38 osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()              | REG0=XMM_R():rw:dq    MEM0:r:dq
 | PMINSB               | SSE            | SSE4           |                | 0f 38 38     | 0b11       | rrr        | 0x0F 0x38 0x38 osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn]                     | REG0=XMM_R():rw:dq    REG1=XMM_B():r:dq
 | PMINSD               | SSE            | SSE4           |                | 0f 38 39     | mm         | rrr        | 0x0F 0x38 0x39 osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()              | REG0=XMM_R():rw:dq    MEM0:r:dq
 | PMINSD               | SSE            | SSE4           |                | 0f 38 39     | 0b11       | rrr        | 0x0F 0x38 0x39 osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn]                     | REG0=XMM_R():rw:dq    REG1=XMM_B():r:dq
 | PMINSW               | MMX            | MMX            | PENTIUMMMX     | 0f ea        | mm         | rrr        | 0x0F 0xEA no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q MEM0:r:q
 | PMINSW               | MMX            | MMX            | PENTIUMMMX     | 0f ea        | 0b11       | rrr        | 0x0F 0xEA no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PMINSW               | SSE            | SSE2           |                | 0f ea        | mm         | rrr        | 0x0F 0xEA osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PMINSW               | SSE            | SSE2           |                | 0f ea        | 0b11       | rrr        | 0x0F 0xEA osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PMINUB               | MMX            | MMX            | PENTIUMMMX     | 0f da        | mm         | rrr        | 0x0F 0xDA no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q MEM0:r:q
 | PMINUB               | MMX            | MMX            | PENTIUMMMX     | 0f da        | 0b11       | rrr        | 0x0F 0xDA no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PMINUB               | SSE            | SSE2           |                | 0f da        | mm         | rrr        | 0x0F 0xDA osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PMINUB               | SSE            | SSE2           |                | 0f da        | 0b11       | rrr        | 0x0F 0xDA osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PMINUD               | SSE            | SSE4           |                | 0f 38 3b     | mm         | rrr        | 0x0F 0x38 0x3B osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()              | REG0=XMM_R():rw:dq    MEM0:r:dq
 | PMINUD               | SSE            | SSE4           |                | 0f 38 3b     | 0b11       | rrr        | 0x0F 0x38 0x3B osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn]                     | REG0=XMM_R():rw:dq    REG1=XMM_B():r:dq
 | PMINUW               | SSE            | SSE4           |                | 0f 38 3a     | mm         | rrr        | 0x0F 0x38 0x3A osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()              | REG0=XMM_R():rw:dq    MEM0:r:dq
 | PMINUW               | SSE            | SSE4           |                | 0f 38 3a     | 0b11       | rrr        | 0x0F 0x38 0x3A osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn]                     | REG0=XMM_R():rw:dq    REG1=XMM_B():r:dq
 | PMOVMSKB             | MMX            | MMX            | SSE            | 0f d7        | 0b11       | rrr        | 0x0F 0xD7 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=GPR32_R():w REG1=MMX_B():r:q:i8
 | PMOVMSKB             | SSE            | SSE2           |                | 0f d7        | 0b11       | rrr        | 0x0F 0xD7 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=GPR32_R():w REG1=XMM_B():r:dq:i8
 | PMOVSXBD             | SSE            | SSE4           |                | 0f 38 21     | mm         | rrr        | 0x0F 0x38 0x21 osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()              | REG0=XMM_R():w:dq:i32    MEM0:r:d:i8
 | PMOVSXBD             | SSE            | SSE4           |                | 0f 38 21     | 0b11       | rrr        | 0x0F 0x38 0x21 osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn]                     | REG0=XMM_R():w:dq:i32    REG1=XMM_B():r:d:i8
 | PMOVSXBQ             | SSE            | SSE4           |                | 0f 38 22     | mm         | rrr        | 0x0F 0x38 0x22 osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()              | REG0=XMM_R():w:dq:i64    MEM0:r:w:i8
 | PMOVSXBQ             | SSE            | SSE4           |                | 0f 38 22     | 0b11       | rrr        | 0x0F 0x38 0x22 osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn]                     | REG0=XMM_R():w:dq:i64    REG1=XMM_B():r:w:i8
 | PMOVSXBW             | SSE            | SSE4           |                | 0f 38 20     | mm         | rrr        | 0x0F 0x38 0x20 osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()              | REG0=XMM_R():w:dq:i16    MEM0:r:q:i8
 | PMOVSXBW             | SSE            | SSE4           |                | 0f 38 20     | 0b11       | rrr        | 0x0F 0x38 0x20 osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn]                     | REG0=XMM_R():w:dq:i16    REG1=XMM_B():r:q:i8
 | PMOVSXDQ             | SSE            | SSE4           |                | 0f 38 25     | mm         | rrr        | 0x0F 0x38 0x25 osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()              | REG0=XMM_R():w:dq:i64    MEM0:r:q:i32
 | PMOVSXDQ             | SSE            | SSE4           |                | 0f 38 25     | 0b11       | rrr        | 0x0F 0x38 0x25 osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn]                     | REG0=XMM_R():w:dq:i64    REG1=XMM_B():r:q:i32
 | PMOVSXWD             | SSE            | SSE4           |                | 0f 38 23     | mm         | rrr        | 0x0F 0x38 0x23 osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()              | REG0=XMM_R():w:dq:i32    MEM0:r:q:i16
 | PMOVSXWD             | SSE            | SSE4           |                | 0f 38 23     | 0b11       | rrr        | 0x0F 0x38 0x23 osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn]                     | REG0=XMM_R():w:dq:i32    REG1=XMM_B():r:q:i16
 | PMOVSXWQ             | SSE            | SSE4           |                | 0f 38 24     | mm         | rrr        | 0x0F 0x38 0x24 osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()              | REG0=XMM_R():w:dq:i64    MEM0:r:d:i16
 | PMOVSXWQ             | SSE            | SSE4           |                | 0f 38 24     | 0b11       | rrr        | 0x0F 0x38 0x24 osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn]                     | REG0=XMM_R():w:dq:i64    REG1=XMM_B():r:d:i16
 | PMOVZXBD             | SSE            | SSE4           |                | 0f 38 31     | mm         | rrr        | 0x0F 0x38 0x31 osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()              | REG0=XMM_R():w:dq:u32    MEM0:r:d:u8
 | PMOVZXBD             | SSE            | SSE4           |                | 0f 38 31     | 0b11       | rrr        | 0x0F 0x38 0x31 osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn]                     | REG0=XMM_R():w:dq:u32    REG1=XMM_B():r:d:u8
 | PMOVZXBQ             | SSE            | SSE4           |                | 0f 38 32     | mm         | rrr        | 0x0F 0x38 0x32 osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()              | REG0=XMM_R():w:dq:u64    MEM0:r:w:u8
 | PMOVZXBQ             | SSE            | SSE4           |                | 0f 38 32     | 0b11       | rrr        | 0x0F 0x38 0x32 osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn]                     | REG0=XMM_R():w:dq:u64    REG1=XMM_B():r:w:u8
 | PMOVZXBW             | SSE            | SSE4           |                | 0f 38 30     | mm         | rrr        | 0x0F 0x38 0x30 osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()              | REG0=XMM_R():w:dq:u16    MEM0:r:q:u8
 | PMOVZXBW             | SSE            | SSE4           |                | 0f 38 30     | 0b11       | rrr        | 0x0F 0x38 0x30 osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn]                     | REG0=XMM_R():w:dq:u16    REG1=XMM_B():r:q:u8
 | PMOVZXDQ             | SSE            | SSE4           |                | 0f 38 35     | mm         | rrr        | 0x0F 0x38 0x35 osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()              | REG0=XMM_R():w:dq:u64    MEM0:r:q:u32
 | PMOVZXDQ             | SSE            | SSE4           |                | 0f 38 35     | 0b11       | rrr        | 0x0F 0x38 0x35 osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn]                     | REG0=XMM_R():w:dq:u64    REG1=XMM_B():r:q:u32
 | PMOVZXWD             | SSE            | SSE4           |                | 0f 38 33     | mm         | rrr        | 0x0F 0x38 0x33 osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()              | REG0=XMM_R():w:dq:u32    MEM0:r:q:u16
 | PMOVZXWD             | SSE            | SSE4           |                | 0f 38 33     | 0b11       | rrr        | 0x0F 0x38 0x33 osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn]                     | REG0=XMM_R():w:dq:u32    REG1=XMM_B():r:q:u16
 | PMOVZXWQ             | SSE            | SSE4           |                | 0f 38 34     | mm         | rrr        | 0x0F 0x38 0x34 osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()              | REG0=XMM_R():w:dq:u64    MEM0:r:d:u16
 | PMOVZXWQ             | SSE            | SSE4           |                | 0f 38 34     | 0b11       | rrr        | 0x0F 0x38 0x34 osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn]                     | REG0=XMM_R():w:dq:u64    REG1=XMM_B():r:d:u16
 | PMULDQ               | SSE            | SSE4           |                | 0f 38 28     | mm         | rrr        | 0x0F 0x38 0x28 osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()              | REG0=XMM_R():rw:dq    MEM0:r:dq
 | PMULDQ               | SSE            | SSE4           |                | 0f 38 28     | 0b11       | rrr        | 0x0F 0x38 0x28 osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn]                     | REG0=XMM_R():rw:dq    REG1=XMM_B():r:dq
 | PMULHRSW             | MMX            | SSSE3          | SSSE3MMX       | 0f 38 0b     | mm         | rrr        | 0x0F 0x38 0x0B no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                           | REG0=MMX_R():rw:q MEM0:r:q
 | PMULHRSW             | MMX            | SSSE3          | SSSE3MMX       | 0f 38 0b     | 0b11       | rrr        | 0x0F 0x38 0x0B no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                   | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PMULHRSW             | SSE            | SSSE3          |                | 0f 38 0b     | mm         | rrr        | 0x0F 0x38 0x0B osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()             | REG0=XMM_R():rw:dq MEM0:r:dq
 | PMULHRSW             | SSE            | SSSE3          |                | 0f 38 0b     | 0b11       | rrr        | 0x0F 0x38 0x0B osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                    | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PMULHRW              | 3DNOW          | 3DNOW          |                | 0f 0f b7     | mm         | rrr        | 0x0F 0x0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() 0xB7                                               | REG0=MMX_R():rw:q MEM0:r:q
 | PMULHRW              | 3DNOW          | 3DNOW          |                | 0f 0f b7     | 0b11       | rrr        | 0x0F 0x0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] 0xB7                                                      | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PMULHUW              | MMX            | MMX            | PENTIUMMMX     | 0f e4        | mm         | rrr        | 0x0F 0xE4 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q:u16 MEM0:r:q:u16
 | PMULHUW              | MMX            | MMX            | PENTIUMMMX     | 0f e4        | 0b11       | rrr        | 0x0F 0xE4 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q:u16 REG1=MMX_B():r:q:u16
 | PMULHUW              | SSE            | SSE2           |                | 0f e4        | mm         | rrr        | 0x0F 0xE4 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq:u16 MEM0:r:dq:u16
 | PMULHUW              | SSE            | SSE2           |                | 0f e4        | 0b11       | rrr        | 0x0F 0xE4 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq:u16 REG1=XMM_B():r:dq:u16
 | PMULHW               | MMX            | MMX            | PENTIUMMMX     | 0f e5        | mm         | rrr        | 0x0F 0xE5 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q:i16 MEM0:r:q:i16
 | PMULHW               | MMX            | MMX            | PENTIUMMMX     | 0f e5        | 0b11       | rrr        | 0x0F 0xE5 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q:i16 REG1=MMX_B():r:q:i16
 | PMULHW               | SSE            | SSE2           |                | 0f e5        | mm         | rrr        | 0x0F 0xE5 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq:i16 MEM0:r:dq:i16
 | PMULHW               | SSE            | SSE2           |                | 0f e5        | 0b11       | rrr        | 0x0F 0xE5 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq:i16 REG1=XMM_B():r:dq:i16
 | PMULLD               | SSE            | SSE4           |                | 0f 38 40     | mm         | rrr        | 0x0F 0x38 0x40 osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()              | REG0=XMM_R():rw:dq    MEM0:r:dq
 | PMULLD               | SSE            | SSE4           |                | 0f 38 40     | 0b11       | rrr        | 0x0F 0x38 0x40 osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn]                     | REG0=XMM_R():rw:dq    REG1=XMM_B():r:dq
 | PMULLW               | MMX            | MMX            | PENTIUMMMX     | 0f d5        | mm         | rrr        | 0x0F 0xD5 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q:i16 MEM0:r:q:i16
 | PMULLW               | MMX            | MMX            | PENTIUMMMX     | 0f d5        | 0b11       | rrr        | 0x0F 0xD5 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q:i16 REG1=MMX_B():r:q:i16
 | PMULLW               | SSE            | SSE2           |                | 0f d5        | mm         | rrr        | 0x0F 0xD5 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PMULLW               | SSE            | SSE2           |                | 0f d5        | 0b11       | rrr        | 0x0F 0xD5 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PMULUDQ              | MMX            | SSE2           | SSE2MMX        | 0f f4        | mm         | rrr        | 0x0F 0xF4 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q:u32 MEM0:r:q:u32
 | PMULUDQ              | MMX            | SSE2           | SSE2MMX        | 0f f4        | 0b11       | rrr        | 0x0F 0xF4 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q:u32 REG1=MMX_B():r:q:u32
 | PMULUDQ              | SSE            | SSE2           |                | 0f f4        | mm         | rrr        | 0x0F 0xF4 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PMULUDQ              | SSE            | SSE2           |                | 0f f4        | 0b11       | rrr        | 0x0F 0xF4 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | POP                  | POP            | BASE           | I86            | 8f           | mm         | 0b000      | 0x8F MOD[mm] MOD!=3 REG[0b000] RM[nnn] DF64() MODRM()                                                | MEM0:w:v REG0=XED_REG_STACKPOP:r:spw:SUPP
 | POP                  | POP            | BASE           | I86            | 8f           | 0b11       | 0b000      | 0x8F MOD[0b11] MOD=3 REG[0b000] RM[nnn] DF64()                                                       | REG0=GPRv_B():w REG1=XED_REG_STACKPOP:r:spw:SUPP
 | POP                  | POP            | BASE           | I86            | 07           |            |            | 0x07 not64                                                                                           | REG0=XED_REG_ES:w:IMPL REG1=XED_REG_STACKPOP:r:spw:SUPP
 | POP                  | POP            | BASE           | I86            | 1f           |            |            | 0x1F not64                                                                                           | REG0=XED_REG_DS:w:IMPL REG1=XED_REG_STACKPOP:r:spw:SUPP
 | POP                  | POP            | BASE           | I86            |              |            |            | 0b0101_1 SRM[rrr] DF64()                                                                             | REG0=GPRv_SB():w REG1=XED_REG_STACKPOP:r:spw:SUPP
 | POP                  | POP            | BASE           | I86            | 0f a1        |            |            | 0x0F 0xA1 DF64()                                                                                     | REG0=XED_REG_FS:w:IMPL REG1=XED_REG_STACKPOP:r:spw:SUPP
 | POP                  | POP            | BASE           | I86            | 0f a9        |            |            | 0x0F 0xA9 DF64()                                                                                     | REG0=XED_REG_GS:w:IMPL REG1=XED_REG_STACKPOP:r:spw:SUPP
 | POPA                 | POP            | BASE           | I186           | 61           |            |            | 0x61 mode16 no66_prefix                                                                              | REG0=XED_REG_STACKPOP:r:spw8:SUPP REG1=XED_REG_AX:w:SUPP REG2=XED_REG_CX:w:SUPP REG3=XED_REG_DX:w:SUPP REG4=XED_REG_BX:w:SUPP REG5=XED_REG_BP:w:SUPP REG6=XED_REG_SI:w:SUPP REG7=XED_REG_DI:w:SUPP
 | POPA                 | POP            | BASE           | I186           | 61           |            |            | 0x61 mode32 66_prefix                                                                                | REG0=XED_REG_STACKPOP:r:spw8:SUPP REG1=XED_REG_AX:w:SUPP REG2=XED_REG_CX:w:SUPP REG3=XED_REG_DX:w:SUPP REG4=XED_REG_BX:w:SUPP REG5=XED_REG_BP:w:SUPP REG6=XED_REG_SI:w:SUPP REG7=XED_REG_DI:w:SUPP
 | POPAD                | POP            | BASE           | I386           | 61           |            |            | 0x61 mode16 66_prefix                                                                                | REG0=XED_REG_STACKPOP:r:spw8:SUPP REG1=XED_REG_EAX:w:SUPP REG2=XED_REG_ECX:w:SUPP REG3=XED_REG_EDX:w:SUPP REG4=XED_REG_EBX:w:SUPP REG5=XED_REG_EBP:w:SUPP REG6=XED_REG_ESI:w:SUPP REG7=XED_REG_EDI:w:SUPP
 | POPAD                | POP            | BASE           | I386           | 61           |            |            | 0x61 mode32 no66_prefix                                                                              | REG0=XED_REG_STACKPOP:r:spw8:SUPP REG1=XED_REG_EAX:w:SUPP REG2=XED_REG_ECX:w:SUPP REG3=XED_REG_EDX:w:SUPP REG4=XED_REG_EBX:w:SUPP REG5=XED_REG_EBP:w:SUPP REG6=XED_REG_ESI:w:SUPP REG7=XED_REG_EDI:w:SUPP
 | POPCNT               | SSE            | SSE4           | POPCNT         | 0f b8        | mm         | rrr        | 0x0F 0xB8  f3_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]   MODRM()                              | REG0=GPRv_R():w:v     MEM0:r:v
 | POPCNT               | SSE            | SSE4           | POPCNT         | 0f b8        | 0b11       | rrr        | 0x0F 0xB8  f3_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_R():w:v     REG1=GPRv_B():r:v
 | POPCNT_VEX           | KNCSCALAR      | KNC            | KNCV           | b8           | 0b11       | rrr        | VV1 0xB8  VL128 VF3 V0F W0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                    | REG0=GPR32_R():w:d   REG1=GPR32_B():r:d
 | POPCNT_VEX           | KNCSCALAR      | KNC            | KNCV           | b8           | 0b11       | rrr        | VV1 0xB8  VL128 VF3 V0F W1 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                    | REG0=GPR64_R():w:q  REG1=GPR64_B():r:q
 | POPF                 | POP            | BASE           | I86            | 9d           |            |            | 0x9D  mode16 no66_prefix                                                                             | REG0=XED_REG_STACKPOP:r:w:SUPP
 | POPF                 | POP            | BASE           | I86            | 9d           |            |            | 0x9D mode32 66_prefix                                                                                | REG0=XED_REG_STACKPOP:r:w:SUPP
 | POPF                 | POP            | BASE           | I86            | 9d           |            |            | 0x9D mode64 norexw_prefix 66_prefix                                                                  | REG0=XED_REG_STACKPOP:r:w:SUPP
 | POPFD                | POP            | BASE           | I386           | 9d           |            |            | 0x9D mode16 66_prefix                                                                                | REG0=XED_REG_STACKPOP:r:d:SUPP
 | POPFD                | POP            | BASE           | I386           | 9d           |            |            | 0x9D mode32 no66_prefix                                                                              | REG0=XED_REG_STACKPOP:r:d:SUPP
 | POPFQ                | POP            | LONGMODE       |                | 9d           |            |            | 0x9D mode64 norexw_prefix no66_prefix DF64()                                                         | REG0=XED_REG_STACKPOP:r:q:SUPP
 | POPFQ                | POP            | LONGMODE       |                | 9d           |            |            | 0x9D mode64 rexw_prefix DF64()                                                                       | REG0=XED_REG_STACKPOP:r:q:SUPP
 | POR                  | LOGICAL        | MMX            | PENTIUMMMX     | 0f eb        | mm         | rrr        | 0x0F 0xEB no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q MEM0:r:q
 | POR                  | LOGICAL        | MMX            | PENTIUMMMX     | 0f eb        | 0b11       | rrr        | 0x0F 0xEB no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | POR                  | LOGICAL        | SSE2           |                | 0f eb        | mm         | rrr        | 0x0F 0xEB osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | POR                  | LOGICAL        | SSE2           |                | 0f eb        | 0b11       | rrr        | 0x0F 0xEB osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PREFETCH_EXCLUSIVE   | PREFETCH       | 3DNOW          | PREFETCH_NOP   | 0f 0d        | mm         | 0b000      | 0x0F 0x0D MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM()                                                  | MEM0:r:mprefetch
 | PREFETCH_RESERVED    | PREFETCH       | 3DNOW          | PREFETCH_NOP   | 0f 0d        | mm         | 0b010      | 0x0F 0x0D MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM()                                                  | MEM0:r:mprefetch
 | PREFETCH_RESERVED    | PREFETCH       | 3DNOW          | PREFETCH_NOP   | 0f 0d        | mm         | 0b100      | 0x0F 0x0D MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM()                                                  | MEM0:r:mprefetch
 | PREFETCH_RESERVED    | PREFETCH       | 3DNOW          | PREFETCH_NOP   | 0f 0d        | mm         | 0b101      | 0x0F 0x0D MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM()                                                  | MEM0:r:mprefetch
 | PREFETCH_RESERVED    | PREFETCH       | 3DNOW          | PREFETCH_NOP   | 0f 0d        | mm         | 0b110      | 0x0F 0x0D MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM()                                                  | MEM0:r:mprefetch
 | PREFETCH_RESERVED    | PREFETCH       | 3DNOW          | PREFETCH_NOP   | 0f 0d        | mm         | 0b111      | 0x0F 0x0D MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM()                                                  | MEM0:r:mprefetch
 | PREFETCHNTA          | PREFETCH       | SSE            | SSE_PREFETCH   | 0f 18        | mm         | 0b000      | 0x0F 0x18 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM()                                                  | MEM0:r:mprefetch
 | PREFETCHT0           | PREFETCH       | SSE            | SSE_PREFETCH   | 0f 18        | mm         | 0b001      | 0x0F 0x18 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM()                                                  | MEM0:r:mprefetch
 | PREFETCHT1           | PREFETCH       | SSE            | SSE_PREFETCH   | 0f 18        | mm         | 0b010      | 0x0F 0x18 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM()                                                  | MEM0:r:mprefetch
 | PREFETCHT2           | PREFETCH       | SSE            | SSE_PREFETCH   | 0f 18        | mm         | 0b011      | 0x0F 0x18 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM()                                                  | MEM0:r:mprefetch
 | PREFETCHW            | PREFETCH       | 3DNOW          | PREFETCH_NOP   | 0f 0d        | mm         | 0b001      | 0x0F 0x0D MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM()                                                  | MEM0:r:mprefetch
 | PREFETCHW            | PREFETCH       | 3DNOW          | PREFETCH_NOP   | 0f 0d        | mm         | 0b011      | 0x0F 0x0D MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM()                                                  | MEM0:r:mprefetch
 | PREFETCHWT1          | PREFETCHWT1    | PREFETCHWT1    | PREFETCHWT1    | 0f 0d        | mm         | 0b010      | 0x0F 0x0D MOD[mm] MOD!=3 REG[0b010] RM[nnn]  MODRM()                                                 | MEM0:r:b:u8
 | PSADBW               | MMX            | MMX            | PENTIUMMMX     | 0f f6        | mm         | rrr        | 0x0F 0xF6 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q MEM0:r:q
 | PSADBW               | MMX            | MMX            | PENTIUMMMX     | 0f f6        | 0b11       | rrr        | 0x0F 0xF6 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PSADBW               | SSE            | SSE2           |                | 0f f6        | mm         | rrr        | 0x0F 0xF6 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PSADBW               | SSE            | SSE2           |                | 0f f6        | 0b11       | rrr        | 0x0F 0xF6 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PSHUFB               | MMX            | SSSE3          | SSSE3MMX       | 0f 38 00     | mm         | rrr        | 0x0F 0x38 0x00 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                           | REG0=MMX_R():rw:q MEM0:r:q
 | PSHUFB               | MMX            | SSSE3          | SSSE3MMX       | 0f 38 00     | 0b11       | rrr        | 0x0F 0x38 0x00 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                   | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PSHUFB               | SSE            | SSSE3          |                | 0f 38 00     | mm         | rrr        | 0x0F 0x38 0x00 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()             | REG0=XMM_R():rw:dq MEM0:r:dq
 | PSHUFB               | SSE            | SSSE3          |                | 0f 38 00     | 0b11       | rrr        | 0x0F 0x38 0x00 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                    | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PSHUFD               | SSE            | SSE2           |                | 0f 70        | mm         | rrr        | 0x0F 0x70 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM() UIMM8()          | REG0=XMM_R():w:dq:u32 MEM0:r:dq:u32 IMM0:r:b
 | PSHUFD               | SSE            | SSE2           |                | 0f 70        | 0b11       | rrr        | 0x0F 0x70 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66() UIMM8()                 | REG0=XMM_R():w:dq:u32 REG1=XMM_B():r:dq:u32 IMM0:r:b
 | PSHUFHW              | SSE            | SSE2           |                | 0f 70        | mm         | rrr        | 0x0F 0x70 f3_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM() UIMM8()             | REG0=XMM_R():w:dq:u16 MEM0:r:dq:u16 IMM0:r:b
 | PSHUFHW              | SSE            | SSE2           |                | 0f 70        | 0b11       | rrr        | 0x0F 0x70 f3_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66() UIMM8()                    | REG0=XMM_R():w:dq:u16 REG1=XMM_B():r:dq:u16 IMM0:r:b
 | PSHUFLW              | SSE            | SSE2           |                | 0f 70        | mm         | rrr        | 0x0F 0x70 f2_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM() UIMM8()             | REG0=XMM_R():w:dq:u16 MEM0:r:dq:u16 IMM0:r:b
 | PSHUFLW              | SSE            | SSE2           |                | 0f 70        | 0b11       | rrr        | 0x0F 0x70 f2_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66() UIMM8()                    | REG0=XMM_R():w:dq:u16 REG1=XMM_B():r:dq:u16 IMM0:r:b
 | PSHUFW               | MMX            | MMX            | PENTIUMMMX     | 0f 70        | mm         | rrr        | 0x0F 0x70 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM() UIMM8()                        | REG0=MMX_R():w:q:u16 MEM0:r:q:u16 IMM0:r:b
 | PSHUFW               | MMX            | MMX            | PENTIUMMMX     | 0f 70        | 0b11       | rrr        | 0x0F 0x70 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  UIMM8()                               | REG0=MMX_R():w:q:u16 REG1=MMX_B():r:q:u16 IMM0:r:b
 | PSIGNB               | MMX            | SSSE3          | SSSE3MMX       | 0f 38 08     | mm         | rrr        | 0x0F 0x38 0x08 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                           | REG0=MMX_R():rw:q MEM0:r:q
 | PSIGNB               | MMX            | SSSE3          | SSSE3MMX       | 0f 38 08     | 0b11       | rrr        | 0x0F 0x38 0x08 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                   | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PSIGNB               | SSE            | SSSE3          |                | 0f 38 08     | mm         | rrr        | 0x0F 0x38 0x08 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()             | REG0=XMM_R():rw:dq MEM0:r:dq
 | PSIGNB               | SSE            | SSSE3          |                | 0f 38 08     | 0b11       | rrr        | 0x0F 0x38 0x08 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                    | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PSIGND               | MMX            | SSSE3          | SSSE3MMX       | 0f 38 0a     | mm         | rrr        | 0x0F 0x38 0x0A no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                           | REG0=MMX_R():rw:q MEM0:r:q
 | PSIGND               | MMX            | SSSE3          | SSSE3MMX       | 0f 38 0a     | 0b11       | rrr        | 0x0F 0x38 0x0A no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                   | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PSIGND               | SSE            | SSSE3          |                | 0f 38 0a     | mm         | rrr        | 0x0F 0x38 0x0A osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()             | REG0=XMM_R():rw:dq MEM0:r:dq
 | PSIGND               | SSE            | SSSE3          |                | 0f 38 0a     | 0b11       | rrr        | 0x0F 0x38 0x0A osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                    | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PSIGNW               | MMX            | SSSE3          | SSSE3MMX       | 0f 38 09     | mm         | rrr        | 0x0F 0x38 0x09 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                           | REG0=MMX_R():rw:q MEM0:r:q
 | PSIGNW               | MMX            | SSSE3          | SSSE3MMX       | 0f 38 09     | 0b11       | rrr        | 0x0F 0x38 0x09 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                   | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PSIGNW               | SSE            | SSSE3          |                | 0f 38 09     | mm         | rrr        | 0x0F 0x38 0x09 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()             | REG0=XMM_R():rw:dq MEM0:r:dq
 | PSIGNW               | SSE            | SSSE3          |                | 0f 38 09     | 0b11       | rrr        | 0x0F 0x38 0x09 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                    | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PSLLD                | MMX            | MMX            | PENTIUMMMX     | 0f 72        | 0b11       | 0b110      | 0x0F 0x72 no_refining_prefix MOD[0b11] MOD=3 REG[0b110] RM[nnn]  UIMM8()                             | REG0=MMX_B():rw:q:u32 IMM0:r:b
 | PSLLD                | MMX            | MMX            | PENTIUMMMX     | 0f f2        | mm         | rrr        | 0x0F 0xF2 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q:u32 MEM0:r:q
 | PSLLD                | MMX            | MMX            | PENTIUMMMX     | 0f f2        | 0b11       | rrr        | 0x0F 0xF2 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q:u32 REG1=MMX_B():r:q
 | PSLLD                | SSE            | SSE2           |                | 0f 72        | 0b11       | 0b110      | 0x0F 0x72 osz_refining_prefix MOD[0b11] MOD=3 REG[0b110] RM[nnn]  REFINING66() UIMM8()               | REG0=XMM_B():rw:dq:u32 IMM0:r:b
 | PSLLD                | SSE            | SSE2           |                | 0f f2        | mm         | rrr        | 0x0F 0xF2 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PSLLD                | SSE            | SSE2           |                | 0f f2        | 0b11       | rrr        | 0x0F 0xF2 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PSLLDQ               | SSE            | SSE2           |                | 0f 73        | 0b11       | 0b111      | 0x0F 0x73 osz_refining_prefix MOD[0b11] MOD=3 REG[0b111] RM[nnn]  REFINING66() UIMM8()               | REG0=XMM_B():rw:dq:u128 IMM0:r:b
 | PSLLQ                | MMX            | MMX            | PENTIUMMMX     | 0f 73        | 0b11       | 0b110      | 0x0F 0x73 no_refining_prefix MOD[0b11] MOD=3 REG[0b110] RM[nnn]  UIMM8()                             | REG0=MMX_B():rw:q:u64 IMM0:r:b
 | PSLLQ                | MMX            | MMX            | PENTIUMMMX     | 0f f3        | mm         | rrr        | 0x0F 0xF3 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q:u64 MEM0:r:q
 | PSLLQ                | MMX            | MMX            | PENTIUMMMX     | 0f f3        | 0b11       | rrr        | 0x0F 0xF3 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q:u64 REG1=MMX_B():r:q
 | PSLLQ                | SSE            | SSE2           |                | 0f 73        | 0b11       | 0b110      | 0x0F 0x73 osz_refining_prefix MOD[0b11] MOD=3 REG[0b110] RM[nnn]  REFINING66() UIMM8()               | REG0=XMM_B():rw:dq:u64 IMM0:r:b
 | PSLLQ                | SSE            | SSE2           |                | 0f f3        | mm         | rrr        | 0x0F 0xF3 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq:u64 MEM0:r:dq:u64
 | PSLLQ                | SSE            | SSE2           |                | 0f f3        | 0b11       | rrr        | 0x0F 0xF3 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq:u64 REG1=XMM_B():r:dq:u64
 | PSLLW                | MMX            | MMX            | PENTIUMMMX     | 0f 71        | 0b11       | 0b110      | 0x0F 0x71 no_refining_prefix MOD[0b11] MOD=3 REG[0b110] RM[nnn]  UIMM8()                             | REG0=MMX_B():rw:q:u16 IMM0:r:b
 | PSLLW                | MMX            | MMX            | PENTIUMMMX     | 0f f1        | mm         | rrr        | 0x0F 0xF1 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q:u16 MEM0:r:q
 | PSLLW                | MMX            | MMX            | PENTIUMMMX     | 0f f1        | 0b11       | rrr        | 0x0F 0xF1 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q:u16 REG1=MMX_B():r:q
 | PSLLW                | SSE            | SSE2           |                | 0f 71        | 0b11       | 0b110      | 0x0F 0x71 osz_refining_prefix MOD[0b11] MOD=3 REG[0b110] RM[nnn]  REFINING66() UIMM8()               | REG0=XMM_B():rw:dq:u16 IMM0:r:b
 | PSLLW                | SSE            | SSE2           |                | 0f f1        | mm         | rrr        | 0x0F 0xF1 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq:u16 MEM0:r:dq
 | PSLLW                | SSE            | SSE2           |                | 0f f1        | 0b11       | rrr        | 0x0F 0xF1 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq:u16 REG1=XMM_B():r:dq
 | PSMASH               | SYSTEM         | SNP            |                | 0f 01        | 0b11       | 0b111      | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b111] RM[0b111] f3_refining_prefix mode64                             | REG0=XED_REG_RAX:rw:IMPL
 | PSRAD                | MMX            | MMX            | PENTIUMMMX     | 0f 72        | 0b11       | 0b100      | 0x0F 0x72 no_refining_prefix MOD[0b11] MOD=3 REG[0b100] RM[nnn]  UIMM8()                             | REG0=MMX_B():rw:q:i32 IMM0:r:b
 | PSRAD                | MMX            | MMX            | PENTIUMMMX     | 0f e2        | mm         | rrr        | 0x0F 0xE2 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q:i32 MEM0:r:q
 | PSRAD                | MMX            | MMX            | PENTIUMMMX     | 0f e2        | 0b11       | rrr        | 0x0F 0xE2 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q:i32 REG1=MMX_B():r:q
 | PSRAD                | SSE            | SSE2           |                | 0f 72        | 0b11       | 0b100      | 0x0F 0x72 osz_refining_prefix MOD[0b11] MOD=3 REG[0b100] RM[nnn]  REFINING66() UIMM8()               | REG0=XMM_B():rw:dq:i32 IMM0:r:b
 | PSRAD                | SSE            | SSE2           |                | 0f e2        | mm         | rrr        | 0x0F 0xE2 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq:i32 MEM0:r:dq:u64
 | PSRAD                | SSE            | SSE2           |                | 0f e2        | 0b11       | rrr        | 0x0F 0xE2 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq:i32 REG1=XMM_B():r:dq:u64
 | PSRAW                | MMX            | MMX            | PENTIUMMMX     | 0f 71        | 0b11       | 0b100      | 0x0F 0x71 no_refining_prefix MOD[0b11] MOD=3 REG[0b100] RM[nnn]  UIMM8()                             | REG0=MMX_B():rw:q:i16 IMM0:r:b
 | PSRAW                | MMX            | MMX            | PENTIUMMMX     | 0f e1        | mm         | rrr        | 0x0F 0xE1 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q:i16 MEM0:r:q
 | PSRAW                | MMX            | MMX            | PENTIUMMMX     | 0f e1        | 0b11       | rrr        | 0x0F 0xE1 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q:i16 REG1=MMX_B():r:q
 | PSRAW                | SSE            | SSE2           |                | 0f 71        | 0b11       | 0b100      | 0x0F 0x71 osz_refining_prefix MOD[0b11] MOD=3 REG[0b100] RM[nnn]  REFINING66() UIMM8()               | REG0=XMM_B():rw:dq:i16 IMM0:r:b
 | PSRAW                | SSE            | SSE2           |                | 0f e1        | mm         | rrr        | 0x0F 0xE1 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq:i16 MEM0:r:dq:u64
 | PSRAW                | SSE            | SSE2           |                | 0f e1        | 0b11       | rrr        | 0x0F 0xE1 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq:i16 REG1=XMM_B():r:dq:u64
 | PSRLD                | MMX            | MMX            | PENTIUMMMX     | 0f 72        | 0b11       | 0b010      | 0x0F 0x72 no_refining_prefix MOD[0b11] MOD=3 REG[0b010] RM[nnn]  UIMM8()                             | REG0=MMX_B():rw:q:u32 IMM0:r:b
 | PSRLD                | MMX            | MMX            | PENTIUMMMX     | 0f d2        | mm         | rrr        | 0x0F 0xD2 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q:u32 MEM0:r:q
 | PSRLD                | MMX            | MMX            | PENTIUMMMX     | 0f d2        | 0b11       | rrr        | 0x0F 0xD2 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q:u32 REG1=MMX_B():r:q
 | PSRLD                | SSE            | SSE2           |                | 0f 72        | 0b11       | 0b010      | 0x0F 0x72 osz_refining_prefix MOD[0b11] MOD=3 REG[0b010] RM[nnn]  REFINING66() UIMM8()               | REG0=XMM_B():rw:dq:u32 IMM0:r:b
 | PSRLD                | SSE            | SSE2           |                | 0f d2        | mm         | rrr        | 0x0F 0xD2 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PSRLD                | SSE            | SSE2           |                | 0f d2        | 0b11       | rrr        | 0x0F 0xD2 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PSRLDQ               | SSE            | SSE2           |                | 0f 73        | 0b11       | 0b011      | 0x0F 0x73 osz_refining_prefix MOD[0b11] MOD=3 REG[0b011] RM[nnn]  REFINING66() UIMM8()               | REG0=XMM_B():rw:dq:u128 IMM0:r:b
 | PSRLQ                | MMX            | MMX            | PENTIUMMMX     | 0f 73        | 0b11       | 0b010      | 0x0F 0x73 no_refining_prefix MOD[0b11] MOD=3 REG[0b010] RM[nnn]  UIMM8()                             | REG0=MMX_B():rw:q:u64 IMM0:r:b
 | PSRLQ                | MMX            | MMX            | PENTIUMMMX     | 0f d3        | mm         | rrr        | 0x0F 0xD3 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q:u64 MEM0:r:q
 | PSRLQ                | MMX            | MMX            | PENTIUMMMX     | 0f d3        | 0b11       | rrr        | 0x0F 0xD3 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q:u64 REG1=MMX_B():r:q
 | PSRLQ                | SSE            | SSE2           |                | 0f 73        | 0b11       | 0b010      | 0x0F 0x73 osz_refining_prefix MOD[0b11] MOD=3 REG[0b010] RM[nnn]  REFINING66() UIMM8()               | REG0=XMM_B():rw:dq:u64 IMM0:r:b
 | PSRLQ                | SSE            | SSE2           |                | 0f d3        | mm         | rrr        | 0x0F 0xD3 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PSRLQ                | SSE            | SSE2           |                | 0f d3        | 0b11       | rrr        | 0x0F 0xD3 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PSRLW                | MMX            | MMX            | PENTIUMMMX     | 0f 71        | 0b11       | 0b010      | 0x0F 0x71 no_refining_prefix MOD[0b11] MOD=3 REG[0b010] RM[nnn]  UIMM8()                             | REG0=MMX_B():rw:q:u16 IMM0:r:b
 | PSRLW                | MMX            | MMX            | PENTIUMMMX     | 0f d1        | mm         | rrr        | 0x0F 0xD1 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q:u16 MEM0:r:q
 | PSRLW                | MMX            | MMX            | PENTIUMMMX     | 0f d1        | 0b11       | rrr        | 0x0F 0xD1 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q:u16 REG1=MMX_B():r:q
 | PSRLW                | SSE            | SSE2           |                | 0f 71        | 0b11       | 0b010      | 0x0F 0x71 osz_refining_prefix MOD[0b11] MOD=3 REG[0b010] RM[nnn]  REFINING66() UIMM8()               | REG0=XMM_B():rw:dq:u16 IMM0:r:b
 | PSRLW                | SSE            | SSE2           |                | 0f d1        | mm         | rrr        | 0x0F 0xD1 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PSRLW                | SSE            | SSE2           |                | 0f d1        | 0b11       | rrr        | 0x0F 0xD1 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PSUBB                | MMX            | MMX            | PENTIUMMMX     | 0f f8        | mm         | rrr        | 0x0F 0xF8 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q MEM0:r:q
 | PSUBB                | MMX            | MMX            | PENTIUMMMX     | 0f f8        | 0b11       | rrr        | 0x0F 0xF8 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PSUBB                | SSE            | SSE2           |                | 0f f8        | mm         | rrr        | 0x0F 0xF8 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PSUBB                | SSE            | SSE2           |                | 0f f8        | 0b11       | rrr        | 0x0F 0xF8 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PSUBD                | MMX            | MMX            | PENTIUMMMX     | 0f fa        | mm         | rrr        | 0x0F 0xFA no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q MEM0:r:q
 | PSUBD                | MMX            | MMX            | PENTIUMMMX     | 0f fa        | 0b11       | rrr        | 0x0F 0xFA no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PSUBD                | SSE            | SSE2           |                | 0f fa        | mm         | rrr        | 0x0F 0xFA osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PSUBD                | SSE            | SSE2           |                | 0f fa        | 0b11       | rrr        | 0x0F 0xFA osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PSUBQ                | MMX            | SSE2           | SSE2MMX        | 0f fb        | mm         | rrr        | 0x0F 0xFB no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q MEM0:r:q
 | PSUBQ                | MMX            | SSE2           | SSE2MMX        | 0f fb        | 0b11       | rrr        | 0x0F 0xFB no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PSUBQ                | SSE            | SSE2           |                | 0f fb        | mm         | rrr        | 0x0F 0xFB osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PSUBQ                | SSE            | SSE2           |                | 0f fb        | 0b11       | rrr        | 0x0F 0xFB osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PSUBSB               | MMX            | MMX            | PENTIUMMMX     | 0f e8        | mm         | rrr        | 0x0F 0xE8 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q MEM0:r:q
 | PSUBSB               | MMX            | MMX            | PENTIUMMMX     | 0f e8        | 0b11       | rrr        | 0x0F 0xE8 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PSUBSB               | SSE            | SSE2           |                | 0f e8        | mm         | rrr        | 0x0F 0xE8 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PSUBSB               | SSE            | SSE2           |                | 0f e8        | 0b11       | rrr        | 0x0F 0xE8 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PSUBSW               | MMX            | MMX            | PENTIUMMMX     | 0f e9        | mm         | rrr        | 0x0F 0xE9 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q MEM0:r:q
 | PSUBSW               | MMX            | MMX            | PENTIUMMMX     | 0f e9        | 0b11       | rrr        | 0x0F 0xE9 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PSUBSW               | SSE            | SSE2           |                | 0f e9        | mm         | rrr        | 0x0F 0xE9 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PSUBSW               | SSE            | SSE2           |                | 0f e9        | 0b11       | rrr        | 0x0F 0xE9 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PSUBUSB              | MMX            | MMX            | PENTIUMMMX     | 0f d8        | mm         | rrr        | 0x0F 0xD8 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q MEM0:r:q
 | PSUBUSB              | MMX            | MMX            | PENTIUMMMX     | 0f d8        | 0b11       | rrr        | 0x0F 0xD8 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PSUBUSB              | SSE            | SSE2           |                | 0f d8        | mm         | rrr        | 0x0F 0xD8 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PSUBUSB              | SSE            | SSE2           |                | 0f d8        | 0b11       | rrr        | 0x0F 0xD8 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PSUBUSW              | MMX            | MMX            | PENTIUMMMX     | 0f d9        | mm         | rrr        | 0x0F 0xD9 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q MEM0:r:q
 | PSUBUSW              | MMX            | MMX            | PENTIUMMMX     | 0f d9        | 0b11       | rrr        | 0x0F 0xD9 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PSUBUSW              | SSE            | SSE2           |                | 0f d9        | mm         | rrr        | 0x0F 0xD9 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PSUBUSW              | SSE            | SSE2           |                | 0f d9        | 0b11       | rrr        | 0x0F 0xD9 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PSUBW                | MMX            | MMX            | PENTIUMMMX     | 0f f9        | mm         | rrr        | 0x0F 0xF9 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q MEM0:r:q
 | PSUBW                | MMX            | MMX            | PENTIUMMMX     | 0f f9        | 0b11       | rrr        | 0x0F 0xF9 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PSUBW                | SSE            | SSE2           |                | 0f f9        | mm         | rrr        | 0x0F 0xF9 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PSUBW                | SSE            | SSE2           |                | 0f f9        | 0b11       | rrr        | 0x0F 0xF9 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | PSWAPD               | 3DNOW          | 3DNOW          |                | 0f 0f bb     | mm         | rrr        | 0x0F 0x0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() 0xBB                                               | REG0=MMX_R():rw:q MEM0:r:q
 | PSWAPD               | 3DNOW          | 3DNOW          |                | 0f 0f bb     | 0b11       | rrr        | 0x0F 0x0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] 0xBB                                                      | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PTEST                | LOGICAL        | SSE4           |                | 0f 38 17     | mm         | rrr        | 0x0F 0x38 0x17 osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()              | REG0=XMM_R():r:dq   MEM0:r:dq
 | PTEST                | LOGICAL        | SSE4           |                | 0f 38 17     | 0b11       | rrr        | 0x0F 0x38 0x17 osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn]                     | REG0=XMM_R():r:dq   REG1=XMM_B():r:dq
 | PTWRITE              | PT             | PT             |                | 0f ae        | 0b11       | 0b100      | 0x0F 0xAE MOD[0b11] MOD=3 REG[0b100]  RM[nnn] f3_refining_prefix  no66_prefix                        | REG0=GPRy_B():r
 | PTWRITE              | PT             | PT             |                | 0f ae        | mm         | 0b100      | 0x0F 0xAE MOD[mm]   MOD!=3 REG[0b100] RM[nnn] f3_refining_prefix no66_prefix MODRM()                 | MEM0:r:y
 | PUNPCKHBW            | MMX            | MMX            | PENTIUMMMX     | 0f 68        | mm         | rrr        | 0x0F 0x68 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q MEM0:r:q
 | PUNPCKHBW            | MMX            | MMX            | PENTIUMMMX     | 0f 68        | 0b11       | rrr        | 0x0F 0x68 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q REG1=MMX_B():r:d
 | PUNPCKHBW            | SSE            | SSE2           |                | 0f 68        | mm         | rrr        | 0x0F 0x68 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PUNPCKHBW            | SSE            | SSE2           |                | 0f 68        | 0b11       | rrr        | 0x0F 0x68 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:q
 | PUNPCKHDQ            | MMX            | MMX            | PENTIUMMMX     | 0f 6a        | mm         | rrr        | 0x0F 0x6A no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q MEM0:r:q
 | PUNPCKHDQ            | MMX            | MMX            | PENTIUMMMX     | 0f 6a        | 0b11       | rrr        | 0x0F 0x6A no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q REG1=MMX_B():r:d
 | PUNPCKHDQ            | SSE            | SSE2           |                | 0f 6a        | mm         | rrr        | 0x0F 0x6A osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PUNPCKHDQ            | SSE            | SSE2           |                | 0f 6a        | 0b11       | rrr        | 0x0F 0x6A osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:q
 | PUNPCKHQDQ           | SSE            | SSE2           |                | 0f 6d        | mm         | rrr        | 0x0F 0x6D osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PUNPCKHQDQ           | SSE            | SSE2           |                | 0f 6d        | 0b11       | rrr        | 0x0F 0x6D osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:q
 | PUNPCKHWD            | MMX            | MMX            | PENTIUMMMX     | 0f 69        | mm         | rrr        | 0x0F 0x69 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q MEM0:r:q
 | PUNPCKHWD            | MMX            | MMX            | PENTIUMMMX     | 0f 69        | 0b11       | rrr        | 0x0F 0x69 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q REG1=MMX_B():r:d
 | PUNPCKHWD            | SSE            | SSE2           |                | 0f 69        | mm         | rrr        | 0x0F 0x69 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PUNPCKHWD            | SSE            | SSE2           |                | 0f 69        | 0b11       | rrr        | 0x0F 0x69 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:q
 | PUNPCKLBW            | MMX            | MMX            | PENTIUMMMX     | 0f 60        | mm         | rrr        | 0x0F 0x60 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q:u8 MEM0:r:d:u8
 | PUNPCKLBW            | MMX            | MMX            | PENTIUMMMX     | 0f 60        | 0b11       | rrr        | 0x0F 0x60 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q:u8 REG1=MMX_B():r:d:u8
 | PUNPCKLBW            | SSE            | SSE2           |                | 0f 60        | mm         | rrr        | 0x0F 0x60 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PUNPCKLBW            | SSE            | SSE2           |                | 0f 60        | 0b11       | rrr        | 0x0F 0x60 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:q
 | PUNPCKLDQ            | MMX            | MMX            | PENTIUMMMX     | 0f 62        | mm         | rrr        | 0x0F 0x62 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q:u32 MEM0:r:d:u32
 | PUNPCKLDQ            | MMX            | MMX            | PENTIUMMMX     | 0f 62        | 0b11       | rrr        | 0x0F 0x62 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q:u32 REG1=MMX_B():r:d:u32
 | PUNPCKLDQ            | SSE            | SSE2           |                | 0f 62        | mm         | rrr        | 0x0F 0x62 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PUNPCKLDQ            | SSE            | SSE2           |                | 0f 62        | 0b11       | rrr        | 0x0F 0x62 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:q
 | PUNPCKLQDQ           | SSE            | SSE2           |                | 0f 6c        | mm         | rrr        | 0x0F 0x6C osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PUNPCKLQDQ           | SSE            | SSE2           |                | 0f 6c        | 0b11       | rrr        | 0x0F 0x6C osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:q
 | PUNPCKLWD            | MMX            | MMX            | PENTIUMMMX     | 0f 61        | mm         | rrr        | 0x0F 0x61 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q:u16 MEM0:r:d:u16
 | PUNPCKLWD            | MMX            | MMX            | PENTIUMMMX     | 0f 61        | 0b11       | rrr        | 0x0F 0x61 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q:u16 REG1=MMX_B():r:d:u16
 | PUNPCKLWD            | SSE            | SSE2           |                | 0f 61        | mm         | rrr        | 0x0F 0x61 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PUNPCKLWD            | SSE            | SSE2           |                | 0f 61        | 0b11       | rrr        | 0x0F 0x61 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:q
 | PUSH                 | PUSH           | BASE           | I186           | 68           |            |            | 0x68 DF64() SIMMz()                                                                                  | IMM0:r:z REG0=XED_REG_STACKPUSH:w:spw:SUPP
 | PUSH                 | PUSH           | BASE           | I186           | 6a           |            |            | 0x6A DF64() SIMM8()                                                                                  | IMM0:r:b:i8 REG0=XED_REG_STACKPUSH:w:spw:SUPP
 | PUSH                 | PUSH           | BASE           | I86            | ff           | mm         | 0b110      | 0xFF MOD[mm] MOD!=3 REG[0b110] RM[nnn] DF64() MODRM()                                                | MEM0:r:v REG0=XED_REG_STACKPUSH:w:spw:SUPP
 | PUSH                 | PUSH           | BASE           | I86            | ff           | 0b11       | 0b110      | 0xFF MOD[0b11] MOD=3 REG[0b110] RM[nnn] DF64()                                                       | REG0=GPRv_B():r REG1=XED_REG_STACKPUSH:w:spw:SUPP
 | PUSH                 | PUSH           | BASE           | I86            | 06           |            |            | 0x06 not64                                                                                           | REG0=XED_REG_ES:r:IMPL REG1=XED_REG_STACKPUSH:w:spw:SUPP
 | PUSH                 | PUSH           | BASE           | I86            | 0e           |            |            | 0x0E not64                                                                                           | REG0=XED_REG_CS:r:IMPL REG1=XED_REG_STACKPUSH:w:spw:SUPP
 | PUSH                 | PUSH           | BASE           | I86            | 16           |            |            | 0x16 not64                                                                                           | REG0=XED_REG_SS:r:IMPL REG1=XED_REG_STACKPUSH:w:spw:SUPP
 | PUSH                 | PUSH           | BASE           | I86            | 1e           |            |            | 0x1E not64                                                                                           | REG0=XED_REG_DS:r:IMPL REG1=XED_REG_STACKPUSH:w:spw:SUPP
 | PUSH                 | PUSH           | BASE           | I86            |              |            |            | 0b0101_0 SRM[rrr] DF64()                                                                             | REG0=GPRv_SB():r REG1=XED_REG_STACKPUSH:w:spw:SUPP
 | PUSH                 | PUSH           | BASE           | I86            | 0f a0        |            |            | 0x0F 0xA0 DF64()                                                                                     | REG0=XED_REG_FS:r:IMPL REG1=XED_REG_STACKPUSH:w:spw:SUPP
 | PUSH                 | PUSH           | BASE           | I86            | 0f a8        |            |            | 0x0F 0xA8 DF64()                                                                                     | REG0=XED_REG_GS:r:IMPL REG1=XED_REG_STACKPUSH:w:spw:SUPP
 | PUSHA                | PUSH           | BASE           | I186           | 60           |            |            | 0x60 mode16 no66_prefix                                                                              | REG0=XED_REG_STACKPUSH:w:spw8:SUPP REG1=XED_REG_AX:r:SUPP REG2=XED_REG_CX:r:SUPP REG3=XED_REG_DX:r:SUPP REG4=XED_REG_BX:r:SUPP REG5=XED_REG_SP:r:SUPP REG6=XED_REG_BP:r:SUPP REG7=XED_REG_SI:r:SUPP REG8=XED_REG_DI:r:SUPP
 | PUSHA                | PUSH           | BASE           | I186           | 60           |            |            | 0x60 mode32 66_prefix                                                                                | REG0=XED_REG_STACKPUSH:w:spw8:SUPP REG1=XED_REG_AX:r:SUPP REG2=XED_REG_CX:r:SUPP REG3=XED_REG_DX:r:SUPP REG4=XED_REG_BX:r:SUPP REG5=XED_REG_SP:r:SUPP REG6=XED_REG_BP:r:SUPP REG7=XED_REG_SI:r:SUPP REG8=XED_REG_DI:r:SUPP
 | PUSHAD               | PUSH           | BASE           | I386           | 60           |            |            | 0x60 mode16 66_prefix                                                                                | REG0=XED_REG_STACKPUSH:w:spw8:SUPP REG1=XED_REG_EAX:r:SUPP REG2=XED_REG_ECX:r:SUPP REG3=XED_REG_EDX:r:SUPP REG4=XED_REG_EBX:r:SUPP REG5=XED_REG_ESP:r:SUPP REG6=XED_REG_EBP:r:SUPP REG7=XED_REG_ESI:r:SUPP REG8=XED_REG_EDI:r:SUPP
 | PUSHAD               | PUSH           | BASE           | I386           | 60           |            |            | 0x60 mode32 no66_prefix                                                                              | REG0=XED_REG_STACKPUSH:w:spw8:SUPP REG1=XED_REG_EAX:r:SUPP REG2=XED_REG_ECX:r:SUPP REG3=XED_REG_EDX:r:SUPP REG4=XED_REG_EBX:r:SUPP REG5=XED_REG_ESP:r:SUPP REG6=XED_REG_EBP:r:SUPP REG7=XED_REG_ESI:r:SUPP REG8=XED_REG_EDI:r:SUPP
 | PUSHF                | PUSH           | BASE           | I86            | 9c           |            |            | 0x9C mode16 no66_prefix                                                                              | REG0=XED_REG_STACKPUSH:w:w:SUPP
 | PUSHF                | PUSH           | BASE           | I86            | 9c           |            |            | 0x9C mode32 66_prefix                                                                                | REG0=XED_REG_STACKPUSH:w:w:SUPP
 | PUSHF                | PUSH           | BASE           | I86            | 9c           |            |            | 0x9C mode64 norexw_prefix 66_prefix                                                                  | REG0=XED_REG_STACKPUSH:w:w:SUPP
 | PUSHFD               | PUSH           | BASE           | I386           | 9c           |            |            | 0x9C mode32 no66_prefix                                                                              | REG0=XED_REG_STACKPUSH:w:d:SUPP
 | PUSHFD               | PUSH           | BASE           | I386           | 9c           |            |            | 0x9C mode16 66_prefix                                                                                | REG0=XED_REG_STACKPUSH:w:d:SUPP
 | PUSHFQ               | PUSH           | LONGMODE       |                | 9c           |            |            | 0x9C mode64 norexw_prefix no66_prefix DF64()                                                         | REG0=XED_REG_STACKPUSH:w:q:SUPP
 | PUSHFQ               | PUSH           | LONGMODE       |                | 9c           |            |            | 0x9C mode64 rexw_prefix  DF64()                                                                      | REG0=XED_REG_STACKPUSH:w:q:SUPP
 | PVALIDATE            | SYSTEM         | SNP            |                | 0f 01        | 0b11       | 0b111      | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b111] RM[0b111] f2_refining_prefix mode64                             | REG0=XED_REG_RAX:rw:IMPL REG1=XED_REG_ECX:r:IMPL REG2=XED_REG_EDX:r:IMPL
 | PXOR                 | LOGICAL        | MMX            | PENTIUMMMX     | 0f ef        | mm         | rrr        | 0x0F 0xEF no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=MMX_R():rw:q MEM0:r:q
 | PXOR                 | LOGICAL        | MMX            | PENTIUMMMX     | 0f ef        | 0b11       | rrr        | 0x0F 0xEF no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=MMX_R():rw:q REG1=MMX_B():r:q
 | PXOR                 | LOGICAL        | SSE2           |                | 0f ef        | mm         | rrr        | 0x0F 0xEF osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:dq MEM0:r:dq
 | PXOR                 | LOGICAL        | SSE2           |                | 0f ef        | 0b11       | rrr        | 0x0F 0xEF osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:dq REG1=XMM_B():r:dq
 | RCL                  | ROTATE         | BASE           | I186           | c0           | mm         | 0b010      | 0xC0 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM() UIMM8()                                               | MEM0:rw:b IMM0:r:b
 | RCL                  | ROTATE         | BASE           | I186           | c0           | 0b11       | 0b010      | 0xC0 MOD[0b11] MOD=3 REG[0b010] RM[nnn] UIMM8()                                                      | REG0=GPR8_B():rw IMM0:r:b
 | RCL                  | ROTATE         | BASE           | I186           | c1           | mm         | 0b010      | 0xC1 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM() UIMM8()                                               | MEM0:rw:v IMM0:r:b
 | RCL                  | ROTATE         | BASE           | I186           | c1           | 0b11       | 0b010      | 0xC1 MOD[0b11] MOD=3 REG[0b010] RM[nnn] UIMM8()                                                      | REG0=GPRv_B():rw IMM0:r:b
 | RCL                  | ROTATE         | BASE           | I86            | d0           | mm         | 0b010      | 0xD0 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM() ONE()                                                 | MEM0:rw:b IMM0:r:b:IMPL
 | RCL                  | ROTATE         | BASE           | I86            | d0           | 0b11       | 0b010      | 0xD0 MOD[0b11] MOD=3 REG[0b010] RM[nnn] ONE()                                                        | REG0=GPR8_B():rw IMM0:r:b:IMPL
 | RCL                  | ROTATE         | BASE           | I86            | d1           | mm         | 0b010      | 0xD1 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM() ONE()                                                 | MEM0:rw:v IMM0:r:b:IMPL
 | RCL                  | ROTATE         | BASE           | I86            | d1           | 0b11       | 0b010      | 0xD1 MOD[0b11] MOD=3 REG[0b010] RM[nnn] ONE()                                                        | REG0=GPRv_B():rw IMM0:r:b:IMPL
 | RCL                  | ROTATE         | BASE           | I86            | d2           | mm         | 0b010      | 0xD2 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM()                                                       | MEM0:rw:b REG0=XED_REG_CL:r:IMPL
 | RCL                  | ROTATE         | BASE           | I86            | d2           | 0b11       | 0b010      | 0xD2 MOD[0b11] MOD=3 REG[0b010] RM[nnn]                                                              | REG0=GPR8_B():rw REG1=XED_REG_CL:r:IMPL
 | RCL                  | ROTATE         | BASE           | I86            | d3           | mm         | 0b010      | 0xD3 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM()                                                       | MEM0:rw:v REG0=XED_REG_CL:r:IMPL
 | RCL                  | ROTATE         | BASE           | I86            | d3           | 0b11       | 0b010      | 0xD3 MOD[0b11] MOD=3 REG[0b010] RM[nnn]                                                              | REG0=GPRv_B():rw REG1=XED_REG_CL:r:IMPL
 | RCPPS                | SSE            | SSE            |                | 0f 53        | mm         | rrr        | 0x0F 0x53 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=XMM_R():w:ps MEM0:r:ps
 | RCPPS                | SSE            | SSE            |                | 0f 53        | 0b11       | rrr        | 0x0F 0x53 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=XMM_R():w:ps REG1=XMM_B():r:ps
 | RCPSS                | SSE            | SSE            |                | 0f 53        | mm         | rrr        | 0x0F 0x53 f3_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM()                     | REG0=XMM_R():w:ss MEM0:r:ss
 | RCPSS                | SSE            | SSE            |                | 0f 53        | 0b11       | rrr        | 0x0F 0x53 f3_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66()                            | REG0=XMM_R():w:ss REG1=XMM_B():r:ss
 | RCR                  | ROTATE         | BASE           | I186           | c0           | mm         | 0b011      | 0xC0 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM() UIMM8()                                               | MEM0:rw:b IMM0:r:b
 | RCR                  | ROTATE         | BASE           | I186           | c0           | 0b11       | 0b011      | 0xC0 MOD[0b11] MOD=3 REG[0b011] RM[nnn] UIMM8()                                                      | REG0=GPR8_B():rw IMM0:r:b
 | RCR                  | ROTATE         | BASE           | I186           | c1           | mm         | 0b011      | 0xC1 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM() UIMM8()                                               | MEM0:rw:v IMM0:r:b
 | RCR                  | ROTATE         | BASE           | I186           | c1           | 0b11       | 0b011      | 0xC1 MOD[0b11] MOD=3 REG[0b011] RM[nnn] UIMM8()                                                      | REG0=GPRv_B():rw IMM0:r:b
 | RCR                  | ROTATE         | BASE           | I86            | d0           | mm         | 0b011      | 0xD0 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM() ONE()                                                 | MEM0:rw:b IMM0:r:b:IMPL
 | RCR                  | ROTATE         | BASE           | I86            | d0           | 0b11       | 0b011      | 0xD0 MOD[0b11] MOD=3 REG[0b011] RM[nnn] ONE()                                                        | REG0=GPR8_B():rw IMM0:r:b:IMPL
 | RCR                  | ROTATE         | BASE           | I86            | d1           | mm         | 0b011      | 0xD1 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM() ONE()                                                 | MEM0:rw:v IMM0:r:b:IMPL
 | RCR                  | ROTATE         | BASE           | I86            | d1           | 0b11       | 0b011      | 0xD1 MOD[0b11] MOD=3 REG[0b011] RM[nnn] ONE()                                                        | REG0=GPRv_B():rw IMM0:r:b:IMPL
 | RCR                  | ROTATE         | BASE           | I86            | d2           | mm         | 0b011      | 0xD2 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM()                                                       | MEM0:rw:b REG0=XED_REG_CL:r:IMPL
 | RCR                  | ROTATE         | BASE           | I86            | d2           | 0b11       | 0b011      | 0xD2 MOD[0b11] MOD=3 REG[0b011] RM[nnn]                                                              | REG0=GPR8_B():rw REG1=XED_REG_CL:r:IMPL
 | RCR                  | ROTATE         | BASE           | I86            | d3           | mm         | 0b011      | 0xD3 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM()                                                       | MEM0:rw:v REG0=XED_REG_CL:r:IMPL
 | RCR                  | ROTATE         | BASE           | I86            | d3           | 0b11       | 0b011      | 0xD3 MOD[0b11] MOD=3 REG[0b011] RM[nnn]                                                              | REG0=GPRv_B():rw REG1=XED_REG_CL:r:IMPL
 | RDFSBASE             | RDWRFSGS       | RDWRFSGS       |                | 0f ae        | 0b11       | 0b000      | 0x0F 0xAE MOD[0b11] MOD=3 REG[0b000] RM[nnn] mode64 f3_refining_prefix                               | REG0=GPRy_B():w  REG1=XED_REG_FSBASE:r:SUPP:y
 | RDGSBASE             | RDWRFSGS       | RDWRFSGS       |                | 0f ae        | 0b11       | 0b001      | 0x0F 0xAE MOD[0b11] MOD=3 REG[0b001] RM[nnn] mode64 f3_refining_prefix                               | REG0=GPRy_B():w  REG1=XED_REG_GSBASE:r:SUPP:y
 | RDMSR                | SYSTEM         | BASE           | PENTIUMREAL    | 0f 32        |            |            | 0x0F 0x32                                                                                            | REG0=XED_REG_EAX:w:SUPP REG1=XED_REG_EDX:w:SUPP REG2=XED_REG_ECX:r:SUPP REG3=XED_REG_MSRS:r:SUPP
 | RDPID                | RDPID          | RDPID          | RDPID          | 0f c7        | 0b11       | 0b111      | 0x0F 0xC7 MOD[0b11] MOD=3  REG[0b111] RM[nnn]  f3_refining_prefix    not64                           | REG0=GPR32_B():w:d:u32 REG1=XED_REG_TSCAUX:r:SUPP:d:u32
 | RDPID                | RDPID          | RDPID          | RDPID          | 0f c7        | 0b11       | 0b111      | 0x0F 0xC7 MOD[0b11] MOD=3  REG[0b111] RM[nnn]  f3_refining_prefix   mode64                           | REG0=GPR64_B():w:q:u64 REG1=XED_REG_TSCAUX:r:SUPP:d:u32
 | RDPKRU               | PKU            | PKU            | PKU            | 0f 01        | 0b11       | 0b101      | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b101] RM[0b110]  no_refining_prefix                                   | REG0=XED_REG_EDX:w:SUPP REG1=XED_REG_EAX:w:SUPP REG2=XED_REG_ECX:r:SUPP
 | RDPMC                | SYSTEM         | BASE           | RDPMC          | 0f 33        |            |            | 0x0F 0x33                                                                                            | REG0=XED_REG_EAX:w:SUPP REG1=XED_REG_EDX:w:SUPP REG2=XED_REG_ECX:r:SUPP REG3=XED_REG_MSRS:r:SUPP
 | RDPRU                | RDPRU          | RDPRU          | RDPRU          | 0f 01        | 0b11       | 0b111      | 0x0F 0x01 MOD[0b11] MOD=3  REG[0b111] RM[0b101]                                                      | REG0=XED_REG_EDX:w:SUPP:d  REG1=XED_REG_EAX:w:SUPP:d REG2=XED_REG_ECX:r:SUPP:d
 | RDRAND               | RDRAND         | RDRAND         | RDRAND         | 0f c7        | 0b11       | 0b110      | 0x0F 0xC7  MOD[0b11] MOD=3 REG[0b110] RM[nnn] not_refining                                           | REG0=GPRv_B():w
 | RDSEED               | RDSEED         | RDSEED         | RDSEED         | 0f c7        | 0b11       | 0b111      | 0x0F 0xC7  MOD[0b11] MOD=3 REG[0b111] RM[nnn] not_refining                                           | REG0=GPRv_B():w
 | RDSSPD               | CET            | CET            | CET            | 0f 1e        | 0b11       | 0b001      | 0x0F 0x1E MOD[0b11] MOD=3  REG[0b001] RM[nnn]  f3_refining_prefix    W0 CET=1                        | REG0=GPR32_B():w:d:u32 REG1=XED_REG_SSP:r:SUPP:u64
 | RDSSPQ               | CET            | CET            | CET            | 0f 1e        | 0b11       | 0b001      | 0x0F 0x1E MOD[0b11] MOD=3  REG[0b001] RM[nnn]  f3_refining_prefix    W1  mode64 CET=1                | REG0=GPR64_B():w:q:u64 REG1=XED_REG_SSP:r:SUPP:u64
 | RDTSC                | SYSTEM         | BASE           | PENTIUMREAL    | 0f 31        |            |            | 0x0F 0x31                                                                                            | REG0=XED_REG_EAX:w:SUPP REG1=XED_REG_EDX:w:SUPP REG2=XED_REG_TSC:r:SUPP
 | RDTSCP               | SYSTEM         | RDTSCP         |                | 0f 01        | 0b11       | 0b111      | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b111] RM[0b001]                                                       | REG0=XED_REG_EAX:w:SUPP REG1=XED_REG_EDX:w:SUPP REG2=XED_REG_ECX:w:SUPP REG3=XED_REG_TSC:r:SUPP REG4=XED_REG_TSCAUX:r:SUPP
 | REP_INSB             | IOSTRINGOP     | BASE           | I186           | 6c           |            |            | 0x6C repe                                                                                            | MEM0:cw:SUPP:b BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP REG1=ArCX():rcw:SUPP
 | REP_INSB             | IOSTRINGOP     | BASE           | I186           | 6c           |            |            | 0x6C repne                                                                                           | MEM0:cw:SUPP:b BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP REG1=ArCX():rcw:SUPP
 | REP_INSD             | IOSTRINGOP     | BASE           | I386           | 6d           |            |            | 0x6D mode16 66_prefix  repe                                                                          | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP REG1=ArCX():rcw:SUPP
 | REP_INSD             | IOSTRINGOP     | BASE           | I386           | 6d           |            |            | 0x6D mode32 no66_prefix  repe                                                                        | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP REG1=ArCX():rcw:SUPP
 | REP_INSD             | IOSTRINGOP     | BASE           | I386           | 6d           |            |            | 0x6D mode64 norexw_prefix no66_prefix  repe                                                          | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP REG1=ArCX():rcw:SUPP
 | REP_INSD             | IOSTRINGOP     | BASE           | I386           | 6d           |            |            | 0x6D mode64 rexw_prefix  repe                                                                        | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP REG1=ArCX():rcw:SUPP
 | REP_INSD             | IOSTRINGOP     | BASE           | I386           | 6d           |            |            | 0x6D mode16 66_prefix  repne                                                                         | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP REG1=ArCX():rcw:SUPP
 | REP_INSD             | IOSTRINGOP     | BASE           | I386           | 6d           |            |            | 0x6D mode32 no66_prefix  repne                                                                       | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP REG1=ArCX():rcw:SUPP
 | REP_INSD             | IOSTRINGOP     | BASE           | I386           | 6d           |            |            | 0x6D mode64 norexw_prefix no66_prefix  repne                                                         | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP REG1=ArCX():rcw:SUPP
 | REP_INSD             | IOSTRINGOP     | BASE           | I386           | 6d           |            |            | 0x6D mode64 rexw_prefix  repne                                                                       | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP REG1=ArCX():rcw:SUPP
 | REP_INSW             | IOSTRINGOP     | BASE           | I186           | 6d           |            |            | 0x6D mode16 no66_prefix  repe                                                                        | MEM0:cw:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP REG1=ArCX():rcw:SUPP
 | REP_INSW             | IOSTRINGOP     | BASE           | I186           | 6d           |            |            | 0x6D mode32 66_prefix  repe                                                                          | MEM0:cw:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP REG1=ArCX():rcw:SUPP
 | REP_INSW             | IOSTRINGOP     | BASE           | I186           | 6d           |            |            | 0x6D mode64 norexw_prefix 66_prefix  repe                                                            | MEM0:cw:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP REG1=ArCX():rcw:SUPP
 | REP_INSW             | IOSTRINGOP     | BASE           | I186           | 6d           |            |            | 0x6D mode16 no66_prefix  repne                                                                       | MEM0:cw:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP REG1=ArCX():rcw:SUPP
 | REP_INSW             | IOSTRINGOP     | BASE           | I186           | 6d           |            |            | 0x6D mode32 66_prefix   repne                                                                        | MEM0:cw:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP REG1=ArCX():rcw:SUPP
 | REP_INSW             | IOSTRINGOP     | BASE           | I186           | 6d           |            |            | 0x6D mode64 norexw_prefix 66_prefix  repne                                                           | MEM0:cw:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP REG1=ArCX():rcw:SUPP
 | REP_LODSB            | STRINGOP       | BASE           | I86            | ac           |            |            | 0xAC repe OVERRIDE_SEG0()                                                                            | REG0=XED_REG_AL:cw:SUPP MEM0:cr:SUPP:b BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REP_LODSB            | STRINGOP       | BASE           | I86            | ac           |            |            | 0xAC repne OVERRIDE_SEG0()                                                                           | REG0=XED_REG_AL:cw:SUPP MEM0:cr:SUPP:b BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REP_LODSD            | STRINGOP       | BASE           | I386           | ad           |            |            | 0xAD mode16 66_prefix  repe OVERRIDE_SEG0()                                                          | REG0=XED_REG_EAX:cw:SUPP MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REP_LODSD            | STRINGOP       | BASE           | I386           | ad           |            |            | 0xAD mode32 no66_prefix  repe OVERRIDE_SEG0()                                                        | REG0=XED_REG_EAX:cw:SUPP MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REP_LODSD            | STRINGOP       | BASE           | I386           | ad           |            |            | 0xAD mode64 norexw_prefix no66_prefix  repe OVERRIDE_SEG0()                                          | REG0=XED_REG_EAX:cw:SUPP MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REP_LODSD            | STRINGOP       | BASE           | I386           | ad           |            |            | 0xAD mode16 66_prefix  repne OVERRIDE_SEG0()                                                         | REG0=XED_REG_EAX:cw:SUPP MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REP_LODSD            | STRINGOP       | BASE           | I386           | ad           |            |            | 0xAD mode32 no66_prefix  repne OVERRIDE_SEG0()                                                       | REG0=XED_REG_EAX:cw:SUPP MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REP_LODSD            | STRINGOP       | BASE           | I386           | ad           |            |            | 0xAD mode64 norexw_prefix no66_prefix  repne OVERRIDE_SEG0()                                         | REG0=XED_REG_EAX:cw:SUPP MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REP_LODSQ            | STRINGOP       | LONGMODE       |                | ad           |            |            | 0xAD mode64 rexw_prefix  repe OVERRIDE_SEG0()                                                        | REG0=XED_REG_RAX:cw:SUPP MEM0:cr:SUPP:q BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REP_LODSQ            | STRINGOP       | LONGMODE       |                | ad           |            |            | 0xAD mode64 rexw_prefix  repne OVERRIDE_SEG0()                                                       | REG0=XED_REG_RAX:cw:SUPP MEM0:cr:SUPP:q BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REP_LODSW            | STRINGOP       | BASE           | I86            | ad           |            |            | 0xAD mode16 no66_prefix   repe OVERRIDE_SEG0()                                                       | REG0=XED_REG_AX:cw:SUPP MEM0:cr:SUPP:w BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REP_LODSW            | STRINGOP       | BASE           | I86            | ad           |            |            | 0xAD mode32 66_prefix  repe OVERRIDE_SEG0()                                                          | REG0=XED_REG_AX:cw:SUPP MEM0:cr:SUPP:w BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REP_LODSW            | STRINGOP       | BASE           | I86            | ad           |            |            | 0xAD mode64 norexw_prefix 66_prefix  repe OVERRIDE_SEG0()                                            | REG0=XED_REG_AX:cw:SUPP MEM0:cr:SUPP:w BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REP_LODSW            | STRINGOP       | BASE           | I86            | ad           |            |            | 0xAD mode16 no66_prefix   repne OVERRIDE_SEG0()                                                      | REG0=XED_REG_AX:cw:SUPP MEM0:cr:SUPP:w BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REP_LODSW            | STRINGOP       | BASE           | I86            | ad           |            |            | 0xAD mode32 66_prefix  repne OVERRIDE_SEG0()                                                         | REG0=XED_REG_AX:cw:SUPP MEM0:cr:SUPP:w BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REP_LODSW            | STRINGOP       | BASE           | I86            | ad           |            |            | 0xAD mode64 norexw_prefix 66_prefix  repne OVERRIDE_SEG0()                                           | REG0=XED_REG_AX:cw:SUPP MEM0:cr:SUPP:w BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REP_MOVSB            | STRINGOP       | BASE           | I86            | a4           |            |            | 0xA4 repe OVERRIDE_SEG1()                                                                            | MEM0:cw:SUPP:b BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP  MEM1:cr:SUPP:b BASE1=ArSI():rcw:SUPP SEG1=FINAL_DSEG1():r:SUPP REG0=ArCX():rcw:SUPP
 | REP_MOVSB            | STRINGOP       | BASE           | I86            | a4           |            |            | 0xA4 repne OVERRIDE_SEG1()                                                                           | MEM0:cw:SUPP:b BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP  MEM1:cr:SUPP:b BASE1=ArSI():rcw:SUPP SEG1=FINAL_DSEG1():r:SUPP REG0=ArCX():rcw:SUPP
 | REP_MOVSD            | STRINGOP       | BASE           | I386           | a5           |            |            | 0xA5 mode16 66_prefix  repe OVERRIDE_SEG1()                                                          | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:cr:SUPP:d BASE1=ArSI():rcw:SUPP SEG1=FINAL_DSEG1():r:SUPP REG0=ArCX():rcw:SUPP
 | REP_MOVSD            | STRINGOP       | BASE           | I386           | a5           |            |            | 0xA5 mode32 no66_prefix  repe OVERRIDE_SEG1()                                                        | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:cr:SUPP:d BASE1=ArSI():rcw:SUPP SEG1=FINAL_DSEG1():r:SUPP REG0=ArCX():rcw:SUPP
 | REP_MOVSD            | STRINGOP       | BASE           | I386           | a5           |            |            | 0xA5 mode64 norexw_prefix no66_prefix  repe OVERRIDE_SEG1()                                          | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:cr:SUPP:d BASE1=ArSI():rcw:SUPP SEG1=FINAL_DSEG1():r:SUPP REG0=ArCX():rcw:SUPP
 | REP_MOVSD            | STRINGOP       | BASE           | I386           | a5           |            |            | 0xA5 mode16 66_prefix  repne OVERRIDE_SEG1()                                                         | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:cr:SUPP:d BASE1=ArSI():rcw:SUPP SEG1=FINAL_DSEG1():r:SUPP REG0=ArCX():rcw:SUPP
 | REP_MOVSD            | STRINGOP       | BASE           | I386           | a5           |            |            | 0xA5 mode32 no66_prefix   repne OVERRIDE_SEG1()                                                      | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:cr:SUPP:d BASE1=ArSI():rcw:SUPP SEG1=FINAL_DSEG1():r:SUPP REG0=ArCX():rcw:SUPP
 | REP_MOVSD            | STRINGOP       | BASE           | I386           | a5           |            |            | 0xA5 mode64 norexw_prefix no66_prefix  repne OVERRIDE_SEG1()                                         | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:cr:SUPP:d BASE1=ArSI():rcw:SUPP SEG1=FINAL_DSEG1():r:SUPP REG0=ArCX():rcw:SUPP
 | REP_MOVSQ            | STRINGOP       | LONGMODE       |                | a5           |            |            | 0xA5 mode64 rexw_prefix repe OVERRIDE_SEG1()                                                         | MEM0:cw:SUPP:q BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:cr:SUPP:q BASE1=ArSI():rcw:SUPP SEG1=FINAL_DSEG1():r:SUPP REG0=ArCX():rcw:SUPP
 | REP_MOVSQ            | STRINGOP       | LONGMODE       |                | a5           |            |            | 0xA5 mode64 rexw_prefix repne OVERRIDE_SEG1()                                                        | MEM0:cw:SUPP:q BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:cr:SUPP:q BASE1=ArSI():rcw:SUPP SEG1=FINAL_DSEG1():r:SUPP REG0=ArCX():rcw:SUPP
 | REP_MOVSW            | STRINGOP       | BASE           | I86            | a5           |            |            | 0xA5 mode16 no66_prefix  repe OVERRIDE_SEG1()                                                        | MEM0:cw:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:cr:SUPP:w BASE1=ArSI():rcw:SUPP SEG1=FINAL_DSEG1():r:SUPP REG0=ArCX():rcw:SUPP
 | REP_MOVSW            | STRINGOP       | BASE           | I86            | a5           |            |            | 0xA5 mode32 66_prefix  repe OVERRIDE_SEG1()                                                          | MEM0:cw:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:cr:SUPP:w BASE1=ArSI():rcw:SUPP SEG1=FINAL_DSEG1():r:SUPP REG0=ArCX():rcw:SUPP
 | REP_MOVSW            | STRINGOP       | BASE           | I86            | a5           |            |            | 0xA5 mode64 norexw_prefix 66_prefix  repe OVERRIDE_SEG1()                                            | MEM0:cw:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:cr:SUPP:w BASE1=ArSI():rcw:SUPP SEG1=FINAL_DSEG1():r:SUPP REG0=ArCX():rcw:SUPP
 | REP_MOVSW            | STRINGOP       | BASE           | I86            | a5           |            |            | 0xA5 mode16 no66_prefix  repne OVERRIDE_SEG1()                                                       | MEM0:cw:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:cr:SUPP:w BASE1=ArSI():rcw:SUPP SEG1=FINAL_DSEG1():r:SUPP REG0=ArCX():rcw:SUPP
 | REP_MOVSW            | STRINGOP       | BASE           | I86            | a5           |            |            | 0xA5 mode32 66_prefix  repne OVERRIDE_SEG1()                                                         | MEM0:cw:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:cr:SUPP:w BASE1=ArSI():rcw:SUPP SEG1=FINAL_DSEG1():r:SUPP REG0=ArCX():rcw:SUPP
 | REP_MOVSW            | STRINGOP       | BASE           | I86            | a5           |            |            | 0xA5 mode64 norexw_prefix 66_prefix  repne OVERRIDE_SEG1()                                           | MEM0:cw:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:cr:SUPP:w BASE1=ArSI():rcw:SUPP SEG1=FINAL_DSEG1():r:SUPP REG0=ArCX():rcw:SUPP
 | REP_OUTSB            | IOSTRINGOP     | BASE           | I186           | 6e           |            |            | 0x6E repe OVERRIDE_SEG0()                                                                            | REG0=XED_REG_DX:r:SUPP MEM0:cr:SUPP:b BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REP_OUTSB            | IOSTRINGOP     | BASE           | I186           | 6e           |            |            | 0x6E repne OVERRIDE_SEG0()                                                                           | REG0=XED_REG_DX:r:SUPP MEM0:cr:SUPP:b BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REP_OUTSD            | IOSTRINGOP     | BASE           | I386           | 6f           |            |            | 0x6F mode16 66_prefix  repe OVERRIDE_SEG0()                                                          | REG0=XED_REG_DX:r:SUPP MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REP_OUTSD            | IOSTRINGOP     | BASE           | I386           | 6f           |            |            | 0x6F mode32 no66_prefix  repe OVERRIDE_SEG0()                                                        | REG0=XED_REG_DX:r:SUPP MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REP_OUTSD            | IOSTRINGOP     | BASE           | I386           | 6f           |            |            | 0x6F mode64 norexw_prefix no66_prefix  repe OVERRIDE_SEG0()                                          | REG0=XED_REG_DX:r:SUPP MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REP_OUTSD            | IOSTRINGOP     | BASE           | I386           | 6f           |            |            | 0x6F mode64 rexw_prefix  repe OVERRIDE_SEG0()                                                        | REG0=XED_REG_DX:r:SUPP MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REP_OUTSD            | IOSTRINGOP     | BASE           | I386           | 6f           |            |            | 0x6F mode16 66_prefix  repne OVERRIDE_SEG0()                                                         | REG0=XED_REG_DX:r:SUPP MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REP_OUTSD            | IOSTRINGOP     | BASE           | I386           | 6f           |            |            | 0x6F mode32 no66_prefix  repne OVERRIDE_SEG0()                                                       | REG0=XED_REG_DX:r:SUPP MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REP_OUTSD            | IOSTRINGOP     | BASE           | I386           | 6f           |            |            | 0x6F mode64 norexw_prefix no66_prefix  repne OVERRIDE_SEG0()                                         | REG0=XED_REG_DX:r:SUPP MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REP_OUTSD            | IOSTRINGOP     | BASE           | I386           | 6f           |            |            | 0x6F mode64 rexw_prefix   repne OVERRIDE_SEG0()                                                      | REG0=XED_REG_DX:r:SUPP MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REP_OUTSW            | IOSTRINGOP     | BASE           | I186           | 6f           |            |            | 0x6F mode16 no66_prefix  repe OVERRIDE_SEG0()                                                        | REG0=XED_REG_DX:r:SUPP MEM0:cr:SUPP:w BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REP_OUTSW            | IOSTRINGOP     | BASE           | I186           | 6f           |            |            | 0x6F mode32 66_prefix  repe OVERRIDE_SEG0()                                                          | REG0=XED_REG_DX:r:SUPP MEM0:cr:SUPP:w BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REP_OUTSW            | IOSTRINGOP     | BASE           | I186           | 6f           |            |            | 0x6F mode64 norexw_prefix 66_prefix  repe OVERRIDE_SEG0()                                            | REG0=XED_REG_DX:r:SUPP MEM0:cr:SUPP:w BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REP_OUTSW            | IOSTRINGOP     | BASE           | I186           | 6f           |            |            | 0x6F mode16 no66_prefix  repne OVERRIDE_SEG0()                                                       | REG0=XED_REG_DX:r:SUPP MEM0:cr:SUPP:w BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REP_OUTSW            | IOSTRINGOP     | BASE           | I186           | 6f           |            |            | 0x6F mode32 66_prefix  repne OVERRIDE_SEG0()                                                         | REG0=XED_REG_DX:r:SUPP MEM0:cr:SUPP:w BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REP_OUTSW            | IOSTRINGOP     | BASE           | I186           | 6f           |            |            | 0x6F mode64 norexw_prefix 66_prefix  repne OVERRIDE_SEG0()                                           | REG0=XED_REG_DX:r:SUPP MEM0:cr:SUPP:w BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REP_STOSB            | STRINGOP       | BASE           | I86            | aa           |            |            | 0xAA repe                                                                                            | MEM0:cw:SUPP:b BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_AL:r:SUPP REG1=ArCX():rcw:SUPP
 | REP_STOSB            | STRINGOP       | BASE           | I86            | aa           |            |            | 0xAA repne                                                                                           | MEM0:cw:SUPP:b BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_AL:r:SUPP REG1=ArCX():rcw:SUPP
 | REP_STOSD            | STRINGOP       | BASE           | I386           | ab           |            |            | 0xAB mode16 66_prefix  repe                                                                          | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_EAX:r:SUPP REG1=ArCX():rcw:SUPP
 | REP_STOSD            | STRINGOP       | BASE           | I386           | ab           |            |            | 0xAB mode32 no66_prefix  repe                                                                        | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_EAX:r:SUPP REG1=ArCX():rcw:SUPP
 | REP_STOSD            | STRINGOP       | BASE           | I386           | ab           |            |            | 0xAB mode64 norexw_prefix no66_prefix  repe                                                          | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_EAX:r:SUPP REG1=ArCX():rcw:SUPP
 | REP_STOSD            | STRINGOP       | BASE           | I386           | ab           |            |            | 0xAB mode16 66_prefix  repne                                                                         | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_EAX:r:SUPP REG1=ArCX():rcw:SUPP
 | REP_STOSD            | STRINGOP       | BASE           | I386           | ab           |            |            | 0xAB mode32 no66_prefix  repne                                                                       | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_EAX:r:SUPP REG1=ArCX():rcw:SUPP
 | REP_STOSD            | STRINGOP       | BASE           | I386           | ab           |            |            | 0xAB mode64 norexw_prefix no66_prefix  repne                                                         | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_EAX:r:SUPP REG1=ArCX():rcw:SUPP
 | REP_STOSQ            | STRINGOP       | LONGMODE       |                | ab           |            |            | 0xAB mode64 rexw_prefix  repe                                                                        | MEM0:cw:SUPP:q BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_RAX:r:SUPP REG1=ArCX():rcw:SUPP
 | REP_STOSQ            | STRINGOP       | LONGMODE       |                | ab           |            |            | 0xAB mode64 rexw_prefix  repne                                                                       | MEM0:cw:SUPP:q BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_RAX:r:SUPP REG1=ArCX():rcw:SUPP
 | REP_STOSW            | STRINGOP       | BASE           | I86            | ab           |            |            | 0xAB mode16 no66_prefix  repe                                                                        | MEM0:cw:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_AX:r:SUPP REG1=ArCX():rcw:SUPP
 | REP_STOSW            | STRINGOP       | BASE           | I86            | ab           |            |            | 0xAB mode32 66_prefix  repe                                                                          | MEM0:cw:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_AX:r:SUPP REG1=ArCX():rcw:SUPP
 | REP_STOSW            | STRINGOP       | BASE           | I86            | ab           |            |            | 0xAB mode64 norexw_prefix 66_prefix repe                                                             | MEM0:cw:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_AX:r:SUPP REG1=ArCX():rcw:SUPP
 | REP_STOSW            | STRINGOP       | BASE           | I86            | ab           |            |            | 0xAB mode16 no66_prefix  repne                                                                       | MEM0:cw:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_AX:r:SUPP REG1=ArCX():rcw:SUPP
 | REP_STOSW            | STRINGOP       | BASE           | I86            | ab           |            |            | 0xAB mode32 66_prefix  repne                                                                         | MEM0:cw:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_AX:r:SUPP REG1=ArCX():rcw:SUPP
 | REP_STOSW            | STRINGOP       | BASE           | I86            | ab           |            |            | 0xAB mode64 norexw_prefix 66_prefix  repne                                                           | MEM0:cw:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_AX:r:SUPP REG1=ArCX():rcw:SUPP
 | REPE_CMPSB           | STRINGOP       | BASE           | I86            | a6           |            |            | 0xA6 repe OVERRIDE_SEG0()                                                                            | MEM0:cr:SUPP:b BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:cr:SUPP:b BASE1=ArDI():rcw:SUPP SEG1=FINAL_ESEG1():r:SUPP REG0=ArCX():rcw:SUPP
 | REPE_CMPSD           | STRINGOP       | BASE           | I386           | a7           |            |            | 0xA7 mode16 66_prefix  repe OVERRIDE_SEG0()                                                          | MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:cr:SUPP:d BASE1=ArDI():rcw:SUPP SEG1=FINAL_ESEG1():r:SUPP REG0=ArCX():rcw:SUPP
 | REPE_CMPSD           | STRINGOP       | BASE           | I386           | a7           |            |            | 0xA7 mode32 no66_prefix  repe OVERRIDE_SEG0()                                                        | MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:cr:SUPP:d BASE1=ArDI():rcw:SUPP SEG1=FINAL_ESEG1():r:SUPP REG0=ArCX():rcw:SUPP
 | REPE_CMPSD           | STRINGOP       | BASE           | I386           | a7           |            |            | 0xA7 mode64 norexw_prefix no66_prefix  repe OVERRIDE_SEG0()                                          | MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:cr:SUPP:d BASE1=ArDI():rcw:SUPP SEG1=FINAL_ESEG1():r:SUPP REG0=ArCX():rcw:SUPP
 | REPE_CMPSQ           | STRINGOP       | LONGMODE       |                | a7           |            |            | 0xA7 mode64 rexw_prefix repe OVERRIDE_SEG0()                                                         | MEM0:cr:SUPP:q BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:cr:SUPP:q BASE1=ArDI():rcw:SUPP SEG1=FINAL_ESEG1():r:SUPP REG0=ArCX():rcw:SUPP
 | REPE_CMPSW           | STRINGOP       | BASE           | I86            | a7           |            |            | 0xA7 mode16 no66_prefix repe OVERRIDE_SEG0()                                                         | MEM0:cr:SUPP:w BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:cr:SUPP:w BASE1=ArDI():rcw:SUPP SEG1=FINAL_ESEG1():r:SUPP REG0=ArCX():rcw:SUPP
 | REPE_CMPSW           | STRINGOP       | BASE           | I86            | a7           |            |            | 0xA7 mode32 66_prefix repe OVERRIDE_SEG0()                                                           | MEM0:cr:SUPP:w BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:cr:SUPP:w BASE1=ArDI():rcw:SUPP SEG1=FINAL_ESEG1():r:SUPP REG0=ArCX():rcw:SUPP
 | REPE_CMPSW           | STRINGOP       | BASE           | I86            | a7           |            |            | 0xA7 mode64 norexw_prefix 66_prefix repe OVERRIDE_SEG0()                                             | MEM0:cr:SUPP:w BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:cr:SUPP:w BASE1=ArDI():rcw:SUPP SEG1=FINAL_ESEG1():r:SUPP REG0=ArCX():rcw:SUPP
 | REPE_SCASB           | STRINGOP       | BASE           | I86            | ae           |            |            | 0xAE repe                                                                                            | REG0=XED_REG_AL:r:SUPP MEM0:cr:SUPP:b BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REPE_SCASD           | STRINGOP       | BASE           | I386           | af           |            |            | 0xAF mode16 66_prefix  repe                                                                          | REG0=XED_REG_EAX:r:SUPP MEM0:cr:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REPE_SCASD           | STRINGOP       | BASE           | I386           | af           |            |            | 0xAF mode32 no66_prefix  repe                                                                        | REG0=XED_REG_EAX:r:SUPP MEM0:cr:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REPE_SCASD           | STRINGOP       | BASE           | I386           | af           |            |            | 0xAF mode64 norexw_prefix no66_prefix repe                                                           | REG0=XED_REG_EAX:r:SUPP MEM0:cr:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REPE_SCASQ           | STRINGOP       | LONGMODE       |                | af           |            |            | 0xAF mode64 rexw_prefix  repe                                                                        | REG0=XED_REG_RAX:r:SUPP MEM0:cr:SUPP:q BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REPE_SCASW           | STRINGOP       | BASE           | I86            | af           |            |            | 0xAF mode16 no66_prefix  repe                                                                        | REG0=XED_REG_AX:r:SUPP MEM0:cr:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REPE_SCASW           | STRINGOP       | BASE           | I86            | af           |            |            | 0xAF mode32 66_prefix  repe                                                                          | REG0=XED_REG_AX:r:SUPP MEM0:cr:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REPE_SCASW           | STRINGOP       | BASE           | I86            | af           |            |            | 0xAF mode64 norexw_prefix 66_prefix  repe                                                            | REG0=XED_REG_AX:r:SUPP MEM0:cr:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REPNE_CMPSB          | STRINGOP       | BASE           | I86            | a6           |            |            | 0xA6 repne OVERRIDE_SEG0()                                                                           | MEM0:cr:SUPP:b BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:cr:SUPP:b BASE1=ArDI():rcw:SUPP SEG1=FINAL_ESEG1():r:SUPP REG0=ArCX():rcw:SUPP
 | REPNE_CMPSD          | STRINGOP       | BASE           | I386           | a7           |            |            | 0xA7 mode16 66_prefix  repne OVERRIDE_SEG0()                                                         | MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:cr:SUPP:d BASE1=ArDI():rcw:SUPP SEG1=FINAL_ESEG1():r:SUPP REG0=ArCX():rcw:SUPP
 | REPNE_CMPSD          | STRINGOP       | BASE           | I386           | a7           |            |            | 0xA7 mode32 no66_prefix  repne OVERRIDE_SEG0()                                                       | MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:cr:SUPP:d BASE1=ArDI():rcw:SUPP SEG1=FINAL_ESEG1():r:SUPP REG0=ArCX():rcw:SUPP
 | REPNE_CMPSD          | STRINGOP       | BASE           | I386           | a7           |            |            | 0xA7 mode64 norexw_prefix no66_prefix  repne OVERRIDE_SEG0()                                         | MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:cr:SUPP:d BASE1=ArDI():rcw:SUPP SEG1=FINAL_ESEG1():r:SUPP REG0=ArCX():rcw:SUPP
 | REPNE_CMPSQ          | STRINGOP       | LONGMODE       |                | a7           |            |            | 0xA7 mode64 rexw_prefix  repne OVERRIDE_SEG0()                                                       | MEM0:cr:SUPP:q BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:cr:SUPP:q BASE1=ArDI():rcw:SUPP SEG1=FINAL_ESEG1():r:SUPP REG0=ArCX():rcw:SUPP
 | REPNE_CMPSW          | STRINGOP       | BASE           | I86            | a7           |            |            | 0xA7 mode16 no66_prefix  repne OVERRIDE_SEG0()                                                       | MEM0:cr:SUPP:w BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:cr:SUPP:w BASE1=ArDI():rcw:SUPP SEG1=FINAL_ESEG1():r:SUPP REG0=ArCX():rcw:SUPP
 | REPNE_CMPSW          | STRINGOP       | BASE           | I86            | a7           |            |            | 0xA7 mode32 66_prefix  repne OVERRIDE_SEG0()                                                         | MEM0:cr:SUPP:w BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:cr:SUPP:w BASE1=ArDI():rcw:SUPP SEG1=FINAL_ESEG1():r:SUPP REG0=ArCX():rcw:SUPP
 | REPNE_CMPSW          | STRINGOP       | BASE           | I86            | a7           |            |            | 0xA7 mode64 norexw_prefix 66_prefix  repne OVERRIDE_SEG0()                                           | MEM0:cr:SUPP:w BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:cr:SUPP:w BASE1=ArDI():rcw:SUPP SEG1=FINAL_ESEG1():r:SUPP REG0=ArCX():rcw:SUPP
 | REPNE_SCASB          | STRINGOP       | BASE           | I86            | ae           |            |            | 0xAE repne                                                                                           | REG0=XED_REG_AL:r:SUPP MEM0:cr:SUPP:b BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REPNE_SCASD          | STRINGOP       | BASE           | I386           | af           |            |            | 0xAF mode16 66_prefix  repne                                                                         | REG0=XED_REG_EAX:r:SUPP MEM0:cr:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REPNE_SCASD          | STRINGOP       | BASE           | I386           | af           |            |            | 0xAF mode32 no66_prefix  repne                                                                       | REG0=XED_REG_EAX:r:SUPP MEM0:cr:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REPNE_SCASD          | STRINGOP       | BASE           | I386           | af           |            |            | 0xAF mode64 norexw_prefix no66_prefix  repne                                                         | REG0=XED_REG_EAX:r:SUPP MEM0:cr:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REPNE_SCASQ          | STRINGOP       | LONGMODE       |                | af           |            |            | 0xAF mode64 rexw_prefix  repne                                                                       | REG0=XED_REG_RAX:r:SUPP MEM0:cr:SUPP:q BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REPNE_SCASW          | STRINGOP       | BASE           | I86            | af           |            |            | 0xAF mode16 no66_prefix  repne                                                                       | REG0=XED_REG_AX:r:SUPP MEM0:cr:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REPNE_SCASW          | STRINGOP       | BASE           | I86            | af           |            |            | 0xAF mode32 66_prefix  repne                                                                         | REG0=XED_REG_AX:r:SUPP MEM0:cr:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG1=ArCX():rcw:SUPP
 | REPNE_SCASW          | STRINGOP       | BASE           | I86            | af           |            |            | 0xAF mode64 norexw_prefix 66_prefix  repne                                                           | REG0=XED_REG_AX:r:SUPP MEM0:cr:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG1=ArCX():rcw:SUPP
 | RET_FAR              | RET            | BASE           | I86            | ca           |            |            | 0xCA UIMM16()                                                                                        | IMM0:r:w REG0=XED_REG_STACKPOP:r:spw2:SUPP REG1=rIP():w:SUPP
 | RET_FAR              | RET            | BASE           | I86            | cb           |            |            | 0xCB                                                                                                 | REG0=XED_REG_STACKPOP:r:spw2:SUPP REG1=rIP():w:SUPP
 | RET_NEAR             | RET            | BASE           | I86            | c2           |            |            | 0xC2 DF64() UIMM16() IMMUNE66_LOOP64()                                                               | IMM0:r:w REG0=XED_REG_STACKPOP:r:spw:SUPP REG1=rIP():w:SUPP
 | RET_NEAR             | RET            | BASE           | I86            | c3           |            |            | 0xC3 DF64() IMMUNE66_LOOP64()                                                                        | REG0=XED_REG_STACKPOP:r:spw:SUPP REG1=rIP():w:SUPP
 | RMPADJUST            | SYSTEM         | SNP            |                | 0f 01        | 0b11       | 0b111      | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b111] RM[0b110] f3_refining_prefix mode64                             | REG0=XED_REG_RAX:rw:IMPL REG1=XED_REG_RCX:r:IMPL REG2=XED_REG_RDX:r:IMPL
 | RMPUPDATE            | SYSTEM         | SNP            |                | 0f 01        | 0b11       | 0b111      | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b111] RM[0b110] f2_refining_prefix mode64                             | REG0=XED_REG_RAX:rw:IMPL REG1=XED_REG_RCX:r:IMPL
 | ROL                  | ROTATE         | BASE           | I186           | c0           | mm         | 0b000      | 0xC0 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() UIMM8()                                               | MEM0:rw:b IMM0:r:b
 | ROL                  | ROTATE         | BASE           | I186           | c0           | 0b11       | 0b000      | 0xC0 MOD[0b11] MOD=3 REG[0b000] RM[nnn] UIMM8()                                                      | REG0=GPR8_B():rw IMM0:r:b
 | ROL                  | ROTATE         | BASE           | I186           | c1           | mm         | 0b000      | 0xC1 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() UIMM8()                                               | MEM0:rw:v IMM0:r:b
 | ROL                  | ROTATE         | BASE           | I186           | c1           | 0b11       | 0b000      | 0xC1 MOD[0b11] MOD=3 REG[0b000] RM[nnn] UIMM8()                                                      | REG0=GPRv_B():rw IMM0:r:b
 | ROL                  | ROTATE         | BASE           | I86            | d0           | mm         | 0b000      | 0xD0 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM()  ONE()                                                | MEM0:rw:b IMM0:r:b:IMPL
 | ROL                  | ROTATE         | BASE           | I86            | d0           | 0b11       | 0b000      | 0xD0 MOD[0b11] MOD=3 REG[0b000] RM[nnn] ONE()                                                        | REG0=GPR8_B():rw IMM0:r:b:IMPL
 | ROL                  | ROTATE         | BASE           | I86            | d1           | mm         | 0b000      | 0xD1 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() ONE()                                                 | MEM0:rw:v IMM0:r:b:IMPL
 | ROL                  | ROTATE         | BASE           | I86            | d1           | 0b11       | 0b000      | 0xD1 MOD[0b11] MOD=3 REG[0b000] RM[nnn] ONE()                                                        | REG0=GPRv_B():rw IMM0:r:b:IMPL
 | ROL                  | ROTATE         | BASE           | I86            | d2           | mm         | 0b000      | 0xD2 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM()                                                       | MEM0:rw:b REG0=XED_REG_CL:r:IMPL
 | ROL                  | ROTATE         | BASE           | I86            | d2           | 0b11       | 0b000      | 0xD2 MOD[0b11] MOD=3 REG[0b000] RM[nnn]                                                              | REG0=GPR8_B():rw REG1=XED_REG_CL:r:IMPL
 | ROL                  | ROTATE         | BASE           | I86            | d3           | mm         | 0b000      | 0xD3 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM()                                                       | MEM0:rw:v REG0=XED_REG_CL:r:IMPL
 | ROL                  | ROTATE         | BASE           | I86            | d3           | 0b11       | 0b000      | 0xD3 MOD[0b11] MOD=3 REG[0b000] RM[nnn]                                                              | REG0=GPRv_B():rw REG1=XED_REG_CL:r:IMPL
 | ROR                  | ROTATE         | BASE           | I186           | c0           | mm         | 0b001      | 0xC0 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM() UIMM8()                                               | MEM0:rw:b IMM0:r:b
 | ROR                  | ROTATE         | BASE           | I186           | c0           | 0b11       | 0b001      | 0xC0 MOD[0b11] MOD=3 REG[0b001] RM[nnn] UIMM8()                                                      | REG0=GPR8_B():rw IMM0:r:b
 | ROR                  | ROTATE         | BASE           | I186           | c1           | 0b11       | 0b001      | 0xC1 MOD[0b11] MOD=3 REG[0b001] RM[nnn] UIMM8()                                                      | REG0=GPRv_B():rw IMM0:r:b
 | ROR                  | ROTATE         | BASE           | I186           | c1           | mm         | 0b001      | 0xC1 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM() UIMM8()                                               | MEM0:rw:v IMM0:r:b
 | ROR                  | ROTATE         | BASE           | I86            | d0           | mm         | 0b001      | 0xD0 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM() ONE()                                                 | MEM0:rw:b IMM0:r:b:IMPL
 | ROR                  | ROTATE         | BASE           | I86            | d0           | 0b11       | 0b001      | 0xD0 MOD[0b11] MOD=3 REG[0b001] RM[nnn] ONE()                                                        | REG0=GPR8_B():rw IMM0:r:b:IMPL
 | ROR                  | ROTATE         | BASE           | I86            | d1           | mm         | 0b001      | 0xD1 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM() ONE()                                                 | MEM0:rw:v IMM0:r:b:IMPL
 | ROR                  | ROTATE         | BASE           | I86            | d1           | 0b11       | 0b001      | 0xD1 MOD[0b11] MOD=3 REG[0b001] RM[nnn] ONE()                                                        | REG0=GPRv_B():rw IMM0:r:b:IMPL
 | ROR                  | ROTATE         | BASE           | I86            | d2           | mm         | 0b001      | 0xD2 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM()                                                       | MEM0:rw:b REG0=XED_REG_CL:r:IMPL
 | ROR                  | ROTATE         | BASE           | I86            | d2           | 0b11       | 0b001      | 0xD2 MOD[0b11] MOD=3 REG[0b001] RM[nnn]                                                              | REG0=GPR8_B():rw REG1=XED_REG_CL:r:IMPL
 | ROR                  | ROTATE         | BASE           | I86            | d3           | mm         | 0b001      | 0xD3 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM()                                                       | MEM0:rw:v REG0=XED_REG_CL:r:IMPL
 | ROR                  | ROTATE         | BASE           | I86            | d3           | 0b11       | 0b001      | 0xD3 MOD[0b11] MOD=3 REG[0b001] RM[nnn]                                                              | REG0=GPRv_B():rw REG1=XED_REG_CL:r:IMPL
 | RORX                 | BMI2           | BMI2           |                | f0           | 0b11       | rrr        | VV1 0xF0 VF2 V0F3A not64 VL128 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]  UIMM8()                       | REG0=VGPR32_R():w:d REG1=VGPR32_B():r:d IMM0:r:b
 | RORX                 | BMI2           | BMI2           |                | f0           | 0b11       | rrr        | VV1 0xF0 VF2 V0F3A W0 mode64 VL128 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]  UIMM8()                   | REG0=VGPR32_R():w:d REG1=VGPR32_B():r:d IMM0:r:b
 | RORX                 | BMI2           | BMI2           |                | f0           | mm         | rrr        | VV1 0xF0 VF2 V0F3A not64 VL128 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM() UIMM8()                | REG0=VGPR32_R():w:d MEM0:r:d IMM0:r:b
 | RORX                 | BMI2           | BMI2           |                | f0           | mm         | rrr        | VV1 0xF0 VF2 V0F3A W0 mode64 VL128 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM() UIMM8()            | REG0=VGPR32_R():w:d MEM0:r:d IMM0:r:b
 | RORX                 | BMI2           | BMI2           |                | f0           | 0b11       | rrr        | VV1 0xF0 VF2 V0F3A W1 VL128 NOVSR mode64 MOD[0b11] MOD=3 REG[rrr] RM[nnn]  UIMM8()                   | REG0=VGPR64_R():w:q REG1=VGPR64_B():r:q IMM0:r:b
 | RORX                 | BMI2           | BMI2           |                | f0           | mm         | rrr        | VV1 0xF0 VF2 V0F3A W1 VL128 NOVSR mode64 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM() UIMM8()            | REG0=VGPR64_R():w:q MEM0:r:q IMM0:r:b
 | ROUNDPD              | SSE            | SSE4           |                | 0f 3a 09     | mm         | rrr        | 0x0F 0x3A 0x09 osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()      | REG0=XMM_R():w:pd   MEM0:r:pd           IMM0:r:b
 | ROUNDPD              | SSE            | SSE4           |                | 0f 3a 09     | 0b11       | rrr        | 0x0F 0x3A 0x09 osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()             | REG0=XMM_R():w:pd   REG1=XMM_B():r:pd   IMM0:r:b
 | ROUNDPS              | SSE            | SSE4           |                | 0f 3a 08     | mm         | rrr        | 0x0F 0x3A 0x08 osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()      | REG0=XMM_R():w:ps   MEM0:r:ps           IMM0:r:b
 | ROUNDPS              | SSE            | SSE4           |                | 0f 3a 08     | 0b11       | rrr        | 0x0F 0x3A 0x08 osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()             | REG0=XMM_R():w:ps   REG1=XMM_B():r:ps   IMM0:r:b
 | ROUNDSD              | SSE            | SSE4           |                | 0f 3a 0b     | mm         | rrr        | 0x0F 0x3A 0x0B osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()      | REG0=XMM_R():w:q   MEM0:r:q           IMM0:r:b
 | ROUNDSD              | SSE            | SSE4           |                | 0f 3a 0b     | 0b11       | rrr        | 0x0F 0x3A 0x0B osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()             | REG0=XMM_R():w:q   REG1=XMM_B():r:q   IMM0:r:b
 | ROUNDSS              | SSE            | SSE4           |                | 0f 3a 0a     | mm         | rrr        | 0x0F 0x3A 0x0A osz_refining_prefix REFINING66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()      | REG0=XMM_R():w:d   MEM0:r:d           IMM0:r:b
 | ROUNDSS              | SSE            | SSE4           |                | 0f 3a 0a     | 0b11       | rrr        | 0x0F 0x3A 0x0A osz_refining_prefix REFINING66() MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()             | REG0=XMM_R():w:d   REG1=XMM_B():r:d   IMM0:r:b
 | RSM                  | SYSRET         | BASE           | I486           | 0f aa        |            |            | 0x0F 0xAA                                                                                            | REG0=rIP():w:SUPP
 | RSQRTPS              | SSE            | SSE            |                | 0f 52        | mm         | rrr        | 0x0F 0x52 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=XMM_R():w:ps MEM0:r:ps
 | RSQRTPS              | SSE            | SSE            |                | 0f 52        | 0b11       | rrr        | 0x0F 0x52 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=XMM_R():w:ps REG1=XMM_B():r:ps
 | RSQRTSS              | SSE            | SSE            |                | 0f 52        | mm         | rrr        | 0x0F 0x52 f3_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM()                     | REG0=XMM_R():w:ss MEM0:r:ss
 | RSQRTSS              | SSE            | SSE            |                | 0f 52        | 0b11       | rrr        | 0x0F 0x52 f3_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66()                            | REG0=XMM_R():w:ss REG1=XMM_B():r:ss
 | RSTORSSP             | CET            | CET            | CET            | 0f 01        | mm         | 0b101      | 0x0F 0x01 MOD[mm] MOD!=3 REG[0b101] RM[nnn]  MODRM()  f3_refining_prefix                             | MEM0:rw:q:u64 REG0=XED_REG_SSP:w:SUPP:u64
 | SAHF                 | FLAGOP         | BASE           | LAHF           | 9e           |            |            | 0x9E                                                                                                 | REG0=XED_REG_AH:r:SUPP
 | SALC                 | FLAGOP         | BASE           | I86            | d6           |            |            | 0xD6 not64                                                                                           | REG0=XED_REG_AL:w:SUPP
 | SAR                  | SHIFT          | BASE           | I186           | c0           | mm         | 0b111      | 0xC0 MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM() UIMM8()                                               | MEM0:rw:b IMM0:r:b
 | SAR                  | SHIFT          | BASE           | I186           | c0           | 0b11       | 0b111      | 0xC0 MOD[0b11] MOD=3 REG[0b111] RM[nnn] UIMM8()                                                      | REG0=GPR8_B():rw IMM0:r:b
 | SAR                  | SHIFT          | BASE           | I186           | c1           | mm         | 0b111      | 0xC1 MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM() UIMM8()                                               | MEM0:rw:v IMM0:r:b
 | SAR                  | SHIFT          | BASE           | I186           | c1           | 0b11       | 0b111      | 0xC1 MOD[0b11] MOD=3 REG[0b111] RM[nnn] UIMM8()                                                      | REG0=GPRv_B():rw IMM0:r:b
 | SAR                  | SHIFT          | BASE           | I86            | d0           | mm         | 0b111      | 0xD0 MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM() ONE()                                                 | MEM0:rw:b IMM0:r:b:IMPL
 | SAR                  | SHIFT          | BASE           | I86            | d0           | 0b11       | 0b111      | 0xD0 MOD[0b11] MOD=3 REG[0b111] RM[nnn] ONE()                                                        | REG0=GPR8_B():rw IMM0:r:b:IMPL
 | SAR                  | SHIFT          | BASE           | I86            | d1           | mm         | 0b111      | 0xD1 MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM() ONE()                                                 | MEM0:rw:v IMM0:r:b:IMPL
 | SAR                  | SHIFT          | BASE           | I86            | d1           | 0b11       | 0b111      | 0xD1 MOD[0b11] MOD=3 REG[0b111] RM[nnn] ONE()                                                        | REG0=GPRv_B():rw IMM0:r:b:IMPL
 | SAR                  | SHIFT          | BASE           | I86            | d2           | mm         | 0b111      | 0xD2 MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM()                                                       | MEM0:rw:b REG0=XED_REG_CL:r:IMPL
 | SAR                  | SHIFT          | BASE           | I86            | d2           | 0b11       | 0b111      | 0xD2 MOD[0b11] MOD=3 REG[0b111] RM[nnn]                                                              | REG0=GPR8_B():rw REG1=XED_REG_CL:r:IMPL
 | SAR                  | SHIFT          | BASE           | I86            | d3           | mm         | 0b111      | 0xD3 MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM()                                                       | MEM0:rw:v REG0=XED_REG_CL:r:IMPL
 | SAR                  | SHIFT          | BASE           | I86            | d3           | 0b11       | 0b111      | 0xD3 MOD[0b11] MOD=3 REG[0b111] RM[nnn]                                                              | REG0=GPRv_B():rw REG1=XED_REG_CL:r:IMPL
 | SARX                 | BMI2           | BMI2           |                | f7           | mm         | rrr        | VV1 0xF7 V0F38 VF3 not64 VL128 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=VGPR32_R():w:d MEM0:r:d REG1=VGPR32_N():r:d
 | SARX                 | BMI2           | BMI2           |                | f7           | mm         | rrr        | VV1 0xF7 V0F38 VF3 W0 mode64 VL128 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                           | REG0=VGPR32_R():w:d MEM0:r:d REG1=VGPR32_N():r:d
 | SARX                 | BMI2           | BMI2           |                | f7           | 0b11       | rrr        | VV1 0xF7 V0F38 VF3 not64 VL128  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=VGPR32_R():w:d REG1=VGPR32_B():r:d REG2=VGPR32_N():r:d
 | SARX                 | BMI2           | BMI2           |                | f7           | 0b11       | rrr        | VV1 0xF7 V0F38 VF3 W0 mode64 VL128  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                 | REG0=VGPR32_R():w:d REG1=VGPR32_B():r:d REG2=VGPR32_N():r:d
 | SARX                 | BMI2           | BMI2           |                | f7           | mm         | rrr        | VV1 0xF7 V0F38 VF3  W1 VL128 mode64  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                         | REG0=VGPR64_R():w:q MEM0:r:q REG1=VGPR64_N():r:q
 | SARX                 | BMI2           | BMI2           |                | f7           | 0b11       | rrr        | VV1 0xF7 V0F38 VF3  W1 VL128 mode64 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                 | REG0=VGPR64_R():w:q REG1=VGPR64_B():r:q  REG2=VGPR64_N():r:q
 | SAVEPREVSSP          | CET            | CET            | CET            | 0f 01        | 0b11       | 0b101      | 0x0F 0x01 MOD[0b11] MOD=3  REG[0b101] RM[0b010]  f3_refining_prefix                                  | REG0=XED_REG_SSP:r:SUPP:u64
 | SBB                  | BINARY         | BASE           | I86            | 80           | mm         | 0b011      | 0x80 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM() SIMM8() nolock_prefix                                 | MEM0:rw:b IMM0:r:b:i8
 | SBB                  | BINARY         | BASE           | I86            | 80           | 0b11       | 0b011      | 0x80 MOD[0b11] MOD=3 REG[0b011] RM[nnn] SIMM8()                                                      | REG0=GPR8_B():rw IMM0:r:b:i8
 | SBB                  | BINARY         | BASE           | I86            | 81           | mm         | 0b011      | 0x81 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM() SIMMz() nolock_prefix                                 | MEM0:rw:v IMM0:r:z
 | SBB                  | BINARY         | BASE           | I86            | 81           | 0b11       | 0b011      | 0x81 MOD[0b11] MOD=3 REG[0b011] RM[nnn] SIMMz()                                                      | REG0=GPRv_B():rw IMM0:r:z
 | SBB                  | BINARY         | BASE           | I86            | 82           | mm         | 0b011      | 0x82 MOD[mm] MOD!=3 REG[0b011] RM[nnn] not64 MODRM() SIMM8() nolock_prefix                           | MEM0:rw:b IMM0:r:b:i8
 | SBB                  | BINARY         | BASE           | I86            | 82           | 0b11       | 0b011      | 0x82 MOD[0b11] MOD=3 REG[0b011] RM[nnn] not64 SIMM8()                                                | REG0=GPR8_B():rw IMM0:r:b:i8
 | SBB                  | BINARY         | BASE           | I86            | 83           | mm         | 0b011      | 0x83 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM() SIMM8() nolock_prefix                                 | MEM0:rw:v IMM0:r:b:i8
 | SBB                  | BINARY         | BASE           | I86            | 83           | 0b11       | 0b011      | 0x83 MOD[0b11] MOD=3 REG[0b011] RM[nnn] SIMM8()                                                      | REG0=GPRv_B():rw IMM0:r:b:i8
 | SBB                  | BINARY         | BASE           | I86            | 18           | mm         | rrr        | 0x18 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                                           | MEM0:rw:b REG0=GPR8_R():r
 | SBB                  | BINARY         | BASE           | I86            | 18           | 0b11       | rrr        | 0x18 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPR8_B():rw REG1=GPR8_R():r
 | SBB                  | BINARY         | BASE           | I86            | 19           | mm         | rrr        | 0x19 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                                           | MEM0:rw:v REG0=GPRv_R():r
 | SBB                  | BINARY         | BASE           | I86            | 19           | 0b11       | rrr        | 0x19 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPRv_B():rw REG1=GPRv_R():r
 | SBB                  | BINARY         | BASE           | I86            | 1a           | 0b11       | rrr        | 0x1A MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPR8_R():rw REG1=GPR8_B():r
 | SBB                  | BINARY         | BASE           | I86            | 1a           | mm         | rrr        | 0x1A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                         | REG0=GPR8_R():rw MEM0:r:b
 | SBB                  | BINARY         | BASE           | I86            | 1b           | 0b11       | rrr        | 0x1B MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPRv_R():rw REG1=GPRv_B():r
 | SBB                  | BINARY         | BASE           | I86            | 1b           | mm         | rrr        | 0x1B MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                         | REG0=GPRv_R():rw MEM0:r:v
 | SBB                  | BINARY         | BASE           | I86            | 1c           |            |            | 0x1C SIMM8()                                                                                         | REG0=XED_REG_AL:rw:IMPL IMM0:r:b:i8
 | SBB                  | BINARY         | BASE           | I86            | 1d           |            |            | 0x1D SIMMz()                                                                                         | REG0=OrAX():rw:IMPL IMM0:r:z
 | SBB_LOCK             | BINARY         | BASE           | I86            | 80           | mm         | 0b011      | 0x80 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM() SIMM8() lock_prefix                                   | MEM0:rw:b IMM0:r:b:i8
 | SBB_LOCK             | BINARY         | BASE           | I86            | 81           | mm         | 0b011      | 0x81 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM() SIMMz() lock_prefix                                   | MEM0:rw:v IMM0:r:z
 | SBB_LOCK             | BINARY         | BASE           | I86            | 82           | mm         | 0b011      | 0x82 MOD[mm] MOD!=3 REG[0b011] RM[nnn] not64 MODRM() SIMM8() lock_prefix                             | MEM0:rw:b IMM0:r:b:i8
 | SBB_LOCK             | BINARY         | BASE           | I86            | 83           | mm         | 0b011      | 0x83 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM() SIMM8() lock_prefix                                   | MEM0:rw:v IMM0:r:b:i8
 | SBB_LOCK             | BINARY         | BASE           | I86            | 18           | mm         | rrr        | 0x18 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                                             | MEM0:rw:b REG0=GPR8_R():r
 | SBB_LOCK             | BINARY         | BASE           | I86            | 19           | mm         | rrr        | 0x19 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                                             | MEM0:rw:v REG0=GPRv_R():r
 | SCASB                | STRINGOP       | BASE           | I86            | ae           |            |            | 0xAE norep                                                                                           | REG0=XED_REG_AL:r:SUPP MEM0:r:SUPP:b BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP
 | SCASD                | STRINGOP       | BASE           | I386           | af           |            |            | 0xAF mode16 66_prefix  norep                                                                         | REG0=XED_REG_EAX:r:SUPP MEM0:r:SUPP:d BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP
 | SCASD                | STRINGOP       | BASE           | I386           | af           |            |            | 0xAF mode32 no66_prefix  norep                                                                       | REG0=XED_REG_EAX:r:SUPP MEM0:r:SUPP:d BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP
 | SCASD                | STRINGOP       | BASE           | I386           | af           |            |            | 0xAF mode64 norexw_prefix no66_prefix norep                                                          | REG0=XED_REG_EAX:r:SUPP MEM0:r:SUPP:d BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP
 | SCASQ                | STRINGOP       | LONGMODE       |                | af           |            |            | 0xAF mode64 rexw_prefix  norep                                                                       | REG0=XED_REG_RAX:r:SUPP MEM0:r:SUPP:q BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP
 | SCASW                | STRINGOP       | BASE           | I86            | af           |            |            | 0xAF mode16 no66_prefix  norep                                                                       | REG0=XED_REG_AX:r:SUPP MEM0:r:SUPP:w BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP
 | SCASW                | STRINGOP       | BASE           | I86            | af           |            |            | 0xAF mode32 66_prefix  norep                                                                         | REG0=XED_REG_AX:r:SUPP MEM0:r:SUPP:w BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP
 | SCASW                | STRINGOP       | BASE           | I86            | af           |            |            | 0xAF mode64 norexw_prefix 66_prefix norep                                                            | REG0=XED_REG_AX:r:SUPP MEM0:r:SUPP:w BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP
 | SERIALIZE            | SERIALIZE      | SERIALIZE      | SERIALIZE      | 0f 01        | 0b11       | 0b101      | 0x0F 0x01 MOD[0b11] MOD=3  REG[0b101] RM[0b000]  no_refining_prefix                                  | 
 | SETB                 | SETCC          | BASE           | I386           | 0f 92        | mm         | rrr        | 0x0F 0x92 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | MEM0:w:b
 | SETB                 | SETCC          | BASE           | I386           | 0f 92        | 0b11       | rrr        | 0x0F 0x92 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPR8_B():w
 | SETBE                | SETCC          | BASE           | I386           | 0f 96        | mm         | rrr        | 0x0F 0x96 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | MEM0:w:b
 | SETBE                | SETCC          | BASE           | I386           | 0f 96        | 0b11       | rrr        | 0x0F 0x96 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPR8_B():w
 | SETL                 | SETCC          | BASE           | I386           | 0f 9c        | mm         | rrr        | 0x0F 0x9C MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | MEM0:w:b
 | SETL                 | SETCC          | BASE           | I386           | 0f 9c        | 0b11       | rrr        | 0x0F 0x9C MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPR8_B():w
 | SETLE                | SETCC          | BASE           | I386           | 0f 9e        | mm         | rrr        | 0x0F 0x9E MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | MEM0:w:b
 | SETLE                | SETCC          | BASE           | I386           | 0f 9e        | 0b11       | rrr        | 0x0F 0x9E MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPR8_B():w
 | SETNB                | SETCC          | BASE           | I386           | 0f 93        | mm         | rrr        | 0x0F 0x93 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | MEM0:w:b
 | SETNB                | SETCC          | BASE           | I386           | 0f 93        | 0b11       | rrr        | 0x0F 0x93 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPR8_B():w
 | SETNBE               | SETCC          | BASE           | I386           | 0f 97        | mm         | rrr        | 0x0F 0x97 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | MEM0:w:b
 | SETNBE               | SETCC          | BASE           | I386           | 0f 97        | 0b11       | rrr        | 0x0F 0x97 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPR8_B():w
 | SETNL                | SETCC          | BASE           | I386           | 0f 9d        | mm         | rrr        | 0x0F 0x9D MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | MEM0:w:b
 | SETNL                | SETCC          | BASE           | I386           | 0f 9d        | 0b11       | rrr        | 0x0F 0x9D MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPR8_B():w
 | SETNLE               | SETCC          | BASE           | I386           | 0f 9f        | mm         | rrr        | 0x0F 0x9F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | MEM0:w:b
 | SETNLE               | SETCC          | BASE           | I386           | 0f 9f        | 0b11       | rrr        | 0x0F 0x9F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPR8_B():w
 | SETNO                | SETCC          | BASE           | I386           | 0f 91        | mm         | rrr        | 0x0F 0x91 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | MEM0:w:b
 | SETNO                | SETCC          | BASE           | I386           | 0f 91        | 0b11       | rrr        | 0x0F 0x91 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPR8_B():w
 | SETNP                | SETCC          | BASE           | I386           | 0f 9b        | mm         | rrr        | 0x0F 0x9B MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | MEM0:w:b
 | SETNP                | SETCC          | BASE           | I386           | 0f 9b        | 0b11       | rrr        | 0x0F 0x9B MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPR8_B():w
 | SETNS                | SETCC          | BASE           | I386           | 0f 99        | mm         | rrr        | 0x0F 0x99 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | MEM0:w:b
 | SETNS                | SETCC          | BASE           | I386           | 0f 99        | 0b11       | rrr        | 0x0F 0x99 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPR8_B():w
 | SETNZ                | SETCC          | BASE           | I386           | 0f 95        | mm         | rrr        | 0x0F 0x95 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | MEM0:w:b
 | SETNZ                | SETCC          | BASE           | I386           | 0f 95        | 0b11       | rrr        | 0x0F 0x95 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPR8_B():w
 | SETO                 | SETCC          | BASE           | I386           | 0f 90        | mm         | rrr        | 0x0F 0x90 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | MEM0:w:b
 | SETO                 | SETCC          | BASE           | I386           | 0f 90        | 0b11       | rrr        | 0x0F 0x90 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPR8_B():w
 | SETP                 | SETCC          | BASE           | I386           | 0f 9a        | mm         | rrr        | 0x0F 0x9A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | MEM0:w:b
 | SETP                 | SETCC          | BASE           | I386           | 0f 9a        | 0b11       | rrr        | 0x0F 0x9A MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPR8_B():w
 | SETS                 | SETCC          | BASE           | I386           | 0f 98        | mm         | rrr        | 0x0F 0x98 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | MEM0:w:b
 | SETS                 | SETCC          | BASE           | I386           | 0f 98        | 0b11       | rrr        | 0x0F 0x98 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPR8_B():w
 | SETSSBSY             | CET            | CET            | CET            | 0f 01        | 0b11       | 0b101      | 0x0F 0x01 MOD[0b11] MOD=3  REG[0b101] RM[0b000]  f3_refining_prefix                                  | 
 | SETZ                 | SETCC          | BASE           | I386           | 0f 94        | mm         | rrr        | 0x0F 0x94 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | MEM0:w:b
 | SETZ                 | SETCC          | BASE           | I386           | 0f 94        | 0b11       | rrr        | 0x0F 0x94 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPR8_B():w
 | SFENCE               | MISC           | SSE            |                | 0f ae        | 0b11       | 0b111      | 0x0F 0xAE  MOD[0b11] MOD=3 REG[0b111] RM[nnn]  no_refining_prefix                                    | 
 | SGDT                 | SYSTEM         | BASE           | I286REAL       | 0f 01        | mm         | 0b000      | 0x0F 0x01 MOD[mm] MOD!=3 REG[0b000] RM[nnn] mode64 FORCE64() MODRM()                                 | MEM0:w:s64 REG0=XED_REG_GDTR:r:SUPP
 | SGDT                 | SYSTEM         | BASE           | I286REAL       | 0f 01        | mm         | 0b000      | 0x0F 0x01 MOD[mm] MOD!=3 REG[0b000] RM[nnn] not64 MODRM()                                            | MEM0:w:s REG0=XED_REG_GDTR:r:SUPP
 | SHA1MSG1             | SHA            | SHA            | SHA            | 0f 38 c9     | 0b11       | rrr        | 0x0F 0x38 0xC9 MOD[0b11] MOD=3  REG[rrr] RM[nnn]  no_refining_prefix                                 | REG0=XMM_R():rw:dq:i32 REG1=XMM_B():r:dq:i32
 | SHA1MSG1             | SHA            | SHA            | SHA            | 0f 38 c9     | mm         | rrr        | 0x0F 0x38 0xC9 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  no_refining_prefix                          | REG0=XMM_R():rw:dq:i32 MEM0:r:dq:i32
 | SHA1MSG2             | SHA            | SHA            | SHA            | 0f 38 ca     | 0b11       | rrr        | 0x0F 0x38 0xCA MOD[0b11] MOD=3  REG[rrr] RM[nnn]  no_refining_prefix                                 | REG0=XMM_R():rw:dq:i32 REG1=XMM_B():r:dq:i32
 | SHA1MSG2             | SHA            | SHA            | SHA            | 0f 38 ca     | mm         | rrr        | 0x0F 0x38 0xCA MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  no_refining_prefix                          | REG0=XMM_R():rw:dq:i32 MEM0:r:dq:i32
 | SHA1NEXTE            | SHA            | SHA            | SHA            | 0f 38 c8     | 0b11       | rrr        | 0x0F 0x38 0xC8 MOD[0b11] MOD=3  REG[rrr] RM[nnn]  no_refining_prefix                                 | REG0=XMM_R():rw:dq:i32 REG1=XMM_B():r:dq:i32
 | SHA1NEXTE            | SHA            | SHA            | SHA            | 0f 38 c8     | mm         | rrr        | 0x0F 0x38 0xC8 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  no_refining_prefix                          | REG0=XMM_R():rw:dq:i32 MEM0:r:dq:i32
 | SHA1RNDS4            | SHA            | SHA            | SHA            | 0f 3a cc     | 0b11       | rrr        | 0x0F 0x3A 0xCC MOD[0b11] MOD=3  REG[rrr] RM[nnn]  no_refining_prefix     UIMM8()                     | REG0=XMM_R():rw:dq:i32 REG1=XMM_B():r:dq:i32 IMM0:r:b
 | SHA1RNDS4            | SHA            | SHA            | SHA            | 0f 3a cc     | mm         | rrr        | 0x0F 0x3A 0xCC MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  no_refining_prefix     UIMM8()              | REG0=XMM_R():rw:dq:i32 MEM0:r:dq:i32 IMM0:r:b
 | SHA256MSG1           | SHA            | SHA            | SHA            | 0f 38 cc     | 0b11       | rrr        | 0x0F 0x38 0xCC MOD[0b11] MOD=3  REG[rrr] RM[nnn]  no_refining_prefix                                 | REG0=XMM_R():rw:dq:i32 REG1=XMM_B():r:dq:i32
 | SHA256MSG1           | SHA            | SHA            | SHA            | 0f 38 cc     | mm         | rrr        | 0x0F 0x38 0xCC MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  no_refining_prefix                          | REG0=XMM_R():rw:dq:i32 MEM0:r:dq:i32
 | SHA256MSG2           | SHA            | SHA            | SHA            | 0f 38 cd     | 0b11       | rrr        | 0x0F 0x38 0xCD MOD[0b11] MOD=3  REG[rrr] RM[nnn]  no_refining_prefix                                 | REG0=XMM_R():rw:dq:i32 REG1=XMM_B():r:dq:i32
 | SHA256MSG2           | SHA            | SHA            | SHA            | 0f 38 cd     | mm         | rrr        | 0x0F 0x38 0xCD MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  no_refining_prefix                          | REG0=XMM_R():rw:dq:i32 MEM0:r:dq:i32
 | SHA256RNDS2          | SHA            | SHA            | SHA            | 0f 38 cb     | 0b11       | rrr        | 0x0F 0x38 0xCB MOD[0b11] MOD=3  REG[rrr] RM[nnn]  no_refining_prefix                                 | REG0=XMM_R():rw:dq:i32 REG1=XMM_B():r:dq:i32 REG2=XED_REG_XMM0:r:SUPP:dq:u8
 | SHA256RNDS2          | SHA            | SHA            | SHA            | 0f 38 cb     | mm         | rrr        | 0x0F 0x38 0xCB MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  no_refining_prefix                          | REG0=XMM_R():rw:dq:i32 MEM0:r:dq:i32 REG1=XED_REG_XMM0:r:SUPP:dq:u8
 | SHL                  | SHIFT          | BASE           | I186           | c0           | mm         | 0b100      | 0xC0 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM() UIMM8()                                               | MEM0:rw:b IMM0:r:b
 | SHL                  | SHIFT          | BASE           | I186           | c0           | 0b11       | 0b100      | 0xC0 MOD[0b11] MOD=3 REG[0b100] RM[nnn] UIMM8()                                                      | REG0=GPR8_B():rw IMM0:r:b
 | SHL                  | SHIFT          | BASE           | I186           | c0           | mm         | 0b110      | 0xC0 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM() UIMM8()                                               | MEM0:rw:b IMM0:r:b
 | SHL                  | SHIFT          | BASE           | I186           | c0           | 0b11       | 0b110      | 0xC0 MOD[0b11] MOD=3 REG[0b110] RM[nnn] UIMM8()                                                      | REG0=GPR8_B():rw IMM0:r:b
 | SHL                  | SHIFT          | BASE           | I186           | c1           | mm         | 0b100      | 0xC1 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM() UIMM8()                                               | MEM0:rw:v IMM0:r:b
 | SHL                  | SHIFT          | BASE           | I186           | c1           | 0b11       | 0b100      | 0xC1 MOD[0b11] MOD=3 REG[0b100] RM[nnn] UIMM8()                                                      | REG0=GPRv_B():rw IMM0:r:b
 | SHL                  | SHIFT          | BASE           | I186           | c1           | mm         | 0b110      | 0xC1 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM() UIMM8()                                               | MEM0:rw:v IMM0:r:b
 | SHL                  | SHIFT          | BASE           | I186           | c1           | 0b11       | 0b110      | 0xC1 MOD[0b11] MOD=3 REG[0b110] RM[nnn] UIMM8()                                                      | REG0=GPRv_B():rw IMM0:r:b
 | SHL                  | SHIFT          | BASE           | I86            | d0           | mm         | 0b100      | 0xD0 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM() ONE()                                                 | MEM0:rw:b IMM0:r:b:IMPL
 | SHL                  | SHIFT          | BASE           | I86            | d0           | 0b11       | 0b100      | 0xD0 MOD[0b11] MOD=3 REG[0b100] RM[nnn] ONE()                                                        | REG0=GPR8_B():rw IMM0:r:b:IMPL
 | SHL                  | SHIFT          | BASE           | I86            | d0           | mm         | 0b110      | 0xD0 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM() ONE()                                                 | MEM0:rw:b IMM0:r:b:IMPL
 | SHL                  | SHIFT          | BASE           | I86            | d0           | 0b11       | 0b110      | 0xD0 MOD[0b11] MOD=3 REG[0b110] RM[nnn] ONE()                                                        | REG0=GPR8_B():rw IMM0:r:b:IMPL
 | SHL                  | SHIFT          | BASE           | I86            | d1           | mm         | 0b110      | 0xD1 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM() ONE()                                                 | MEM0:rw:v IMM0:r:b:IMPL
 | SHL                  | SHIFT          | BASE           | I86            | d1           | 0b11       | 0b110      | 0xD1 MOD[0b11] MOD=3 REG[0b110] RM[nnn] ONE()                                                        | REG0=GPRv_B():rw IMM0:r:b:IMPL
 | SHL                  | SHIFT          | BASE           | I86            | d1           | mm         | 0b100      | 0xD1 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM() ONE()                                                 | MEM0:rw:v IMM0:r:b:IMPL
 | SHL                  | SHIFT          | BASE           | I86            | d1           | 0b11       | 0b100      | 0xD1 MOD[0b11] MOD=3 REG[0b100] RM[nnn] ONE()                                                        | REG0=GPRv_B():rw IMM0:r:b:IMPL
 | SHL                  | SHIFT          | BASE           | I86            | d2           | mm         | 0b100      | 0xD2 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM()                                                       | MEM0:rw:b REG0=XED_REG_CL:r:IMPL
 | SHL                  | SHIFT          | BASE           | I86            | d2           | 0b11       | 0b100      | 0xD2 MOD[0b11] MOD=3 REG[0b100] RM[nnn]                                                              | REG0=GPR8_B():rw REG1=XED_REG_CL:r:IMPL
 | SHL                  | SHIFT          | BASE           | I86            | d2           | mm         | 0b110      | 0xD2 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM()                                                       | MEM0:rw:b REG0=XED_REG_CL:r:IMPL
 | SHL                  | SHIFT          | BASE           | I86            | d2           | 0b11       | 0b110      | 0xD2 MOD[0b11] MOD=3 REG[0b110] RM[nnn]                                                              | REG0=GPR8_B():rw REG1=XED_REG_CL:r:IMPL
 | SHL                  | SHIFT          | BASE           | I86            | d3           | mm         | 0b100      | 0xD3 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM()                                                       | MEM0:rw:v REG0=XED_REG_CL:r:IMPL
 | SHL                  | SHIFT          | BASE           | I86            | d3           | 0b11       | 0b100      | 0xD3 MOD[0b11] MOD=3 REG[0b100] RM[nnn]                                                              | REG0=GPRv_B():rw REG1=XED_REG_CL:r:IMPL
 | SHL                  | SHIFT          | BASE           | I86            | d3           | mm         | 0b110      | 0xD3 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM()                                                       | MEM0:rw:v REG0=XED_REG_CL:r:IMPL
 | SHL                  | SHIFT          | BASE           | I86            | d3           | 0b11       | 0b110      | 0xD3 MOD[0b11] MOD=3 REG[0b110] RM[nnn]                                                              | REG0=GPRv_B():rw REG1=XED_REG_CL:r:IMPL
 | SHLD                 | SHIFT          | BASE           | I386           | 0f a4        | mm         | rrr        | 0x0F 0xA4 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                                            | MEM0:rcw:v REG0=GPRv_R():r IMM0:r:b
 | SHLD                 | SHIFT          | BASE           | I386           | 0f a4        | 0b11       | rrr        | 0x0F 0xA4 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                                   | REG0=GPRv_B():rcw REG1=GPRv_R():r IMM0:r:b
 | SHLD                 | SHIFT          | BASE           | I386           | 0f a5        | mm         | rrr        | 0x0F 0xA5 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | MEM0:rcw:v REG0=GPRv_R():r REG1=XED_REG_CL:r:IMPL
 | SHLD                 | SHIFT          | BASE           | I386           | 0f a5        | 0b11       | rrr        | 0x0F 0xA5 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_B():rcw REG1=GPRv_R():r REG2=XED_REG_CL:r:IMPL
 | SHLX                 | BMI2           | BMI2           |                | f7           | mm         | rrr        | VV1 0xF7 V0F38 V66 not64 VL128 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=VGPR32_R():w:d MEM0:r:d REG1=VGPR32_N():r:d
 | SHLX                 | BMI2           | BMI2           |                | f7           | mm         | rrr        | VV1 0xF7 V0F38 V66 W0 mode64 VL128 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                           | REG0=VGPR32_R():w:d MEM0:r:d REG1=VGPR32_N():r:d
 | SHLX                 | BMI2           | BMI2           |                | f7           | 0b11       | rrr        | VV1 0xF7 V0F38 V66 not64 VL128  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=VGPR32_R():w:d REG1=VGPR32_B():r:d REG2=VGPR32_N():r:d
 | SHLX                 | BMI2           | BMI2           |                | f7           | 0b11       | rrr        | VV1 0xF7 V0F38 V66 W0 mode64 VL128  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                 | REG0=VGPR32_R():w:d REG1=VGPR32_B():r:d REG2=VGPR32_N():r:d
 | SHLX                 | BMI2           | BMI2           |                | f7           | mm         | rrr        | VV1 0xF7 V0F38 V66  W1 VL128 mode64  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                         | REG0=VGPR64_R():w:q MEM0:r:q REG1=VGPR64_N():r:q
 | SHLX                 | BMI2           | BMI2           |                | f7           | 0b11       | rrr        | VV1 0xF7 V0F38 V66  W1 VL128 mode64 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                 | REG0=VGPR64_R():w:q REG1=VGPR64_B():r:q  REG2=VGPR64_N():r:q
 | SHR                  | SHIFT          | BASE           | I186           | c0           | mm         | 0b101      | 0xC0 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM() UIMM8()                                               | MEM0:rw:b IMM0:r:b
 | SHR                  | SHIFT          | BASE           | I186           | c0           | 0b11       | 0b101      | 0xC0 MOD[0b11] MOD=3 REG[0b101] RM[nnn] UIMM8()                                                      | REG0=GPR8_B():rw IMM0:r:b
 | SHR                  | SHIFT          | BASE           | I186           | c1           | mm         | 0b101      | 0xC1 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM() UIMM8()                                               | MEM0:rw:v IMM0:r:b
 | SHR                  | SHIFT          | BASE           | I186           | c1           | 0b11       | 0b101      | 0xC1 MOD[0b11] MOD=3 REG[0b101] RM[nnn] UIMM8()                                                      | REG0=GPRv_B():rw IMM0:r:b
 | SHR                  | SHIFT          | BASE           | I86            | d0           | mm         | 0b101      | 0xD0 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM() ONE()                                                 | MEM0:rw:b IMM0:r:b:IMPL
 | SHR                  | SHIFT          | BASE           | I86            | d0           | 0b11       | 0b101      | 0xD0 MOD[0b11] MOD=3 REG[0b101] RM[nnn] ONE()                                                        | REG0=GPR8_B():rw IMM0:r:b:IMPL
 | SHR                  | SHIFT          | BASE           | I86            | d1           | mm         | 0b101      | 0xD1 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM() ONE()                                                 | MEM0:rw:v IMM0:r:b:IMPL
 | SHR                  | SHIFT          | BASE           | I86            | d1           | 0b11       | 0b101      | 0xD1 MOD[0b11] MOD=3 REG[0b101] RM[nnn] ONE()                                                        | REG0=GPRv_B():rw IMM0:r:b:IMPL
 | SHR                  | SHIFT          | BASE           | I86            | d2           | mm         | 0b101      | 0xD2 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM()                                                       | MEM0:rw:b REG0=XED_REG_CL:r:IMPL
 | SHR                  | SHIFT          | BASE           | I86            | d2           | 0b11       | 0b101      | 0xD2 MOD[0b11] MOD=3 REG[0b101] RM[nnn]                                                              | REG0=GPR8_B():rw REG1=XED_REG_CL:r:IMPL
 | SHR                  | SHIFT          | BASE           | I86            | d3           | mm         | 0b101      | 0xD3 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM()                                                       | MEM0:rw:v REG0=XED_REG_CL:r:IMPL
 | SHR                  | SHIFT          | BASE           | I86            | d3           | 0b11       | 0b101      | 0xD3 MOD[0b11] MOD=3 REG[0b101] RM[nnn]                                                              | REG0=GPRv_B():rw REG1=XED_REG_CL:r:IMPL
 | SHRD                 | SHIFT          | BASE           | I386           | 0f ac        | mm         | rrr        | 0x0F 0xAC MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                                            | MEM0:rcw:v REG0=GPRv_R():r IMM0:r:b
 | SHRD                 | SHIFT          | BASE           | I386           | 0f ac        | 0b11       | rrr        | 0x0F 0xAC MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                                   | REG0=GPRv_B():rcw REG1=GPRv_R():r IMM0:r:b
 | SHRD                 | SHIFT          | BASE           | I386           | 0f ad        | mm         | rrr        | 0x0F 0xAD MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                    | MEM0:rcw:v REG0=GPRv_R():r REG1=XED_REG_CL:r:IMPL
 | SHRD                 | SHIFT          | BASE           | I386           | 0f ad        | 0b11       | rrr        | 0x0F 0xAD MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_B():rcw REG1=GPRv_R():r REG2=XED_REG_CL:r:IMPL
 | SHRX                 | BMI2           | BMI2           |                | f7           | mm         | rrr        | VV1 0xF7 V0F38 VF2 not64 VL128 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=VGPR32_R():w:d MEM0:r:d REG1=VGPR32_N():r:d
 | SHRX                 | BMI2           | BMI2           |                | f7           | mm         | rrr        | VV1 0xF7 V0F38 VF2 W0 mode64 VL128 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                           | REG0=VGPR32_R():w:d MEM0:r:d REG1=VGPR32_N():r:d
 | SHRX                 | BMI2           | BMI2           |                | f7           | 0b11       | rrr        | VV1 0xF7 V0F38 VF2 not64 VL128  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=VGPR32_R():w:d REG1=VGPR32_B():r:d REG2=VGPR32_N():r:d
 | SHRX                 | BMI2           | BMI2           |                | f7           | 0b11       | rrr        | VV1 0xF7 V0F38 VF2 W0 mode64 VL128  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                 | REG0=VGPR32_R():w:d REG1=VGPR32_B():r:d REG2=VGPR32_N():r:d
 | SHRX                 | BMI2           | BMI2           |                | f7           | mm         | rrr        | VV1 0xF7 V0F38 VF2  W1 VL128 mode64  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                         | REG0=VGPR64_R():w:q MEM0:r:q REG1=VGPR64_N():r:q
 | SHRX                 | BMI2           | BMI2           |                | f7           | 0b11       | rrr        | VV1 0xF7 V0F38 VF2  W1 VL128 mode64 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                 | REG0=VGPR64_R():w:q REG1=VGPR64_B():r:q  REG2=VGPR64_N():r:q
 | SHUFPD               | SSE            | SSE2           |                | 0f c6        | mm         | rrr        | 0x0F 0xC6 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM() UIMM8()          | REG0=XMM_R():rw:pd MEM0:r:pd IMM0:r:b
 | SHUFPD               | SSE            | SSE2           |                | 0f c6        | 0b11       | rrr        | 0x0F 0xC6 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66() UIMM8()                 | REG0=XMM_R():rw:pd REG1=XMM_B():r:pd IMM0:r:b
 | SHUFPS               | SSE            | SSE            |                | 0f c6        | mm         | rrr        | 0x0F 0xC6 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM() UIMM8()                        | REG0=XMM_R():rw:ps MEM0:r:ps IMM0:r:b
 | SHUFPS               | SSE            | SSE            |                | 0f c6        | 0b11       | rrr        | 0x0F 0xC6 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  UIMM8()                               | REG0=XMM_R():rw:ps REG1=XMM_B():r:ps IMM0:r:b
 | SIDT                 | SYSTEM         | BASE           | I286REAL       | 0f 01        | mm         | 0b001      | 0x0F 0x01 MOD[mm] MOD!=3 REG[0b001] RM[nnn] not64 MODRM()                                            | MEM0:w:s REG0=XED_REG_IDTR:r:SUPP
 | SIDT                 | SYSTEM         | BASE           | I286REAL       | 0f 01        | mm         | 0b001      | 0x0F 0x01 MOD[mm] MOD!=3 REG[0b001] RM[nnn] mode64 FORCE64() MODRM()                                 | MEM0:w:s64 REG0=XED_REG_IDTR:r:SUPP
 | SKINIT               | SYSTEM         | SVM            |                | 0f 01        | 0b11       | 0b011      | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b011] RM[0b110]                                                       | REG0=XED_REG_EAX:r:IMPL
 | SLDT                 | SYSTEM         | BASE           | I286PROTECTED  | 0f 00        | mm         | 0b000      | 0x0F 0x00 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM()                                                  | MEM0:w:w REG0=XED_REG_LDTR:r:SUPP
 | SLDT                 | SYSTEM         | BASE           | I286PROTECTED  | 0f 00        | 0b11       | 0b000      | 0x0F 0x00 MOD[0b11] MOD=3 REG[0b000] RM[nnn]                                                         | REG0=GPRv_B():w REG1=XED_REG_LDTR:r:SUPP
 | SLWPCB               | XOP            | XOP            | XOP            | 12           | 0b11       | 0b001      | XOPV 0x12 VNP VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[0b001] RM[nnn]                                   | REG0=GPRy_B():w:y
 | SMSW                 | SYSTEM         | BASE           | I286REAL       | 0f 01        | mm         | 0b100      | 0x0F 0x01 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM()                                                  | MEM0:w:w REG0=XED_REG_CR0:r:SUPP
 | SMSW                 | SYSTEM         | BASE           | I286REAL       | 0f 01        | 0b11       | 0b100      | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b100] RM[nnn]                                                         | REG0=GPRv_B():w REG1=XED_REG_CR0:r:SUPP
 | SPFLT                | KNCSCALAR      | KNC            | KNCV           | ae           | 0b11       | 0b110      | VV1 0xAE  VL128 VF2 V0F NOVSR MOD[0b11] MOD=3 REG[0b110] RM[nnn]  W0                                 | REG0=GPR32_B():r:d
 | SPFLT                | KNCSCALAR      | KNC            | KNCV           | ae           | 0b11       | 0b110      | VV1 0xAE  VL128 VF2 V0F NOVSR MOD[0b11] MOD=3 REG[0b110] RM[nnn]  W1                                 | REG0=GPR64_B():r:q
 | SQRTPD               | SSE            | SSE2           |                | 0f 51        | mm         | rrr        | 0x0F 0x51 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():w:pd MEM0:r:pd
 | SQRTPD               | SSE            | SSE2           |                | 0f 51        | 0b11       | rrr        | 0x0F 0x51 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():w:pd REG1=XMM_B():r:pd
 | SQRTPS               | SSE            | SSE            |                | 0f 51        | mm         | rrr        | 0x0F 0x51 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=XMM_R():w:ps MEM0:r:ps
 | SQRTPS               | SSE            | SSE            |                | 0f 51        | 0b11       | rrr        | 0x0F 0x51 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=XMM_R():w:ps REG1=XMM_B():r:ps
 | SQRTSD               | SSE            | SSE2           |                | 0f 51        | mm         | rrr        | 0x0F 0x51 f2_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM()                     | REG0=XMM_R():w:sd MEM0:r:sd
 | SQRTSD               | SSE            | SSE2           |                | 0f 51        | 0b11       | rrr        | 0x0F 0x51 f2_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66()                            | REG0=XMM_R():w:sd REG1=XMM_B():r:sd
 | SQRTSS               | SSE            | SSE            |                | 0f 51        | mm         | rrr        | 0x0F 0x51 f3_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM()                     | REG0=XMM_R():w:ss MEM0:r:ss
 | SQRTSS               | SSE            | SSE            |                | 0f 51        | 0b11       | rrr        | 0x0F 0x51 f3_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66()                            | REG0=XMM_R():w:ss REG1=XMM_B():r:ss
 | STAC                 | SMAP           | SMAP           |                | 0f 01        | 0b11       | 0b001      | 0x0F 0x01  MOD[0b11] MOD=3 REG[0b001] RM[0b011] no_refining_prefix                                   | 
 | STC                  | FLAGOP         | BASE           | I86            | f9           |            |            | 0xF9                                                                                                 | 
 | STD                  | FLAGOP         | BASE           | I86            | fd           |            |            | 0xFD                                                                                                 | 
 | STGI                 | SYSTEM         | SVM            |                | 0f 01        | 0b11       | 0b011      | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b011] RM[0b100]                                                       | 
 | STI                  | FLAGOP         | BASE           | I86            | fb           |            |            | 0xFB                                                                                                 | 
 | STMXCSR              | SSE            | SSE            | SSEMXCSR       | 0f ae        | mm         | 0b011      | 0x0F 0xAE MOD[mm] MOD!=3 REG[0b011] RM[nnn] no_refining_prefix  MODRM()                              | MEM0:w:d REG0=XED_REG_MXCSR:r:SUPP
 | STOSB                | STRINGOP       | BASE           | I86            | aa           |            |            | 0xAA norep                                                                                           | MEM0:w:SUPP:b BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_AL:r:SUPP
 | STOSD                | STRINGOP       | BASE           | I386           | ab           |            |            | 0xAB mode16 66_prefix  norep                                                                         | MEM0:w:SUPP:d BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_EAX:r:SUPP
 | STOSD                | STRINGOP       | BASE           | I386           | ab           |            |            | 0xAB mode32 no66_prefix  norep                                                                       | MEM0:w:SUPP:d BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_EAX:r:SUPP
 | STOSD                | STRINGOP       | BASE           | I386           | ab           |            |            | 0xAB mode64 norexw_prefix no66_prefix  norep                                                         | MEM0:w:SUPP:d BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_EAX:r:SUPP
 | STOSQ                | STRINGOP       | LONGMODE       |                | ab           |            |            | 0xAB mode64 rexw_prefix  norep                                                                       | MEM0:w:SUPP:q BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_RAX:r:SUPP
 | STOSW                | STRINGOP       | BASE           | I86            | ab           |            |            | 0xAB mode16 no66_prefix  norep                                                                       | MEM0:w:SUPP:w BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_AX:r:SUPP
 | STOSW                | STRINGOP       | BASE           | I86            | ab           |            |            | 0xAB mode32 66_prefix  norep                                                                         | MEM0:w:SUPP:w BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_AX:r:SUPP
 | STOSW                | STRINGOP       | BASE           | I86            | ab           |            |            | 0xAB mode64 norexw_prefix 66_prefix  norep                                                           | MEM0:w:SUPP:w BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_AX:r:SUPP
 | STR                  | SYSTEM         | BASE           | I286PROTECTED  | 0f 00        | mm         | 0b001      | 0x0F 0x00 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM()                                                  | MEM0:w:w REG0=XED_REG_TR:r:SUPP
 | STR                  | SYSTEM         | BASE           | I286PROTECTED  | 0f 00        | 0b11       | 0b001      | 0x0F 0x00 MOD[0b11] MOD=3 REG[0b001] RM[nnn]                                                         | REG0=GPRv_B():w REG1=XED_REG_TR:r:SUPP
 | SUB                  | BINARY         | BASE           | I86            | 80           | mm         | 0b101      | 0x80 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM() SIMM8() nolock_prefix                                 | MEM0:rw:b IMM0:r:b:i8
 | SUB                  | BINARY         | BASE           | I86            | 80           | 0b11       | 0b101      | 0x80 MOD[0b11] MOD=3 REG[0b101] RM[nnn] SIMM8()                                                      | REG0=GPR8_B():rw IMM0:r:b:i8
 | SUB                  | BINARY         | BASE           | I86            | 81           | mm         | 0b101      | 0x81 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM() SIMMz() nolock_prefix                                 | MEM0:rw:v IMM0:r:z
 | SUB                  | BINARY         | BASE           | I86            | 81           | 0b11       | 0b101      | 0x81 MOD[0b11] MOD=3 REG[0b101] RM[nnn] SIMMz()                                                      | REG0=GPRv_B():rw IMM0:r:z
 | SUB                  | BINARY         | BASE           | I86            | 82           | mm         | 0b101      | 0x82 MOD[mm] MOD!=3 REG[0b101] RM[nnn] not64 MODRM() SIMM8() nolock_prefix                           | MEM0:rw:b IMM0:r:b:i8
 | SUB                  | BINARY         | BASE           | I86            | 82           | 0b11       | 0b101      | 0x82 MOD[0b11] MOD=3 REG[0b101] RM[nnn] not64 SIMM8()                                                | REG0=GPR8_B():rw IMM0:r:b:i8
 | SUB                  | BINARY         | BASE           | I86            | 83           | mm         | 0b101      | 0x83 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM() SIMM8() nolock_prefix                                 | MEM0:rw:v IMM0:r:b:i8
 | SUB                  | BINARY         | BASE           | I86            | 83           | 0b11       | 0b101      | 0x83 MOD[0b11] MOD=3 REG[0b101] RM[nnn] SIMM8()                                                      | REG0=GPRv_B():rw IMM0:r:b:i8
 | SUB                  | BINARY         | BASE           | I86            | 28           | mm         | rrr        | 0x28 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                                           | MEM0:rw:b REG0=GPR8_R():r
 | SUB                  | BINARY         | BASE           | I86            | 28           | 0b11       | rrr        | 0x28 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPR8_B():rw REG1=GPR8_R():r
 | SUB                  | BINARY         | BASE           | I86            | 29           | mm         | rrr        | 0x29 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                                           | MEM0:rw:v REG0=GPRv_R():r
 | SUB                  | BINARY         | BASE           | I86            | 29           | 0b11       | rrr        | 0x29 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPRv_B():rw REG1=GPRv_R():r
 | SUB                  | BINARY         | BASE           | I86            | 2a           | 0b11       | rrr        | 0x2A MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPR8_R():rw REG1=GPR8_B():r
 | SUB                  | BINARY         | BASE           | I86            | 2a           | mm         | rrr        | 0x2A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                         | REG0=GPR8_R():rw MEM0:r:b
 | SUB                  | BINARY         | BASE           | I86            | 2b           | 0b11       | rrr        | 0x2B MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPRv_R():rw REG1=GPRv_B():r
 | SUB                  | BINARY         | BASE           | I86            | 2b           | mm         | rrr        | 0x2B MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                         | REG0=GPRv_R():rw MEM0:r:v
 | SUB                  | BINARY         | BASE           | I86            | 2c           |            |            | 0x2C SIMM8()                                                                                         | REG0=XED_REG_AL:rw:IMPL IMM0:r:b:i8
 | SUB                  | BINARY         | BASE           | I86            | 2d           |            |            | 0x2D SIMMz()                                                                                         | REG0=OrAX():rw:IMPL IMM0:r:z
 | SUB_LOCK             | BINARY         | BASE           | I86            | 80           | mm         | 0b101      | 0x80 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM() SIMM8() lock_prefix                                   | MEM0:rw:b IMM0:r:b:i8
 | SUB_LOCK             | BINARY         | BASE           | I86            | 81           | mm         | 0b101      | 0x81 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM() SIMMz() lock_prefix                                   | MEM0:rw:v IMM0:r:z
 | SUB_LOCK             | BINARY         | BASE           | I86            | 82           | mm         | 0b101      | 0x82 MOD[mm] MOD!=3 REG[0b101] RM[nnn] not64 MODRM() SIMM8() lock_prefix                             | MEM0:rw:b IMM0:r:b:i8
 | SUB_LOCK             | BINARY         | BASE           | I86            | 83           | mm         | 0b101      | 0x83 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM() SIMM8() lock_prefix                                   | MEM0:rw:v IMM0:r:b:i8
 | SUB_LOCK             | BINARY         | BASE           | I86            | 28           | mm         | rrr        | 0x28 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                                             | MEM0:rw:b REG0=GPR8_R():r
 | SUB_LOCK             | BINARY         | BASE           | I86            | 29           | mm         | rrr        | 0x29 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                                             | MEM0:rw:v REG0=GPRv_R():r
 | SUBPD                | SSE            | SSE2           |                | 0f 5c        | mm         | rrr        | 0x0F 0x5C osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:pd MEM0:r:pd
 | SUBPD                | SSE            | SSE2           |                | 0f 5c        | 0b11       | rrr        | 0x0F 0x5C osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:pd REG1=XMM_B():r:pd
 | SUBPS                | SSE            | SSE            |                | 0f 5c        | mm         | rrr        | 0x0F 0x5C no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=XMM_R():rw:ps MEM0:r:ps
 | SUBPS                | SSE            | SSE            |                | 0f 5c        | 0b11       | rrr        | 0x0F 0x5C no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=XMM_R():rw:ps REG1=XMM_B():r:ps
 | SUBSD                | SSE            | SSE2           |                | 0f 5c        | mm         | rrr        | 0x0F 0x5C f2_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM()                     | REG0=XMM_R():rw:sd MEM0:r:sd
 | SUBSD                | SSE            | SSE2           |                | 0f 5c        | 0b11       | rrr        | 0x0F 0x5C f2_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66()                            | REG0=XMM_R():rw:sd REG1=XMM_B():r:sd
 | SUBSS                | SSE            | SSE            |                | 0f 5c        | mm         | rrr        | 0x0F 0x5C f3_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  IGNORE66() MODRM()                     | REG0=XMM_R():rw:ss MEM0:r:ss
 | SUBSS                | SSE            | SSE            |                | 0f 5c        | 0b11       | rrr        | 0x0F 0x5C f3_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  IGNORE66()                            | REG0=XMM_R():rw:ss REG1=XMM_B():r:ss
 | SWAPGS               | SYSTEM         | LONGMODE       |                | 0f 01        | 0b11       | 0b111      | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b111] RM[0b000] mode64                                                | 
 | SYSCALL              | SYSCALL        | LONGMODE       | LONGMODE       | 0f 05        |            |            | 0x0F 0x05 mode64 FORCE64()                                                                           | REG0=XED_REG_RIP:w:SUPP REG1=XED_REG_RCX:w:SUPP REG2=XED_REG_R11:w:SUPP
 | SYSCALL_AMD          | SYSCALL        | BASE           | AMD            | 0f 05        |            |            | 0x0F 0x05 not64 IGNORE66()                                                                           | REG0=rIP():w:SUPP
 | SYSENTER             | SYSCALL        | BASE           | PPRO           | 0f 34        |            |            | 0x0F 0x34 not64                                                                                      | REG0=XED_REG_EIP:w:SUPP REG1=XED_REG_ESP:w:SUPP
 | SYSENTER             | SYSCALL        | BASE           | PPRO           | 0f 34        |            |            | 0x0F 0x34 mode64                                                                                     | REG0=XED_REG_RIP:w:SUPP REG1=XED_REG_RSP:w:SUPP
 | SYSEXIT              | SYSRET         | BASE           | PPRO           | 0f 35        |            |            | 0x0F 0x35 not64                                                                                      | REG0=XED_REG_EIP:w:SUPP  REG1=XED_REG_ESP:w:SUPP REG2=XED_REG_ECX:r:SUPP REG3=XED_REG_EDX:r:SUPP
 | SYSEXIT              | SYSRET         | BASE           | PPRO           | 0f 35        |            |            | 0x0F 0x35 mode64                                                                                     | REG0=XED_REG_RIP:w:SUPP  REG1=XED_REG_RSP:w:SUPP REG2=XED_REG_RCX:r:SUPP REG3=XED_REG_RDX:r:SUPP
 | SYSRET               | SYSRET         | LONGMODE       | LONGMODE       | 0f 07        |            |            | 0x0F 0x07 mode64 norexw_prefix                                                                       | REG0=XED_REG_EIP:w:SUPP  REG1=XED_REG_ECX:r:SUPP
 | SYSRET_AMD           | SYSRET         | BASE           | AMD            | 0f 07        |            |            | 0x0F 0x07 not64                                                                                      | REG0=XED_REG_EIP:w:SUPP
 | SYSRET64             | SYSRET         | LONGMODE       | LONGMODE       | 0f 07        |            |            | 0x0F 0x07 mode64 rexw_prefix                                                                         | REG0=XED_REG_RIP:w:SUPP  REG1=XED_REG_RCX:r:SUPP  REG2=XED_REG_R11:r:SUPP
 | T1MSKC               | TBM            | TBM            | TBM            | 01           | mm         | 0b111      | XOPV 0x01 VNP not64 VL128  XMAP9 MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM()                           | REG0=VGPR32_N():w:d MEM0:r:d
 | T1MSKC               | TBM            | TBM            | TBM            | 01           | mm         | 0b111      | XOPV 0x01 VNP mode64 VL128  XMAP9 MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM()                          | REG0=VGPRy_N():w:y MEM0:r:y
 | T1MSKC               | TBM            | TBM            | TBM            | 01           | 0b11       | 0b111      | XOPV 0x01 VNP not64 VL128  XMAP9 MOD[0b11] MOD=3 REG[0b111] RM[nnn]                                  | REG0=VGPR32_N():w:d REG1=GPR32_B():r:d
 | T1MSKC               | TBM            | TBM            | TBM            | 01           | 0b11       | 0b111      | XOPV 0x01 VNP mode64 VL128  XMAP9 MOD[0b11] MOD=3 REG[0b111] RM[nnn]                                 | REG0=VGPRy_N():w:y REG1=GPRy_B():r:y
 | TEST                 | LOGICAL        | BASE           | I86            | f6           | mm         | 0b000      | 0xF6 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() SIMM8()                                               | MEM0:r:b IMM0:r:b:i8
 | TEST                 | LOGICAL        | BASE           | I86            | f6           | mm         | 0b001      | 0xF6 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM() SIMM8()                                               | MEM0:r:b IMM0:r:b:i8
 | TEST                 | LOGICAL        | BASE           | I86            | f6           | 0b11       | 0b000      | 0xF6 MOD[0b11] MOD=3 REG[0b000] RM[nnn] SIMM8()                                                      | REG0=GPR8_B():r IMM0:r:b:i8
 | TEST                 | LOGICAL        | BASE           | I86            | f6           | 0b11       | 0b001      | 0xF6 MOD[0b11] MOD=3 REG[0b001] RM[nnn] SIMM8()                                                      | REG0=GPR8_B():r IMM0:r:b:i8
 | TEST                 | LOGICAL        | BASE           | I86            | f7           | mm         | 0b000      | 0xF7 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() SIMMz()                                               | MEM0:r:v IMM0:r:z
 | TEST                 | LOGICAL        | BASE           | I86            | f7           | mm         | 0b001      | 0xF7 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM() SIMMz()                                               | MEM0:r:v IMM0:r:z
 | TEST                 | LOGICAL        | BASE           | I86            | f7           | 0b11       | 0b000      | 0xF7 MOD[0b11] MOD=3 REG[0b000] RM[nnn] SIMMz()                                                      | REG0=GPRv_B():r IMM0:r:z
 | TEST                 | LOGICAL        | BASE           | I86            | f7           | 0b11       | 0b001      | 0xF7 MOD[0b11] MOD=3 REG[0b001] RM[nnn] SIMMz()                                                      | REG0=GPRv_B():r IMM0:r:z
 | TEST                 | LOGICAL        | BASE           | I86            | 84           | mm         | rrr        | 0x84 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                         | MEM0:r:b REG0=GPR8_R():r
 | TEST                 | LOGICAL        | BASE           | I86            | 84           | 0b11       | rrr        | 0x84 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPR8_B():r REG1=GPR8_R():r
 | TEST                 | LOGICAL        | BASE           | I86            | 85           | mm         | rrr        | 0x85 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                         | MEM0:r:v REG0=GPRv_R():r
 | TEST                 | LOGICAL        | BASE           | I86            | 85           | 0b11       | rrr        | 0x85 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPRv_B():r REG1=GPRv_R():r
 | TEST                 | LOGICAL        | BASE           | I86            | a8           |            |            | 0xA8 SIMM8()                                                                                         | REG0=XED_REG_AL:r:IMPL IMM0:r:b:i8
 | TEST                 | LOGICAL        | BASE           | I86            | a9           |            |            | 0xA9 SIMMz()                                                                                         | REG0=OrAX():r:IMPL IMM0:r:z
 | TPAUSE               | WAITPKG        | WAITPKG        | WAITPKG        | 0f ae        | 0b11       | 0b110      | 0x0F 0xAE MOD[0b11] MOD=3  REG[0b110] RM[nnn]  osz_refining_prefix      norexw_prefix                | REG0=GPR32_B():r:d:u32 REG1=XED_REG_EDX:r:SUPP:d:u32 REG2=XED_REG_EAX:r:SUPP:d:u32
 | TPAUSE               | WAITPKG        | WAITPKG        | WAITPKG        | 0f ae        | 0b11       | 0b110      | 0x0F 0xAE MOD[0b11] MOD=3  REG[0b110] RM[nnn]  osz_refining_prefix      mode64 rexw_prefix           | REG0=GPR64_B():r:q:u64 REG1=XED_REG_EDX:r:SUPP:d:u32 REG2=XED_REG_EAX:r:SUPP:d:u32
 | TZCNT                | BMI1           | BMI1           |                | 0f bc        | mm         | rrr        | 0x0F 0xBC refining_f3  TZCNT=1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=GPRv_R():w MEM0:r:v
 | TZCNT                | BMI1           | BMI1           |                | 0f bc        | 0b11       | rrr        | 0x0F 0xBC refining_f3 TZCNT=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_R():w  REG1=GPRv_B():r
 | TZCNT_VEX            | KNCSCALAR      | KNC            | KNCV           | bc           | 0b11       | rrr        | VV1 0xBC  VL128 VF3 V0F W0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                    | REG0=GPR32_R():w:d   REG1=GPR32_B():r:d
 | TZCNT_VEX            | KNCSCALAR      | KNC            | KNCV           | bc           | 0b11       | rrr        | VV1 0xBC  VL128 VF3 V0F W1 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                    | REG0=GPR64_R():w:q  REG1=GPR64_B():r:q
 | TZCNTI               | KNCSCALAR      | KNC            | KNCV           | bc           | 0b11       | rrr        | VV1 0xBC  VL128 VF2 V0F W0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                    | REG0=GPR32_R():rw:d   REG1=GPR32_B():r:d
 | TZCNTI               | KNCSCALAR      | KNC            | KNCV           | bc           | 0b11       | rrr        | VV1 0xBC  VL128 VF2 V0F W1 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                    | REG0=GPR64_R():rw:q  REG1=GPR64_B():r:q
 | TZMSK                | TBM            | TBM            | TBM            | 01           | mm         | 0b100      | XOPV 0x01 VNP not64 VL128  XMAP9 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM()                           | REG0=VGPR32_N():w:d MEM0:r:d
 | TZMSK                | TBM            | TBM            | TBM            | 01           | mm         | 0b100      | XOPV 0x01 VNP mode64 VL128  XMAP9 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM()                          | REG0=VGPRy_N():w:y MEM0:r:y
 | TZMSK                | TBM            | TBM            | TBM            | 01           | 0b11       | 0b100      | XOPV 0x01 VNP not64 VL128  XMAP9 MOD[0b11] MOD=3 REG[0b100] RM[nnn]                                  | REG0=VGPR32_N():w:d REG1=GPR32_B():r:d
 | TZMSK                | TBM            | TBM            | TBM            | 01           | 0b11       | 0b100      | XOPV 0x01 VNP mode64 VL128  XMAP9 MOD[0b11] MOD=3 REG[0b100] RM[nnn]                                 | REG0=VGPRy_N():w:y REG1=GPRy_B():r:y
 | UCOMISD              | SSE            | SSE2           |                | 0f 2e        | mm         | rrr        | 0x0F 0x2E osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():r:sd MEM0:r:sd
 | UCOMISD              | SSE            | SSE2           |                | 0f 2e        | 0b11       | rrr        | 0x0F 0x2E osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():r:sd REG1=XMM_B():r:sd
 | UCOMISS              | SSE            | SSE            |                | 0f 2e        | mm         | rrr        | 0x0F 0x2E no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=XMM_R():r:ss MEM0:r:ss
 | UCOMISS              | SSE            | SSE            |                | 0f 2e        | 0b11       | rrr        | 0x0F 0x2E no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=XMM_R():r:ss REG1=XMM_B():r:ss
 | UD0                  | MISC           | BASE           | PPRO           | 0f ff        | mm         | rrr        | 0x0F 0xFF MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                                   | REG0=GPR32_R():r MEM0:r:d
 | UD0                  | MISC           | BASE           | PPRO           | 0f ff        | 0b11       | rrr        | 0x0F 0xFF MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPR32_R():r REG1=GPR32_B():r
 | UD1                  | MISC           | BASE           | PPRO           | 0f b9        | mm         | rrr        | 0x0F 0xB9 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                                   | REG0=GPR32_R():r MEM0:r:d
 | UD1                  | MISC           | BASE           | PPRO           | 0f b9        | 0b11       | rrr        | 0x0F 0xB9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPR32_R():r REG1=GPR32_B():r
 | UD2                  | MISC           | BASE           | PPRO           | 0f 0b        |            |            | 0x0F 0x0B                                                                                            | 
 | UMONITOR             | WAITPKG        | WAITPKG        | WAITPKG        | 0f ae        | 0b11       | 0b110      | 0x0F 0xAE MOD[0b11] MOD=3  REG[0b110] RM[nnn]  f3_refining_prefix                                    | REG0=A_GPR_B():r
 | UMWAIT               | WAITPKG        | WAITPKG        | WAITPKG        | 0f ae        | 0b11       | 0b110      | 0x0F 0xAE MOD[0b11] MOD=3  REG[0b110] RM[nnn]  f2_refining_prefix      norexw_prefix                 | REG0=GPR32_B():r:d:u32 REG1=XED_REG_EDX:r:SUPP:d:u32 REG2=XED_REG_EAX:r:SUPP:d:u32
 | UMWAIT               | WAITPKG        | WAITPKG        | WAITPKG        | 0f ae        | 0b11       | 0b110      | 0x0F 0xAE MOD[0b11] MOD=3  REG[0b110] RM[nnn]  f2_refining_prefix      mode64 rexw_prefix            | REG0=GPR64_B():r:q:u64 REG1=XED_REG_EDX:r:SUPP:d:u32 REG2=XED_REG_EAX:r:SUPP:d:u32
 | UNPCKHPD             | SSE            | SSE2           |                | 0f 15        | mm         | rrr        | 0x0F 0x15 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:pd MEM0:r:dq
 | UNPCKHPD             | SSE            | SSE2           |                | 0f 15        | 0b11       | rrr        | 0x0F 0x15 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:pd REG1=XMM_B():r:q
 | UNPCKHPS             | SSE            | SSE            |                | 0f 15        | mm         | rrr        | 0x0F 0x15 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=XMM_R():rw:ps MEM0:r:dq
 | UNPCKHPS             | SSE            | SSE            |                | 0f 15        | 0b11       | rrr        | 0x0F 0x15 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=XMM_R():rw:ps REG1=XMM_B():r:dq
 | UNPCKLPD             | SSE            | SSE2           |                | 0f 14        | mm         | rrr        | 0x0F 0x14 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:pd MEM0:r:dq
 | UNPCKLPD             | SSE            | SSE2           |                | 0f 14        | 0b11       | rrr        | 0x0F 0x14 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:pd REG1=XMM_B():r:q
 | UNPCKLPS             | SSE            | SSE            |                | 0f 14        | mm         | rrr        | 0x0F 0x14 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=XMM_R():rw:ps MEM0:r:dq
 | UNPCKLPS             | SSE            | SSE            |                | 0f 14        | 0b11       | rrr        | 0x0F 0x14 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=XMM_R():rw:ps REG1=XMM_B():r:q
 | V4FMADDPS            | AVX512_4FMAPS  | AVX512EVEX     | AVX512_4FMAPS_512 | 9a           | mm         | rrr        | EVV 0x9A VF2 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_TUPLE1_4X() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32:MULTISOURCE4 MEM0:r:dq:f32
 | V4FMADDSS            | AVX512_4FMAPS  | AVX512EVEX     | AVX512_4FMAPS_SCALAR | 9b           | mm         | rrr        | EVV 0x9B VF2 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_TUPLE1_4X() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32:MULTISOURCE4 MEM0:r:dq:f32
 | V4FNMADDPS           | AVX512_4FMAPS  | AVX512EVEX     | AVX512_4FMAPS_512 | aa           | mm         | rrr        | EVV 0xAA VF2 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_TUPLE1_4X() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32:MULTISOURCE4 MEM0:r:dq:f32
 | V4FNMADDSS           | AVX512_4FMAPS  | AVX512EVEX     | AVX512_4FMAPS_SCALAR | ab           | mm         | rrr        | EVV 0xAB VF2 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_TUPLE1_4X() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32:MULTISOURCE4 MEM0:r:dq:f32
 | VADDNPD              | KNC            | KNCE           |                | 50           | mm         | rrr        | KVV 0x50 V0F38 V66  REXW=1 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT64()                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VADDNPD              | KNC            | KNCE           |                | 50           | 0b11       | rrr        | KVV 0x50 V0F38 V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE64()                     | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=REGSWIZ
 | VADDNPD              | KNC            | KNCE           |                | 50           | 0b11       | rrr        | KVV 0x50 V0F38 V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                   | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=ROUNDC:TXT=SAEC
 | VADDNPS              | KNC            | KNCE           |                | 50           | mm         | rrr        | KVV 0x50 V0F38 V66  REXW=0 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT32()                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VADDNPS              | KNC            | KNCE           |                | 50           | 0b11       | rrr        | KVV 0x50 V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VADDNPS              | KNC            | KNCE           |                | 50           | 0b11       | rrr        | KVV 0x50 V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                   | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=ROUNDC:TXT=SAEC
 | VADDPD               | AVX            | AVX            |                | 58           | mm         | rrr        | VV1 0x58  V66 VL128 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
 | VADDPD               | AVX            | AVX            |                | 58           | 0b11       | rrr        | VV1 0x58  V66 VL128 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
 | VADDPD               | AVX            | AVX            |                | 58           | mm         | rrr        | VV1 0x58  V66 VL256 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
 | VADDPD               | AVX            | AVX            |                | 58           | 0b11       | rrr        | VV1 0x58  V66 VL256 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
 | VADDPD               | AVX512         | AVX512EVEX     | AVX512F_128    | 58           | 0b11       | rrr        | EVV 0x58 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                  | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VADDPD               | AVX512         | AVX512EVEX     | AVX512F_128    | 58           | mm         | rrr        | EVV 0x58 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VADDPD               | AVX512         | AVX512EVEX     | AVX512F_256    | 58           | 0b11       | rrr        | EVV 0x58 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                  | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
 | VADDPD               | AVX512         | AVX512EVEX     | AVX512F_256    | 58           | mm         | rrr        | EVV 0x58 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VADDPD               | AVX512         | AVX512EVEX     | AVX512F_512    | 58           | 0b11       | rrr        | EVV 0x58 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                  | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VADDPD               | AVX512         | AVX512EVEX     | AVX512F_512    | 58           | 0b11       | rrr        | EVV 0x58 V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1       | REG0=ZMM_R3():w:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VADDPD               | AVX512         | AVX512EVEX     | AVX512F_512    | 58           | mm         | rrr        | EVV 0x58 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VADDPD               | KNC            | KNCE           |                | 58           | mm         | rrr        | KVV 0x58 V0F V66  REXW=1 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT64()                  | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VADDPD               | KNC            | KNCE           |                | 58           | 0b11       | rrr        | KVV 0x58 V0F V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE64()                       | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=REGSWIZ
 | VADDPD               | KNC            | KNCE           |                | 58           | 0b11       | rrr        | KVV 0x58 V0F V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                     | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=ROUNDC:TXT=SAEC
 | VADDPS               | AVX            | AVX            |                | 58           | mm         | rrr        | VV1 0x58  VNP VL128 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
 | VADDPS               | AVX            | AVX            |                | 58           | 0b11       | rrr        | VV1 0x58  VNP VL128 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
 | VADDPS               | AVX            | AVX            |                | 58           | mm         | rrr        | VV1 0x58  VNP VL256 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
 | VADDPS               | AVX            | AVX            |                | 58           | 0b11       | rrr        | VV1 0x58  VNP VL256 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
 | VADDPS               | AVX512         | AVX512EVEX     | AVX512F_128    | 58           | 0b11       | rrr        | EVV 0x58 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                  | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VADDPS               | AVX512         | AVX512EVEX     | AVX512F_128    | 58           | mm         | rrr        | EVV 0x58 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VADDPS               | AVX512         | AVX512EVEX     | AVX512F_256    | 58           | 0b11       | rrr        | EVV 0x58 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                  | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
 | VADDPS               | AVX512         | AVX512EVEX     | AVX512F_256    | 58           | mm         | rrr        | EVV 0x58 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VADDPS               | AVX512         | AVX512EVEX     | AVX512F_512    | 58           | 0b11       | rrr        | EVV 0x58 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                  | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VADDPS               | AVX512         | AVX512EVEX     | AVX512F_512    | 58           | 0b11       | rrr        | EVV 0x58 VNP V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0       | REG0=ZMM_R3():w:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VADDPS               | AVX512         | AVX512EVEX     | AVX512F_512    | 58           | mm         | rrr        | EVV 0x58 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VADDPS               | KNC            | KNCE           |                | 58           | mm         | rrr        | KVV 0x58 V0F VNP  REXW=0 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT32()                  | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VADDPS               | KNC            | KNCE           |                | 58           | 0b11       | rrr        | KVV 0x58 V0F VNP  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                       | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VADDPS               | KNC            | KNCE           |                | 58           | 0b11       | rrr        | KVV 0x58 V0F VNP  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=ROUNDC:TXT=SAEC
 | VADDSD               | AVX            | AVX            |                | 58           | mm         | rrr        | VV1 0x58  VF2  V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                          | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:q:f64
 | VADDSD               | AVX            | AVX            |                | 58           | 0b11       | rrr        | VV1 0x58  VF2  V0F  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                 | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:q:f64
 | VADDSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 58           | 0b11       | rrr        | EVV 0x58 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                                         | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VADDSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 58           | 0b11       | rrr        | EVV 0x58 VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1       | REG0=XMM_R3():w:dq:f64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VADDSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 58           | mm         | rrr        | EVV 0x58 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
 | VADDSETSPS           | KNC            | KNCE           |                | cc           | 0b11       | uuu        | KVV 0xCC V0F38 V66 W0   MOD[0b11] MOD=3 REG[uuu] RM[www] NR=0 REG_SWIZZLE32()                        | REG0=ZMM_R3():rw:zf32 REG1=MASK1():rw:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VADDSETSPS           | KNC            | KNCE           |                | cc           | 0b11       | uuu        | KVV 0xCC V0F38 V66 W0   MOD[0b11] MOD=3 REG[uuu] RM[www] NR=1 ROUND() KNC_SAE()                      | REG0=ZMM_R3():rw:zf32 REG1=MASK1():rw:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=ROUNDC:TXT=SAEC
 | VADDSETSPS           | KNC            | KNCE           |                | cc           | mm         | uuu        | KVV 0xCC V0F38 V66 W0  MOD[mm] MOD!=3 REG[uuu] RM[rrr] MODRM() UPCONVERT_FLT32()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():rw:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VADDSS               | AVX            | AVX            |                | 58           | mm         | rrr        | VV1 0x58  VF3  V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                          | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:d:f32
 | VADDSS               | AVX            | AVX            |                | 58           | 0b11       | rrr        | VV1 0x58  VF3  V0F  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                 | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:d:f32
 | VADDSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 58           | 0b11       | rrr        | EVV 0x58 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                                         | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VADDSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 58           | 0b11       | rrr        | EVV 0x58 VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W0       | REG0=XMM_R3():w:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VADDSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 58           | mm         | rrr        | EVV 0x58 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
 | VADDSUBPD            | AVX            | AVX            |                | d0           | mm         | rrr        | VV1 0xD0  VL128 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
 | VADDSUBPD            | AVX            | AVX            |                | d0           | 0b11       | rrr        | VV1 0xD0  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
 | VADDSUBPD            | AVX            | AVX            |                | d0           | mm         | rrr        | VV1 0xD0  VL256 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
 | VADDSUBPD            | AVX            | AVX            |                | d0           | 0b11       | rrr        | VV1 0xD0  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
 | VADDSUBPS            | AVX            | AVX            |                | d0           | mm         | rrr        | VV1 0xD0  VL128 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
 | VADDSUBPS            | AVX            | AVX            |                | d0           | 0b11       | rrr        | VV1 0xD0  VL128 VF2 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
 | VADDSUBPS            | AVX            | AVX            |                | d0           | mm         | rrr        | VV1 0xD0  VL256 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
 | VADDSUBPS            | AVX            | AVX            |                | d0           | 0b11       | rrr        | VV1 0xD0  VL256 VF2 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
 | VAESDEC              | AES            | AVXAES         |                | de           | 0b11       | rrr        | VV1 0xDE V66 V0F38 MOD[0b11] MOD=3  REG[rrr] RM[nnn] VL128                                           | REG0=XMM_R():w:dq REG1=XMM_N():r:dq  REG2=XMM_B():r:dq
 | VAESDEC              | AES            | AVXAES         |                | de           | mm         | rrr        | VV1 0xDE V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() VL128                                     | REG0=XMM_R():w:dq REG1=XMM_N():r:dq  MEM0:r:dq
 | VAESDEC              | VAES           | AVX512EVEX     | AVX512_VAES_128 | de           | 0b11       | rrr        | EVV 0xDE V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128      ZEROING=0 MASK=0              | REG0=XMM_R3():w:dq:u128 REG1=XMM_N3():r:dq:u128 REG2=XMM_B3():r:dq:u128
 | VAESDEC              | VAES           | AVX512EVEX     | AVX512_VAES_128 | de           | mm         | rrr        | EVV 0xDE V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ZEROING=0 MASK=0  ESIZE_128_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u128 REG1=XMM_N3():r:dq:u128 MEM0:r:dq:u128
 | VAESDEC              | VAES           | AVX512EVEX     | AVX512_VAES_256 | de           | 0b11       | rrr        | EVV 0xDE V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256      ZEROING=0 MASK=0              | REG0=YMM_R3():w:qq:u128 REG1=YMM_N3():r:qq:u128 REG2=YMM_B3():r:qq:u128
 | VAESDEC              | VAES           | AVX512EVEX     | AVX512_VAES_256 | de           | mm         | rrr        | EVV 0xDE V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ZEROING=0 MASK=0  ESIZE_128_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u128 REG1=YMM_N3():r:qq:u128 MEM0:r:qq:u128
 | VAESDEC              | VAES           | AVX512EVEX     | AVX512_VAES_512 | de           | 0b11       | rrr        | EVV 0xDE V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512      ZEROING=0 MASK=0              | REG0=ZMM_R3():w:zu128 REG1=ZMM_N3():r:zu128 REG2=ZMM_B3():r:zu128
 | VAESDEC              | VAES           | AVX512EVEX     | AVX512_VAES_512 | de           | mm         | rrr        | EVV 0xDE V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ZEROING=0 MASK=0  ESIZE_128_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu128 REG1=ZMM_N3():r:zu128 MEM0:r:zd:u128
 | VAESDEC              | VAES           | VAES           | VAES           | de           | 0b11       | rrr        | VV1 0xDE V66 V0F38 MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256                                          | REG0=YMM_R():w:qq:u128 REG1=YMM_N():r:qq:u128 REG2=YMM_B():r:qq:u128
 | VAESDEC              | VAES           | VAES           | VAES           | de           | mm         | rrr        | VV1 0xDE V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256                                   | REG0=YMM_R():w:qq:u128 REG1=YMM_N():r:qq:u128 MEM0:r:qq:u128
 | VAESDECLAST          | AES            | AVXAES         |                | df           | 0b11       | rrr        | VV1 0xDF V66 V0F38 MOD[0b11] MOD=3  REG[rrr] RM[nnn] VL128                                           | REG0=XMM_R():w:dq REG1=XMM_N():r:dq  REG2=XMM_B():r:dq
 | VAESDECLAST          | AES            | AVXAES         |                | df           | mm         | rrr        | VV1 0xDF V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() VL128                                     | REG0=XMM_R():w:dq REG1=XMM_N():r:dq  MEM0:r:dq
 | VAESDECLAST          | VAES           | AVX512EVEX     | AVX512_VAES_128 | df           | 0b11       | rrr        | EVV 0xDF V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128      ZEROING=0 MASK=0              | REG0=XMM_R3():w:dq:u128 REG1=XMM_N3():r:dq:u128 REG2=XMM_B3():r:dq:u128
 | VAESDECLAST          | VAES           | AVX512EVEX     | AVX512_VAES_128 | df           | mm         | rrr        | EVV 0xDF V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ZEROING=0 MASK=0  ESIZE_128_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u128 REG1=XMM_N3():r:dq:u128 MEM0:r:dq:u128
 | VAESDECLAST          | VAES           | AVX512EVEX     | AVX512_VAES_256 | df           | 0b11       | rrr        | EVV 0xDF V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256      ZEROING=0 MASK=0              | REG0=YMM_R3():w:qq:u128 REG1=YMM_N3():r:qq:u128 REG2=YMM_B3():r:qq:u128
 | VAESDECLAST          | VAES           | AVX512EVEX     | AVX512_VAES_256 | df           | mm         | rrr        | EVV 0xDF V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ZEROING=0 MASK=0  ESIZE_128_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u128 REG1=YMM_N3():r:qq:u128 MEM0:r:qq:u128
 | VAESDECLAST          | VAES           | AVX512EVEX     | AVX512_VAES_512 | df           | 0b11       | rrr        | EVV 0xDF V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512      ZEROING=0 MASK=0              | REG0=ZMM_R3():w:zu128 REG1=ZMM_N3():r:zu128 REG2=ZMM_B3():r:zu128
 | VAESDECLAST          | VAES           | AVX512EVEX     | AVX512_VAES_512 | df           | mm         | rrr        | EVV 0xDF V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ZEROING=0 MASK=0  ESIZE_128_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu128 REG1=ZMM_N3():r:zu128 MEM0:r:zd:u128
 | VAESDECLAST          | VAES           | VAES           | VAES           | df           | 0b11       | rrr        | VV1 0xDF V66 V0F38 MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256                                          | REG0=YMM_R():w:qq:u128 REG1=YMM_N():r:qq:u128 REG2=YMM_B():r:qq:u128
 | VAESDECLAST          | VAES           | VAES           | VAES           | df           | mm         | rrr        | VV1 0xDF V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256                                   | REG0=YMM_R():w:qq:u128 REG1=YMM_N():r:qq:u128 MEM0:r:qq:u128
 | VAESENC              | AES            | AVXAES         |                | dc           | 0b11       | rrr        | VV1 0xDC V66 V0F38  MOD[0b11] MOD=3  REG[rrr] RM[nnn] VL128                                          | REG0=XMM_R():w:dq REG1=XMM_N():r:dq  REG2=XMM_B():r:dq
 | VAESENC              | AES            | AVXAES         |                | dc           | mm         | rrr        | VV1 0xDC V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM() VL128                                   | REG0=XMM_R():w:dq REG1=XMM_N():r:dq  MEM0:r:dq
 | VAESENC              | VAES           | AVX512EVEX     | AVX512_VAES_128 | dc           | 0b11       | rrr        | EVV 0xDC V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128      ZEROING=0 MASK=0              | REG0=XMM_R3():w:dq:u128 REG1=XMM_N3():r:dq:u128 REG2=XMM_B3():r:dq:u128
 | VAESENC              | VAES           | AVX512EVEX     | AVX512_VAES_128 | dc           | mm         | rrr        | EVV 0xDC V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ZEROING=0 MASK=0  ESIZE_128_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u128 REG1=XMM_N3():r:dq:u128 MEM0:r:dq:u128
 | VAESENC              | VAES           | AVX512EVEX     | AVX512_VAES_256 | dc           | 0b11       | rrr        | EVV 0xDC V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256      ZEROING=0 MASK=0              | REG0=YMM_R3():w:qq:u128 REG1=YMM_N3():r:qq:u128 REG2=YMM_B3():r:qq:u128
 | VAESENC              | VAES           | AVX512EVEX     | AVX512_VAES_256 | dc           | mm         | rrr        | EVV 0xDC V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ZEROING=0 MASK=0  ESIZE_128_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u128 REG1=YMM_N3():r:qq:u128 MEM0:r:qq:u128
 | VAESENC              | VAES           | AVX512EVEX     | AVX512_VAES_512 | dc           | 0b11       | rrr        | EVV 0xDC V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512      ZEROING=0 MASK=0              | REG0=ZMM_R3():w:zu128 REG1=ZMM_N3():r:zu128 REG2=ZMM_B3():r:zu128
 | VAESENC              | VAES           | AVX512EVEX     | AVX512_VAES_512 | dc           | mm         | rrr        | EVV 0xDC V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ZEROING=0 MASK=0  ESIZE_128_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu128 REG1=ZMM_N3():r:zu128 MEM0:r:zd:u128
 | VAESENC              | VAES           | VAES           | VAES           | dc           | 0b11       | rrr        | VV1 0xDC V66 V0F38 MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256                                          | REG0=YMM_R():w:qq:u128 REG1=YMM_N():r:qq:u128 REG2=YMM_B():r:qq:u128
 | VAESENC              | VAES           | VAES           | VAES           | dc           | mm         | rrr        | VV1 0xDC V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256                                   | REG0=YMM_R():w:qq:u128 REG1=YMM_N():r:qq:u128 MEM0:r:qq:u128
 | VAESENCLAST          | AES            | AVXAES         |                | dd           | 0b11       | rrr        | VV1 0xDD V66 V0F38 MOD[0b11] MOD=3  REG[rrr] RM[nnn] VL128                                           | REG0=XMM_R():w:dq REG1=XMM_N():r:dq  REG2=XMM_B():r:dq
 | VAESENCLAST          | AES            | AVXAES         |                | dd           | mm         | rrr        | VV1 0xDD  V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() VL128                                    | REG0=XMM_R():w:dq REG1=XMM_N():r:dq  MEM0:r:dq
 | VAESENCLAST          | VAES           | AVX512EVEX     | AVX512_VAES_128 | dd           | 0b11       | rrr        | EVV 0xDD V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128      ZEROING=0 MASK=0              | REG0=XMM_R3():w:dq:u128 REG1=XMM_N3():r:dq:u128 REG2=XMM_B3():r:dq:u128
 | VAESENCLAST          | VAES           | AVX512EVEX     | AVX512_VAES_128 | dd           | mm         | rrr        | EVV 0xDD V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ZEROING=0 MASK=0  ESIZE_128_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u128 REG1=XMM_N3():r:dq:u128 MEM0:r:dq:u128
 | VAESENCLAST          | VAES           | AVX512EVEX     | AVX512_VAES_256 | dd           | 0b11       | rrr        | EVV 0xDD V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256      ZEROING=0 MASK=0              | REG0=YMM_R3():w:qq:u128 REG1=YMM_N3():r:qq:u128 REG2=YMM_B3():r:qq:u128
 | VAESENCLAST          | VAES           | AVX512EVEX     | AVX512_VAES_256 | dd           | mm         | rrr        | EVV 0xDD V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ZEROING=0 MASK=0  ESIZE_128_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u128 REG1=YMM_N3():r:qq:u128 MEM0:r:qq:u128
 | VAESENCLAST          | VAES           | AVX512EVEX     | AVX512_VAES_512 | dd           | 0b11       | rrr        | EVV 0xDD V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512      ZEROING=0 MASK=0              | REG0=ZMM_R3():w:zu128 REG1=ZMM_N3():r:zu128 REG2=ZMM_B3():r:zu128
 | VAESENCLAST          | VAES           | AVX512EVEX     | AVX512_VAES_512 | dd           | mm         | rrr        | EVV 0xDD V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ZEROING=0 MASK=0  ESIZE_128_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu128 REG1=ZMM_N3():r:zu128 MEM0:r:zd:u128
 | VAESENCLAST          | VAES           | VAES           | VAES           | dd           | 0b11       | rrr        | VV1 0xDD V66 V0F38 MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256                                          | REG0=YMM_R():w:qq:u128 REG1=YMM_N():r:qq:u128 REG2=YMM_B():r:qq:u128
 | VAESENCLAST          | VAES           | VAES           | VAES           | dd           | mm         | rrr        | VV1 0xDD V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256                                   | REG0=YMM_R():w:qq:u128 REG1=YMM_N():r:qq:u128 MEM0:r:qq:u128
 | VAESIMC              | AES            | AVXAES         |                | db           | 0b11       | rrr        | VV1 0xDB VL128 V66 V0F38 NOVSR MOD[0b11] MOD=3  REG[rrr] RM[nnn]                                     | REG0=XMM_R():w:dq  REG1=XMM_B():r:dq
 | VAESIMC              | AES            | AVXAES         |                | db           | mm         | rrr        | VV1 0xDB VL128 V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=XMM_R():w:dq  MEM0:r:dq
 | VAESKEYGENASSIST     | AES            | AVXAES         |                | df           | 0b11       | rrr        | VV1 0xDF VL128 V66 V0F3A  NOVSR MOD[0b11] MOD=3  REG[rrr] RM[nnn] UIMM8()                            | REG0=XMM_R():w:dq  REG1=XMM_B():r:dq IMM0:r:b
 | VAESKEYGENASSIST     | AES            | AVXAES         |                | df           | mm         | rrr        | VV1 0xDF  VL128 V66 V0F3A NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                      | REG0=XMM_R():w:dq  MEM0:r:dq IMM0:r:b
 | VALIGND              | AVX512         | AVX512EVEX     | AVX512F_128    | 03           | 0b11       | rrr        | EVV 0x03 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0   UIMM8()                      | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32 IMM0:r:b
 | VALIGND              | AVX512         | AVX512EVEX     | AVX512F_128    | 03           | mm         | rrr        | EVV 0x03 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
 | VALIGND              | AVX512         | AVX512EVEX     | AVX512F_256    | 03           | 0b11       | rrr        | EVV 0x03 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0   UIMM8()                      | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32 IMM0:r:b
 | VALIGND              | AVX512         | AVX512EVEX     | AVX512F_256    | 03           | mm         | rrr        | EVV 0x03 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
 | VALIGND              | AVX512         | AVX512EVEX     | AVX512F_512    | 03           | 0b11       | rrr        | EVV 0x03 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0   UIMM8()                      | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32 IMM0:r:b
 | VALIGND              | AVX512         | AVX512EVEX     | AVX512F_512    | 03           | mm         | rrr        | EVV 0x03 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
 | VALIGND              | KNC            | KNCE           |                | 03           | mm         | rrr        | KVV 0x03 V0F3A V66  W0 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() NOSWIZD()                    | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zd:TXT=NT IMM0:r:b
 | VALIGND              | KNC            | KNCE           |                | 03           | 0b11       | rrr        | KVV 0x03 V0F3A V66  W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0  UIMM8() SWIZ=0                         | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32 IMM0:r:b
 | VALIGNQ              | AVX512         | AVX512EVEX     | AVX512F_128    | 03           | 0b11       | rrr        | EVV 0x03 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1   UIMM8()                      | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64 IMM0:r:b
 | VALIGNQ              | AVX512         | AVX512EVEX     | AVX512F_128    | 03           | mm         | rrr        | EVV 0x03 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VALIGNQ              | AVX512         | AVX512EVEX     | AVX512F_256    | 03           | 0b11       | rrr        | EVV 0x03 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1   UIMM8()                      | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64 IMM0:r:b
 | VALIGNQ              | AVX512         | AVX512EVEX     | AVX512F_256    | 03           | mm         | rrr        | EVV 0x03 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VALIGNQ              | AVX512         | AVX512EVEX     | AVX512F_512    | 03           | 0b11       | rrr        | EVV 0x03 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1   UIMM8()                      | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64 IMM0:r:b
 | VALIGNQ              | AVX512         | AVX512EVEX     | AVX512F_512    | 03           | mm         | rrr        | EVV 0x03 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VANDNPD              | LOGICAL_FP     | AVX            |                | 55           | mm         | rrr        | VV1 0x55  VL128 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 MEM0:r:dq:u64
 | VANDNPD              | LOGICAL_FP     | AVX            |                | 55           | 0b11       | rrr        | VV1 0x55  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 REG2=XMM_B():r:dq:u64
 | VANDNPD              | LOGICAL_FP     | AVX            |                | 55           | mm         | rrr        | VV1 0x55  VL256 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=YMM_R():w:qq:u64 REG1=YMM_N():r:qq:u64 MEM0:r:qq:u64
 | VANDNPD              | LOGICAL_FP     | AVX            |                | 55           | 0b11       | rrr        | VV1 0x55  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:u64 REG1=YMM_N():r:qq:u64 REG2=YMM_B():r:qq:u64
 | VANDNPD              | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_128   | 55           | 0b11       | rrr        | EVV 0x55 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                  | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
 | VANDNPD              | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_128   | 55           | mm         | rrr        | EVV 0x55 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VANDNPD              | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_256   | 55           | 0b11       | rrr        | EVV 0x55 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                  | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
 | VANDNPD              | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_256   | 55           | mm         | rrr        | EVV 0x55 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VANDNPD              | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_512   | 55           | 0b11       | rrr        | EVV 0x55 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                  | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
 | VANDNPD              | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_512   | 55           | mm         | rrr        | EVV 0x55 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VANDNPS              | LOGICAL_FP     | AVX            |                | 55           | mm         | rrr        | VV1 0x55  VL128 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq REG1=XMM_N():r:dq MEM0:r:dq
 | VANDNPS              | LOGICAL_FP     | AVX            |                | 55           | 0b11       | rrr        | VV1 0x55  VL128 VNP V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq REG1=XMM_N():r:dq REG2=XMM_B():r:dq
 | VANDNPS              | LOGICAL_FP     | AVX            |                | 55           | mm         | rrr        | VV1 0x55  VL256 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=YMM_R():w:qq REG1=YMM_N():r:qq MEM0:r:qq
 | VANDNPS              | LOGICAL_FP     | AVX            |                | 55           | 0b11       | rrr        | VV1 0x55  VL256 VNP V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq REG1=YMM_N():r:qq REG2=YMM_B():r:qq
 | VANDNPS              | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_128   | 55           | 0b11       | rrr        | EVV 0x55 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                  | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
 | VANDNPS              | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_128   | 55           | mm         | rrr        | EVV 0x55 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VANDNPS              | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_256   | 55           | 0b11       | rrr        | EVV 0x55 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                  | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
 | VANDNPS              | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_256   | 55           | mm         | rrr        | EVV 0x55 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VANDNPS              | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_512   | 55           | 0b11       | rrr        | EVV 0x55 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                  | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
 | VANDNPS              | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_512   | 55           | mm         | rrr        | EVV 0x55 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VANDPD               | LOGICAL_FP     | AVX            |                | 54           | mm         | rrr        | VV1 0x54  VL128 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 MEM0:r:dq:u64
 | VANDPD               | LOGICAL_FP     | AVX            |                | 54           | 0b11       | rrr        | VV1 0x54  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 REG2=XMM_B():r:dq:u64
 | VANDPD               | LOGICAL_FP     | AVX            |                | 54           | mm         | rrr        | VV1 0x54  VL256 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=YMM_R():w:qq:u64 REG1=YMM_N():r:qq:u64 MEM0:r:qq:u64
 | VANDPD               | LOGICAL_FP     | AVX            |                | 54           | 0b11       | rrr        | VV1 0x54  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:u64 REG1=YMM_N():r:qq:u64 REG2=YMM_B():r:qq:u64
 | VANDPD               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_128   | 54           | 0b11       | rrr        | EVV 0x54 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                  | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
 | VANDPD               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_128   | 54           | mm         | rrr        | EVV 0x54 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VANDPD               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_256   | 54           | 0b11       | rrr        | EVV 0x54 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                  | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
 | VANDPD               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_256   | 54           | mm         | rrr        | EVV 0x54 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VANDPD               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_512   | 54           | 0b11       | rrr        | EVV 0x54 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                  | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
 | VANDPD               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_512   | 54           | mm         | rrr        | EVV 0x54 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VANDPS               | LOGICAL_FP     | AVX            |                | 54           | mm         | rrr        | VV1 0x54  VL128 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq REG1=XMM_N():r:dq MEM0:r:dq
 | VANDPS               | LOGICAL_FP     | AVX            |                | 54           | 0b11       | rrr        | VV1 0x54  VL128 VNP V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq REG1=XMM_N():r:dq REG2=XMM_B():r:dq
 | VANDPS               | LOGICAL_FP     | AVX            |                | 54           | mm         | rrr        | VV1 0x54  VL256 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=YMM_R():w:qq REG1=YMM_N():r:qq MEM0:r:qq
 | VANDPS               | LOGICAL_FP     | AVX            |                | 54           | 0b11       | rrr        | VV1 0x54  VL256 VNP V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq REG1=YMM_N():r:qq REG2=YMM_B():r:qq
 | VANDPS               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_128   | 54           | 0b11       | rrr        | EVV 0x54 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                  | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
 | VANDPS               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_128   | 54           | mm         | rrr        | EVV 0x54 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VANDPS               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_256   | 54           | 0b11       | rrr        | EVV 0x54 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                  | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
 | VANDPS               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_256   | 54           | mm         | rrr        | EVV 0x54 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VANDPS               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_512   | 54           | 0b11       | rrr        | EVV 0x54 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                  | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
 | VANDPS               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_512   | 54           | mm         | rrr        | EVV 0x54 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VBLENDMPD            | BLEND          | AVX512EVEX     | AVX512F_128    | 65           | 0b11       | rrr        | EVV 0x65 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VBLENDMPD            | BLEND          | AVX512EVEX     | AVX512F_128    | 65           | mm         | rrr        | EVV 0x65 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VBLENDMPD            | BLEND          | AVX512EVEX     | AVX512F_256    | 65           | 0b11       | rrr        | EVV 0x65 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
 | VBLENDMPD            | BLEND          | AVX512EVEX     | AVX512F_256    | 65           | mm         | rrr        | EVV 0x65 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VBLENDMPD            | BLEND          | AVX512EVEX     | AVX512F_512    | 65           | 0b11       | rrr        | EVV 0x65 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VBLENDMPD            | BLEND          | AVX512EVEX     | AVX512F_512    | 65           | mm         | rrr        | EVV 0x65 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VBLENDMPD            | BLEND          | KNCE           |                | 65           | mm         | rrr        | KVV 0x65 V0F38 V66  REXW=1 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT64()                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VBLENDMPD            | BLEND          | KNCE           |                | 65           | 0b11       | rrr        | KVV 0x65 V0F38 V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE64()                     | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=REGSWIZ
 | VBLENDMPD            | BLEND          | KNCE           |                | 65           | 0b11       | rrr        | KVV 0x65 V0F38 V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                   | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=ROUNDC:TXT=SAEC
 | VBLENDMPS            | BLEND          | AVX512EVEX     | AVX512F_128    | 65           | 0b11       | rrr        | EVV 0x65 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VBLENDMPS            | BLEND          | AVX512EVEX     | AVX512F_128    | 65           | mm         | rrr        | EVV 0x65 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VBLENDMPS            | BLEND          | AVX512EVEX     | AVX512F_256    | 65           | 0b11       | rrr        | EVV 0x65 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
 | VBLENDMPS            | BLEND          | AVX512EVEX     | AVX512F_256    | 65           | mm         | rrr        | EVV 0x65 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VBLENDMPS            | BLEND          | AVX512EVEX     | AVX512F_512    | 65           | 0b11       | rrr        | EVV 0x65 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VBLENDMPS            | BLEND          | AVX512EVEX     | AVX512F_512    | 65           | mm         | rrr        | EVV 0x65 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VBLENDMPS            | BLEND          | KNCE           |                | 65           | mm         | rrr        | KVV 0x65 V0F38 V66  REXW=0 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT32()                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VBLENDMPS            | BLEND          | KNCE           |                | 65           | 0b11       | rrr        | KVV 0x65 V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VBLENDMPS            | BLEND          | KNCE           |                | 65           | 0b11       | rrr        | KVV 0x65 V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                   | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=ROUNDC:TXT=SAEC
 | VBLENDPD             | AVX            | AVX            |                | 0d           | mm         | rrr        | VV1 0x0D  VL128 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                            | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64 IMM0:r:b
 | VBLENDPD             | AVX            | AVX            |                | 0d           | 0b11       | rrr        | VV1 0x0D  VL128 V66 V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                   | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64 IMM0:r:b
 | VBLENDPD             | AVX            | AVX            |                | 0d           | mm         | rrr        | VV1 0x0D  VL256 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                            | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64 IMM0:r:b
 | VBLENDPD             | AVX            | AVX            |                | 0d           | 0b11       | rrr        | VV1 0x0D  VL256 V66 V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                   | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64 IMM0:r:b
 | VBLENDPS             | AVX            | AVX            |                | 0c           | mm         | rrr        | VV1 0x0C  VL128 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                            | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32 IMM0:r:b
 | VBLENDPS             | AVX            | AVX            |                | 0c           | 0b11       | rrr        | VV1 0x0C  VL128 V66 V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                   | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32 IMM0:r:b
 | VBLENDPS             | AVX            | AVX            |                | 0c           | mm         | rrr        | VV1 0x0C  VL256 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                            | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32 IMM0:r:b
 | VBLENDPS             | AVX            | AVX            |                | 0c           | 0b11       | rrr        | VV1 0x0C  VL256 V66 V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                   | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32 IMM0:r:b
 | VBLENDVPD            | AVX            | AVX            |                | 4b           | mm         | rrr        | VV1 0x4B   V66 V0F3A VL128 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()           | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64 REG2=XMM_SE():r:dq:u64
 | VBLENDVPD            | AVX            | AVX            |                | 4b           | 0b11       | rrr        | VV1 0x4B   V66 V0F3A VL128 norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                  | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64 REG3=XMM_SE():r:dq:u64
 | VBLENDVPD            | AVX            | AVX            |                | 4b           | mm         | rrr        | VV1 0x4B   V66 V0F3A VL256 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()           | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64 REG2=YMM_SE():r:qq:u64
 | VBLENDVPD            | AVX            | AVX            |                | 4b           | 0b11       | rrr        | VV1 0x4B   V66 V0F3A VL256 norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                  | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64 REG3=YMM_SE():r:qq:u64
 | VBLENDVPS            | AVX            | AVX            |                | 4a           | mm         | rrr        | VV1 0x4A   V66 V0F3A VL128 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()           | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32 REG2=XMM_SE():r:dq:u32
 | VBLENDVPS            | AVX            | AVX            |                | 4a           | 0b11       | rrr        | VV1 0x4A   V66 V0F3A VL128 norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                  | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32 REG3=XMM_SE():r:dq:u32
 | VBLENDVPS            | AVX            | AVX            |                | 4a           | mm         | rrr        | VV1 0x4A   V66 V0F3A VL256 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()           | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32 REG2=YMM_SE():r:qq:u32
 | VBLENDVPS            | AVX            | AVX            |                | 4a           | 0b11       | rrr        | VV1 0x4A   V66 V0F3A VL256 norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                  | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32 REG3=YMM_SE():r:qq:u32
 | VBROADCASTF128       | BROADCAST      | AVX            |                | 1a           | mm         | rrr        | VV1 0x1A norexw_prefix VL256 V66 V0F38 NOVSR  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                | REG0=YMM_R():w:qq:f64 MEM0:r:dq:f64 EMX_BROADCAST_2TO4_64
 | VBROADCASTF32X2      | BROADCAST      | AVX512EVEX     | AVX512DQ_256   | 19           | 0b11       | rrr        | EVV 0x19 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32 EMX_BROADCAST_2TO8_32
 | VBROADCASTF32X2      | BROADCAST      | AVX512EVEX     | AVX512DQ_256   | 19           | mm         | rrr        | EVV 0x19 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_TUPLE2() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:f32 EMX_BROADCAST_2TO8_32
 | VBROADCASTF32X2      | BROADCAST      | AVX512EVEX     | AVX512DQ_512   | 19           | 0b11       | rrr        | EVV 0x19 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32 EMX_BROADCAST_2TO16_32
 | VBROADCASTF32X2      | BROADCAST      | AVX512EVEX     | AVX512DQ_512   | 19           | mm         | rrr        | EVV 0x19 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_TUPLE2() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:f32 EMX_BROADCAST_2TO16_32
 | VBROADCASTF32X4      | BROADCAST      | AVX512EVEX     | AVX512F_256    | 1a           | mm         | rrr        | EVV 0x1A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_TUPLE4() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:f32 EMX_BROADCAST_4TO8_32
 | VBROADCASTF32X4      | BROADCAST      | AVX512EVEX     | AVX512F_512    | 1a           | mm         | rrr        | EVV 0x1A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_TUPLE4() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:f32 EMX_BROADCAST_4TO16_32
 | VBROADCASTF32X4      | BROADCAST      | KNCE           |                | 1a           | mm         | rrr        | KVV 0x1A V0F38 V66 REXW=0  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM()  UPCONVERT_FLT32_LOAD()   | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT  NELEM=4:SUPP EMX_BROADCAST_4TO16_32
 | VBROADCASTF32X8      | BROADCAST      | AVX512EVEX     | AVX512DQ_512   | 1b           | mm         | rrr        | EVV 0x1B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_TUPLE8() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:f32 EMX_BROADCAST_8TO16_32
 | VBROADCASTF64X2      | BROADCAST      | AVX512EVEX     | AVX512DQ_256   | 1a           | mm         | rrr        | EVV 0x1A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_TUPLE2() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:f64 EMX_BROADCAST_2TO4_64
 | VBROADCASTF64X2      | BROADCAST      | AVX512EVEX     | AVX512DQ_512   | 1a           | mm         | rrr        | EVV 0x1A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_TUPLE2() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:f64 EMX_BROADCAST_2TO8_64
 | VBROADCASTF64x4      | BROADCAST      | KNCE           |                | 1b           | mm         | rrr        | KVV 0x1B V0F38 V66 REXW=1  REXW=1  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM()  UPCONVERT_FLT64_LOAD() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT  NELEM=4:SUPP EMX_BROADCAST_4TO8_64
 | VBROADCASTF64X4      | BROADCAST      | AVX512EVEX     | AVX512F_512    | 1b           | mm         | rrr        | EVV 0x1B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_TUPLE4() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:f64 EMX_BROADCAST_4TO8_64
 | VBROADCASTI128       | BROADCAST      | AVX2           |                | 5a           | mm         | rrr        | VV1 0x5A VL256 V66 V0F38 W0 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                            | REG0=YMM_R():w:qq:u128  MEM0:r:dq:u128 EMX_BROADCAST_2TO4_64
 | VBROADCASTI32X2      | BROADCAST      | AVX512EVEX     | AVX512DQ_128   | 59           | 0b11       | rrr        | EVV 0x59 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32 EMX_BROADCAST_2TO4_32
 | VBROADCASTI32X2      | BROADCAST      | AVX512EVEX     | AVX512DQ_128   | 59           | mm         | rrr        | EVV 0x59 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_TUPLE2() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:u32 EMX_BROADCAST_2TO4_32
 | VBROADCASTI32X2      | BROADCAST      | AVX512EVEX     | AVX512DQ_256   | 59           | 0b11       | rrr        | EVV 0x59 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32 EMX_BROADCAST_2TO8_32
 | VBROADCASTI32X2      | BROADCAST      | AVX512EVEX     | AVX512DQ_256   | 59           | mm         | rrr        | EVV 0x59 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_TUPLE2() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:u32 EMX_BROADCAST_2TO8_32
 | VBROADCASTI32X2      | BROADCAST      | AVX512EVEX     | AVX512DQ_512   | 59           | 0b11       | rrr        | EVV 0x59 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32 EMX_BROADCAST_2TO16_32
 | VBROADCASTI32X2      | BROADCAST      | AVX512EVEX     | AVX512DQ_512   | 59           | mm         | rrr        | EVV 0x59 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_TUPLE2() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:u32 EMX_BROADCAST_2TO16_32
 | VBROADCASTI32X4      | BROADCAST      | AVX512EVEX     | AVX512F_256    | 5a           | mm         | rrr        | EVV 0x5A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_TUPLE4() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u32 EMX_BROADCAST_4TO8_32
 | VBROADCASTI32X4      | BROADCAST      | AVX512EVEX     | AVX512F_512    | 5a           | mm         | rrr        | EVV 0x5A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_TUPLE4() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u32 EMX_BROADCAST_4TO16_32
 | VBROADCASTI32X4      | BROADCAST      | KNCE           |                | 5a           | mm         | rrr        | KVV 0x5A V0F38 V66 REXW=0  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM()  UPCONVERT_INT32_LOAD()   | REG0=ZMM_R3():rw:zd:i32 REG1=MASK1():r:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT  NELEM=4:SUPP EMX_BROADCAST_4TO16_32
 | VBROADCASTI32X8      | BROADCAST      | AVX512EVEX     | AVX512DQ_512   | 5b           | mm         | rrr        | EVV 0x5B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_TUPLE8() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u32 EMX_BROADCAST_8TO16_32
 | VBROADCASTI64X2      | BROADCAST      | AVX512EVEX     | AVX512DQ_256   | 5a           | mm         | rrr        | EVV 0x5A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_TUPLE2() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u64 EMX_BROADCAST_2TO4_64
 | VBROADCASTI64X2      | BROADCAST      | AVX512EVEX     | AVX512DQ_512   | 5a           | mm         | rrr        | EVV 0x5A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_TUPLE2() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u64 EMX_BROADCAST_2TO8_64
 | VBROADCASTI64x4      | BROADCAST      | KNCE           |                | 5b           | mm         | rrr        | KVV 0x5B V0F38 V66 REXW=1  REXW=1  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM()  UPCONVERT_FLT64_LOAD() | REG0=ZMM_R3():rw:zq:i64 REG1=MASK1():r:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT  NELEM=4:SUPP EMX_BROADCAST_4TO8_64
 | VBROADCASTI64X4      | BROADCAST      | AVX512EVEX     | AVX512F_512    | 5b           | mm         | rrr        | EVV 0x5B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_TUPLE4() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u64 EMX_BROADCAST_4TO8_64
 | VBROADCASTSD         | BROADCAST      | AVX            |                | 19           | mm         | rrr        | VV1 0x19  norexw_prefix VL256 V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                | REG0=YMM_R():w:qq:f64 MEM0:r:q:f64 EMX_BROADCAST_1TO4_64
 | VBROADCASTSD         | BROADCAST      | AVX2           |                | 19           | 0b11       | rrr        | VV1 0x19  VL256 V66 V0F38 W0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                  | REG0=YMM_R():w:qq:f64  REG1=XMM_B():r:dq:f64 EMX_BROADCAST_1TO4_64
 | VBROADCASTSD         | BROADCAST      | AVX512EVEX     | AVX512F_256    | 19           | mm         | rrr        | EVV 0x19 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_TUPLE1() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:f64 EMX_BROADCAST_1TO4_64
 | VBROADCASTSD         | BROADCAST      | AVX512EVEX     | AVX512F_256    | 19           | 0b11       | rrr        | EVV 0x19 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                        | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64 EMX_BROADCAST_1TO4_64
 | VBROADCASTSD         | BROADCAST      | AVX512EVEX     | AVX512F_512    | 19           | mm         | rrr        | EVV 0x19 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_TUPLE1() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:f64 EMX_BROADCAST_1TO8_64
 | VBROADCASTSD         | BROADCAST      | AVX512EVEX     | AVX512F_512    | 19           | 0b11       | rrr        | EVV 0x19 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                        | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64 EMX_BROADCAST_1TO8_64
 | VBROADCASTSD         | BROADCAST      | KNCE           |                | 19           | mm         | rrr        | KVV 0x19 V0F38 V66 REXW=1  REXW=1  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM()  UPCONVERT_FLT64_LOAD() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT  NELEM=1:SUPP EMX_BROADCAST_1TO8_64
 | VBROADCASTSS         | BROADCAST      | AVX            |                | 18           | mm         | rrr        | VV1 0x18  norexw_prefix VL128 V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                | REG0=XMM_R():w:dq:f32 MEM0:r:d:f32 EMX_BROADCAST_1TO4_32
 | VBROADCASTSS         | BROADCAST      | AVX            |                | 18           | mm         | rrr        | VV1 0x18  norexw_prefix VL256 V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                | REG0=YMM_R():w:qq:f32 MEM0:r:d:f32 EMX_BROADCAST_1TO8_32
 | VBROADCASTSS         | BROADCAST      | AVX2           |                | 18           | 0b11       | rrr        | VV1 0x18  VL128 V66 V0F38 W0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                  | REG0=XMM_R():w:dq:f32  REG1=XMM_B():r:dq:f32 EMX_BROADCAST_1TO4_32
 | VBROADCASTSS         | BROADCAST      | AVX2           |                | 18           | 0b11       | rrr        | VV1 0x18  VL256 V66 V0F38 W0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                  | REG0=YMM_R():w:qq:f32  REG1=XMM_B():r:dq:f32 EMX_BROADCAST_1TO8_32
 | VBROADCASTSS         | BROADCAST      | AVX512EVEX     | AVX512F_128    | 18           | mm         | rrr        | EVV 0x18 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_TUPLE1() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:d:f32 EMX_BROADCAST_1TO4_32
 | VBROADCASTSS         | BROADCAST      | AVX512EVEX     | AVX512F_128    | 18           | 0b11       | rrr        | EVV 0x18 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32 EMX_BROADCAST_1TO4_32
 | VBROADCASTSS         | BROADCAST      | AVX512EVEX     | AVX512F_256    | 18           | mm         | rrr        | EVV 0x18 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_TUPLE1() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:d:f32 EMX_BROADCAST_1TO8_32
 | VBROADCASTSS         | BROADCAST      | AVX512EVEX     | AVX512F_256    | 18           | 0b11       | rrr        | EVV 0x18 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32 EMX_BROADCAST_1TO8_32
 | VBROADCASTSS         | BROADCAST      | AVX512EVEX     | AVX512F_512    | 18           | mm         | rrr        | EVV 0x18 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_TUPLE1() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:d:f32 EMX_BROADCAST_1TO16_32
 | VBROADCASTSS         | BROADCAST      | AVX512EVEX     | AVX512F_512    | 18           | 0b11       | rrr        | EVV 0x18 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32 EMX_BROADCAST_1TO16_32
 | VBROADCASTSS         | BROADCAST      | KNCE           |                | 18           | mm         | rrr        | KVV 0x18 V0F38 V66 REXW=0  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM()  UPCONVERT_FLT32_LOAD()   | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT  NELEM=1:SUPP EMX_BROADCAST_1TO16_32
 | VCMPPD               | AVX            | AVX            |                | c2           | mm         | rrr        | VV1 0xC2  V66 VL128 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                              | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64 IMM0:r:b
 | VCMPPD               | AVX            | AVX            |                | c2           | 0b11       | rrr        | VV1 0xC2  V66 VL128 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                     | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64 IMM0:r:b
 | VCMPPD               | AVX            | AVX            |                | c2           | mm         | rrr        | VV1 0xC2  V66 VL256 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                              | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64 IMM0:r:b
 | VCMPPD               | AVX            | AVX            |                | c2           | 0b11       | rrr        | VV1 0xC2  V66 VL256 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                     | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64 IMM0:r:b
 | VCMPPD               | AVX512         | AVX512EVEX     | AVX512F_128    | c2           | 0b11       | rrr        | EVV 0xC2 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1    ZEROING=0 UIMM8()             | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64 IMM0:r:b
 | VCMPPD               | AVX512         | AVX512EVEX     | AVX512F_128    | c2           | mm         | rrr        | EVV 0xC2 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
 | VCMPPD               | AVX512         | AVX512EVEX     | AVX512F_256    | c2           | 0b11       | rrr        | EVV 0xC2 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1    ZEROING=0 UIMM8()             | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64 IMM0:r:b
 | VCMPPD               | AVX512         | AVX512EVEX     | AVX512F_256    | c2           | mm         | rrr        | EVV 0xC2 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
 | VCMPPD               | AVX512         | AVX512EVEX     | AVX512F_512    | c2           | 0b11       | rrr        | EVV 0xC2 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1    ZEROING=0 UIMM8()             | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64 IMM0:r:b
 | VCMPPD               | AVX512         | AVX512EVEX     | AVX512F_512    | c2           | 0b11       | rrr        | EVV 0xC2 V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W1    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw:TXT=SAESTR REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64 IMM0:r:b
 | VCMPPD               | AVX512         | AVX512EVEX     | AVX512F_512    | c2           | mm         | rrr        | EVV 0xC2 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
 | VCMPPD               | KNC            | KNCE           |                | c2           | mm         | rrr        | KVV 0xC2 V66 V0F  W1 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() UPCONVERT_FLT64()              | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 MEM0:r:zv:TXT=CONVERT:TXT=NT IMM0:r:b
 | VCMPPD               | KNC            | KNCE           |                | c2           | 0b11       | rrr        | KVV 0xC2 V66 V0F  W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8() NR=0 REG_SWIZZLE64()                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=REGSWIZ IMM0:r:b
 | VCMPPD               | KNC            | KNCE           |                | c2           | 0b11       | rrr        | KVV 0xC2 V66 V0F  W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8() NR=1  KNC_SAE()                        | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=SAEC IMM0:r:b
 | VCMPPS               | AVX            | AVX            |                | c2           | mm         | rrr        | VV1 0xC2  VNP VL128 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                              | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32 IMM0:r:b
 | VCMPPS               | AVX            | AVX            |                | c2           | 0b11       | rrr        | VV1 0xC2  VNP VL128 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                     | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32 IMM0:r:b
 | VCMPPS               | AVX            | AVX            |                | c2           | mm         | rrr        | VV1 0xC2  VNP VL256 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                              | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32 IMM0:r:b
 | VCMPPS               | AVX            | AVX            |                | c2           | 0b11       | rrr        | VV1 0xC2  VNP VL256 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                     | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32 IMM0:r:b
 | VCMPPS               | AVX512         | AVX512EVEX     | AVX512F_128    | c2           | 0b11       | rrr        | EVV 0xC2 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0    ZEROING=0 UIMM8()             | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32 IMM0:r:b
 | VCMPPS               | AVX512         | AVX512EVEX     | AVX512F_128    | c2           | mm         | rrr        | EVV 0xC2 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
 | VCMPPS               | AVX512         | AVX512EVEX     | AVX512F_256    | c2           | 0b11       | rrr        | EVV 0xC2 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0    ZEROING=0 UIMM8()             | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32 IMM0:r:b
 | VCMPPS               | AVX512         | AVX512EVEX     | AVX512F_256    | c2           | mm         | rrr        | EVV 0xC2 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
 | VCMPPS               | AVX512         | AVX512EVEX     | AVX512F_512    | c2           | 0b11       | rrr        | EVV 0xC2 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0    ZEROING=0 UIMM8()             | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32 IMM0:r:b
 | VCMPPS               | AVX512         | AVX512EVEX     | AVX512F_512    | c2           | 0b11       | rrr        | EVV 0xC2 VNP V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw:TXT=SAESTR REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32 IMM0:r:b
 | VCMPPS               | AVX512         | AVX512EVEX     | AVX512F_512    | c2           | mm         | rrr        | EVV 0xC2 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
 | VCMPPS               | KNC            | KNCE           |                | c2           | mm         | rrr        | KVV 0xC2 VNP V0F  W0 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() UPCONVERT_FLT32()              | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT IMM0:r:b
 | VCMPPS               | KNC            | KNCE           |                | c2           | 0b11       | rrr        | KVV 0xC2 VNP V0F  W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8() NR=0 REG_SWIZZLE64()                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ IMM0:r:b
 | VCMPPS               | KNC            | KNCE           |                | c2           | 0b11       | rrr        | KVV 0xC2 VNP V0F  W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8() NR=1  KNC_SAE()                        | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=SAEC IMM0:r:b
 | VCMPSD               | AVX            | AVX            |                | c2           | mm         | rrr        | VV1 0xC2   VF2 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                                  | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:q:f64 IMM0:r:b
 | VCMPSD               | AVX            | AVX            |                | c2           | 0b11       | rrr        | VV1 0xC2   VF2 V0F  MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                         | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:q:f64 IMM0:r:b
 | VCMPSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | c2           | 0b11       | rrr        | EVV 0xC2 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1    ZEROING=0 UIMM8()                    | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64 IMM0:r:b
 | VCMPSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | c2           | 0b11       | rrr        | EVV 0xC2 VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W1    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw:TXT=SAESTR REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64 IMM0:r:b
 | VCMPSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | c2           | mm         | rrr        | EVV 0xC2 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_SCALAR() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64 IMM0:r:b
 | VCMPSS               | AVX            | AVX            |                | c2           | mm         | rrr        | VV1 0xC2   VF3 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                                  | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:d:f32 IMM0:r:b
 | VCMPSS               | AVX            | AVX            |                | c2           | 0b11       | rrr        | VV1 0xC2   VF3 V0F  MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                         | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:d:f32 IMM0:r:b
 | VCMPSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | c2           | 0b11       | rrr        | EVV 0xC2 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0    ZEROING=0 UIMM8()                    | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32 IMM0:r:b
 | VCMPSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | c2           | 0b11       | rrr        | EVV 0xC2 VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W0    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw:TXT=SAESTR REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32 IMM0:r:b
 | VCMPSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | c2           | mm         | rrr        | EVV 0xC2 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_SCALAR() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32 IMM0:r:b
 | VCOMISD              | AVX            | AVX            |                | 2f           | mm         | rrr        | VV1 0x2F   V66 V0F  NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():r:q:f64 MEM0:r:q:f64
 | VCOMISD              | AVX            | AVX            |                | 2f           | 0b11       | rrr        | VV1 0x2F   V66 V0F  NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=XMM_R():r:q:f64 REG1=XMM_B():r:q:f64
 | VCOMISD              | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 2f           | 0b11       | rrr        | EVV 0x2F V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1  NOEVSR  ZEROING=0 MASK=0  FIX_ROUND_LEN128() | REG0=XMM_R3():r:dq:f64 REG1=XMM_B3():r:dq:f64
 | VCOMISD              | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 2f           | 0b11       | rrr        | EVV 0x2F V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W1  NOEVSR  ZEROING=0 MASK=0 | REG0=XMM_R3():r:dq:f64:TXT=SAESTR REG1=XMM_B3():r:dq:f64
 | VCOMISD              | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 2f           | mm         | rrr        | EVV 0x2F V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_SCALAR() FIX_ROUND_LEN128() | REG0=XMM_R3():r:dq:f64 MEM0:r:q:f64
 | VCOMISS              | AVX            | AVX            |                | 2f           | mm         | rrr        | VV1 0x2F   VNP V0F  NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():r:d:f32 MEM0:r:d:f32
 | VCOMISS              | AVX            | AVX            |                | 2f           | 0b11       | rrr        | VV1 0x2F   VNP V0F  NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=XMM_R():r:d:f32 REG1=XMM_B():r:d:f32
 | VCOMISS              | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 2f           | 0b11       | rrr        | EVV 0x2F VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0  NOEVSR  ZEROING=0 MASK=0 FIX_ROUND_LEN128() | REG0=XMM_R3():r:dq:f32 REG1=XMM_B3():r:dq:f32
 | VCOMISS              | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 2f           | 0b11       | rrr        | EVV 0x2F VNP V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W0  NOEVSR  ZEROING=0 MASK=0 | REG0=XMM_R3():r:dq:f32:TXT=SAESTR REG1=XMM_B3():r:dq:f32
 | VCOMISS              | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 2f           | mm         | rrr        | EVV 0x2F VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_SCALAR() FIX_ROUND_LEN128() | REG0=XMM_R3():r:dq:f32 MEM0:r:d:f32
 | VCOMPRESSPD          | COMPRESS       | AVX512EVEX     | AVX512F_128    | 8a           | mm         | rrr        | EVV 0x8A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:w:dq:f64 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:f64
 | VCOMPRESSPD          | COMPRESS       | AVX512EVEX     | AVX512F_128    | 8a           | 0b11       | rrr        | EVV 0x8A V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                        | REG0=XMM_B3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:f64
 | VCOMPRESSPD          | COMPRESS       | AVX512EVEX     | AVX512F_256    | 8a           | mm         | rrr        | EVV 0x8A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:w:qq:f64 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:f64
 | VCOMPRESSPD          | COMPRESS       | AVX512EVEX     | AVX512F_256    | 8a           | 0b11       | rrr        | EVV 0x8A V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                        | REG0=YMM_B3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:f64
 | VCOMPRESSPD          | COMPRESS       | AVX512EVEX     | AVX512F_512    | 8a           | mm         | rrr        | EVV 0x8A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:w:zd:f64 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zf64
 | VCOMPRESSPD          | COMPRESS       | AVX512EVEX     | AVX512F_512    | 8a           | 0b11       | rrr        | EVV 0x8A V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                        | REG0=ZMM_B3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zf64
 | VCOMPRESSPS          | COMPRESS       | AVX512EVEX     | AVX512F_128    | 8a           | mm         | rrr        | EVV 0x8A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:w:dq:f32 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:f32
 | VCOMPRESSPS          | COMPRESS       | AVX512EVEX     | AVX512F_128    | 8a           | 0b11       | rrr        | EVV 0x8A V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_B3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:f32
 | VCOMPRESSPS          | COMPRESS       | AVX512EVEX     | AVX512F_256    | 8a           | mm         | rrr        | EVV 0x8A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:w:qq:f32 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:f32
 | VCOMPRESSPS          | COMPRESS       | AVX512EVEX     | AVX512F_256    | 8a           | 0b11       | rrr        | EVV 0x8A V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=YMM_B3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:f32
 | VCOMPRESSPS          | COMPRESS       | AVX512EVEX     | AVX512F_512    | 8a           | mm         | rrr        | EVV 0x8A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:w:zd:f32 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zf32
 | VCOMPRESSPS          | COMPRESS       | AVX512EVEX     | AVX512F_512    | 8a           | 0b11       | rrr        | EVV 0x8A V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=ZMM_B3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zf32
 | VCVTDQ2PD            | CONVERT        | AVX            |                | e6           | mm         | rrr        | VV1 0xE6  VL128 VF3 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:f64 MEM0:r:q:i32
 | VCVTDQ2PD            | CONVERT        | AVX            |                | e6           | 0b11       | rrr        | VV1 0xE6  VL128 VF3 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:f64 REG1=XMM_B():r:q:i32
 | VCVTDQ2PD            | CONVERT        | AVX            |                | e6           | mm         | rrr        | VV1 0xE6  VL256 VF3 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=YMM_R():w:qq:f64 MEM0:r:dq:i32
 | VCVTDQ2PD            | CONVERT        | AVX            |                | e6           | 0b11       | rrr        | VV1 0xE6  VL256 VF3 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=YMM_R():w:qq:f64 REG1=XMM_B():r:dq:i32
 | VCVTDQ2PD            | CONVERT        | AVX512EVEX     | AVX512F_128    | e6           | 0b11       | rrr        | EVV 0xE6 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                          | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i32
 | VCVTDQ2PD            | CONVERT        | AVX512EVEX     | AVX512F_128    | e6           | mm         | rrr        | EVV 0xE6 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:i32:TXT=BCASTSTR
 | VCVTDQ2PD            | CONVERT        | AVX512EVEX     | AVX512F_256    | e6           | 0b11       | rrr        | EVV 0xE6 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                          | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i32
 | VCVTDQ2PD            | CONVERT        | AVX512EVEX     | AVX512F_256    | e6           | mm         | rrr        | EVV 0xE6 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:i32:TXT=BCASTSTR
 | VCVTDQ2PD            | CONVERT        | AVX512EVEX     | AVX512F_512    | e6           | 0b11       | rrr        | EVV 0xE6 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                          | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:i32
 | VCVTDQ2PD            | CONVERT        | AVX512EVEX     | AVX512F_512    | e6           | mm         | rrr        | EVV 0xE6 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:i32:TXT=BCASTSTR
 | VCVTDQ2PD            | CONVERT        | KNCE           |                | e6           | mm         | rrr        | KVV 0xE6 V0F W0 VF3    NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT32_HALF()        | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VCVTDQ2PD            | CONVERT        | KNCE           |                | e6           | 0b11       | rrr        | KVV 0xE6 V0F W0 VF3   NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                  | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_B3():r:zf64:TXT=REGSWIZ
 | VCVTDQ2PD            | CONVERT        | KNCE           |                | e6           | 0b11       | rrr        | KVV 0xE6 V0F W0 VF3   NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] NR=1                                  | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_B3():r:zf64
 | VCVTDQ2PS            | CONVERT        | AVX            |                | 5b           | mm         | rrr        | VV1 0x5B  VL128 VNP V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:f32 MEM0:r:dq:i32
 | VCVTDQ2PS            | CONVERT        | AVX            |                | 5b           | 0b11       | rrr        | VV1 0x5B  VL128 VNP V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:f32 REG1=XMM_B():r:dq:i32
 | VCVTDQ2PS            | CONVERT        | AVX            |                | 5b           | mm         | rrr        | VV1 0x5B  VL256 VNP V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=YMM_R():w:qq:f32 MEM0:r:qq:i32
 | VCVTDQ2PS            | CONVERT        | AVX            |                | 5b           | 0b11       | rrr        | VV1 0x5B  VL256 VNP V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=YMM_R():w:qq:f32 REG1=YMM_B():r:qq:i32
 | VCVTDQ2PS            | CONVERT        | AVX512EVEX     | AVX512F_128    | 5b           | 0b11       | rrr        | EVV 0x5B VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                          | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i32
 | VCVTDQ2PS            | CONVERT        | AVX512EVEX     | AVX512F_128    | 5b           | mm         | rrr        | EVV 0x5B VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:i32:TXT=BCASTSTR
 | VCVTDQ2PS            | CONVERT        | AVX512EVEX     | AVX512F_256    | 5b           | 0b11       | rrr        | EVV 0x5B VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                          | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:i32
 | VCVTDQ2PS            | CONVERT        | AVX512EVEX     | AVX512F_256    | 5b           | mm         | rrr        | EVV 0x5B VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:i32:TXT=BCASTSTR
 | VCVTDQ2PS            | CONVERT        | AVX512EVEX     | AVX512F_512    | 5b           | 0b11       | rrr        | EVV 0x5B VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                          | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zi32
 | VCVTDQ2PS            | CONVERT        | AVX512EVEX     | AVX512F_512    | 5b           | 0b11       | rrr        | EVV 0x5B VNP V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0  NOEVSR | REG0=ZMM_R3():w:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zi32
 | VCVTDQ2PS            | CONVERT        | AVX512EVEX     | AVX512F_512    | 5b           | mm         | rrr        | EVV 0x5B VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:i32:TXT=BCASTSTR
 | VCVTFXPNTDQ2PS       | CONVERT        | KNCE           |                | cb           | mm         | rrr        | KVV 0xCB V0F3A VNP  W0 NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() UPCONVERT_INT32()     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT IMM0:r:b
 | VCVTFXPNTDQ2PS       | CONVERT        | KNCE           |                | cb           | 0b11       | rrr        | KVV 0xCB V0F3A VNP  W0 NOEVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8() NR=0 REG_SWIZZLE32()          | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_B3():r:zd:TXT=REGSWIZ  IMM0:r:b
 | VCVTFXPNTDQ2PS       | CONVERT        | KNCE           |                | cb           | 0b11       | rrr        | KVV 0xCB V0F3A VNP  W0 NOEVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8() NR=1  KNC_SAE()               | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_B3():r:zd:TXT=SAEC IMM0:r:b
 | VCVTFXPNTPD2DQ       | CONVERT        | KNCE           |                | e6           | mm         | rrr        | KVV 0xE6 V0F3A VF2  W1 NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() UPCONVERT_FLT64()     | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT IMM0:r:b
 | VCVTFXPNTPD2DQ       | CONVERT        | KNCE           |                | e6           | 0b11       | rrr        | KVV 0xE6 V0F3A VF2  W1 NOEVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8() NR=0 REG_SWIZZLE64()          | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw REG2=ZMM_B3():r:zf64:TXT=REGSWIZ  IMM0:r:b
 | VCVTFXPNTPD2DQ       | CONVERT        | KNCE           |                | e6           | 0b11       | rrr        | KVV 0xE6 V0F3A VF2  W1 NOEVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8() NR=1 KNC_SAE()                | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw REG2=ZMM_B3():r:zf64:TXT=SAEC IMM0:r:b
 | VCVTFXPNTPD2UDQ      | CONVERT        | KNCE           |                | ca           | mm         | rrr        | KVV 0xCA V0F3A VF2  W1  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() UPCONVERT_FLT64()    | REG0=ZMM_R3():rw:zud REG1=MASK1():r:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT IMM0:r:b
 | VCVTFXPNTPD2UDQ      | CONVERT        | KNCE           |                | ca           | 0b11       | rrr        | KVV 0xCA V0F3A VF2  W1  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] UIMM8() NR=1 KNC_SAE()              | REG0=ZMM_R3():rw:zud REG1=MASK1():r:mskw REG2=ZMM_B3():r:zf64:TXT=SAEC  IMM0:r:b
 | VCVTFXPNTPD2UDQ      | CONVERT        | KNCE           |                | ca           | 0b11       | rrr        | KVV 0xCA V0F3A VF2  W1  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] UIMM8() NR=0 REG_SWIZZLE64()        | REG0=ZMM_R3():rw:zud REG1=MASK1():r:mskw REG2=ZMM_B3():r:zf64 IMM0:r:b
 | VCVTFXPNTPS2DQ       | CONVERT        | KNCE           | KNCE           | cb           | mm         | rrr        | KVV 0xCB V0F3A V66  W0  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() UPCONVERT_FLT32()    | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  MEM0:r:zv:TXT=CONVERT:TXT=NT IMM0:r:b
 | VCVTFXPNTPS2DQ       | CONVERT        | KNCE           | KNCE           | cb           | 0b11       | rrr        | KVV 0xCB V0F3A V66  W0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] UIMM8() NR=1 KNC_SAE()              | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  REG2=ZMM_B3():r:zf32:TXT=SAEC  IMM0:r:b
 | VCVTFXPNTPS2DQ       | CONVERT        | KNCE           | KNCE           | cb           | 0b11       | rrr        | KVV 0xCB V0F3A V66  W0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] UIMM8() NR=0 REG_SWIZZLE32()        | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  REG2=ZMM_B3():r:zf32:TXT=REGSWIZ IMM0:r:b
 | VCVTFXPNTPS2UDQ      | CONVERT        | KNCE           | KNCE           | ca           | mm         | rrr        | KVV 0xCA V0F3A V66  W0  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() UPCONVERT_FLT32()    | REG0=ZMM_R3():rw:zud REG1=MASK1():r:mskw  MEM0:r:zv:TXT=CONVERT:TXT=NT IMM0:r:b
 | VCVTFXPNTPS2UDQ      | CONVERT        | KNCE           | KNCE           | ca           | 0b11       | rrr        | KVV 0xCA V0F3A V66  W0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] UIMM8() NR=1 KNC_SAE()              | REG0=ZMM_R3():rw:zud REG1=MASK1():r:mskw  REG2=ZMM_B3():r:zf32:TXT=SAEC  IMM0:r:b
 | VCVTFXPNTPS2UDQ      | CONVERT        | KNCE           | KNCE           | ca           | 0b11       | rrr        | KVV 0xCA V0F3A V66  W0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] UIMM8() NR=0 REG_SWIZZLE32()        | REG0=ZMM_R3():rw:zud REG1=MASK1():r:mskw  REG2=ZMM_B3():r:zf32:TXT=REGSWIZ IMM0:r:b
 | VCVTFXPNTUDQ2PS      | CONVERT        | KNCE           |                | ca           | mm         | rrr        | KVV 0xCA V0F3A VNP  W0 NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() UPCONVERT_INT32()     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT IMM0:r:b
 | VCVTFXPNTUDQ2PS      | CONVERT        | KNCE           |                | ca           | 0b11       | rrr        | KVV 0xCA V0F3A VNP  W0 NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] UIMM8() NR=0 REG_SWIZZLE32()         | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_B3():r:zud:TXT=REGSWIZ IMM0:r:b
 | VCVTFXPNTUDQ2PS      | CONVERT        | KNCE           |                | ca           | 0b11       | rrr        | KVV 0xCA V0F3A VNP  W0 NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] UIMM8() NR=1 KNC_SAE()               | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_B3():r:zud:TXT=SAEC IMM0:r:b
 | VCVTNE2PS2BF16       | CONVERT        | AVX512EVEX     | AVX512_BF16_128 | 72           | 0b11       | rrr        | EVV 0x72 VF2 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():w:dq:bf16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VCVTNE2PS2BF16       | CONVERT        | AVX512EVEX     | AVX512_BF16_128 | 72           | mm         | rrr        | EVV 0x72 VF2 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:bf16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VCVTNE2PS2BF16       | CONVERT        | AVX512EVEX     | AVX512_BF16_256 | 72           | 0b11       | rrr        | EVV 0x72 VF2 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():w:qq:bf16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
 | VCVTNE2PS2BF16       | CONVERT        | AVX512EVEX     | AVX512_BF16_256 | 72           | mm         | rrr        | EVV 0x72 VF2 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:bf16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VCVTNE2PS2BF16       | CONVERT        | AVX512EVEX     | AVX512_BF16_512 | 72           | 0b11       | rrr        | EVV 0x72 VF2 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():w:zbf16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VCVTNE2PS2BF16       | CONVERT        | AVX512EVEX     | AVX512_BF16_512 | 72           | mm         | rrr        | EVV 0x72 VF2 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zbf16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VCVTNEPS2BF16        | CONVERT        | AVX512EVEX     | AVX512_BF16_128 | 72           | 0b11       | rrr        | EVV 0x72 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_R3():w:dq:bf16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
 | VCVTNEPS2BF16        | CONVERT        | AVX512EVEX     | AVX512_BF16_128 | 72           | mm         | rrr        | EVV 0x72 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:bf16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VCVTNEPS2BF16        | CONVERT        | AVX512EVEX     | AVX512_BF16_256 | 72           | 0b11       | rrr        | EVV 0x72 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=XMM_R3():w:dq:bf16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
 | VCVTNEPS2BF16        | CONVERT        | AVX512EVEX     | AVX512_BF16_256 | 72           | mm         | rrr        | EVV 0x72 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:bf16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VCVTNEPS2BF16        | CONVERT        | AVX512EVEX     | AVX512_BF16_512 | 72           | 0b11       | rrr        | EVV 0x72 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=YMM_R3():w:qq:bf16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
 | VCVTNEPS2BF16        | CONVERT        | AVX512EVEX     | AVX512_BF16_512 | 72           | mm         | rrr        | EVV 0x72 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:bf16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VCVTPD2DQ            | CONVERT        | AVX            |                | e6           | mm         | rrr        | VV1 0xE6  VL128 VF2 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:i32 MEM0:r:dq:f64
 | VCVTPD2DQ            | CONVERT        | AVX            |                | e6           | 0b11       | rrr        | VV1 0xE6  VL128 VF2 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:i32 REG1=XMM_B():r:dq:f64
 | VCVTPD2DQ            | CONVERT        | AVX            |                | e6           | mm         | rrr        | VV1 0xE6  VL256 VF2 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:i32 MEM0:r:qq:f64
 | VCVTPD2DQ            | CONVERT        | AVX            |                | e6           | 0b11       | rrr        | VV1 0xE6  VL256 VF2 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:i32 REG1=YMM_B():r:qq:f64
 | VCVTPD2DQ            | CONVERT        | AVX512EVEX     | AVX512F_128    | e6           | 0b11       | rrr        | EVV 0xE6 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                          | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64
 | VCVTPD2DQ            | CONVERT        | AVX512EVEX     | AVX512F_128    | e6           | mm         | rrr        | EVV 0xE6 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VCVTPD2DQ            | CONVERT        | AVX512EVEX     | AVX512F_256    | e6           | 0b11       | rrr        | EVV 0xE6 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                          | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64
 | VCVTPD2DQ            | CONVERT        | AVX512EVEX     | AVX512F_256    | e6           | mm         | rrr        | EVV 0xE6 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VCVTPD2DQ            | CONVERT        | AVX512EVEX     | AVX512F_512    | e6           | 0b11       | rrr        | EVV 0xE6 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                          | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
 | VCVTPD2DQ            | CONVERT        | AVX512EVEX     | AVX512F_512    | e6           | 0b11       | rrr        | EVV 0xE6 VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1  NOEVSR | REG0=YMM_R3():w:qq:i32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
 | VCVTPD2DQ            | CONVERT        | AVX512EVEX     | AVX512F_512    | e6           | mm         | rrr        | EVV 0xE6 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VCVTPD2PS            | CONVERT        | AVX            |                | 5a           | mm         | rrr        | VV1 0x5A  V66 VL128 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:f32 MEM0:r:dq:f64
 | VCVTPD2PS            | CONVERT        | AVX            |                | 5a           | 0b11       | rrr        | VV1 0x5A  V66 VL128 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:f32 REG1=XMM_B():r:dq:f64
 | VCVTPD2PS            | CONVERT        | AVX            |                | 5a           | mm         | rrr        | VV1 0x5A  V66 VL256 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:f32 MEM0:r:qq:f64
 | VCVTPD2PS            | CONVERT        | AVX            |                | 5a           | 0b11       | rrr        | VV1 0x5A  V66 VL256 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:f32 REG1=YMM_B():r:qq:f64
 | VCVTPD2PS            | CONVERT        | AVX512EVEX     | AVX512F_128    | 5a           | 0b11       | rrr        | EVV 0x5A V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                          | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64
 | VCVTPD2PS            | CONVERT        | AVX512EVEX     | AVX512F_128    | 5a           | mm         | rrr        | EVV 0x5A V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VCVTPD2PS            | CONVERT        | AVX512EVEX     | AVX512F_256    | 5a           | 0b11       | rrr        | EVV 0x5A V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                          | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64
 | VCVTPD2PS            | CONVERT        | AVX512EVEX     | AVX512F_256    | 5a           | mm         | rrr        | EVV 0x5A V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VCVTPD2PS            | CONVERT        | AVX512EVEX     | AVX512F_512    | 5a           | 0b11       | rrr        | EVV 0x5A V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                          | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
 | VCVTPD2PS            | CONVERT        | AVX512EVEX     | AVX512F_512    | 5a           | 0b11       | rrr        | EVV 0x5A V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1  NOEVSR | REG0=YMM_R3():w:qq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
 | VCVTPD2PS            | CONVERT        | AVX512EVEX     | AVX512F_512    | 5a           | mm         | rrr        | EVV 0x5A V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VCVTPD2PS            | CONVERT        | KNCE           |                | 5a           | mm         | rrr        | KVV 0x5A V0F V66  W1 NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT64()               | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VCVTPD2PS            | CONVERT        | KNCE           |                | 5a           | 0b11       | rrr        | KVV 0x5A V0F V66  W1 NOEVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE64()                    | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_B3():r:zf64:TXT=REGSWIZ
 | VCVTPD2PS            | CONVERT        | KNCE           |                | 5a           | 0b11       | rrr        | KVV 0x5A V0F V66  W1 NOEVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]  NR=1 ROUND() KNC_SAE()                 | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_B3():r:zf64:TXT=ROUNDC:TXT=SAEC
 | VCVTPD2QQ            | CONVERT        | AVX512EVEX     | AVX512DQ_128   | 7b           | 0b11       | rrr        | EVV 0x7B V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                          | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64
 | VCVTPD2QQ            | CONVERT        | AVX512EVEX     | AVX512DQ_128   | 7b           | mm         | rrr        | EVV 0x7B V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VCVTPD2QQ            | CONVERT        | AVX512EVEX     | AVX512DQ_256   | 7b           | 0b11       | rrr        | EVV 0x7B V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                          | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64
 | VCVTPD2QQ            | CONVERT        | AVX512EVEX     | AVX512DQ_256   | 7b           | mm         | rrr        | EVV 0x7B V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VCVTPD2QQ            | CONVERT        | AVX512EVEX     | AVX512DQ_512   | 7b           | 0b11       | rrr        | EVV 0x7B V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                          | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
 | VCVTPD2QQ            | CONVERT        | AVX512EVEX     | AVX512DQ_512   | 7b           | 0b11       | rrr        | EVV 0x7B V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1  NOEVSR | REG0=ZMM_R3():w:zi64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
 | VCVTPD2QQ            | CONVERT        | AVX512EVEX     | AVX512DQ_512   | 7b           | mm         | rrr        | EVV 0x7B V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VCVTPD2UDQ           | CONVERT        | AVX512EVEX     | AVX512F_128    | 79           | 0b11       | rrr        | EVV 0x79 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                          | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64
 | VCVTPD2UDQ           | CONVERT        | AVX512EVEX     | AVX512F_128    | 79           | mm         | rrr        | EVV 0x79 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VCVTPD2UDQ           | CONVERT        | AVX512EVEX     | AVX512F_256    | 79           | 0b11       | rrr        | EVV 0x79 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                          | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64
 | VCVTPD2UDQ           | CONVERT        | AVX512EVEX     | AVX512F_256    | 79           | mm         | rrr        | EVV 0x79 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VCVTPD2UDQ           | CONVERT        | AVX512EVEX     | AVX512F_512    | 79           | 0b11       | rrr        | EVV 0x79 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                          | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
 | VCVTPD2UDQ           | CONVERT        | AVX512EVEX     | AVX512F_512    | 79           | 0b11       | rrr        | EVV 0x79 VNP V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1  NOEVSR | REG0=YMM_R3():w:qq:u32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
 | VCVTPD2UDQ           | CONVERT        | AVX512EVEX     | AVX512F_512    | 79           | mm         | rrr        | EVV 0x79 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VCVTPD2UQQ           | CONVERT        | AVX512EVEX     | AVX512DQ_128   | 79           | 0b11       | rrr        | EVV 0x79 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                          | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64
 | VCVTPD2UQQ           | CONVERT        | AVX512EVEX     | AVX512DQ_128   | 79           | mm         | rrr        | EVV 0x79 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VCVTPD2UQQ           | CONVERT        | AVX512EVEX     | AVX512DQ_256   | 79           | 0b11       | rrr        | EVV 0x79 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                          | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64
 | VCVTPD2UQQ           | CONVERT        | AVX512EVEX     | AVX512DQ_256   | 79           | mm         | rrr        | EVV 0x79 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VCVTPD2UQQ           | CONVERT        | AVX512EVEX     | AVX512DQ_512   | 79           | 0b11       | rrr        | EVV 0x79 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                          | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
 | VCVTPD2UQQ           | CONVERT        | AVX512EVEX     | AVX512DQ_512   | 79           | 0b11       | rrr        | EVV 0x79 V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1  NOEVSR | REG0=ZMM_R3():w:zu64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
 | VCVTPD2UQQ           | CONVERT        | AVX512EVEX     | AVX512DQ_512   | 79           | mm         | rrr        | EVV 0x79 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VCVTPH2PS            | CONVERT        | AVX512EVEX     | AVX512F_128    | 13           | 0b11       | rrr        | EVV 0x13 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f16
 | VCVTPH2PS            | CONVERT        | AVX512EVEX     | AVX512F_128    | 13           | mm         | rrr        | EVV 0x13 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_16_BITS() NELEM_HALFMEM() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:f16
 | VCVTPH2PS            | CONVERT        | AVX512EVEX     | AVX512F_256    | 13           | 0b11       | rrr        | EVV 0x13 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f16
 | VCVTPH2PS            | CONVERT        | AVX512EVEX     | AVX512F_256    | 13           | mm         | rrr        | EVV 0x13 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_16_BITS() NELEM_HALFMEM() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:f16
 | VCVTPH2PS            | CONVERT        | AVX512EVEX     | AVX512F_512    | 13           | 0b11       | rrr        | EVV 0x13 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f16
 | VCVTPH2PS            | CONVERT        | AVX512EVEX     | AVX512F_512    | 13           | 0b11       | rrr        | EVV 0x13 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0  NOEVSR      | REG0=ZMM_R3():w:zf32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f16
 | VCVTPH2PS            | CONVERT        | AVX512EVEX     | AVX512F_512    | 13           | mm         | rrr        | EVV 0x13 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_16_BITS() NELEM_HALFMEM() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:f16
 | VCVTPH2PS            | CONVERT        | F16C           |                | 13           | mm         | rrr        | VV1 0x13 VL128 V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()  W0                           | REG0=XMM_R():w:dq:f32 MEM0:r:q:f16
 | VCVTPH2PS            | CONVERT        | F16C           |                | 13           | 0b11       | rrr        | VV1 0x13 VL128 V66 V0F38 NOVSR  MOD[0b11] MOD=3 REG[rrr] RM[nnn] W0                                  | REG0=XMM_R():w:dq:f32  REG1=XMM_B():r:q:f16
 | VCVTPH2PS            | CONVERT        | F16C           |                | 13           | mm         | rrr        | VV1 0x13 VL256 V66 V0F38 NOVSR  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() W0                           | REG0=YMM_R():w:qq:f32 MEM0:r:dq:f16
 | VCVTPH2PS            | CONVERT        | F16C           |                | 13           | 0b11       | rrr        | VV1 0x13 VL256 V66 V0F38 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]  W0                                  | REG0=YMM_R():w:qq:f32  REG1=XMM_B():r:dq:f16
 | VCVTPS2DQ            | CONVERT        | AVX            |                | 5b           | mm         | rrr        | VV1 0x5B  VL128 V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:i32 MEM0:r:dq:f32
 | VCVTPS2DQ            | CONVERT        | AVX            |                | 5b           | 0b11       | rrr        | VV1 0x5B  VL128 V66 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:i32 REG1=XMM_B():r:dq:f32
 | VCVTPS2DQ            | CONVERT        | AVX            |                | 5b           | mm         | rrr        | VV1 0x5B  VL256 V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=YMM_R():w:qq:i32 MEM0:r:qq:f32
 | VCVTPS2DQ            | CONVERT        | AVX            |                | 5b           | 0b11       | rrr        | VV1 0x5B  VL256 V66 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=YMM_R():w:qq:i32 REG1=YMM_B():r:qq:f32
 | VCVTPS2DQ            | CONVERT        | AVX512EVEX     | AVX512F_128    | 5b           | 0b11       | rrr        | EVV 0x5B V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                          | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
 | VCVTPS2DQ            | CONVERT        | AVX512EVEX     | AVX512F_128    | 5b           | mm         | rrr        | EVV 0x5B V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VCVTPS2DQ            | CONVERT        | AVX512EVEX     | AVX512F_256    | 5b           | 0b11       | rrr        | EVV 0x5B V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                          | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
 | VCVTPS2DQ            | CONVERT        | AVX512EVEX     | AVX512F_256    | 5b           | mm         | rrr        | EVV 0x5B V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VCVTPS2DQ            | CONVERT        | AVX512EVEX     | AVX512F_512    | 5b           | 0b11       | rrr        | EVV 0x5B V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                          | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
 | VCVTPS2DQ            | CONVERT        | AVX512EVEX     | AVX512F_512    | 5b           | 0b11       | rrr        | EVV 0x5B V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0  NOEVSR | REG0=ZMM_R3():w:zi32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
 | VCVTPS2DQ            | CONVERT        | AVX512EVEX     | AVX512F_512    | 5b           | mm         | rrr        | EVV 0x5B V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VCVTPS2PD            | CONVERT        | AVX            |                | 5a           | mm         | rrr        | VV1 0x5A  VNP VL128 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:f64 MEM0:r:q:f32
 | VCVTPS2PD            | CONVERT        | AVX            |                | 5a           | 0b11       | rrr        | VV1 0x5A  VNP VL128 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:f64 REG1=XMM_B():r:q:f32
 | VCVTPS2PD            | CONVERT        | AVX            |                | 5a           | mm         | rrr        | VV1 0x5A  VNP VL256 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=YMM_R():w:qq:f64 MEM0:r:dq:f32
 | VCVTPS2PD            | CONVERT        | AVX            |                | 5a           | 0b11       | rrr        | VV1 0x5A  VNP VL256 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=YMM_R():w:qq:f64 REG1=XMM_B():r:dq:f32
 | VCVTPS2PD            | CONVERT        | AVX512EVEX     | AVX512F_128    | 5a           | 0b11       | rrr        | EVV 0x5A VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                          | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
 | VCVTPS2PD            | CONVERT        | AVX512EVEX     | AVX512F_128    | 5a           | mm         | rrr        | EVV 0x5A VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VCVTPS2PD            | CONVERT        | AVX512EVEX     | AVX512F_256    | 5a           | 0b11       | rrr        | EVV 0x5A VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                          | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
 | VCVTPS2PD            | CONVERT        | AVX512EVEX     | AVX512F_256    | 5a           | mm         | rrr        | EVV 0x5A VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VCVTPS2PD            | CONVERT        | AVX512EVEX     | AVX512F_512    | 5a           | 0b11       | rrr        | EVV 0x5A VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                          | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
 | VCVTPS2PD            | CONVERT        | AVX512EVEX     | AVX512F_512    | 5a           | 0b11       | rrr        | EVV 0x5A VNP V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0  NOEVSR        | REG0=ZMM_R3():w:zf64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
 | VCVTPS2PD            | CONVERT        | AVX512EVEX     | AVX512F_512    | 5a           | mm         | rrr        | EVV 0x5A VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VCVTPS2PD            | CONVERT        | KNCE           |                | 5a           | mm         | rrr        | KVV 0x5A V0F REXW=0 VNP    NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT32_HALF()    | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VCVTPS2PD            | CONVERT        | KNCE           |                | 5a           | 0b11       | rrr        | KVV 0x5A V0F REXW=0 VNP   NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()              | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VCVTPS2PD            | CONVERT        | KNCE           |                | 5a           | 0b11       | rrr        | KVV 0x5A V0F REXW=0 VNP   NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] NR=1 KNC_SAE()                    | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_B3():r:zf32:TXT=SAEC
 | VCVTPS2PH            | CONVERT        | AVX512EVEX     | AVX512F_128    | 1d           | 0b11       | rrr        | EVV 0x1D V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR UIMM8()                | REG0=XMM_B3():w:dq:f16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:f32 IMM0:r:b
 | VCVTPS2PH            | CONVERT        | AVX512EVEX     | AVX512F_128    | 1d           | mm         | rrr        | EVV 0x1D V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0 UIMM8()  ESIZE_16_BITS() NELEM_HALFMEM() | MEM0:w:q:f16 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:f32 IMM0:r:b
 | VCVTPS2PH            | CONVERT        | AVX512EVEX     | AVX512F_256    | 1d           | 0b11       | rrr        | EVV 0x1D V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR UIMM8()                | REG0=XMM_B3():w:dq:f16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:f32 IMM0:r:b
 | VCVTPS2PH            | CONVERT        | AVX512EVEX     | AVX512F_256    | 1d           | mm         | rrr        | EVV 0x1D V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0 UIMM8()  ESIZE_16_BITS() NELEM_HALFMEM() | MEM0:w:dq:f16 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:f32 IMM0:r:b
 | VCVTPS2PH            | CONVERT        | AVX512EVEX     | AVX512F_512    | 1d           | 0b11       | rrr        | EVV 0x1D V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR UIMM8()                | REG0=YMM_B3():w:qq:f16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zf32 IMM0:r:b
 | VCVTPS2PH            | CONVERT        | AVX512EVEX     | AVX512F_512    | 1d           | 0b11       | rrr        | EVV 0x1D V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0  NOEVSR UIMM8() | REG0=YMM_B3():w:qq:f16:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zf32 IMM0:r:b
 | VCVTPS2PH            | CONVERT        | AVX512EVEX     | AVX512F_512    | 1d           | mm         | rrr        | EVV 0x1D V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0 UIMM8()  ESIZE_16_BITS() NELEM_HALFMEM() | MEM0:w:qq:f16 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zf32 IMM0:r:b
 | VCVTPS2PH            | CONVERT        | F16C           |                | 1d           | mm         | rrr        | VV1 0x1D VL128 V66 V0F3A NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() W0                    | MEM0:w:q:f16 REG0=XMM_R():r:dq:f32  IMM0:r:b
 | VCVTPS2PH            | CONVERT        | F16C           |                | 1d           | 0b11       | rrr        | VV1 0x1D VL128 V66 V0F3A NOVSR  MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8() W0                          | REG0=XMM_B():w:q:f16 REG1=XMM_R():r:dq:f32   IMM0:r:b
 | VCVTPS2PH            | CONVERT        | F16C           |                | 1d           | mm         | rrr        | VV1 0x1D VL256 V66 V0F3A NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() W0                    | MEM0:w:dq:f16 REG0=YMM_R():r:qq:f32  IMM0:r:b
 | VCVTPS2PH            | CONVERT        | F16C           |                | 1d           | 0b11       | rrr        | VV1 0x1D VL256 V66 V0F3A NOVSR  MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8() W0                          | REG0=XMM_B():w:dq:f16 REG1=YMM_R():r:qq:f32    IMM0:r:b
 | VCVTPS2QQ            | CONVERT        | AVX512EVEX     | AVX512DQ_128   | 7b           | 0b11       | rrr        | EVV 0x7B V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                          | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
 | VCVTPS2QQ            | CONVERT        | AVX512EVEX     | AVX512DQ_128   | 7b           | mm         | rrr        | EVV 0x7B V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VCVTPS2QQ            | CONVERT        | AVX512EVEX     | AVX512DQ_256   | 7b           | 0b11       | rrr        | EVV 0x7B V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                          | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
 | VCVTPS2QQ            | CONVERT        | AVX512EVEX     | AVX512DQ_256   | 7b           | mm         | rrr        | EVV 0x7B V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VCVTPS2QQ            | CONVERT        | AVX512EVEX     | AVX512DQ_512   | 7b           | 0b11       | rrr        | EVV 0x7B V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                          | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
 | VCVTPS2QQ            | CONVERT        | AVX512EVEX     | AVX512DQ_512   | 7b           | 0b11       | rrr        | EVV 0x7B V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0  NOEVSR | REG0=ZMM_R3():w:zi64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
 | VCVTPS2QQ            | CONVERT        | AVX512EVEX     | AVX512DQ_512   | 7b           | mm         | rrr        | EVV 0x7B V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VCVTPS2UDQ           | CONVERT        | AVX512EVEX     | AVX512F_128    | 79           | 0b11       | rrr        | EVV 0x79 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                          | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
 | VCVTPS2UDQ           | CONVERT        | AVX512EVEX     | AVX512F_128    | 79           | mm         | rrr        | EVV 0x79 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VCVTPS2UDQ           | CONVERT        | AVX512EVEX     | AVX512F_256    | 79           | 0b11       | rrr        | EVV 0x79 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                          | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
 | VCVTPS2UDQ           | CONVERT        | AVX512EVEX     | AVX512F_256    | 79           | mm         | rrr        | EVV 0x79 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VCVTPS2UDQ           | CONVERT        | AVX512EVEX     | AVX512F_512    | 79           | 0b11       | rrr        | EVV 0x79 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                          | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
 | VCVTPS2UDQ           | CONVERT        | AVX512EVEX     | AVX512F_512    | 79           | 0b11       | rrr        | EVV 0x79 VNP V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0  NOEVSR | REG0=ZMM_R3():w:zu32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
 | VCVTPS2UDQ           | CONVERT        | AVX512EVEX     | AVX512F_512    | 79           | mm         | rrr        | EVV 0x79 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VCVTPS2UQQ           | CONVERT        | AVX512EVEX     | AVX512DQ_128   | 79           | 0b11       | rrr        | EVV 0x79 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                          | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
 | VCVTPS2UQQ           | CONVERT        | AVX512EVEX     | AVX512DQ_128   | 79           | mm         | rrr        | EVV 0x79 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VCVTPS2UQQ           | CONVERT        | AVX512EVEX     | AVX512DQ_256   | 79           | 0b11       | rrr        | EVV 0x79 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                          | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
 | VCVTPS2UQQ           | CONVERT        | AVX512EVEX     | AVX512DQ_256   | 79           | mm         | rrr        | EVV 0x79 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VCVTPS2UQQ           | CONVERT        | AVX512EVEX     | AVX512DQ_512   | 79           | 0b11       | rrr        | EVV 0x79 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                          | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
 | VCVTPS2UQQ           | CONVERT        | AVX512EVEX     | AVX512DQ_512   | 79           | 0b11       | rrr        | EVV 0x79 V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0  NOEVSR | REG0=ZMM_R3():w:zu64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
 | VCVTPS2UQQ           | CONVERT        | AVX512EVEX     | AVX512DQ_512   | 79           | mm         | rrr        | EVV 0x79 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VCVTQQ2PD            | CONVERT        | AVX512EVEX     | AVX512DQ_128   | e6           | 0b11       | rrr        | EVV 0xE6 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                          | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64
 | VCVTQQ2PD            | CONVERT        | AVX512EVEX     | AVX512DQ_128   | e6           | mm         | rrr        | EVV 0xE6 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VCVTQQ2PD            | CONVERT        | AVX512EVEX     | AVX512DQ_256   | e6           | 0b11       | rrr        | EVV 0xE6 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                          | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64
 | VCVTQQ2PD            | CONVERT        | AVX512EVEX     | AVX512DQ_256   | e6           | mm         | rrr        | EVV 0xE6 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VCVTQQ2PD            | CONVERT        | AVX512EVEX     | AVX512DQ_512   | e6           | 0b11       | rrr        | EVV 0xE6 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                          | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
 | VCVTQQ2PD            | CONVERT        | AVX512EVEX     | AVX512DQ_512   | e6           | 0b11       | rrr        | EVV 0xE6 VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1  NOEVSR | REG0=ZMM_R3():w:zi64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
 | VCVTQQ2PD            | CONVERT        | AVX512EVEX     | AVX512DQ_512   | e6           | mm         | rrr        | EVV 0xE6 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VCVTQQ2PS            | CONVERT        | AVX512EVEX     | AVX512DQ_128   | 5b           | 0b11       | rrr        | EVV 0x5B VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                          | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u64
 | VCVTQQ2PS            | CONVERT        | AVX512EVEX     | AVX512DQ_128   | 5b           | mm         | rrr        | EVV 0x5B VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR
 | VCVTQQ2PS            | CONVERT        | AVX512EVEX     | AVX512DQ_256   | 5b           | 0b11       | rrr        | EVV 0x5B VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                          | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u64
 | VCVTQQ2PS            | CONVERT        | AVX512EVEX     | AVX512DQ_256   | 5b           | mm         | rrr        | EVV 0x5B VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR
 | VCVTQQ2PS            | CONVERT        | AVX512EVEX     | AVX512DQ_512   | 5b           | 0b11       | rrr        | EVV 0x5B VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                          | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu64
 | VCVTQQ2PS            | CONVERT        | AVX512EVEX     | AVX512DQ_512   | 5b           | 0b11       | rrr        | EVV 0x5B VNP V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1  NOEVSR | REG0=YMM_R3():w:qq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu64
 | VCVTQQ2PS            | CONVERT        | AVX512EVEX     | AVX512DQ_512   | 5b           | mm         | rrr        | EVV 0x5B VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR
 | VCVTSD2SI            | CONVERT        | AVX            |                | 2d           | mm         | rrr        | VV1 0x2D   VF2 V0F  NOVSR not64 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                              | REG0=GPR32_R():w:d:i32 MEM0:r:q:f64
 | VCVTSD2SI            | CONVERT        | AVX            |                | 2d           | 0b11       | rrr        | VV1 0x2D   VF2 V0F  NOVSR not64 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=GPR32_R():w:d:i32 REG1=XMM_B():r:q:f64
 | VCVTSD2SI            | CONVERT        | AVX            |                | 2d           | mm         | rrr        | VV1 0x2D   VF2 V0F  NOVSR mode64 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()               | REG0=GPR32_R():w:d:i32 MEM0:r:q:f64
 | VCVTSD2SI            | CONVERT        | AVX            |                | 2d           | 0b11       | rrr        | VV1 0x2D   VF2 V0F  NOVSR mode64 norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                      | REG0=GPR32_R():w:d:i32 REG1=XMM_B():r:q:f64
 | VCVTSD2SI            | CONVERT        | AVX            |                | 2d           | mm         | rrr        | VV1 0x2D   VF2 V0F  NOVSR mode64 rexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                 | REG0=GPR64_R():w:q:i64 MEM0:r:q:f64
 | VCVTSD2SI            | CONVERT        | AVX            |                | 2d           | 0b11       | rrr        | VV1 0x2D   VF2 V0F  NOVSR mode64 rexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                        | REG0=GPR64_R():w:q:i64 REG1=XMM_B():r:q:f64
 | VCVTSD2SI            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2d           | 0b11       | rrr        | EVV 0x2D VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  not64  NOEVSR  ZEROING=0 MASK=0  FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:i32 REG1=XMM_B3():r:dq:f64
 | VCVTSD2SI            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2d           | 0b11       | rrr        | EVV 0x2D VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64 W0  NOEVSR  ZEROING=0 MASK=0 EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:i32 REG1=XMM_B3():r:dq:f64
 | VCVTSD2SI            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2d           | 0b11       | rrr        | EVV 0x2D VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  not64  NOEVSR  ZEROING=0 MASK=0 | REG0=GPR32_R():w:d:i32:TXT=ROUNDC REG1=XMM_B3():r:dq:f64
 | VCVTSD2SI            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2d           | 0b11       | rrr        | EVV 0x2D VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  mode64 W0  NOEVSR  ZEROING=0 MASK=0 EVEXRR_ONE | REG0=GPR32_R():w:d:i32:TXT=ROUNDC REG1=XMM_B3():r:dq:f64
 | VCVTSD2SI            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2d           | mm         | rrr        | EVV 0x2D VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  not64  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_GPR_WRITER_LDOP_Q()  FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:i32 MEM0:r:q:f64
 | VCVTSD2SI            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2d           | mm         | rrr        | EVV 0x2D VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64 W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_GPR_WRITER_LDOP_Q() EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:i32 MEM0:r:q:f64
 | VCVTSD2SI            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2d           | 0b11       | rrr        | EVV 0x2D VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64  W1  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR64_R():w:q:i64 REG1=XMM_B3():r:dq:f64
 | VCVTSD2SI            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2d           | 0b11       | rrr        | EVV 0x2D VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  mode64  W1  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE | REG0=GPR64_R():w:q:i64:TXT=ROUNDC REG1=XMM_B3():r:dq:f64
 | VCVTSD2SI            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2d           | mm         | rrr        | EVV 0x2D VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64  W1  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_GPR_WRITER_LDOP_Q() EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR64_R():w:q:i64 MEM0:r:q:f64
 | VCVTSD2SS            | CONVERT        | AVX            |                | 5a           | mm         | rrr        | VV1 0x5A  VF2 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                           | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:q:f64
 | VCVTSD2SS            | CONVERT        | AVX            |                | 5a           | 0b11       | rrr        | VV1 0x5A  VF2 V0F  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                  | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:q:f64
 | VCVTSD2SS            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 5a           | 0b11       | rrr        | EVV 0x5A VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                                         | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VCVTSD2SS            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 5a           | 0b11       | rrr        | EVV 0x5A VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1       | REG0=XMM_R3():w:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VCVTSD2SS            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 5a           | mm         | rrr        | EVV 0x5A VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
 | VCVTSD2USI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 79           | 0b11       | rrr        | EVV 0x79 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  not64  NOEVSR  ZEROING=0 MASK=0  FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:u32 REG1=XMM_B3():r:dq:f64
 | VCVTSD2USI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 79           | 0b11       | rrr        | EVV 0x79 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64 W0  NOEVSR  ZEROING=0 MASK=0 EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:u32 REG1=XMM_B3():r:dq:f64
 | VCVTSD2USI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 79           | 0b11       | rrr        | EVV 0x79 VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  not64  NOEVSR  ZEROING=0 MASK=0 | REG0=GPR32_R():w:d:u32:TXT=ROUNDC REG1=XMM_B3():r:dq:f64
 | VCVTSD2USI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 79           | 0b11       | rrr        | EVV 0x79 VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  mode64 W0  NOEVSR  ZEROING=0 MASK=0 EVEXRR_ONE | REG0=GPR32_R():w:d:u32:TXT=ROUNDC REG1=XMM_B3():r:dq:f64
 | VCVTSD2USI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 79           | mm         | rrr        | EVV 0x79 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  not64  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_GPR_WRITER_LDOP_Q() FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:u32 MEM0:r:q:f64
 | VCVTSD2USI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 79           | mm         | rrr        | EVV 0x79 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64 W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_GPR_WRITER_LDOP_Q() EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:u32 MEM0:r:q:f64
 | VCVTSD2USI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 79           | 0b11       | rrr        | EVV 0x79 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64  W1  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR64_R():w:q:u64 REG1=XMM_B3():r:dq:f64
 | VCVTSD2USI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 79           | 0b11       | rrr        | EVV 0x79 VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  mode64  W1  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE | REG0=GPR64_R():w:q:u64:TXT=ROUNDC REG1=XMM_B3():r:dq:f64
 | VCVTSD2USI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 79           | mm         | rrr        | EVV 0x79 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64  W1  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_GPR_WRITER_LDOP_Q() EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR64_R():w:q:u64 MEM0:r:q:f64
 | VCVTSI2SD            | CONVERT        | AVX            |                | 2a           | mm         | rrr        | VV1 0x2A  VF2 V0F not64 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:d:i32
 | VCVTSI2SD            | CONVERT        | AVX            |                | 2a           | 0b11       | rrr        | VV1 0x2A  VF2 V0F not64 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=GPR32_B():r:d:i32
 | VCVTSI2SD            | CONVERT        | AVX            |                | 2a           | mm         | rrr        | VV1 0x2A  VF2 V0F mode64 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                       | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:d:i32
 | VCVTSI2SD            | CONVERT        | AVX            |                | 2a           | 0b11       | rrr        | VV1 0x2A  VF2 V0F mode64 norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                              | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=GPR32_B():r:d:i32
 | VCVTSI2SD            | CONVERT        | AVX            |                | 2a           | mm         | rrr        | VV1 0x2A  VF2 V0F mode64 rexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                         | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:q:i64
 | VCVTSI2SD            | CONVERT        | AVX            |                | 2a           | 0b11       | rrr        | VV1 0x2A  VF2 V0F mode64 rexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=GPR64_B():r:q:i64
 | VCVTSI2SD            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2a           | 0b11       | rrr        | EVV 0x2A VF2 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  not64    ZEROING=0 MASK=0 FIX_ROUND_LEN128()     | REG0=XMM_R3():w:dq:f64 REG1=XMM_N3():r:dq:f64 REG2=GPR32_B():r:d:i32
 | VCVTSI2SD            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2a           | 0b11       | rrr        | EVV 0x2A VF2 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  mode64 W0    ZEROING=0 MASK=0  FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f64 REG1=XMM_N3():r:dq:f64 REG2=GPR32_B():r:d:i32
 | VCVTSI2SD            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2a           | mm         | rrr        | EVV 0x2A VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  not64 ZEROING=0 MASK=0 BCRC=0  ESIZE_32_BITS() NELEM_GPR_READER() FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f64 REG1=XMM_N3():r:dq:f64 MEM0:r:d:i32
 | VCVTSI2SD            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2a           | mm         | rrr        | EVV 0x2A VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  mode64 W0 ZEROING=0 MASK=0 BCRC=0  ESIZE_32_BITS() NELEM_GPR_READER() FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f64 REG1=XMM_N3():r:dq:f64 MEM0:r:d:i32
 | VCVTSI2SD            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2a           | 0b11       | rrr        | EVV 0x2A VF2 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] mode64 W1 ZEROING=0 MASK=0 BCRC=0  FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f64 REG1=XMM_N3():r:dq:f64 REG2=GPR64_B():r:q:i64
 | VCVTSI2SD            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2a           | 0b11       | rrr        | EVV 0x2A VF2 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn] mode64 W1 ZEROING=0 MASK=0 BCRC=1 FIX_ROUND_LEN128() AVX512_ROUND() | REG0=XMM_R3():w:dq:f64:TXT=ROUNDC REG1=XMM_N3():r:dq:f64 REG2=GPR64_B():r:q:i64
 | VCVTSI2SD            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2a           | mm         | rrr        | EVV 0x2A VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM() mode64 W1 ZEROING=0 MASK=0 BCRC=0  ESIZE_64_BITS() NELEM_GPR_READER() FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f64 REG1=XMM_N3():r:dq:f64 MEM0:r:q:i64
 | VCVTSI2SS            | CONVERT        | AVX            |                | 2a           | mm         | rrr        | VV1 0x2A   VF3 V0F not64 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:d:i32
 | VCVTSI2SS            | CONVERT        | AVX            |                | 2a           | 0b11       | rrr        | VV1 0x2A   VF3 V0F not64 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=GPR32_B():r:d:i32
 | VCVTSI2SS            | CONVERT        | AVX            |                | 2a           | mm         | rrr        | VV1 0x2A   VF3 V0F mode64 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                      | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:d:i32
 | VCVTSI2SS            | CONVERT        | AVX            |                | 2a           | 0b11       | rrr        | VV1 0x2A   VF3 V0F mode64 norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                             | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=GPR32_B():r:d:i32
 | VCVTSI2SS            | CONVERT        | AVX            |                | 2a           | mm         | rrr        | VV1 0x2A   VF3 V0F mode64 rexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                        | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:q:i64
 | VCVTSI2SS            | CONVERT        | AVX            |                | 2a           | 0b11       | rrr        | VV1 0x2A   VF3 V0F mode64 rexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                               | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=GPR64_B():r:q:i64
 | VCVTSI2SS            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2a           | 0b11       | rrr        | EVV 0x2A VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  not64    ZEROING=0 MASK=0 FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f32 REG1=XMM_N3():r:dq:f32 REG2=GPR32_B():r:d:i32
 | VCVTSI2SS            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2a           | 0b11       | rrr        | EVV 0x2A VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64 W0    ZEROING=0 MASK=0 FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f32 REG1=XMM_N3():r:dq:f32 REG2=GPR32_B():r:d:i32
 | VCVTSI2SS            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2a           | 0b11       | rrr        | EVV 0x2A VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  not64    ZEROING=0 MASK=0 | REG0=XMM_R3():w:dq:f32:TXT=ROUNDC REG1=XMM_N3():r:dq:f32 REG2=GPR32_B():r:d:i32
 | VCVTSI2SS            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2a           | 0b11       | rrr        | EVV 0x2A VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  mode64 W0    ZEROING=0 MASK=0 | REG0=XMM_R3():w:dq:f32:TXT=ROUNDC REG1=XMM_N3():r:dq:f32 REG2=GPR32_B():r:d:i32
 | VCVTSI2SS            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2a           | mm         | rrr        | EVV 0x2A VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  not64    ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_READER() FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f32 REG1=XMM_N3():r:dq:f32 MEM0:r:d:i32
 | VCVTSI2SS            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2a           | mm         | rrr        | EVV 0x2A VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64 W0    ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_READER() FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f32 REG1=XMM_N3():r:dq:f32 MEM0:r:d:i32
 | VCVTSI2SS            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2a           | 0b11       | rrr        | EVV 0x2A VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64  W1    ZEROING=0 MASK=0  FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f32 REG1=XMM_N3():r:dq:f32 REG2=GPR64_B():r:q:i64
 | VCVTSI2SS            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2a           | 0b11       | rrr        | EVV 0x2A VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  mode64  W1    ZEROING=0 MASK=0 | REG0=XMM_R3():w:dq:f32:TXT=ROUNDC REG1=XMM_N3():r:dq:f32 REG2=GPR64_B():r:q:i64
 | VCVTSI2SS            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2a           | mm         | rrr        | EVV 0x2A VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64  W1    ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_GPR_READER() FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f32 REG1=XMM_N3():r:dq:f32 MEM0:r:q:i64
 | VCVTSS2SD            | CONVERT        | AVX            |                | 5a           | mm         | rrr        | VV1 0x5A  VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                            | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:d:f32
 | VCVTSS2SD            | CONVERT        | AVX            |                | 5a           | 0b11       | rrr        | VV1 0x5A  VF3 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                   | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:d:f32
 | VCVTSS2SD            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 5a           | 0b11       | rrr        | EVV 0x5A VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                                         | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VCVTSS2SD            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 5a           | 0b11       | rrr        | EVV 0x5A VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W0                | REG0=XMM_R3():w:dq:f64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VCVTSS2SD            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 5a           | mm         | rrr        | EVV 0x5A VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
 | VCVTSS2SI            | CONVERT        | AVX            |                | 2d           | mm         | rrr        | VV1 0x2D   VF3 V0F  NOVSR not64 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                              | REG0=GPR32_R():w:d:i32 MEM0:r:d:f32
 | VCVTSS2SI            | CONVERT        | AVX            |                | 2d           | 0b11       | rrr        | VV1 0x2D   VF3 V0F  NOVSR not64 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=GPR32_R():w:d:i32 REG1=XMM_B():r:d:f32
 | VCVTSS2SI            | CONVERT        | AVX            |                | 2d           | mm         | rrr        | VV1 0x2D   VF3 V0F  NOVSR mode64 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()               | REG0=GPR32_R():w:d:i32 MEM0:r:d:f32
 | VCVTSS2SI            | CONVERT        | AVX            |                | 2d           | 0b11       | rrr        | VV1 0x2D   VF3 V0F  NOVSR mode64 norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                      | REG0=GPR32_R():w:d:i32 REG1=XMM_B():r:d:f32
 | VCVTSS2SI            | CONVERT        | AVX            |                | 2d           | mm         | rrr        | VV1 0x2D   VF3 V0F  NOVSR mode64 rexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                 | REG0=GPR64_R():w:q:i64 MEM0:r:d:f32
 | VCVTSS2SI            | CONVERT        | AVX            |                | 2d           | 0b11       | rrr        | VV1 0x2D   VF3 V0F  NOVSR mode64 rexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                        | REG0=GPR64_R():w:q:i64 REG1=XMM_B():r:d:f32
 | VCVTSS2SI            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2d           | 0b11       | rrr        | EVV 0x2D VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  not64  NOEVSR  ZEROING=0 MASK=0 FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:i32 REG1=XMM_B3():r:dq:f32
 | VCVTSS2SI            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2d           | 0b11       | rrr        | EVV 0x2D VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64 W0  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:i32 REG1=XMM_B3():r:dq:f32
 | VCVTSS2SI            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2d           | 0b11       | rrr        | EVV 0x2D VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  not64  NOEVSR  ZEROING=0 MASK=0 | REG0=GPR32_R():w:d:i32:TXT=ROUNDC REG1=XMM_B3():r:dq:f32
 | VCVTSS2SI            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2d           | 0b11       | rrr        | EVV 0x2D VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  mode64 W0  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE | REG0=GPR32_R():w:d:i32:TXT=ROUNDC REG1=XMM_B3():r:dq:f32
 | VCVTSS2SI            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2d           | mm         | rrr        | EVV 0x2D VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  not64  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_WRITER_LDOP_D() FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:i32 MEM0:r:d:f32
 | VCVTSS2SI            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2d           | mm         | rrr        | EVV 0x2D VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64 W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_WRITER_LDOP_D() EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:i32 MEM0:r:d:f32
 | VCVTSS2SI            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2d           | 0b11       | rrr        | EVV 0x2D VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64  W1  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR64_R():w:q:i64 REG1=XMM_B3():r:dq:f32
 | VCVTSS2SI            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2d           | 0b11       | rrr        | EVV 0x2D VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  mode64  W1  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE | REG0=GPR64_R():w:q:i64:TXT=ROUNDC REG1=XMM_B3():r:dq:f32
 | VCVTSS2SI            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2d           | mm         | rrr        | EVV 0x2D VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64  W1  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_WRITER_LDOP_D() EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR64_R():w:q:i64 MEM0:r:d:f32
 | VCVTSS2USI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 79           | 0b11       | rrr        | EVV 0x79 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  not64  NOEVSR  ZEROING=0 MASK=0  FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:u32 REG1=XMM_B3():r:dq:f32
 | VCVTSS2USI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 79           | 0b11       | rrr        | EVV 0x79 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64 W0  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:u32 REG1=XMM_B3():r:dq:f32
 | VCVTSS2USI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 79           | 0b11       | rrr        | EVV 0x79 VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  not64  NOEVSR  ZEROING=0 MASK=0 | REG0=GPR32_R():w:d:u32:TXT=ROUNDC REG1=XMM_B3():r:dq:f32
 | VCVTSS2USI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 79           | 0b11       | rrr        | EVV 0x79 VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  mode64 W0  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE | REG0=GPR32_R():w:d:u32:TXT=ROUNDC REG1=XMM_B3():r:dq:f32
 | VCVTSS2USI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 79           | mm         | rrr        | EVV 0x79 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  not64  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_WRITER_LDOP_D() FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:u32 MEM0:r:d:f32
 | VCVTSS2USI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 79           | mm         | rrr        | EVV 0x79 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64 W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_WRITER_LDOP_D() EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:u32 MEM0:r:d:f32
 | VCVTSS2USI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 79           | 0b11       | rrr        | EVV 0x79 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64  W1  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR64_R():w:q:u64 REG1=XMM_B3():r:dq:f32
 | VCVTSS2USI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 79           | 0b11       | rrr        | EVV 0x79 VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  mode64  W1  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE | REG0=GPR64_R():w:q:u64:TXT=ROUNDC REG1=XMM_B3():r:dq:f32
 | VCVTSS2USI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 79           | mm         | rrr        | EVV 0x79 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64  W1  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_WRITER_LDOP_D() EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR64_R():w:q:u64 MEM0:r:d:f32
 | VCVTTPD2DQ           | CONVERT        | AVX            |                | e6           | mm         | rrr        | VV1 0xE6  VL128 V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:i32 MEM0:r:dq:f64
 | VCVTTPD2DQ           | CONVERT        | AVX            |                | e6           | 0b11       | rrr        | VV1 0xE6  VL128 V66 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:i32 REG1=XMM_B():r:dq:f64
 | VCVTTPD2DQ           | CONVERT        | AVX            |                | e6           | mm         | rrr        | VV1 0xE6  VL256 V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:i32 MEM0:r:qq:f64
 | VCVTTPD2DQ           | CONVERT        | AVX            |                | e6           | 0b11       | rrr        | VV1 0xE6  VL256 V66 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:i32 REG1=YMM_B():r:qq:f64
 | VCVTTPD2DQ           | CONVERT        | AVX512EVEX     | AVX512F_128    | e6           | 0b11       | rrr        | EVV 0xE6 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                          | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64
 | VCVTTPD2DQ           | CONVERT        | AVX512EVEX     | AVX512F_128    | e6           | mm         | rrr        | EVV 0xE6 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VCVTTPD2DQ           | CONVERT        | AVX512EVEX     | AVX512F_256    | e6           | 0b11       | rrr        | EVV 0xE6 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                          | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64
 | VCVTTPD2DQ           | CONVERT        | AVX512EVEX     | AVX512F_256    | e6           | mm         | rrr        | EVV 0xE6 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VCVTTPD2DQ           | CONVERT        | AVX512EVEX     | AVX512F_512    | e6           | 0b11       | rrr        | EVV 0xE6 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                          | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
 | VCVTTPD2DQ           | CONVERT        | AVX512EVEX     | AVX512F_512    | e6           | 0b11       | rrr        | EVV 0xE6 V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W1  NOEVSR        | REG0=YMM_R3():w:qq:i32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
 | VCVTTPD2DQ           | CONVERT        | AVX512EVEX     | AVX512F_512    | e6           | mm         | rrr        | EVV 0xE6 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VCVTTPD2QQ           | CONVERT        | AVX512EVEX     | AVX512DQ_128   | 7a           | 0b11       | rrr        | EVV 0x7A V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                          | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64
 | VCVTTPD2QQ           | CONVERT        | AVX512EVEX     | AVX512DQ_128   | 7a           | mm         | rrr        | EVV 0x7A V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VCVTTPD2QQ           | CONVERT        | AVX512EVEX     | AVX512DQ_256   | 7a           | 0b11       | rrr        | EVV 0x7A V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                          | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64
 | VCVTTPD2QQ           | CONVERT        | AVX512EVEX     | AVX512DQ_256   | 7a           | mm         | rrr        | EVV 0x7A V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VCVTTPD2QQ           | CONVERT        | AVX512EVEX     | AVX512DQ_512   | 7a           | 0b11       | rrr        | EVV 0x7A V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                          | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
 | VCVTTPD2QQ           | CONVERT        | AVX512EVEX     | AVX512DQ_512   | 7a           | 0b11       | rrr        | EVV 0x7A V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W1  NOEVSR        | REG0=ZMM_R3():w:zi64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
 | VCVTTPD2QQ           | CONVERT        | AVX512EVEX     | AVX512DQ_512   | 7a           | mm         | rrr        | EVV 0x7A V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VCVTTPD2UDQ          | CONVERT        | AVX512EVEX     | AVX512F_128    | 78           | 0b11       | rrr        | EVV 0x78 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                          | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64
 | VCVTTPD2UDQ          | CONVERT        | AVX512EVEX     | AVX512F_128    | 78           | mm         | rrr        | EVV 0x78 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VCVTTPD2UDQ          | CONVERT        | AVX512EVEX     | AVX512F_256    | 78           | 0b11       | rrr        | EVV 0x78 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                          | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64
 | VCVTTPD2UDQ          | CONVERT        | AVX512EVEX     | AVX512F_256    | 78           | mm         | rrr        | EVV 0x78 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VCVTTPD2UDQ          | CONVERT        | AVX512EVEX     | AVX512F_512    | 78           | 0b11       | rrr        | EVV 0x78 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                          | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
 | VCVTTPD2UDQ          | CONVERT        | AVX512EVEX     | AVX512F_512    | 78           | 0b11       | rrr        | EVV 0x78 VNP V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W1  NOEVSR        | REG0=YMM_R3():w:qq:u32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
 | VCVTTPD2UDQ          | CONVERT        | AVX512EVEX     | AVX512F_512    | 78           | mm         | rrr        | EVV 0x78 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VCVTTPD2UQQ          | CONVERT        | AVX512EVEX     | AVX512DQ_128   | 78           | 0b11       | rrr        | EVV 0x78 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                          | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64
 | VCVTTPD2UQQ          | CONVERT        | AVX512EVEX     | AVX512DQ_128   | 78           | mm         | rrr        | EVV 0x78 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VCVTTPD2UQQ          | CONVERT        | AVX512EVEX     | AVX512DQ_256   | 78           | 0b11       | rrr        | EVV 0x78 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                          | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64
 | VCVTTPD2UQQ          | CONVERT        | AVX512EVEX     | AVX512DQ_256   | 78           | mm         | rrr        | EVV 0x78 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VCVTTPD2UQQ          | CONVERT        | AVX512EVEX     | AVX512DQ_512   | 78           | 0b11       | rrr        | EVV 0x78 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                          | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
 | VCVTTPD2UQQ          | CONVERT        | AVX512EVEX     | AVX512DQ_512   | 78           | 0b11       | rrr        | EVV 0x78 V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W1  NOEVSR        | REG0=ZMM_R3():w:zu64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
 | VCVTTPD2UQQ          | CONVERT        | AVX512EVEX     | AVX512DQ_512   | 78           | mm         | rrr        | EVV 0x78 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VCVTTPS2DQ           | CONVERT        | AVX            |                | 5b           | mm         | rrr        | VV1 0x5B  VL128 VF3 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:i32 MEM0:r:dq:f32
 | VCVTTPS2DQ           | CONVERT        | AVX            |                | 5b           | 0b11       | rrr        | VV1 0x5B  VL128 VF3 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:i32 REG1=XMM_B():r:dq:f32
 | VCVTTPS2DQ           | CONVERT        | AVX            |                | 5b           | mm         | rrr        | VV1 0x5B  VL256 VF3 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=YMM_R():w:qq:i32 MEM0:r:qq:f32
 | VCVTTPS2DQ           | CONVERT        | AVX            |                | 5b           | 0b11       | rrr        | VV1 0x5B  VL256 VF3 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=YMM_R():w:qq:i32 REG1=YMM_B():r:qq:f32
 | VCVTTPS2DQ           | CONVERT        | AVX512EVEX     | AVX512F_128    | 5b           | 0b11       | rrr        | EVV 0x5B VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                          | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
 | VCVTTPS2DQ           | CONVERT        | AVX512EVEX     | AVX512F_128    | 5b           | mm         | rrr        | EVV 0x5B VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VCVTTPS2DQ           | CONVERT        | AVX512EVEX     | AVX512F_256    | 5b           | 0b11       | rrr        | EVV 0x5B VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                          | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
 | VCVTTPS2DQ           | CONVERT        | AVX512EVEX     | AVX512F_256    | 5b           | mm         | rrr        | EVV 0x5B VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VCVTTPS2DQ           | CONVERT        | AVX512EVEX     | AVX512F_512    | 5b           | 0b11       | rrr        | EVV 0x5B VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                          | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
 | VCVTTPS2DQ           | CONVERT        | AVX512EVEX     | AVX512F_512    | 5b           | 0b11       | rrr        | EVV 0x5B VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0  NOEVSR        | REG0=ZMM_R3():w:zi32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
 | VCVTTPS2DQ           | CONVERT        | AVX512EVEX     | AVX512F_512    | 5b           | mm         | rrr        | EVV 0x5B VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VCVTTPS2QQ           | CONVERT        | AVX512EVEX     | AVX512DQ_128   | 7a           | 0b11       | rrr        | EVV 0x7A V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                          | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
 | VCVTTPS2QQ           | CONVERT        | AVX512EVEX     | AVX512DQ_128   | 7a           | mm         | rrr        | EVV 0x7A V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VCVTTPS2QQ           | CONVERT        | AVX512EVEX     | AVX512DQ_256   | 7a           | 0b11       | rrr        | EVV 0x7A V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                          | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
 | VCVTTPS2QQ           | CONVERT        | AVX512EVEX     | AVX512DQ_256   | 7a           | mm         | rrr        | EVV 0x7A V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VCVTTPS2QQ           | CONVERT        | AVX512EVEX     | AVX512DQ_512   | 7a           | 0b11       | rrr        | EVV 0x7A V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                          | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
 | VCVTTPS2QQ           | CONVERT        | AVX512EVEX     | AVX512DQ_512   | 7a           | 0b11       | rrr        | EVV 0x7A V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0  NOEVSR        | REG0=ZMM_R3():w:zi64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
 | VCVTTPS2QQ           | CONVERT        | AVX512EVEX     | AVX512DQ_512   | 7a           | mm         | rrr        | EVV 0x7A V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VCVTTPS2UDQ          | CONVERT        | AVX512EVEX     | AVX512F_128    | 78           | 0b11       | rrr        | EVV 0x78 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                          | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
 | VCVTTPS2UDQ          | CONVERT        | AVX512EVEX     | AVX512F_128    | 78           | mm         | rrr        | EVV 0x78 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VCVTTPS2UDQ          | CONVERT        | AVX512EVEX     | AVX512F_256    | 78           | 0b11       | rrr        | EVV 0x78 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                          | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
 | VCVTTPS2UDQ          | CONVERT        | AVX512EVEX     | AVX512F_256    | 78           | mm         | rrr        | EVV 0x78 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VCVTTPS2UDQ          | CONVERT        | AVX512EVEX     | AVX512F_512    | 78           | 0b11       | rrr        | EVV 0x78 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                          | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
 | VCVTTPS2UDQ          | CONVERT        | AVX512EVEX     | AVX512F_512    | 78           | 0b11       | rrr        | EVV 0x78 VNP V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0  NOEVSR        | REG0=ZMM_R3():w:zu32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
 | VCVTTPS2UDQ          | CONVERT        | AVX512EVEX     | AVX512F_512    | 78           | mm         | rrr        | EVV 0x78 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VCVTTPS2UQQ          | CONVERT        | AVX512EVEX     | AVX512DQ_128   | 78           | 0b11       | rrr        | EVV 0x78 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                          | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
 | VCVTTPS2UQQ          | CONVERT        | AVX512EVEX     | AVX512DQ_128   | 78           | mm         | rrr        | EVV 0x78 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VCVTTPS2UQQ          | CONVERT        | AVX512EVEX     | AVX512DQ_256   | 78           | 0b11       | rrr        | EVV 0x78 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                          | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
 | VCVTTPS2UQQ          | CONVERT        | AVX512EVEX     | AVX512DQ_256   | 78           | mm         | rrr        | EVV 0x78 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VCVTTPS2UQQ          | CONVERT        | AVX512EVEX     | AVX512DQ_512   | 78           | 0b11       | rrr        | EVV 0x78 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                          | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
 | VCVTTPS2UQQ          | CONVERT        | AVX512EVEX     | AVX512DQ_512   | 78           | 0b11       | rrr        | EVV 0x78 V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0  NOEVSR        | REG0=ZMM_R3():w:zu64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
 | VCVTTPS2UQQ          | CONVERT        | AVX512EVEX     | AVX512DQ_512   | 78           | mm         | rrr        | EVV 0x78 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VCVTTSD2SI           | CONVERT        | AVX            |                | 2c           | mm         | rrr        | VV1 0x2C   VF2 V0F  NOVSR not64 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                              | REG0=GPR32_R():w:d:i32 MEM0:r:q:f64
 | VCVTTSD2SI           | CONVERT        | AVX            |                | 2c           | 0b11       | rrr        | VV1 0x2C   VF2 V0F  NOVSR not64 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=GPR32_R():w:d:i32 REG1=XMM_B():r:q:f64
 | VCVTTSD2SI           | CONVERT        | AVX            |                | 2c           | mm         | rrr        | VV1 0x2C   VF2 V0F  NOVSR mode64 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()               | REG0=GPR32_R():w:d:i32 MEM0:r:q:f64
 | VCVTTSD2SI           | CONVERT        | AVX            |                | 2c           | 0b11       | rrr        | VV1 0x2C   VF2 V0F  NOVSR mode64 norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                      | REG0=GPR32_R():w:d:i32 REG1=XMM_B():r:q:f64
 | VCVTTSD2SI           | CONVERT        | AVX            |                | 2c           | mm         | rrr        | VV1 0x2C   VF2 V0F  NOVSR mode64 rexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                 | REG0=GPR64_R():w:q:i64 MEM0:r:q:f64
 | VCVTTSD2SI           | CONVERT        | AVX            |                | 2c           | 0b11       | rrr        | VV1 0x2C   VF2 V0F  NOVSR mode64 rexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                        | REG0=GPR64_R():w:q:i64 REG1=XMM_B():r:q:f64
 | VCVTTSD2SI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2c           | 0b11       | rrr        | EVV 0x2C VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  not64  NOEVSR  ZEROING=0 MASK=0 FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:i32 REG1=XMM_B3():r:dq:f64
 | VCVTTSD2SI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2c           | 0b11       | rrr        | EVV 0x2C VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64 W0  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:i32 REG1=XMM_B3():r:dq:f64
 | VCVTTSD2SI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2c           | 0b11       | rrr        | EVV 0x2C VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  not64  NOEVSR  ZEROING=0 MASK=0 | REG0=GPR32_R():w:d:i32:TXT=SAESTR REG1=XMM_B3():r:dq:f64
 | VCVTTSD2SI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2c           | 0b11       | rrr        | EVV 0x2C VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  mode64 W0  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE | REG0=GPR32_R():w:d:i32:TXT=SAESTR REG1=XMM_B3():r:dq:f64
 | VCVTTSD2SI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2c           | mm         | rrr        | EVV 0x2C VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  not64  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_GPR_WRITER_LDOP_Q() FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:i32 MEM0:r:q:f64
 | VCVTTSD2SI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2c           | mm         | rrr        | EVV 0x2C VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64 W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_GPR_WRITER_LDOP_Q() EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:i32 MEM0:r:q:f64
 | VCVTTSD2SI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2c           | 0b11       | rrr        | EVV 0x2C VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64  W1  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR64_R():w:q:i64 REG1=XMM_B3():r:dq:f64
 | VCVTTSD2SI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2c           | 0b11       | rrr        | EVV 0x2C VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  mode64  W1  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE | REG0=GPR64_R():w:q:i64:TXT=SAESTR REG1=XMM_B3():r:dq:f64
 | VCVTTSD2SI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2c           | mm         | rrr        | EVV 0x2C VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64  W1  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_GPR_WRITER_LDOP_Q() EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR64_R():w:q:i64 MEM0:r:q:f64
 | VCVTTSD2USI          | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 78           | 0b11       | rrr        | EVV 0x78 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  not64  NOEVSR  ZEROING=0 MASK=0 FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:u32 REG1=XMM_B3():r:dq:f64
 | VCVTTSD2USI          | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 78           | 0b11       | rrr        | EVV 0x78 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64 W0  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:u32 REG1=XMM_B3():r:dq:f64
 | VCVTTSD2USI          | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 78           | 0b11       | rrr        | EVV 0x78 VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  not64  NOEVSR  ZEROING=0 MASK=0 | REG0=GPR32_R():w:d:u32:TXT=SAESTR REG1=XMM_B3():r:dq:f64
 | VCVTTSD2USI          | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 78           | 0b11       | rrr        | EVV 0x78 VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  mode64 W0  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE | REG0=GPR32_R():w:d:u32:TXT=SAESTR REG1=XMM_B3():r:dq:f64
 | VCVTTSD2USI          | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 78           | mm         | rrr        | EVV 0x78 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  not64  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_GPR_WRITER_LDOP_Q() FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:u32 MEM0:r:q:f64
 | VCVTTSD2USI          | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 78           | mm         | rrr        | EVV 0x78 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64 W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_GPR_WRITER_LDOP_Q() EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:u32 MEM0:r:q:f64
 | VCVTTSD2USI          | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 78           | 0b11       | rrr        | EVV 0x78 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64  W1  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR64_R():w:q:u64 REG1=XMM_B3():r:dq:f64
 | VCVTTSD2USI          | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 78           | 0b11       | rrr        | EVV 0x78 VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  mode64  W1  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE | REG0=GPR64_R():w:q:u64:TXT=SAESTR REG1=XMM_B3():r:dq:f64
 | VCVTTSD2USI          | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 78           | mm         | rrr        | EVV 0x78 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64  W1  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_GPR_WRITER_LDOP_Q() EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR64_R():w:q:u64 MEM0:r:q:f64
 | VCVTTSS2SI           | CONVERT        | AVX            |                | 2c           | mm         | rrr        | VV1 0x2C   VF3 V0F  NOVSR not64 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                              | REG0=GPR32_R():w:d:i32 MEM0:r:d:f32
 | VCVTTSS2SI           | CONVERT        | AVX            |                | 2c           | 0b11       | rrr        | VV1 0x2C   VF3 V0F  NOVSR not64 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=GPR32_R():w:d:i32 REG1=XMM_B():r:d:f32
 | VCVTTSS2SI           | CONVERT        | AVX            |                | 2c           | mm         | rrr        | VV1 0x2C   VF3 V0F  NOVSR mode64 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()               | REG0=GPR32_R():w:d:i32 MEM0:r:d:f32
 | VCVTTSS2SI           | CONVERT        | AVX            |                | 2c           | 0b11       | rrr        | VV1 0x2C   VF3 V0F  NOVSR mode64 norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                      | REG0=GPR32_R():w:d:i32 REG1=XMM_B():r:d:f32
 | VCVTTSS2SI           | CONVERT        | AVX            |                | 2c           | mm         | rrr        | VV1 0x2C   VF3 V0F  NOVSR mode64 rexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                 | REG0=GPR64_R():w:q:i64 MEM0:r:d:f32
 | VCVTTSS2SI           | CONVERT        | AVX            |                | 2c           | 0b11       | rrr        | VV1 0x2C   VF3 V0F  NOVSR mode64 rexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                        | REG0=GPR64_R():w:q:i64 REG1=XMM_B():r:d:f32
 | VCVTTSS2SI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2c           | 0b11       | rrr        | EVV 0x2C VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  not64  NOEVSR  ZEROING=0 MASK=0 FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:i32 REG1=XMM_B3():r:dq:f32
 | VCVTTSS2SI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2c           | 0b11       | rrr        | EVV 0x2C VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64 W0  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:i32 REG1=XMM_B3():r:dq:f32
 | VCVTTSS2SI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2c           | 0b11       | rrr        | EVV 0x2C VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  not64  NOEVSR  ZEROING=0 MASK=0 | REG0=GPR32_R():w:d:i32:TXT=SAESTR REG1=XMM_B3():r:dq:f32
 | VCVTTSS2SI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2c           | 0b11       | rrr        | EVV 0x2C VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  mode64 W0  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE | REG0=GPR32_R():w:d:i32:TXT=SAESTR REG1=XMM_B3():r:dq:f32
 | VCVTTSS2SI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2c           | mm         | rrr        | EVV 0x2C VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  not64  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_WRITER_LDOP_D() FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:i32 MEM0:r:d:f32
 | VCVTTSS2SI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2c           | mm         | rrr        | EVV 0x2C VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64 W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_WRITER_LDOP_D() EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:i32 MEM0:r:d:f32
 | VCVTTSS2SI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2c           | 0b11       | rrr        | EVV 0x2C VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64  W1  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR64_R():w:q:i64 REG1=XMM_B3():r:dq:f32
 | VCVTTSS2SI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2c           | 0b11       | rrr        | EVV 0x2C VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  mode64  W1  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE | REG0=GPR64_R():w:q:i64:TXT=SAESTR REG1=XMM_B3():r:dq:f32
 | VCVTTSS2SI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 2c           | mm         | rrr        | EVV 0x2C VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64  W1  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_WRITER_LDOP_D() EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR64_R():w:q:i64 MEM0:r:d:f32
 | VCVTTSS2USI          | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 78           | 0b11       | rrr        | EVV 0x78 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  not64  NOEVSR  ZEROING=0 MASK=0  FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:u32 REG1=XMM_B3():r:dq:f32
 | VCVTTSS2USI          | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 78           | 0b11       | rrr        | EVV 0x78 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64 W0  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:u32 REG1=XMM_B3():r:dq:f32
 | VCVTTSS2USI          | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 78           | 0b11       | rrr        | EVV 0x78 VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  not64  NOEVSR  ZEROING=0 MASK=0 | REG0=GPR32_R():w:d:u32:TXT=SAESTR REG1=XMM_B3():r:dq:f32
 | VCVTTSS2USI          | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 78           | 0b11       | rrr        | EVV 0x78 VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  mode64 W0  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE | REG0=GPR32_R():w:d:u32:TXT=SAESTR REG1=XMM_B3():r:dq:f32
 | VCVTTSS2USI          | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 78           | mm         | rrr        | EVV 0x78 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  not64  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_WRITER_LDOP_D() FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:u32 MEM0:r:d:f32
 | VCVTTSS2USI          | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 78           | mm         | rrr        | EVV 0x78 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64 W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_WRITER_LDOP_D() EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:u32 MEM0:r:d:f32
 | VCVTTSS2USI          | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 78           | 0b11       | rrr        | EVV 0x78 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64  W1  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR64_R():w:q:u64 REG1=XMM_B3():r:dq:f32
 | VCVTTSS2USI          | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 78           | 0b11       | rrr        | EVV 0x78 VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  mode64  W1  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE | REG0=GPR64_R():w:q:u64:TXT=SAESTR REG1=XMM_B3():r:dq:f32
 | VCVTTSS2USI          | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 78           | mm         | rrr        | EVV 0x78 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64  W1  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_WRITER_LDOP_D() EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR64_R():w:q:u64 MEM0:r:d:f32
 | VCVTUDQ2PD           | CONVERT        | AVX512EVEX     | AVX512F_128    | 7a           | 0b11       | rrr        | EVV 0x7A VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                          | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32
 | VCVTUDQ2PD           | CONVERT        | AVX512EVEX     | AVX512F_128    | 7a           | mm         | rrr        | EVV 0x7A VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR
 | VCVTUDQ2PD           | CONVERT        | AVX512EVEX     | AVX512F_256    | 7a           | 0b11       | rrr        | EVV 0x7A VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                          | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32
 | VCVTUDQ2PD           | CONVERT        | AVX512EVEX     | AVX512F_256    | 7a           | mm         | rrr        | EVV 0x7A VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR
 | VCVTUDQ2PD           | CONVERT        | AVX512EVEX     | AVX512F_512    | 7a           | 0b11       | rrr        | EVV 0x7A VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                          | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u32
 | VCVTUDQ2PD           | CONVERT        | AVX512EVEX     | AVX512F_512    | 7a           | mm         | rrr        | EVV 0x7A VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR
 | VCVTUDQ2PD           | CONVERT        | KNCE           |                | 7a           | mm         | rrr        | KVV 0x7A V0F W0 VF3    NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT32_HALF()        | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VCVTUDQ2PD           | CONVERT        | KNCE           |                | 7a           | 0b11       | rrr        | KVV 0x7A V0F W0 VF3   NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                  | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_B3():r:zf64:TXT=REGSWIZ
 | VCVTUDQ2PD           | CONVERT        | KNCE           |                | 7a           | 0b11       | rrr        | KVV 0x7A V0F W0 VF3   NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] NR=1                                  | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_B3():r:zf64
 | VCVTUDQ2PS           | CONVERT        | AVX512EVEX     | AVX512F_128    | 7a           | 0b11       | rrr        | EVV 0x7A VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                          | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32
 | VCVTUDQ2PS           | CONVERT        | AVX512EVEX     | AVX512F_128    | 7a           | mm         | rrr        | EVV 0x7A VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR
 | VCVTUDQ2PS           | CONVERT        | AVX512EVEX     | AVX512F_256    | 7a           | 0b11       | rrr        | EVV 0x7A VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                          | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u32
 | VCVTUDQ2PS           | CONVERT        | AVX512EVEX     | AVX512F_256    | 7a           | mm         | rrr        | EVV 0x7A VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR
 | VCVTUDQ2PS           | CONVERT        | AVX512EVEX     | AVX512F_512    | 7a           | 0b11       | rrr        | EVV 0x7A VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                          | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu32
 | VCVTUDQ2PS           | CONVERT        | AVX512EVEX     | AVX512F_512    | 7a           | 0b11       | rrr        | EVV 0x7A VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0  NOEVSR | REG0=ZMM_R3():w:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu32
 | VCVTUDQ2PS           | CONVERT        | AVX512EVEX     | AVX512F_512    | 7a           | mm         | rrr        | EVV 0x7A VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR
 | VCVTUQQ2PD           | CONVERT        | AVX512EVEX     | AVX512DQ_128   | 7a           | 0b11       | rrr        | EVV 0x7A VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                          | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u64
 | VCVTUQQ2PD           | CONVERT        | AVX512EVEX     | AVX512DQ_128   | 7a           | mm         | rrr        | EVV 0x7A VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR
 | VCVTUQQ2PD           | CONVERT        | AVX512EVEX     | AVX512DQ_256   | 7a           | 0b11       | rrr        | EVV 0x7A VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                          | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u64
 | VCVTUQQ2PD           | CONVERT        | AVX512EVEX     | AVX512DQ_256   | 7a           | mm         | rrr        | EVV 0x7A VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR
 | VCVTUQQ2PD           | CONVERT        | AVX512EVEX     | AVX512DQ_512   | 7a           | 0b11       | rrr        | EVV 0x7A VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                          | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu64
 | VCVTUQQ2PD           | CONVERT        | AVX512EVEX     | AVX512DQ_512   | 7a           | 0b11       | rrr        | EVV 0x7A VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1  NOEVSR | REG0=ZMM_R3():w:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu64
 | VCVTUQQ2PD           | CONVERT        | AVX512EVEX     | AVX512DQ_512   | 7a           | mm         | rrr        | EVV 0x7A VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR
 | VCVTUQQ2PS           | CONVERT        | AVX512EVEX     | AVX512DQ_128   | 7a           | 0b11       | rrr        | EVV 0x7A VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                          | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u64
 | VCVTUQQ2PS           | CONVERT        | AVX512EVEX     | AVX512DQ_128   | 7a           | mm         | rrr        | EVV 0x7A VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR
 | VCVTUQQ2PS           | CONVERT        | AVX512EVEX     | AVX512DQ_256   | 7a           | 0b11       | rrr        | EVV 0x7A VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                          | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u64
 | VCVTUQQ2PS           | CONVERT        | AVX512EVEX     | AVX512DQ_256   | 7a           | mm         | rrr        | EVV 0x7A VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR
 | VCVTUQQ2PS           | CONVERT        | AVX512EVEX     | AVX512DQ_512   | 7a           | 0b11       | rrr        | EVV 0x7A VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                          | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu64
 | VCVTUQQ2PS           | CONVERT        | AVX512EVEX     | AVX512DQ_512   | 7a           | 0b11       | rrr        | EVV 0x7A VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1  NOEVSR | REG0=YMM_R3():w:qq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu64
 | VCVTUQQ2PS           | CONVERT        | AVX512EVEX     | AVX512DQ_512   | 7a           | mm         | rrr        | EVV 0x7A VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR
 | VCVTUSI2SD           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 7b           | 0b11       | rrr        | EVV 0x7B VF2 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] not64 ZEROING=0 MASK=0 FIX_ROUND_LEN128()          | REG0=XMM_R3():w:dq:f64 REG1=XMM_N3():r:dq:f64 REG2=GPR32_B():r:d:u32
 | VCVTUSI2SD           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 7b           | 0b11       | rrr        | EVV 0x7B VF2 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] mode64 W0 ZEROING=0 MASK=0  FIX_ROUND_LEN128()     | REG0=XMM_R3():w:dq:f64 REG1=XMM_N3():r:dq:f64 REG2=GPR32_B():r:d:u32
 | VCVTUSI2SD           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 7b           | mm         | rrr        | EVV 0x7B VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  not64    ZEROING=0 MASK=0 BCRC=0  ESIZE_32_BITS() NELEM_GPR_READER() FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f64 REG1=XMM_N3():r:dq:f64 MEM0:r:d:u32
 | VCVTUSI2SD           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 7b           | mm         | rrr        | EVV 0x7B VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  mode64 W0    ZEROING=0 MASK=0 BCRC=0 ESIZE_32_BITS() NELEM_GPR_READER() FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f64 REG1=XMM_N3():r:dq:f64 MEM0:r:d:u32
 | VCVTUSI2SD           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 7b           | 0b11       | rrr        | EVV 0x7B VF2 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  mode64 W1 ZEROING=0 MASK=0 BCRC=0 FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f64 REG1=XMM_N3():r:dq:f64 REG2=GPR64_B():r:q:u64
 | VCVTUSI2SD           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 7b           | 0b11       | rrr        | EVV 0x7B VF2 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn] mode64 W1 ZEROING=0 MASK=0 BCRC=1 FIX_ROUND_LEN128() AVX512_ROUND() | REG0=XMM_R3():w:dq:f64:TXT=ROUNDC REG1=XMM_N3():r:dq:f64 REG2=GPR64_B():r:q:u64
 | VCVTUSI2SD           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 7b           | mm         | rrr        | EVV 0x7B VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM() mode64 W1 ZEROING=0 MASK=0 BCRC=0  ESIZE_64_BITS() NELEM_GPR_READER() FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f64 REG1=XMM_N3():r:dq:f64 MEM0:r:q:u64
 | VCVTUSI2SS           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 7b           | 0b11       | rrr        | EVV 0x7B VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  not64    ZEROING=0 MASK=0 FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f32 REG1=XMM_N3():r:dq:f32 REG2=GPR32_B():r:d:u32
 | VCVTUSI2SS           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 7b           | 0b11       | rrr        | EVV 0x7B VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64 W0    ZEROING=0 MASK=0  FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f32 REG1=XMM_N3():r:dq:f32 REG2=GPR32_B():r:d:u32
 | VCVTUSI2SS           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 7b           | 0b11       | rrr        | EVV 0x7B VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  not64    ZEROING=0 MASK=0 | REG0=XMM_R3():w:dq:f32:TXT=ROUNDC REG1=XMM_N3():r:dq:f32 REG2=GPR32_B():r:d:u32
 | VCVTUSI2SS           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 7b           | 0b11       | rrr        | EVV 0x7B VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  mode64 W0    ZEROING=0 MASK=0 | REG0=XMM_R3():w:dq:f32:TXT=ROUNDC REG1=XMM_N3():r:dq:f32 REG2=GPR32_B():r:d:u32
 | VCVTUSI2SS           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 7b           | mm         | rrr        | EVV 0x7B VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  not64    ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_READER() FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f32 REG1=XMM_N3():r:dq:f32 MEM0:r:d:u32
 | VCVTUSI2SS           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 7b           | mm         | rrr        | EVV 0x7B VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64 W0    ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_READER() FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f32 REG1=XMM_N3():r:dq:f32 MEM0:r:d:u32
 | VCVTUSI2SS           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 7b           | 0b11       | rrr        | EVV 0x7B VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64  W1    ZEROING=0 MASK=0  FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f32 REG1=XMM_N3():r:dq:f32 REG2=GPR64_B():r:q:u64
 | VCVTUSI2SS           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 7b           | 0b11       | rrr        | EVV 0x7B VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  mode64  W1    ZEROING=0 MASK=0 | REG0=XMM_R3():w:dq:f32:TXT=ROUNDC REG1=XMM_N3():r:dq:f32 REG2=GPR64_B():r:q:u64
 | VCVTUSI2SS           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | 7b           | mm         | rrr        | EVV 0x7B VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64  W1    ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_GPR_READER() FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f32 REG1=XMM_N3():r:dq:f32 MEM0:r:q:u64
 | VDBPSADBW            | AVX512         | AVX512EVEX     | AVX512BW_128   | 42           | 0b11       | rrr        | EVV 0x42 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0   UIMM8()                      | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8 IMM0:r:b
 | VDBPSADBW            | AVX512         | AVX512EVEX     | AVX512BW_128   | 42           | mm         | rrr        | EVV 0x42 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0   UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8 IMM0:r:b
 | VDBPSADBW            | AVX512         | AVX512EVEX     | AVX512BW_256   | 42           | 0b11       | rrr        | EVV 0x42 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0   UIMM8()                      | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8 IMM0:r:b
 | VDBPSADBW            | AVX512         | AVX512EVEX     | AVX512BW_256   | 42           | mm         | rrr        | EVV 0x42 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0   UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8 IMM0:r:b
 | VDBPSADBW            | AVX512         | AVX512EVEX     | AVX512BW_512   | 42           | 0b11       | rrr        | EVV 0x42 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0   UIMM8()                      | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8 IMM0:r:b
 | VDBPSADBW            | AVX512         | AVX512EVEX     | AVX512BW_512   | 42           | mm         | rrr        | EVV 0x42 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0   UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8 IMM0:r:b
 | VDIVPD               | AVX            | AVX            |                | 5e           | mm         | rrr        | VV1 0x5E  V66 V0F VL128 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
 | VDIVPD               | AVX            | AVX            |                | 5e           | 0b11       | rrr        | VV1 0x5E  V66 V0F VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
 | VDIVPD               | AVX            | AVX            |                | 5e           | mm         | rrr        | VV1 0x5E  V66 V0F VL256 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
 | VDIVPD               | AVX            | AVX            |                | 5e           | 0b11       | rrr        | VV1 0x5E  V66 V0F VL256 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
 | VDIVPD               | AVX512         | AVX512EVEX     | AVX512F_128    | 5e           | 0b11       | rrr        | EVV 0x5E V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                  | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VDIVPD               | AVX512         | AVX512EVEX     | AVX512F_128    | 5e           | mm         | rrr        | EVV 0x5E V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VDIVPD               | AVX512         | AVX512EVEX     | AVX512F_256    | 5e           | 0b11       | rrr        | EVV 0x5E V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                  | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
 | VDIVPD               | AVX512         | AVX512EVEX     | AVX512F_256    | 5e           | mm         | rrr        | EVV 0x5E V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VDIVPD               | AVX512         | AVX512EVEX     | AVX512F_512    | 5e           | 0b11       | rrr        | EVV 0x5E V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                  | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VDIVPD               | AVX512         | AVX512EVEX     | AVX512F_512    | 5e           | 0b11       | rrr        | EVV 0x5E V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1       | REG0=ZMM_R3():w:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VDIVPD               | AVX512         | AVX512EVEX     | AVX512F_512    | 5e           | mm         | rrr        | EVV 0x5E V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VDIVPS               | AVX            | AVX            |                | 5e           | mm         | rrr        | VV1 0x5E  VNP V0F VL128 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
 | VDIVPS               | AVX            | AVX            |                | 5e           | 0b11       | rrr        | VV1 0x5E  VNP V0F VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
 | VDIVPS               | AVX            | AVX            |                | 5e           | mm         | rrr        | VV1 0x5E  VNP V0F VL256 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
 | VDIVPS               | AVX            | AVX            |                | 5e           | 0b11       | rrr        | VV1 0x5E  VNP V0F VL256 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
 | VDIVPS               | AVX512         | AVX512EVEX     | AVX512F_128    | 5e           | 0b11       | rrr        | EVV 0x5E VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                  | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VDIVPS               | AVX512         | AVX512EVEX     | AVX512F_128    | 5e           | mm         | rrr        | EVV 0x5E VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VDIVPS               | AVX512         | AVX512EVEX     | AVX512F_256    | 5e           | 0b11       | rrr        | EVV 0x5E VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                  | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
 | VDIVPS               | AVX512         | AVX512EVEX     | AVX512F_256    | 5e           | mm         | rrr        | EVV 0x5E VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VDIVPS               | AVX512         | AVX512EVEX     | AVX512F_512    | 5e           | 0b11       | rrr        | EVV 0x5E VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                  | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VDIVPS               | AVX512         | AVX512EVEX     | AVX512F_512    | 5e           | 0b11       | rrr        | EVV 0x5E VNP V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0       | REG0=ZMM_R3():w:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VDIVPS               | AVX512         | AVX512EVEX     | AVX512F_512    | 5e           | mm         | rrr        | EVV 0x5E VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VDIVSD               | AVX            | AVX            |                | 5e           | mm         | rrr        | VV1 0x5E  VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                            | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:q:f64
 | VDIVSD               | AVX            | AVX            |                | 5e           | 0b11       | rrr        | VV1 0x5E  VF2 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                   | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:q:f64
 | VDIVSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 5e           | 0b11       | rrr        | EVV 0x5E VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                                         | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VDIVSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 5e           | 0b11       | rrr        | EVV 0x5E VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1       | REG0=XMM_R3():w:dq:f64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VDIVSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 5e           | mm         | rrr        | EVV 0x5E VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
 | VDIVSS               | AVX            | AVX            |                | 5e           | mm         | rrr        | VV1 0x5E  VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                            | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:d:f32
 | VDIVSS               | AVX            | AVX            |                | 5e           | 0b11       | rrr        | VV1 0x5E  VF3 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                   | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:d:f32
 | VDIVSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 5e           | 0b11       | rrr        | EVV 0x5E VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                                         | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VDIVSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 5e           | 0b11       | rrr        | EVV 0x5E VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W0       | REG0=XMM_R3():w:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VDIVSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 5e           | mm         | rrr        | EVV 0x5E VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
 | VDPBF16PS            | AVX512         | AVX512EVEX     | AVX512_BF16_128 | 52           | 0b11       | rrr        | EVV 0x52 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
 | VDPBF16PS            | AVX512         | AVX512EVEX     | AVX512_BF16_128 | 52           | mm         | rrr        | EVV 0x52 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VDPBF16PS            | AVX512         | AVX512EVEX     | AVX512_BF16_256 | 52           | 0b11       | rrr        | EVV 0x52 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
 | VDPBF16PS            | AVX512         | AVX512EVEX     | AVX512_BF16_256 | 52           | mm         | rrr        | EVV 0x52 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VDPBF16PS            | AVX512         | AVX512EVEX     | AVX512_BF16_512 | 52           | 0b11       | rrr        | EVV 0x52 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
 | VDPBF16PS            | AVX512         | AVX512EVEX     | AVX512_BF16_512 | 52           | mm         | rrr        | EVV 0x52 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VDPPD                | AVX            | AVX            |                | 41           | mm         | rrr        | VV1 0x41  VL128 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                            | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64 IMM0:r:b
 | VDPPD                | AVX            | AVX            |                | 41           | 0b11       | rrr        | VV1 0x41  VL128 V66 V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                   | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64 IMM0:r:b
 | VDPPS                | AVX            | AVX            |                | 40           | mm         | rrr        | VV1 0x40  VL128 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                            | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32 IMM0:r:b
 | VDPPS                | AVX            | AVX            |                | 40           | 0b11       | rrr        | VV1 0x40  VL128 V66 V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                   | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32 IMM0:r:b
 | VDPPS                | AVX            | AVX            |                | 40           | mm         | rrr        | VV1 0x40  VL256 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                            | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32 IMM0:r:b
 | VDPPS                | AVX            | AVX            |                | 40           | 0b11       | rrr        | VV1 0x40  VL256 V66 V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                   | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32 IMM0:r:b
 | VERR                 | SYSTEM         | BASE           | I286PROTECTED  | 0f 00        | mm         | 0b100      | 0x0F 0x00 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM()                                                  | MEM0:r:w
 | VERR                 | SYSTEM         | BASE           | I286PROTECTED  | 0f 00        | 0b11       | 0b100      | 0x0F 0x00 MOD[0b11] MOD=3 REG[0b100] RM[nnn]                                                         | REG0=GPR16_B():r
 | VERW                 | SYSTEM         | BASE           | I286PROTECTED  | 0f 00        | mm         | 0b101      | 0x0F 0x00 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM()                                                  | MEM0:r:w
 | VERW                 | SYSTEM         | BASE           | I286PROTECTED  | 0f 00        | 0b11       | 0b101      | 0x0F 0x00 MOD[0b11] MOD=3 REG[0b101] RM[nnn]                                                         | REG0=GPR16_B():r
 | VEXP223PS            | KNC            | KNCE           | KNCE           | c8           | mm         | rrr        | KVV 0xC8 V0F38 V66  W0  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() NOSWIZF32()                  | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  MEM0:r:zd:TXT=NT
 | VEXP223PS            | KNC            | KNCE           | KNCE           | c8           | 0b11       | rrr        | KVV 0xC8 V0F38 V66  W0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] NR=1 KNC_SAE()                      | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw REG2=ZMM_B3():r:zd:TXT=SAEC
 | VEXP223PS            | KNC            | KNCE           | KNCE           | c8           | 0b11       | rrr        | KVV 0xC8 V0F38 V66  W0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] NR=0 SWIZ=0                         | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw REG2=ZMM_B3():r:zd
 | VEXP2PD              | AVX512         | AVX512EVEX     | AVX512ER_512   | c8           | 0b11       | rrr        | EVV 0xC8 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                        | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
 | VEXP2PD              | AVX512         | AVX512EVEX     | AVX512ER_512   | c8           | 0b11       | rrr        | EVV 0xC8 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W1  NOEVSR      | REG0=ZMM_R3():w:zf64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
 | VEXP2PD              | AVX512         | AVX512EVEX     | AVX512ER_512   | c8           | mm         | rrr        | EVV 0xC8 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VEXP2PS              | AVX512         | AVX512EVEX     | AVX512ER_512   | c8           | 0b11       | rrr        | EVV 0xC8 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
 | VEXP2PS              | AVX512         | AVX512EVEX     | AVX512ER_512   | c8           | 0b11       | rrr        | EVV 0xC8 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0  NOEVSR      | REG0=ZMM_R3():w:zf32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
 | VEXP2PS              | AVX512         | AVX512EVEX     | AVX512ER_512   | c8           | mm         | rrr        | EVV 0xC8 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VEXPANDPD            | EXPAND         | AVX512EVEX     | AVX512F_128    | 88           | mm         | rrr        | EVV 0x88 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_GSCAT() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:f64
 | VEXPANDPD            | EXPAND         | AVX512EVEX     | AVX512F_128    | 88           | 0b11       | rrr        | EVV 0x88 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                        | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64
 | VEXPANDPD            | EXPAND         | AVX512EVEX     | AVX512F_256    | 88           | mm         | rrr        | EVV 0x88 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_GSCAT() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:f64
 | VEXPANDPD            | EXPAND         | AVX512EVEX     | AVX512F_256    | 88           | 0b11       | rrr        | EVV 0x88 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                        | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64
 | VEXPANDPD            | EXPAND         | AVX512EVEX     | AVX512F_512    | 88           | mm         | rrr        | EVV 0x88 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_GSCAT() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:f64
 | VEXPANDPD            | EXPAND         | AVX512EVEX     | AVX512F_512    | 88           | 0b11       | rrr        | EVV 0x88 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                        | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
 | VEXPANDPS            | EXPAND         | AVX512EVEX     | AVX512F_128    | 88           | mm         | rrr        | EVV 0x88 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_GSCAT() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:f32
 | VEXPANDPS            | EXPAND         | AVX512EVEX     | AVX512F_128    | 88           | 0b11       | rrr        | EVV 0x88 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
 | VEXPANDPS            | EXPAND         | AVX512EVEX     | AVX512F_256    | 88           | mm         | rrr        | EVV 0x88 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_GSCAT() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:f32
 | VEXPANDPS            | EXPAND         | AVX512EVEX     | AVX512F_256    | 88           | 0b11       | rrr        | EVV 0x88 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
 | VEXPANDPS            | EXPAND         | AVX512EVEX     | AVX512F_512    | 88           | mm         | rrr        | EVV 0x88 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_GSCAT() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:f32
 | VEXPANDPS            | EXPAND         | AVX512EVEX     | AVX512F_512    | 88           | 0b11       | rrr        | EVV 0x88 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
 | VEXTRACTF128         | AVX            | AVX            |                | 19           | mm         | rrr        | VV1 0x19  norexw_prefix VL256 V66 V0F3A NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()        | MEM0:w:dq:f64 REG0=YMM_R():r:dq:f64  IMM0:r:b
 | VEXTRACTF128         | AVX            | AVX            |                | 19           | 0b11       | rrr        | VV1 0x19  norexw_prefix VL256 V66 V0F3A NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()               | REG0=XMM_B():w:dq:f64 REG1=YMM_R():r:dq:f64 IMM0:r:b
 | VEXTRACTF32X4        | AVX512         | AVX512EVEX     | AVX512F_256    | 19           | 0b11       | rrr        | EVV 0x19 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR UIMM8()                | REG0=XMM_B3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:f32 IMM0:r:b
 | VEXTRACTF32X4        | AVX512         | AVX512EVEX     | AVX512F_256    | 19           | mm         | rrr        | EVV 0x19 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_TUPLE4() | MEM0:w:dq:f32 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:f32 IMM0:r:b
 | VEXTRACTF32X4        | AVX512         | AVX512EVEX     | AVX512F_512    | 19           | 0b11       | rrr        | EVV 0x19 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR UIMM8()                | REG0=XMM_B3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zf32 IMM0:r:b
 | VEXTRACTF32X4        | AVX512         | AVX512EVEX     | AVX512F_512    | 19           | mm         | rrr        | EVV 0x19 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_TUPLE4() | MEM0:w:dq:f32 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zf32 IMM0:r:b
 | VEXTRACTF32X8        | AVX512         | AVX512EVEX     | AVX512DQ_512   | 1b           | 0b11       | rrr        | EVV 0x1B V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR UIMM8()                | REG0=YMM_B3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zf32 IMM0:r:b
 | VEXTRACTF32X8        | AVX512         | AVX512EVEX     | AVX512DQ_512   | 1b           | mm         | rrr        | EVV 0x1B V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_TUPLE8() | MEM0:w:qq:f32 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zf32 IMM0:r:b
 | VEXTRACTF64X2        | AVX512         | AVX512EVEX     | AVX512DQ_256   | 19           | 0b11       | rrr        | EVV 0x19 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR UIMM8()                | REG0=XMM_B3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:f64 IMM0:r:b
 | VEXTRACTF64X2        | AVX512         | AVX512EVEX     | AVX512DQ_256   | 19           | mm         | rrr        | EVV 0x19 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_TUPLE2() | MEM0:w:dq:f64 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:f64 IMM0:r:b
 | VEXTRACTF64X2        | AVX512         | AVX512EVEX     | AVX512DQ_512   | 19           | 0b11       | rrr        | EVV 0x19 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR UIMM8()                | REG0=XMM_B3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zf64 IMM0:r:b
 | VEXTRACTF64X2        | AVX512         | AVX512EVEX     | AVX512DQ_512   | 19           | mm         | rrr        | EVV 0x19 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_TUPLE2() | MEM0:w:dq:f64 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zf64 IMM0:r:b
 | VEXTRACTF64X4        | AVX512         | AVX512EVEX     | AVX512F_512    | 1b           | 0b11       | rrr        | EVV 0x1B V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR UIMM8()                | REG0=YMM_B3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zf64 IMM0:r:b
 | VEXTRACTF64X4        | AVX512         | AVX512EVEX     | AVX512F_512    | 1b           | mm         | rrr        | EVV 0x1B V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_TUPLE4() | MEM0:w:qq:f64 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zf64 IMM0:r:b
 | VEXTRACTI128         | AVX2           | AVX2           |                | 39           | mm         | rrr        | VV1 0x39  VL256 V66 V0F3A W0  NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                  | MEM0:w:dq:u128 REG0=YMM_R():r:qq:u128  IMM0:r:b
 | VEXTRACTI128         | AVX2           | AVX2           |                | 39           | 0b11       | rrr        | VV1 0x39  VL256 V66 V0F3A W0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                          | REG0=XMM_B():w:dq:u128 REG1=YMM_R():r:qq:u128  IMM0:r:b
 | VEXTRACTI32X4        | AVX512         | AVX512EVEX     | AVX512F_256    | 39           | 0b11       | rrr        | EVV 0x39 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR UIMM8()                | REG0=XMM_B3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u32 IMM0:r:b
 | VEXTRACTI32X4        | AVX512         | AVX512EVEX     | AVX512F_256    | 39           | mm         | rrr        | EVV 0x39 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_TUPLE4() | MEM0:w:dq:u32 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u32 IMM0:r:b
 | VEXTRACTI32X4        | AVX512         | AVX512EVEX     | AVX512F_512    | 39           | 0b11       | rrr        | EVV 0x39 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR UIMM8()                | REG0=XMM_B3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu32 IMM0:r:b
 | VEXTRACTI32X4        | AVX512         | AVX512EVEX     | AVX512F_512    | 39           | mm         | rrr        | EVV 0x39 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_TUPLE4() | MEM0:w:dq:u32 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu32 IMM0:r:b
 | VEXTRACTI32X8        | AVX512         | AVX512EVEX     | AVX512DQ_512   | 3b           | 0b11       | rrr        | EVV 0x3B V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR UIMM8()                | REG0=YMM_B3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu32 IMM0:r:b
 | VEXTRACTI32X8        | AVX512         | AVX512EVEX     | AVX512DQ_512   | 3b           | mm         | rrr        | EVV 0x3B V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_TUPLE8() | MEM0:w:qq:u32 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu32 IMM0:r:b
 | VEXTRACTI64X2        | AVX512         | AVX512EVEX     | AVX512DQ_256   | 39           | 0b11       | rrr        | EVV 0x39 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR UIMM8()                | REG0=XMM_B3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u64 IMM0:r:b
 | VEXTRACTI64X2        | AVX512         | AVX512EVEX     | AVX512DQ_256   | 39           | mm         | rrr        | EVV 0x39 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_TUPLE2() | MEM0:w:dq:u64 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u64 IMM0:r:b
 | VEXTRACTI64X2        | AVX512         | AVX512EVEX     | AVX512DQ_512   | 39           | 0b11       | rrr        | EVV 0x39 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR UIMM8()                | REG0=XMM_B3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu64 IMM0:r:b
 | VEXTRACTI64X2        | AVX512         | AVX512EVEX     | AVX512DQ_512   | 39           | mm         | rrr        | EVV 0x39 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_TUPLE2() | MEM0:w:dq:u64 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu64 IMM0:r:b
 | VEXTRACTI64X4        | AVX512         | AVX512EVEX     | AVX512F_512    | 3b           | 0b11       | rrr        | EVV 0x3B V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR UIMM8()                | REG0=YMM_B3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu64 IMM0:r:b
 | VEXTRACTI64X4        | AVX512         | AVX512EVEX     | AVX512F_512    | 3b           | mm         | rrr        | EVV 0x3B V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_TUPLE4() | MEM0:w:qq:u64 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu64 IMM0:r:b
 | VEXTRACTPS           | AVX            | AVX            |                | 17           | mm         | rrr        | VV1 0x17  VL128 V66 V0F3A  NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                     | MEM0:w:d:f32  REG0=XMM_R():r:dq:f32  IMM0:r:b
 | VEXTRACTPS           | AVX            | AVX            |                | 17           | 0b11       | rrr        | VV1 0x17  VL128 V66 V0F3A  NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                            | REG0=GPR32_B():w  REG1=XMM_R():r:dq:f32  IMM0:r:b
 | VEXTRACTPS           | AVX512         | AVX512EVEX     | AVX512F_128N   | 17           | 0b11       | rrr        | EVV 0x17 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR  ZEROING=0 MASK=0 UIMM8() | REG0=GPR32_B():w:d:f32 REG1=XMM_R3():r:dq:f32 IMM0:r:b
 | VEXTRACTPS           | AVX512         | AVX512EVEX     | AVX512F_128N   | 17           | mm         | rrr        | EVV 0x17 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128    NOEVSR  ZEROING=0 MASK=0 UIMM8()  ESIZE_32_BITS() NELEM_GPR_WRITER_STORE() | MEM0:w:d:f32 REG0=XMM_R3():r:dq:f32 IMM0:r:b
 | VFIXUPIMMPD          | AVX512         | AVX512EVEX     | AVX512F_128    | 54           | 0b11       | rrr        | EVV 0x54 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1   UIMM8()                      | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64 IMM0:r:b
 | VFIXUPIMMPD          | AVX512         | AVX512EVEX     | AVX512F_128    | 54           | mm         | rrr        | EVV 0x54 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
 | VFIXUPIMMPD          | AVX512         | AVX512EVEX     | AVX512F_256    | 54           | 0b11       | rrr        | EVV 0x54 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1   UIMM8()                      | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64 IMM0:r:b
 | VFIXUPIMMPD          | AVX512         | AVX512EVEX     | AVX512F_256    | 54           | mm         | rrr        | EVV 0x54 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
 | VFIXUPIMMPD          | AVX512         | AVX512EVEX     | AVX512F_512    | 54           | 0b11       | rrr        | EVV 0x54 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1   UIMM8()                      | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64 IMM0:r:b
 | VFIXUPIMMPD          | AVX512         | AVX512EVEX     | AVX512F_512    | 54           | 0b11       | rrr        | EVV 0x54 V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W1   UIMM8()    | REG0=ZMM_R3():rw:zf64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64 IMM0:r:b
 | VFIXUPIMMPD          | AVX512         | AVX512EVEX     | AVX512F_512    | 54           | mm         | rrr        | EVV 0x54 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
 | VFIXUPIMMPS          | AVX512         | AVX512EVEX     | AVX512F_128    | 54           | 0b11       | rrr        | EVV 0x54 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0   UIMM8()                      | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32 IMM0:r:b
 | VFIXUPIMMPS          | AVX512         | AVX512EVEX     | AVX512F_128    | 54           | mm         | rrr        | EVV 0x54 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
 | VFIXUPIMMPS          | AVX512         | AVX512EVEX     | AVX512F_256    | 54           | 0b11       | rrr        | EVV 0x54 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0   UIMM8()                      | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32 IMM0:r:b
 | VFIXUPIMMPS          | AVX512         | AVX512EVEX     | AVX512F_256    | 54           | mm         | rrr        | EVV 0x54 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
 | VFIXUPIMMPS          | AVX512         | AVX512EVEX     | AVX512F_512    | 54           | 0b11       | rrr        | EVV 0x54 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0   UIMM8()                      | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32 IMM0:r:b
 | VFIXUPIMMPS          | AVX512         | AVX512EVEX     | AVX512F_512    | 54           | 0b11       | rrr        | EVV 0x54 V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0   UIMM8()    | REG0=ZMM_R3():rw:zf32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32 IMM0:r:b
 | VFIXUPIMMPS          | AVX512         | AVX512EVEX     | AVX512F_512    | 54           | mm         | rrr        | EVV 0x54 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
 | VFIXUPIMMSD          | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 55           | 0b11       | rrr        | EVV 0x55 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1   UIMM8()                             | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64 IMM0:r:b
 | VFIXUPIMMSD          | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 55           | 0b11       | rrr        | EVV 0x55 V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W1   UIMM8()    | REG0=XMM_R3():rw:dq:f64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64 IMM0:r:b
 | VFIXUPIMMSD          | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 55           | mm         | rrr        | EVV 0x55 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1   UIMM8()  ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64 IMM0:r:b
 | VFIXUPIMMSS          | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 55           | 0b11       | rrr        | EVV 0x55 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0   UIMM8()                             | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32 IMM0:r:b
 | VFIXUPIMMSS          | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 55           | 0b11       | rrr        | EVV 0x55 V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W0   UIMM8()    | REG0=XMM_R3():rw:dq:f32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32 IMM0:r:b
 | VFIXUPIMMSS          | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 55           | mm         | rrr        | EVV 0x55 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0   UIMM8()  ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32 IMM0:r:b
 | VFIXUPNANPD          | KNC            | KNCE           |                | 55           | mm         | rrr        | KVV 0x55 V0F38 REXW=1 V66  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT64()                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VFIXUPNANPD          | KNC            | KNCE           |                | 55           | 0b11       | rrr        | KVV 0x55 V0F38 REXW=1 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE64()                     | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=REGSWIZ
 | VFIXUPNANPD          | KNC            | KNCE           |                | 55           | 0b11       | rrr        | KVV 0x55 V0F38 REXW=1 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 KNC_SAE()                           | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=SAEC
 | VFIXUPNANPS          | KNC            | KNCE           |                | 55           | mm         | rrr        | KVV 0x55 V0F38 REXW=0 V66  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT32()                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VFIXUPNANPS          | KNC            | KNCE           |                | 55           | 0b11       | rrr        | KVV 0x55 V0F38 REXW=0 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VFIXUPNANPS          | KNC            | KNCE           |                | 55           | 0b11       | rrr        | KVV 0x55 V0F38 REXW=0 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 KNC_SAE()                           | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=SAEC
 | VFMADD132PD          | UFMA           | KNCE           |                | 98           | mm         | rrr        | KVV 0x98 V0F38 V66  REXW=1 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT64()                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VFMADD132PD          | UFMA           | KNCE           |                | 98           | 0b11       | rrr        | KVV 0x98 V0F38 V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE64()                     | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=REGSWIZ
 | VFMADD132PD          | UFMA           | KNCE           |                | 98           | 0b11       | rrr        | KVV 0x98 V0F38 V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                   | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=ROUNDC:TXT=SAEC
 | VFMADD132PD          | VFMA           | AVX512EVEX     | AVX512F_128    | 98           | 0b11       | rrr        | EVV 0x98 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFMADD132PD          | VFMA           | AVX512EVEX     | AVX512F_128    | 98           | mm         | rrr        | EVV 0x98 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFMADD132PD          | VFMA           | AVX512EVEX     | AVX512F_256    | 98           | 0b11       | rrr        | EVV 0x98 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
 | VFMADD132PD          | VFMA           | AVX512EVEX     | AVX512F_256    | 98           | mm         | rrr        | EVV 0x98 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFMADD132PD          | VFMA           | AVX512EVEX     | AVX512F_512    | 98           | 0b11       | rrr        | EVV 0x98 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VFMADD132PD          | VFMA           | AVX512EVEX     | AVX512F_512    | 98           | 0b11       | rrr        | EVV 0x98 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1     | REG0=ZMM_R3():rw:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VFMADD132PD          | VFMA           | AVX512EVEX     | AVX512F_512    | 98           | mm         | rrr        | EVV 0x98 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFMADD132PD          | VFMA           | FMA            |                | 98           | mm         | rrr        | VV1 0x98 VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
 | VFMADD132PD          | VFMA           | FMA            |                | 98           | 0b11       | rrr        | VV1 0x98 VL128 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
 | VFMADD132PD          | VFMA           | FMA            |                | 98           | mm         | rrr        | VV1 0x98 VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
 | VFMADD132PD          | VFMA           | FMA            |                | 98           | 0b11       | rrr        | VV1 0x98 VL256 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
 | VFMADD132PS          | UFMA           | KNCE           |                | 98           | mm         | rrr        | KVV 0x98 V0F38 V66  REXW=0 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT32()                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VFMADD132PS          | UFMA           | KNCE           |                | 98           | 0b11       | rrr        | KVV 0x98 V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VFMADD132PS          | UFMA           | KNCE           |                | 98           | 0b11       | rrr        | KVV 0x98 V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                   | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=ROUNDC:TXT=SAEC
 | VFMADD132PS          | VFMA           | AVX512EVEX     | AVX512F_128    | 98           | 0b11       | rrr        | EVV 0x98 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFMADD132PS          | VFMA           | AVX512EVEX     | AVX512F_128    | 98           | mm         | rrr        | EVV 0x98 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFMADD132PS          | VFMA           | AVX512EVEX     | AVX512F_256    | 98           | 0b11       | rrr        | EVV 0x98 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
 | VFMADD132PS          | VFMA           | AVX512EVEX     | AVX512F_256    | 98           | mm         | rrr        | EVV 0x98 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFMADD132PS          | VFMA           | AVX512EVEX     | AVX512F_512    | 98           | 0b11       | rrr        | EVV 0x98 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VFMADD132PS          | VFMA           | AVX512EVEX     | AVX512F_512    | 98           | 0b11       | rrr        | EVV 0x98 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0     | REG0=ZMM_R3():rw:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VFMADD132PS          | VFMA           | AVX512EVEX     | AVX512F_512    | 98           | mm         | rrr        | EVV 0x98 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFMADD132PS          | VFMA           | FMA            |                | 98           | mm         | rrr        | VV1 0x98 VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
 | VFMADD132PS          | VFMA           | FMA            |                | 98           | 0b11       | rrr        | VV1 0x98 VL128 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
 | VFMADD132PS          | VFMA           | FMA            |                | 98           | mm         | rrr        | VV1 0x98 VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
 | VFMADD132PS          | VFMA           | FMA            |                | 98           | 0b11       | rrr        | VV1 0x98 VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
 | VFMADD132SD          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | 99           | 0b11       | rrr        | EVV 0x99 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                                       | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFMADD132SD          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | 99           | 0b11       | rrr        | EVV 0x99 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1     | REG0=XMM_R3():rw:dq:f64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFMADD132SD          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | 99           | mm         | rrr        | EVV 0x99 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
 | VFMADD132SD          | VFMA           | FMA            |                | 99           | mm         | rrr        | VV1 0x99 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                        | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 MEM0:r:q:f64
 | VFMADD132SD          | VFMA           | FMA            |                | 99           | 0b11       | rrr        | VV1 0x99 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                               | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_B():r:q:f64
 | VFMADD132SS          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | 99           | 0b11       | rrr        | EVV 0x99 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                                       | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFMADD132SS          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | 99           | 0b11       | rrr        | EVV 0x99 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W0     | REG0=XMM_R3():rw:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFMADD132SS          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | 99           | mm         | rrr        | EVV 0x99 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
 | VFMADD132SS          | VFMA           | FMA            |                | 99           | mm         | rrr        | VV1 0x99  V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                       | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 MEM0:r:d:f32
 | VFMADD132SS          | VFMA           | FMA            |                | 99           | 0b11       | rrr        | VV1 0x99  V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                              | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_B():r:d:f32
 | VFMADD213PD          | UFMA           | KNCE           |                | a8           | mm         | rrr        | KVV 0xA8 V0F38 V66  REXW=1 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT64()                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VFMADD213PD          | UFMA           | KNCE           |                | a8           | 0b11       | rrr        | KVV 0xA8 V0F38 V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE64()                     | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=REGSWIZ
 | VFMADD213PD          | UFMA           | KNCE           |                | a8           | 0b11       | rrr        | KVV 0xA8 V0F38 V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                   | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=ROUNDC:TXT=SAEC
 | VFMADD213PD          | VFMA           | AVX512EVEX     | AVX512F_128    | a8           | 0b11       | rrr        | EVV 0xA8 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFMADD213PD          | VFMA           | AVX512EVEX     | AVX512F_128    | a8           | mm         | rrr        | EVV 0xA8 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFMADD213PD          | VFMA           | AVX512EVEX     | AVX512F_256    | a8           | 0b11       | rrr        | EVV 0xA8 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
 | VFMADD213PD          | VFMA           | AVX512EVEX     | AVX512F_256    | a8           | mm         | rrr        | EVV 0xA8 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFMADD213PD          | VFMA           | AVX512EVEX     | AVX512F_512    | a8           | 0b11       | rrr        | EVV 0xA8 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VFMADD213PD          | VFMA           | AVX512EVEX     | AVX512F_512    | a8           | 0b11       | rrr        | EVV 0xA8 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1     | REG0=ZMM_R3():rw:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VFMADD213PD          | VFMA           | AVX512EVEX     | AVX512F_512    | a8           | mm         | rrr        | EVV 0xA8 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFMADD213PD          | VFMA           | FMA            |                | a8           | mm         | rrr        | VV1 0xA8 VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64    MEM0:r:dq:f64
 | VFMADD213PD          | VFMA           | FMA            |                | a8           | 0b11       | rrr        | VV1 0xA8 VL128 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
 | VFMADD213PD          | VFMA           | FMA            |                | a8           | mm         | rrr        | VV1 0xA8 VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64    MEM0:r:qq:f64
 | VFMADD213PD          | VFMA           | FMA            |                | a8           | 0b11       | rrr        | VV1 0xA8 VL256 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
 | VFMADD213PS          | UFMA           | KNCE           |                | a8           | mm         | rrr        | KVV 0xA8 V0F38 V66  REXW=0 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT32()                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VFMADD213PS          | UFMA           | KNCE           |                | a8           | 0b11       | rrr        | KVV 0xA8 V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VFMADD213PS          | UFMA           | KNCE           |                | a8           | 0b11       | rrr        | KVV 0xA8 V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                   | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=ROUNDC:TXT=SAEC
 | VFMADD213PS          | VFMA           | AVX512EVEX     | AVX512F_128    | a8           | 0b11       | rrr        | EVV 0xA8 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFMADD213PS          | VFMA           | AVX512EVEX     | AVX512F_128    | a8           | mm         | rrr        | EVV 0xA8 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFMADD213PS          | VFMA           | AVX512EVEX     | AVX512F_256    | a8           | 0b11       | rrr        | EVV 0xA8 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
 | VFMADD213PS          | VFMA           | AVX512EVEX     | AVX512F_256    | a8           | mm         | rrr        | EVV 0xA8 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFMADD213PS          | VFMA           | AVX512EVEX     | AVX512F_512    | a8           | 0b11       | rrr        | EVV 0xA8 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VFMADD213PS          | VFMA           | AVX512EVEX     | AVX512F_512    | a8           | 0b11       | rrr        | EVV 0xA8 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0     | REG0=ZMM_R3():rw:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VFMADD213PS          | VFMA           | AVX512EVEX     | AVX512F_512    | a8           | mm         | rrr        | EVV 0xA8 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFMADD213PS          | VFMA           | FMA            |                | a8           | mm         | rrr        | VV1 0xA8 VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
 | VFMADD213PS          | VFMA           | FMA            |                | a8           | 0b11       | rrr        | VV1 0xA8 VL128 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
 | VFMADD213PS          | VFMA           | FMA            |                | a8           | mm         | rrr        | VV1 0xA8 VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
 | VFMADD213PS          | VFMA           | FMA            |                | a8           | 0b11       | rrr        | VV1 0xA8 VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
 | VFMADD213SD          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | a9           | 0b11       | rrr        | EVV 0xA9 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                                       | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFMADD213SD          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | a9           | 0b11       | rrr        | EVV 0xA9 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1     | REG0=XMM_R3():rw:dq:f64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFMADD213SD          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | a9           | mm         | rrr        | EVV 0xA9 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
 | VFMADD213SD          | VFMA           | FMA            |                | a9           | mm         | rrr        | VV1 0xA9  V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                       | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64     MEM0:r:q:f64
 | VFMADD213SD          | VFMA           | FMA            |                | a9           | 0b11       | rrr        | VV1 0xA9  V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                              | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_B():r:q:f64
 | VFMADD213SS          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | a9           | 0b11       | rrr        | EVV 0xA9 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                                       | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFMADD213SS          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | a9           | 0b11       | rrr        | EVV 0xA9 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W0     | REG0=XMM_R3():rw:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFMADD213SS          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | a9           | mm         | rrr        | EVV 0xA9 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
 | VFMADD213SS          | VFMA           | FMA            |                | a9           | mm         | rrr        | VV1 0xA9  V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                       | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32     MEM0:r:d:f32
 | VFMADD213SS          | VFMA           | FMA            |                | a9           | 0b11       | rrr        | VV1 0xA9  V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                              | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_B():r:d:f32
 | VFMADD231PD          | UFMA           | KNCE           |                | b8           | mm         | rrr        | KVV 0xB8 V0F38 V66  REXW=1 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT64()                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VFMADD231PD          | UFMA           | KNCE           |                | b8           | 0b11       | rrr        | KVV 0xB8 V0F38 V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE64()                     | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=REGSWIZ
 | VFMADD231PD          | UFMA           | KNCE           |                | b8           | 0b11       | rrr        | KVV 0xB8 V0F38 V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                   | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=ROUNDC:TXT=SAEC
 | VFMADD231PD          | VFMA           | AVX512EVEX     | AVX512F_128    | b8           | 0b11       | rrr        | EVV 0xB8 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFMADD231PD          | VFMA           | AVX512EVEX     | AVX512F_128    | b8           | mm         | rrr        | EVV 0xB8 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFMADD231PD          | VFMA           | AVX512EVEX     | AVX512F_256    | b8           | 0b11       | rrr        | EVV 0xB8 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
 | VFMADD231PD          | VFMA           | AVX512EVEX     | AVX512F_256    | b8           | mm         | rrr        | EVV 0xB8 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFMADD231PD          | VFMA           | AVX512EVEX     | AVX512F_512    | b8           | 0b11       | rrr        | EVV 0xB8 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VFMADD231PD          | VFMA           | AVX512EVEX     | AVX512F_512    | b8           | 0b11       | rrr        | EVV 0xB8 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1     | REG0=ZMM_R3():rw:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VFMADD231PD          | VFMA           | AVX512EVEX     | AVX512F_512    | b8           | mm         | rrr        | EVV 0xB8 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFMADD231PD          | VFMA           | FMA            |                | b8           | mm         | rrr        | VV1 0xB8 VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
 | VFMADD231PD          | VFMA           | FMA            |                | b8           | 0b11       | rrr        | VV1 0xB8 VL128 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
 | VFMADD231PD          | VFMA           | FMA            |                | b8           | mm         | rrr        | VV1 0xB8 VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
 | VFMADD231PD          | VFMA           | FMA            |                | b8           | 0b11       | rrr        | VV1 0xB8 VL256 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
 | VFMADD231PS          | UFMA           | KNCE           |                | b8           | mm         | rrr        | KVV 0xB8 V0F38 V66  REXW=0 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT32()                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VFMADD231PS          | UFMA           | KNCE           |                | b8           | 0b11       | rrr        | KVV 0xB8 V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VFMADD231PS          | UFMA           | KNCE           |                | b8           | 0b11       | rrr        | KVV 0xB8 V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                   | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=ROUNDC:TXT=SAEC
 | VFMADD231PS          | VFMA           | AVX512EVEX     | AVX512F_128    | b8           | 0b11       | rrr        | EVV 0xB8 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFMADD231PS          | VFMA           | AVX512EVEX     | AVX512F_128    | b8           | mm         | rrr        | EVV 0xB8 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFMADD231PS          | VFMA           | AVX512EVEX     | AVX512F_256    | b8           | 0b11       | rrr        | EVV 0xB8 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
 | VFMADD231PS          | VFMA           | AVX512EVEX     | AVX512F_256    | b8           | mm         | rrr        | EVV 0xB8 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFMADD231PS          | VFMA           | AVX512EVEX     | AVX512F_512    | b8           | 0b11       | rrr        | EVV 0xB8 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VFMADD231PS          | VFMA           | AVX512EVEX     | AVX512F_512    | b8           | 0b11       | rrr        | EVV 0xB8 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0     | REG0=ZMM_R3():rw:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VFMADD231PS          | VFMA           | AVX512EVEX     | AVX512F_512    | b8           | mm         | rrr        | EVV 0xB8 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFMADD231PS          | VFMA           | FMA            |                | b8           | mm         | rrr        | VV1 0xB8 VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
 | VFMADD231PS          | VFMA           | FMA            |                | b8           | 0b11       | rrr        | VV1 0xB8 VL128 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
 | VFMADD231PS          | VFMA           | FMA            |                | b8           | mm         | rrr        | VV1 0xB8 VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
 | VFMADD231PS          | VFMA           | FMA            |                | b8           | 0b11       | rrr        | VV1 0xB8 VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
 | VFMADD231SD          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | b9           | 0b11       | rrr        | EVV 0xB9 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                                       | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFMADD231SD          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | b9           | 0b11       | rrr        | EVV 0xB9 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1     | REG0=XMM_R3():rw:dq:f64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFMADD231SD          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | b9           | mm         | rrr        | EVV 0xB9 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
 | VFMADD231SD          | VFMA           | FMA            |                | b9           | mm         | rrr        | VV1 0xB9 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                        | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 MEM0:r:q:f64
 | VFMADD231SD          | VFMA           | FMA            |                | b9           | 0b11       | rrr        | VV1 0xB9 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                               | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_B():r:q:f64
 | VFMADD231SS          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | b9           | 0b11       | rrr        | EVV 0xB9 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                                       | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFMADD231SS          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | b9           | 0b11       | rrr        | EVV 0xB9 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W0     | REG0=XMM_R3():rw:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFMADD231SS          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | b9           | mm         | rrr        | EVV 0xB9 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
 | VFMADD231SS          | VFMA           | FMA            |                | b9           | mm         | rrr        | VV1 0xB9 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                        | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 MEM0:r:d:f32
 | VFMADD231SS          | VFMA           | FMA            |                | b9           | 0b11       | rrr        | VV1 0xB9 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                               | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_B():r:d:f32
 | VFMADD233PS          | UFMA           | KNCE           |                | a4           | mm         | rrr        | KVV 0xA4 V0F38 V66  REXW=0 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT32_LIMITED()        | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VFMADD233PS          | UFMA           | KNCE           |                | a4           | 0b11       | rrr        | KVV 0xA4 V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VFMADD233PS          | UFMA           | KNCE           |                | a4           | 0b11       | rrr        | KVV 0xA4 V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                   | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=ROUNDC:TXT=SAEC
 | VFMADDPD             | FMA4           | FMA4           | FMA4           | 69           | mm         | rrr        | VV1 0x69 V66 W0 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64 REG2=XMM_SE():r:dq:f64
 | VFMADDPD             | FMA4           | FMA4           | FMA4           | 69           | 0b11       | rrr        | VV1 0x69 V66 W0 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64 REG3=XMM_SE():r:dq:f64
 | VFMADDPD             | FMA4           | FMA4           | FMA4           | 69           | mm         | rrr        | VV1 0x69 V66 W1 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_SE():r:dq:f64 MEM0:r:dq:f64
 | VFMADDPD             | FMA4           | FMA4           | FMA4           | 69           | 0b11       | rrr        | VV1 0x69 V66 W1 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_SE():r:dq:f64 REG3=XMM_B():r:dq:f64
 | VFMADDPD             | FMA4           | FMA4           | FMA4           | 69           | mm         | rrr        | VV1 0x69 V66 W0 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64 REG2=YMM_SE():r:qq:f64
 | VFMADDPD             | FMA4           | FMA4           | FMA4           | 69           | 0b11       | rrr        | VV1 0x69 V66 W0 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64 REG3=YMM_SE():r:qq:f64
 | VFMADDPD             | FMA4           | FMA4           | FMA4           | 69           | mm         | rrr        | VV1 0x69 V66 W1 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_SE():r:qq:f64 MEM0:r:qq:f64
 | VFMADDPD             | FMA4           | FMA4           | FMA4           | 69           | 0b11       | rrr        | VV1 0x69 V66 W1 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_SE():r:qq:f64 REG3=YMM_B():r:qq:f64
 | VFMADDPS             | FMA4           | FMA4           | FMA4           | 68           | mm         | rrr        | VV1 0x68 V66 W0 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32 REG2=XMM_SE():r:dq:f32
 | VFMADDPS             | FMA4           | FMA4           | FMA4           | 68           | 0b11       | rrr        | VV1 0x68 V66 W0 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32 REG3=XMM_SE():r:dq:f32
 | VFMADDPS             | FMA4           | FMA4           | FMA4           | 68           | mm         | rrr        | VV1 0x68 V66 W1 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_SE():r:dq:f32 MEM0:r:dq:f32
 | VFMADDPS             | FMA4           | FMA4           | FMA4           | 68           | 0b11       | rrr        | VV1 0x68 V66 W1 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_SE():r:dq:f32 REG3=XMM_B():r:dq:f32
 | VFMADDPS             | FMA4           | FMA4           | FMA4           | 68           | mm         | rrr        | VV1 0x68 V66 W0 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32 REG2=YMM_SE():r:qq:f32
 | VFMADDPS             | FMA4           | FMA4           | FMA4           | 68           | 0b11       | rrr        | VV1 0x68 V66 W0 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32 REG3=YMM_SE():r:qq:f32
 | VFMADDPS             | FMA4           | FMA4           | FMA4           | 68           | mm         | rrr        | VV1 0x68 V66 W1 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_SE():r:qq:f32 MEM0:r:qq:f32
 | VFMADDPS             | FMA4           | FMA4           | FMA4           | 68           | 0b11       | rrr        | VV1 0x68 V66 W1 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_SE():r:qq:f32 REG3=YMM_B():r:qq:f32
 | VFMADDSD             | FMA4           | FMA4           | FMA4           | 6b           | mm         | rrr        | VV1 0x6B V66 W0  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                             | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:q:f64 MEM0:r:q:f64 REG2=XMM_SE():r:q:f64
 | VFMADDSD             | FMA4           | FMA4           | FMA4           | 6b           | 0b11       | rrr        | VV1 0x6B V66 W0  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                                    | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_B():r:q:f64 REG3=XMM_SE():r:q:f64
 | VFMADDSD             | FMA4           | FMA4           | FMA4           | 6b           | mm         | rrr        | VV1 0x6B V66 W1  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                             | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_SE():r:q:f64 MEM0:r:q:f64
 | VFMADDSD             | FMA4           | FMA4           | FMA4           | 6b           | 0b11       | rrr        | VV1 0x6B V66 W1  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                                    | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_SE():r:q:f64 REG3=XMM_B():r:q:f64
 | VFMADDSS             | FMA4           | FMA4           | FMA4           | 6a           | mm         | rrr        | VV1 0x6A V66 W0  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                             | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:d:f32 MEM0:r:d:f32 REG2=XMM_SE():r:d:f32
 | VFMADDSS             | FMA4           | FMA4           | FMA4           | 6a           | 0b11       | rrr        | VV1 0x6A V66 W0  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                                    | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_B():r:d:f32 REG3=XMM_SE():r:d:f32
 | VFMADDSS             | FMA4           | FMA4           | FMA4           | 6a           | mm         | rrr        | VV1 0x6A V66 W1  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                             | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_SE():r:d:f32 MEM0:r:d:f32
 | VFMADDSS             | FMA4           | FMA4           | FMA4           | 6a           | 0b11       | rrr        | VV1 0x6A V66 W1  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                                    | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_SE():r:d:f32 REG3=XMM_B():r:d:f32
 | VFMADDSUB132PD       | VFMA           | AVX512EVEX     | AVX512F_128    | 96           | 0b11       | rrr        | EVV 0x96 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFMADDSUB132PD       | VFMA           | AVX512EVEX     | AVX512F_128    | 96           | mm         | rrr        | EVV 0x96 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFMADDSUB132PD       | VFMA           | AVX512EVEX     | AVX512F_256    | 96           | 0b11       | rrr        | EVV 0x96 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
 | VFMADDSUB132PD       | VFMA           | AVX512EVEX     | AVX512F_256    | 96           | mm         | rrr        | EVV 0x96 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFMADDSUB132PD       | VFMA           | AVX512EVEX     | AVX512F_512    | 96           | 0b11       | rrr        | EVV 0x96 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VFMADDSUB132PD       | VFMA           | AVX512EVEX     | AVX512F_512    | 96           | 0b11       | rrr        | EVV 0x96 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1     | REG0=ZMM_R3():rw:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VFMADDSUB132PD       | VFMA           | AVX512EVEX     | AVX512F_512    | 96           | mm         | rrr        | EVV 0x96 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFMADDSUB132PD       | VFMA           | FMA            |                | 96           | mm         | rrr        | VV1 0x96 VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
 | VFMADDSUB132PD       | VFMA           | FMA            |                | 96           | 0b11       | rrr        | VV1 0x96 VL128 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
 | VFMADDSUB132PD       | VFMA           | FMA            |                | 96           | mm         | rrr        | VV1 0x96 VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
 | VFMADDSUB132PD       | VFMA           | FMA            |                | 96           | 0b11       | rrr        | VV1 0x96 VL256 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
 | VFMADDSUB132PS       | VFMA           | AVX512EVEX     | AVX512F_128    | 96           | 0b11       | rrr        | EVV 0x96 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFMADDSUB132PS       | VFMA           | AVX512EVEX     | AVX512F_128    | 96           | mm         | rrr        | EVV 0x96 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFMADDSUB132PS       | VFMA           | AVX512EVEX     | AVX512F_256    | 96           | 0b11       | rrr        | EVV 0x96 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
 | VFMADDSUB132PS       | VFMA           | AVX512EVEX     | AVX512F_256    | 96           | mm         | rrr        | EVV 0x96 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFMADDSUB132PS       | VFMA           | AVX512EVEX     | AVX512F_512    | 96           | 0b11       | rrr        | EVV 0x96 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VFMADDSUB132PS       | VFMA           | AVX512EVEX     | AVX512F_512    | 96           | 0b11       | rrr        | EVV 0x96 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0     | REG0=ZMM_R3():rw:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VFMADDSUB132PS       | VFMA           | AVX512EVEX     | AVX512F_512    | 96           | mm         | rrr        | EVV 0x96 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFMADDSUB132PS       | VFMA           | FMA            |                | 96           | mm         | rrr        | VV1 0x96 VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
 | VFMADDSUB132PS       | VFMA           | FMA            |                | 96           | 0b11       | rrr        | VV1 0x96 VL128 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
 | VFMADDSUB132PS       | VFMA           | FMA            |                | 96           | mm         | rrr        | VV1 0x96 VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
 | VFMADDSUB132PS       | VFMA           | FMA            |                | 96           | 0b11       | rrr        | VV1 0x96 VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
 | VFMADDSUB213PD       | VFMA           | AVX512EVEX     | AVX512F_128    | a6           | 0b11       | rrr        | EVV 0xA6 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFMADDSUB213PD       | VFMA           | AVX512EVEX     | AVX512F_128    | a6           | mm         | rrr        | EVV 0xA6 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFMADDSUB213PD       | VFMA           | AVX512EVEX     | AVX512F_256    | a6           | 0b11       | rrr        | EVV 0xA6 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
 | VFMADDSUB213PD       | VFMA           | AVX512EVEX     | AVX512F_256    | a6           | mm         | rrr        | EVV 0xA6 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFMADDSUB213PD       | VFMA           | AVX512EVEX     | AVX512F_512    | a6           | 0b11       | rrr        | EVV 0xA6 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VFMADDSUB213PD       | VFMA           | AVX512EVEX     | AVX512F_512    | a6           | 0b11       | rrr        | EVV 0xA6 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1     | REG0=ZMM_R3():rw:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VFMADDSUB213PD       | VFMA           | AVX512EVEX     | AVX512F_512    | a6           | mm         | rrr        | EVV 0xA6 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFMADDSUB213PD       | VFMA           | FMA            |                | a6           | mm         | rrr        | VV1 0xA6 VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64    MEM0:r:dq:f64
 | VFMADDSUB213PD       | VFMA           | FMA            |                | a6           | 0b11       | rrr        | VV1 0xA6 VL128 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
 | VFMADDSUB213PD       | VFMA           | FMA            |                | a6           | mm         | rrr        | VV1 0xA6 VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64    MEM0:r:qq:f64
 | VFMADDSUB213PD       | VFMA           | FMA            |                | a6           | 0b11       | rrr        | VV1 0xA6 VL256 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
 | VFMADDSUB213PS       | VFMA           | AVX512EVEX     | AVX512F_128    | a6           | 0b11       | rrr        | EVV 0xA6 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFMADDSUB213PS       | VFMA           | AVX512EVEX     | AVX512F_128    | a6           | mm         | rrr        | EVV 0xA6 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFMADDSUB213PS       | VFMA           | AVX512EVEX     | AVX512F_256    | a6           | 0b11       | rrr        | EVV 0xA6 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
 | VFMADDSUB213PS       | VFMA           | AVX512EVEX     | AVX512F_256    | a6           | mm         | rrr        | EVV 0xA6 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFMADDSUB213PS       | VFMA           | AVX512EVEX     | AVX512F_512    | a6           | 0b11       | rrr        | EVV 0xA6 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VFMADDSUB213PS       | VFMA           | AVX512EVEX     | AVX512F_512    | a6           | 0b11       | rrr        | EVV 0xA6 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0     | REG0=ZMM_R3():rw:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VFMADDSUB213PS       | VFMA           | AVX512EVEX     | AVX512F_512    | a6           | mm         | rrr        | EVV 0xA6 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFMADDSUB213PS       | VFMA           | FMA            |                | a6           | mm         | rrr        | VV1 0xA6 VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
 | VFMADDSUB213PS       | VFMA           | FMA            |                | a6           | 0b11       | rrr        | VV1 0xA6 VL128 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
 | VFMADDSUB213PS       | VFMA           | FMA            |                | a6           | mm         | rrr        | VV1 0xA6 VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
 | VFMADDSUB213PS       | VFMA           | FMA            |                | a6           | 0b11       | rrr        | VV1 0xA6 VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
 | VFMADDSUB231PD       | VFMA           | AVX512EVEX     | AVX512F_128    | b6           | 0b11       | rrr        | EVV 0xB6 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFMADDSUB231PD       | VFMA           | AVX512EVEX     | AVX512F_128    | b6           | mm         | rrr        | EVV 0xB6 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFMADDSUB231PD       | VFMA           | AVX512EVEX     | AVX512F_256    | b6           | 0b11       | rrr        | EVV 0xB6 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
 | VFMADDSUB231PD       | VFMA           | AVX512EVEX     | AVX512F_256    | b6           | mm         | rrr        | EVV 0xB6 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFMADDSUB231PD       | VFMA           | AVX512EVEX     | AVX512F_512    | b6           | 0b11       | rrr        | EVV 0xB6 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VFMADDSUB231PD       | VFMA           | AVX512EVEX     | AVX512F_512    | b6           | 0b11       | rrr        | EVV 0xB6 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1     | REG0=ZMM_R3():rw:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VFMADDSUB231PD       | VFMA           | AVX512EVEX     | AVX512F_512    | b6           | mm         | rrr        | EVV 0xB6 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFMADDSUB231PD       | VFMA           | FMA            |                | b6           | mm         | rrr        | VV1 0xB6 VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
 | VFMADDSUB231PD       | VFMA           | FMA            |                | b6           | 0b11       | rrr        | VV1 0xB6 VL128 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
 | VFMADDSUB231PD       | VFMA           | FMA            |                | b6           | mm         | rrr        | VV1 0xB6 VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
 | VFMADDSUB231PD       | VFMA           | FMA            |                | b6           | 0b11       | rrr        | VV1 0xB6 VL256 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
 | VFMADDSUB231PS       | VFMA           | AVX512EVEX     | AVX512F_128    | b6           | 0b11       | rrr        | EVV 0xB6 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFMADDSUB231PS       | VFMA           | AVX512EVEX     | AVX512F_128    | b6           | mm         | rrr        | EVV 0xB6 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFMADDSUB231PS       | VFMA           | AVX512EVEX     | AVX512F_256    | b6           | 0b11       | rrr        | EVV 0xB6 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
 | VFMADDSUB231PS       | VFMA           | AVX512EVEX     | AVX512F_256    | b6           | mm         | rrr        | EVV 0xB6 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFMADDSUB231PS       | VFMA           | AVX512EVEX     | AVX512F_512    | b6           | 0b11       | rrr        | EVV 0xB6 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VFMADDSUB231PS       | VFMA           | AVX512EVEX     | AVX512F_512    | b6           | 0b11       | rrr        | EVV 0xB6 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0     | REG0=ZMM_R3():rw:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VFMADDSUB231PS       | VFMA           | AVX512EVEX     | AVX512F_512    | b6           | mm         | rrr        | EVV 0xB6 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFMADDSUB231PS       | VFMA           | FMA            |                | b6           | mm         | rrr        | VV1 0xB6 VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
 | VFMADDSUB231PS       | VFMA           | FMA            |                | b6           | 0b11       | rrr        | VV1 0xB6 VL128 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
 | VFMADDSUB231PS       | VFMA           | FMA            |                | b6           | mm         | rrr        | VV1 0xB6 VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
 | VFMADDSUB231PS       | VFMA           | FMA            |                | b6           | 0b11       | rrr        | VV1 0xB6 VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
 | VFMADDSUBPD          | FMA4           | FMA4           | FMA4           | 5d           | mm         | rrr        | VV1 0x5D V66 W0 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64 REG2=XMM_SE():r:dq:f64
 | VFMADDSUBPD          | FMA4           | FMA4           | FMA4           | 5d           | 0b11       | rrr        | VV1 0x5D V66 W0 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64 REG3=XMM_SE():r:dq:f64
 | VFMADDSUBPD          | FMA4           | FMA4           | FMA4           | 5d           | mm         | rrr        | VV1 0x5D V66 W1 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_SE():r:dq:f64 MEM0:r:dq:f64
 | VFMADDSUBPD          | FMA4           | FMA4           | FMA4           | 5d           | 0b11       | rrr        | VV1 0x5D V66 W1 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_SE():r:dq:f64 REG3=XMM_B():r:dq:f64
 | VFMADDSUBPD          | FMA4           | FMA4           | FMA4           | 5d           | mm         | rrr        | VV1 0x5D V66 W0 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64 REG2=YMM_SE():r:qq:f64
 | VFMADDSUBPD          | FMA4           | FMA4           | FMA4           | 5d           | 0b11       | rrr        | VV1 0x5D V66 W0 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64 REG3=YMM_SE():r:qq:f64
 | VFMADDSUBPD          | FMA4           | FMA4           | FMA4           | 5d           | mm         | rrr        | VV1 0x5D V66 W1 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_SE():r:qq:f64 MEM0:r:qq:f64
 | VFMADDSUBPD          | FMA4           | FMA4           | FMA4           | 5d           | 0b11       | rrr        | VV1 0x5D V66 W1 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_SE():r:qq:f64 REG3=YMM_B():r:qq:f64
 | VFMADDSUBPS          | FMA4           | FMA4           | FMA4           | 5c           | mm         | rrr        | VV1 0x5C V66 W0 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32 REG2=XMM_SE():r:dq:f32
 | VFMADDSUBPS          | FMA4           | FMA4           | FMA4           | 5c           | 0b11       | rrr        | VV1 0x5C V66 W0 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32 REG3=XMM_SE():r:dq:f32
 | VFMADDSUBPS          | FMA4           | FMA4           | FMA4           | 5c           | mm         | rrr        | VV1 0x5C V66 W1 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_SE():r:dq:f32 MEM0:r:dq:f32
 | VFMADDSUBPS          | FMA4           | FMA4           | FMA4           | 5c           | 0b11       | rrr        | VV1 0x5C V66 W1 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_SE():r:dq:f32 REG3=XMM_B():r:dq:f32
 | VFMADDSUBPS          | FMA4           | FMA4           | FMA4           | 5c           | mm         | rrr        | VV1 0x5C V66 W0 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32 REG2=YMM_SE():r:qq:f32
 | VFMADDSUBPS          | FMA4           | FMA4           | FMA4           | 5c           | 0b11       | rrr        | VV1 0x5C V66 W0 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32 REG3=YMM_SE():r:qq:f32
 | VFMADDSUBPS          | FMA4           | FMA4           | FMA4           | 5c           | mm         | rrr        | VV1 0x5C V66 W1 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_SE():r:qq:f32 MEM0:r:qq:f32
 | VFMADDSUBPS          | FMA4           | FMA4           | FMA4           | 5c           | 0b11       | rrr        | VV1 0x5C V66 W1 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_SE():r:qq:f32 REG3=YMM_B():r:qq:f32
 | VFMSUB132PD          | UFMA           | KNCE           |                | 9a           | mm         | rrr        | KVV 0x9A V0F38 V66  REXW=1 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT64()                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VFMSUB132PD          | UFMA           | KNCE           |                | 9a           | 0b11       | rrr        | KVV 0x9A V0F38 V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE64()                     | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=REGSWIZ
 | VFMSUB132PD          | UFMA           | KNCE           |                | 9a           | 0b11       | rrr        | KVV 0x9A V0F38 V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                   | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=ROUNDC:TXT=SAEC
 | VFMSUB132PD          | VFMA           | AVX512EVEX     | AVX512F_128    | 9a           | 0b11       | rrr        | EVV 0x9A V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFMSUB132PD          | VFMA           | AVX512EVEX     | AVX512F_128    | 9a           | mm         | rrr        | EVV 0x9A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFMSUB132PD          | VFMA           | AVX512EVEX     | AVX512F_256    | 9a           | 0b11       | rrr        | EVV 0x9A V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
 | VFMSUB132PD          | VFMA           | AVX512EVEX     | AVX512F_256    | 9a           | mm         | rrr        | EVV 0x9A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFMSUB132PD          | VFMA           | AVX512EVEX     | AVX512F_512    | 9a           | 0b11       | rrr        | EVV 0x9A V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VFMSUB132PD          | VFMA           | AVX512EVEX     | AVX512F_512    | 9a           | 0b11       | rrr        | EVV 0x9A V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1     | REG0=ZMM_R3():rw:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VFMSUB132PD          | VFMA           | AVX512EVEX     | AVX512F_512    | 9a           | mm         | rrr        | EVV 0x9A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFMSUB132PD          | VFMA           | FMA            |                | 9a           | mm         | rrr        | VV1 0x9A VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
 | VFMSUB132PD          | VFMA           | FMA            |                | 9a           | 0b11       | rrr        | VV1 0x9A VL128 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
 | VFMSUB132PD          | VFMA           | FMA            |                | 9a           | mm         | rrr        | VV1 0x9A VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
 | VFMSUB132PD          | VFMA           | FMA            |                | 9a           | 0b11       | rrr        | VV1 0x9A VL256 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
 | VFMSUB132PS          | UFMA           | KNCE           |                | 9a           | mm         | rrr        | KVV 0x9A V0F38 V66  REXW=0 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT32()                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VFMSUB132PS          | UFMA           | KNCE           |                | 9a           | 0b11       | rrr        | KVV 0x9A V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VFMSUB132PS          | UFMA           | KNCE           |                | 9a           | 0b11       | rrr        | KVV 0x9A V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                   | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=ROUNDC:TXT=SAEC
 | VFMSUB132PS          | VFMA           | AVX512EVEX     | AVX512F_128    | 9a           | 0b11       | rrr        | EVV 0x9A V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFMSUB132PS          | VFMA           | AVX512EVEX     | AVX512F_128    | 9a           | mm         | rrr        | EVV 0x9A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFMSUB132PS          | VFMA           | AVX512EVEX     | AVX512F_256    | 9a           | 0b11       | rrr        | EVV 0x9A V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
 | VFMSUB132PS          | VFMA           | AVX512EVEX     | AVX512F_256    | 9a           | mm         | rrr        | EVV 0x9A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFMSUB132PS          | VFMA           | AVX512EVEX     | AVX512F_512    | 9a           | 0b11       | rrr        | EVV 0x9A V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VFMSUB132PS          | VFMA           | AVX512EVEX     | AVX512F_512    | 9a           | 0b11       | rrr        | EVV 0x9A V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0     | REG0=ZMM_R3():rw:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VFMSUB132PS          | VFMA           | AVX512EVEX     | AVX512F_512    | 9a           | mm         | rrr        | EVV 0x9A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFMSUB132PS          | VFMA           | FMA            |                | 9a           | mm         | rrr        | VV1 0x9A VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
 | VFMSUB132PS          | VFMA           | FMA            |                | 9a           | 0b11       | rrr        | VV1 0x9A VL128 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
 | VFMSUB132PS          | VFMA           | FMA            |                | 9a           | mm         | rrr        | VV1 0x9A VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
 | VFMSUB132PS          | VFMA           | FMA            |                | 9a           | 0b11       | rrr        | VV1 0x9A VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
 | VFMSUB132SD          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | 9b           | 0b11       | rrr        | EVV 0x9B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                                       | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFMSUB132SD          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | 9b           | 0b11       | rrr        | EVV 0x9B V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1     | REG0=XMM_R3():rw:dq:f64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFMSUB132SD          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | 9b           | mm         | rrr        | EVV 0x9B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
 | VFMSUB132SD          | VFMA           | FMA            |                | 9b           | mm         | rrr        | VV1 0x9B V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                        | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 MEM0:r:q:f64
 | VFMSUB132SD          | VFMA           | FMA            |                | 9b           | 0b11       | rrr        | VV1 0x9B V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                               | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_B():r:q:f64
 | VFMSUB132SS          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | 9b           | 0b11       | rrr        | EVV 0x9B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                                       | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFMSUB132SS          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | 9b           | 0b11       | rrr        | EVV 0x9B V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W0     | REG0=XMM_R3():rw:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFMSUB132SS          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | 9b           | mm         | rrr        | EVV 0x9B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
 | VFMSUB132SS          | VFMA           | FMA            |                | 9b           | mm         | rrr        | VV1 0x9B  V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                       | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 MEM0:r:d:f32
 | VFMSUB132SS          | VFMA           | FMA            |                | 9b           | 0b11       | rrr        | VV1 0x9B  V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                              | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_B():r:d:f32
 | VFMSUB213PD          | UFMA           | KNCE           |                | aa           | mm         | rrr        | KVV 0xAA V0F38 V66  REXW=1 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT64()                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VFMSUB213PD          | UFMA           | KNCE           |                | aa           | 0b11       | rrr        | KVV 0xAA V0F38 V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE64()                     | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=REGSWIZ
 | VFMSUB213PD          | UFMA           | KNCE           |                | aa           | 0b11       | rrr        | KVV 0xAA V0F38 V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                   | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=ROUNDC:TXT=SAEC
 | VFMSUB213PD          | VFMA           | AVX512EVEX     | AVX512F_128    | aa           | 0b11       | rrr        | EVV 0xAA V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFMSUB213PD          | VFMA           | AVX512EVEX     | AVX512F_128    | aa           | mm         | rrr        | EVV 0xAA V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFMSUB213PD          | VFMA           | AVX512EVEX     | AVX512F_256    | aa           | 0b11       | rrr        | EVV 0xAA V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
 | VFMSUB213PD          | VFMA           | AVX512EVEX     | AVX512F_256    | aa           | mm         | rrr        | EVV 0xAA V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFMSUB213PD          | VFMA           | AVX512EVEX     | AVX512F_512    | aa           | 0b11       | rrr        | EVV 0xAA V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VFMSUB213PD          | VFMA           | AVX512EVEX     | AVX512F_512    | aa           | 0b11       | rrr        | EVV 0xAA V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1     | REG0=ZMM_R3():rw:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VFMSUB213PD          | VFMA           | AVX512EVEX     | AVX512F_512    | aa           | mm         | rrr        | EVV 0xAA V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFMSUB213PD          | VFMA           | FMA            |                | aa           | mm         | rrr        | VV1 0xAA VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64    MEM0:r:dq:f64
 | VFMSUB213PD          | VFMA           | FMA            |                | aa           | 0b11       | rrr        | VV1 0xAA VL128 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
 | VFMSUB213PD          | VFMA           | FMA            |                | aa           | mm         | rrr        | VV1 0xAA VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64    MEM0:r:qq:f64
 | VFMSUB213PD          | VFMA           | FMA            |                | aa           | 0b11       | rrr        | VV1 0xAA VL256 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
 | VFMSUB213PS          | UFMA           | KNCE           |                | aa           | mm         | rrr        | KVV 0xAA V0F38 V66  REXW=0 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT32()                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VFMSUB213PS          | UFMA           | KNCE           |                | aa           | 0b11       | rrr        | KVV 0xAA V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VFMSUB213PS          | UFMA           | KNCE           |                | aa           | 0b11       | rrr        | KVV 0xAA V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                   | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=ROUNDC:TXT=SAEC
 | VFMSUB213PS          | VFMA           | AVX512EVEX     | AVX512F_128    | aa           | 0b11       | rrr        | EVV 0xAA V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFMSUB213PS          | VFMA           | AVX512EVEX     | AVX512F_128    | aa           | mm         | rrr        | EVV 0xAA V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFMSUB213PS          | VFMA           | AVX512EVEX     | AVX512F_256    | aa           | 0b11       | rrr        | EVV 0xAA V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
 | VFMSUB213PS          | VFMA           | AVX512EVEX     | AVX512F_256    | aa           | mm         | rrr        | EVV 0xAA V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFMSUB213PS          | VFMA           | AVX512EVEX     | AVX512F_512    | aa           | 0b11       | rrr        | EVV 0xAA V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VFMSUB213PS          | VFMA           | AVX512EVEX     | AVX512F_512    | aa           | 0b11       | rrr        | EVV 0xAA V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0     | REG0=ZMM_R3():rw:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VFMSUB213PS          | VFMA           | AVX512EVEX     | AVX512F_512    | aa           | mm         | rrr        | EVV 0xAA V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFMSUB213PS          | VFMA           | FMA            |                | aa           | mm         | rrr        | VV1 0xAA VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
 | VFMSUB213PS          | VFMA           | FMA            |                | aa           | 0b11       | rrr        | VV1 0xAA VL128 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
 | VFMSUB213PS          | VFMA           | FMA            |                | aa           | mm         | rrr        | VV1 0xAA VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
 | VFMSUB213PS          | VFMA           | FMA            |                | aa           | 0b11       | rrr        | VV1 0xAA VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
 | VFMSUB213SD          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | ab           | 0b11       | rrr        | EVV 0xAB V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                                       | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFMSUB213SD          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | ab           | 0b11       | rrr        | EVV 0xAB V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1     | REG0=XMM_R3():rw:dq:f64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFMSUB213SD          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | ab           | mm         | rrr        | EVV 0xAB V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
 | VFMSUB213SD          | VFMA           | FMA            |                | ab           | mm         | rrr        | VV1 0xAB  V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                       | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64     MEM0:r:q:f64
 | VFMSUB213SD          | VFMA           | FMA            |                | ab           | 0b11       | rrr        | VV1 0xAB  V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                              | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_B():r:q:f64
 | VFMSUB213SS          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | ab           | 0b11       | rrr        | EVV 0xAB V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                                       | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFMSUB213SS          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | ab           | 0b11       | rrr        | EVV 0xAB V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W0     | REG0=XMM_R3():rw:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFMSUB213SS          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | ab           | mm         | rrr        | EVV 0xAB V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
 | VFMSUB213SS          | VFMA           | FMA            |                | ab           | mm         | rrr        | VV1 0xAB  V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                       | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32     MEM0:r:d:f32
 | VFMSUB213SS          | VFMA           | FMA            |                | ab           | 0b11       | rrr        | VV1 0xAB  V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                              | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_B():r:d:f32
 | VFMSUB231PD          | UFMA           | KNCE           |                | ba           | mm         | rrr        | KVV 0xBA V0F38 V66  REXW=1 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT64()                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VFMSUB231PD          | UFMA           | KNCE           |                | ba           | 0b11       | rrr        | KVV 0xBA V0F38 V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE64()                     | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=REGSWIZ
 | VFMSUB231PD          | UFMA           | KNCE           |                | ba           | 0b11       | rrr        | KVV 0xBA V0F38 V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                   | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=ROUNDC:TXT=SAEC
 | VFMSUB231PD          | VFMA           | AVX512EVEX     | AVX512F_128    | ba           | 0b11       | rrr        | EVV 0xBA V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFMSUB231PD          | VFMA           | AVX512EVEX     | AVX512F_128    | ba           | mm         | rrr        | EVV 0xBA V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFMSUB231PD          | VFMA           | AVX512EVEX     | AVX512F_256    | ba           | 0b11       | rrr        | EVV 0xBA V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
 | VFMSUB231PD          | VFMA           | AVX512EVEX     | AVX512F_256    | ba           | mm         | rrr        | EVV 0xBA V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFMSUB231PD          | VFMA           | AVX512EVEX     | AVX512F_512    | ba           | 0b11       | rrr        | EVV 0xBA V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VFMSUB231PD          | VFMA           | AVX512EVEX     | AVX512F_512    | ba           | 0b11       | rrr        | EVV 0xBA V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1     | REG0=ZMM_R3():rw:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VFMSUB231PD          | VFMA           | AVX512EVEX     | AVX512F_512    | ba           | mm         | rrr        | EVV 0xBA V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFMSUB231PD          | VFMA           | FMA            |                | ba           | mm         | rrr        | VV1 0xBA VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
 | VFMSUB231PD          | VFMA           | FMA            |                | ba           | 0b11       | rrr        | VV1 0xBA VL128 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
 | VFMSUB231PD          | VFMA           | FMA            |                | ba           | mm         | rrr        | VV1 0xBA VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
 | VFMSUB231PD          | VFMA           | FMA            |                | ba           | 0b11       | rrr        | VV1 0xBA VL256 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
 | VFMSUB231PS          | UFMA           | KNCE           |                | ba           | mm         | rrr        | KVV 0xBA V0F38 V66  REXW=0 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT32()                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VFMSUB231PS          | UFMA           | KNCE           |                | ba           | 0b11       | rrr        | KVV 0xBA V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VFMSUB231PS          | UFMA           | KNCE           |                | ba           | 0b11       | rrr        | KVV 0xBA V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                   | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=ROUNDC:TXT=SAEC
 | VFMSUB231PS          | VFMA           | AVX512EVEX     | AVX512F_128    | ba           | 0b11       | rrr        | EVV 0xBA V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFMSUB231PS          | VFMA           | AVX512EVEX     | AVX512F_128    | ba           | mm         | rrr        | EVV 0xBA V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFMSUB231PS          | VFMA           | AVX512EVEX     | AVX512F_256    | ba           | 0b11       | rrr        | EVV 0xBA V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
 | VFMSUB231PS          | VFMA           | AVX512EVEX     | AVX512F_256    | ba           | mm         | rrr        | EVV 0xBA V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFMSUB231PS          | VFMA           | AVX512EVEX     | AVX512F_512    | ba           | 0b11       | rrr        | EVV 0xBA V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VFMSUB231PS          | VFMA           | AVX512EVEX     | AVX512F_512    | ba           | 0b11       | rrr        | EVV 0xBA V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0     | REG0=ZMM_R3():rw:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VFMSUB231PS          | VFMA           | AVX512EVEX     | AVX512F_512    | ba           | mm         | rrr        | EVV 0xBA V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFMSUB231PS          | VFMA           | FMA            |                | ba           | mm         | rrr        | VV1 0xBA VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
 | VFMSUB231PS          | VFMA           | FMA            |                | ba           | 0b11       | rrr        | VV1 0xBA VL128 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
 | VFMSUB231PS          | VFMA           | FMA            |                | ba           | mm         | rrr        | VV1 0xBA VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
 | VFMSUB231PS          | VFMA           | FMA            |                | ba           | 0b11       | rrr        | VV1 0xBA VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
 | VFMSUB231SD          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | bb           | 0b11       | rrr        | EVV 0xBB V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                                       | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFMSUB231SD          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | bb           | 0b11       | rrr        | EVV 0xBB V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1     | REG0=XMM_R3():rw:dq:f64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFMSUB231SD          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | bb           | mm         | rrr        | EVV 0xBB V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
 | VFMSUB231SD          | VFMA           | FMA            |                | bb           | mm         | rrr        | VV1 0xBB V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                        | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 MEM0:r:q:f64
 | VFMSUB231SD          | VFMA           | FMA            |                | bb           | 0b11       | rrr        | VV1 0xBB V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                               | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_B():r:q:f64
 | VFMSUB231SS          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | bb           | 0b11       | rrr        | EVV 0xBB V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                                       | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFMSUB231SS          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | bb           | 0b11       | rrr        | EVV 0xBB V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W0     | REG0=XMM_R3():rw:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFMSUB231SS          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | bb           | mm         | rrr        | EVV 0xBB V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
 | VFMSUB231SS          | VFMA           | FMA            |                | bb           | mm         | rrr        | VV1 0xBB V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                        | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 MEM0:r:d:f32
 | VFMSUB231SS          | VFMA           | FMA            |                | bb           | 0b11       | rrr        | VV1 0xBB V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                               | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_B():r:d:f32
 | VFMSUBADD132PD       | VFMA           | AVX512EVEX     | AVX512F_128    | 97           | 0b11       | rrr        | EVV 0x97 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFMSUBADD132PD       | VFMA           | AVX512EVEX     | AVX512F_128    | 97           | mm         | rrr        | EVV 0x97 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFMSUBADD132PD       | VFMA           | AVX512EVEX     | AVX512F_256    | 97           | 0b11       | rrr        | EVV 0x97 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
 | VFMSUBADD132PD       | VFMA           | AVX512EVEX     | AVX512F_256    | 97           | mm         | rrr        | EVV 0x97 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFMSUBADD132PD       | VFMA           | AVX512EVEX     | AVX512F_512    | 97           | 0b11       | rrr        | EVV 0x97 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VFMSUBADD132PD       | VFMA           | AVX512EVEX     | AVX512F_512    | 97           | 0b11       | rrr        | EVV 0x97 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1     | REG0=ZMM_R3():rw:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VFMSUBADD132PD       | VFMA           | AVX512EVEX     | AVX512F_512    | 97           | mm         | rrr        | EVV 0x97 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFMSUBADD132PD       | VFMA           | FMA            |                | 97           | mm         | rrr        | VV1 0x97 VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
 | VFMSUBADD132PD       | VFMA           | FMA            |                | 97           | 0b11       | rrr        | VV1 0x97 VL128 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
 | VFMSUBADD132PD       | VFMA           | FMA            |                | 97           | mm         | rrr        | VV1 0x97 VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
 | VFMSUBADD132PD       | VFMA           | FMA            |                | 97           | 0b11       | rrr        | VV1 0x97 VL256 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
 | VFMSUBADD132PS       | VFMA           | AVX512EVEX     | AVX512F_128    | 97           | 0b11       | rrr        | EVV 0x97 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFMSUBADD132PS       | VFMA           | AVX512EVEX     | AVX512F_128    | 97           | mm         | rrr        | EVV 0x97 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFMSUBADD132PS       | VFMA           | AVX512EVEX     | AVX512F_256    | 97           | 0b11       | rrr        | EVV 0x97 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
 | VFMSUBADD132PS       | VFMA           | AVX512EVEX     | AVX512F_256    | 97           | mm         | rrr        | EVV 0x97 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFMSUBADD132PS       | VFMA           | AVX512EVEX     | AVX512F_512    | 97           | 0b11       | rrr        | EVV 0x97 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VFMSUBADD132PS       | VFMA           | AVX512EVEX     | AVX512F_512    | 97           | 0b11       | rrr        | EVV 0x97 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0     | REG0=ZMM_R3():rw:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VFMSUBADD132PS       | VFMA           | AVX512EVEX     | AVX512F_512    | 97           | mm         | rrr        | EVV 0x97 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFMSUBADD132PS       | VFMA           | FMA            |                | 97           | mm         | rrr        | VV1 0x97 VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
 | VFMSUBADD132PS       | VFMA           | FMA            |                | 97           | 0b11       | rrr        | VV1 0x97 VL128 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
 | VFMSUBADD132PS       | VFMA           | FMA            |                | 97           | mm         | rrr        | VV1 0x97 VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
 | VFMSUBADD132PS       | VFMA           | FMA            |                | 97           | 0b11       | rrr        | VV1 0x97 VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
 | VFMSUBADD213PD       | VFMA           | AVX512EVEX     | AVX512F_128    | a7           | 0b11       | rrr        | EVV 0xA7 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFMSUBADD213PD       | VFMA           | AVX512EVEX     | AVX512F_128    | a7           | mm         | rrr        | EVV 0xA7 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFMSUBADD213PD       | VFMA           | AVX512EVEX     | AVX512F_256    | a7           | 0b11       | rrr        | EVV 0xA7 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
 | VFMSUBADD213PD       | VFMA           | AVX512EVEX     | AVX512F_256    | a7           | mm         | rrr        | EVV 0xA7 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFMSUBADD213PD       | VFMA           | AVX512EVEX     | AVX512F_512    | a7           | 0b11       | rrr        | EVV 0xA7 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VFMSUBADD213PD       | VFMA           | AVX512EVEX     | AVX512F_512    | a7           | 0b11       | rrr        | EVV 0xA7 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1     | REG0=ZMM_R3():rw:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VFMSUBADD213PD       | VFMA           | AVX512EVEX     | AVX512F_512    | a7           | mm         | rrr        | EVV 0xA7 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFMSUBADD213PD       | VFMA           | FMA            |                | a7           | mm         | rrr        | VV1 0xA7 VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64    MEM0:r:dq:f64
 | VFMSUBADD213PD       | VFMA           | FMA            |                | a7           | 0b11       | rrr        | VV1 0xA7 VL128 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
 | VFMSUBADD213PD       | VFMA           | FMA            |                | a7           | mm         | rrr        | VV1 0xA7 VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64    MEM0:r:qq:f64
 | VFMSUBADD213PD       | VFMA           | FMA            |                | a7           | 0b11       | rrr        | VV1 0xA7 VL256 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
 | VFMSUBADD213PS       | VFMA           | AVX512EVEX     | AVX512F_128    | a7           | 0b11       | rrr        | EVV 0xA7 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFMSUBADD213PS       | VFMA           | AVX512EVEX     | AVX512F_128    | a7           | mm         | rrr        | EVV 0xA7 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFMSUBADD213PS       | VFMA           | AVX512EVEX     | AVX512F_256    | a7           | 0b11       | rrr        | EVV 0xA7 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
 | VFMSUBADD213PS       | VFMA           | AVX512EVEX     | AVX512F_256    | a7           | mm         | rrr        | EVV 0xA7 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFMSUBADD213PS       | VFMA           | AVX512EVEX     | AVX512F_512    | a7           | 0b11       | rrr        | EVV 0xA7 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VFMSUBADD213PS       | VFMA           | AVX512EVEX     | AVX512F_512    | a7           | 0b11       | rrr        | EVV 0xA7 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0     | REG0=ZMM_R3():rw:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VFMSUBADD213PS       | VFMA           | AVX512EVEX     | AVX512F_512    | a7           | mm         | rrr        | EVV 0xA7 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFMSUBADD213PS       | VFMA           | FMA            |                | a7           | mm         | rrr        | VV1 0xA7 VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
 | VFMSUBADD213PS       | VFMA           | FMA            |                | a7           | 0b11       | rrr        | VV1 0xA7 VL128 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
 | VFMSUBADD213PS       | VFMA           | FMA            |                | a7           | mm         | rrr        | VV1 0xA7 VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
 | VFMSUBADD213PS       | VFMA           | FMA            |                | a7           | 0b11       | rrr        | VV1 0xA7 VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
 | VFMSUBADD231PD       | VFMA           | AVX512EVEX     | AVX512F_128    | b7           | 0b11       | rrr        | EVV 0xB7 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFMSUBADD231PD       | VFMA           | AVX512EVEX     | AVX512F_128    | b7           | mm         | rrr        | EVV 0xB7 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFMSUBADD231PD       | VFMA           | AVX512EVEX     | AVX512F_256    | b7           | 0b11       | rrr        | EVV 0xB7 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
 | VFMSUBADD231PD       | VFMA           | AVX512EVEX     | AVX512F_256    | b7           | mm         | rrr        | EVV 0xB7 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFMSUBADD231PD       | VFMA           | AVX512EVEX     | AVX512F_512    | b7           | 0b11       | rrr        | EVV 0xB7 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VFMSUBADD231PD       | VFMA           | AVX512EVEX     | AVX512F_512    | b7           | 0b11       | rrr        | EVV 0xB7 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1     | REG0=ZMM_R3():rw:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VFMSUBADD231PD       | VFMA           | AVX512EVEX     | AVX512F_512    | b7           | mm         | rrr        | EVV 0xB7 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFMSUBADD231PD       | VFMA           | FMA            |                | b7           | mm         | rrr        | VV1 0xB7 VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
 | VFMSUBADD231PD       | VFMA           | FMA            |                | b7           | 0b11       | rrr        | VV1 0xB7 VL128 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
 | VFMSUBADD231PD       | VFMA           | FMA            |                | b7           | mm         | rrr        | VV1 0xB7 VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
 | VFMSUBADD231PD       | VFMA           | FMA            |                | b7           | 0b11       | rrr        | VV1 0xB7 VL256 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
 | VFMSUBADD231PS       | VFMA           | AVX512EVEX     | AVX512F_128    | b7           | 0b11       | rrr        | EVV 0xB7 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFMSUBADD231PS       | VFMA           | AVX512EVEX     | AVX512F_128    | b7           | mm         | rrr        | EVV 0xB7 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFMSUBADD231PS       | VFMA           | AVX512EVEX     | AVX512F_256    | b7           | 0b11       | rrr        | EVV 0xB7 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
 | VFMSUBADD231PS       | VFMA           | AVX512EVEX     | AVX512F_256    | b7           | mm         | rrr        | EVV 0xB7 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFMSUBADD231PS       | VFMA           | AVX512EVEX     | AVX512F_512    | b7           | 0b11       | rrr        | EVV 0xB7 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VFMSUBADD231PS       | VFMA           | AVX512EVEX     | AVX512F_512    | b7           | 0b11       | rrr        | EVV 0xB7 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0     | REG0=ZMM_R3():rw:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VFMSUBADD231PS       | VFMA           | AVX512EVEX     | AVX512F_512    | b7           | mm         | rrr        | EVV 0xB7 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFMSUBADD231PS       | VFMA           | FMA            |                | b7           | mm         | rrr        | VV1 0xB7 VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
 | VFMSUBADD231PS       | VFMA           | FMA            |                | b7           | 0b11       | rrr        | VV1 0xB7 VL128 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
 | VFMSUBADD231PS       | VFMA           | FMA            |                | b7           | mm         | rrr        | VV1 0xB7 VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
 | VFMSUBADD231PS       | VFMA           | FMA            |                | b7           | 0b11       | rrr        | VV1 0xB7 VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
 | VFMSUBADDPD          | FMA4           | FMA4           | FMA4           | 5f           | mm         | rrr        | VV1 0x5F V66 W0 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64 REG2=XMM_SE():r:dq:f64
 | VFMSUBADDPD          | FMA4           | FMA4           | FMA4           | 5f           | 0b11       | rrr        | VV1 0x5F V66 W0 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64 REG3=XMM_SE():r:dq:f64
 | VFMSUBADDPD          | FMA4           | FMA4           | FMA4           | 5f           | mm         | rrr        | VV1 0x5F V66 W1 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_SE():r:dq:f64 MEM0:r:dq:f64
 | VFMSUBADDPD          | FMA4           | FMA4           | FMA4           | 5f           | 0b11       | rrr        | VV1 0x5F V66 W1 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_SE():r:dq:f64 REG3=XMM_B():r:dq:f64
 | VFMSUBADDPD          | FMA4           | FMA4           | FMA4           | 5f           | mm         | rrr        | VV1 0x5F V66 W0 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64 REG2=YMM_SE():r:qq:f64
 | VFMSUBADDPD          | FMA4           | FMA4           | FMA4           | 5f           | 0b11       | rrr        | VV1 0x5F V66 W0 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64 REG3=YMM_SE():r:qq:f64
 | VFMSUBADDPD          | FMA4           | FMA4           | FMA4           | 5f           | mm         | rrr        | VV1 0x5F V66 W1 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_SE():r:qq:f64 MEM0:r:qq:f64
 | VFMSUBADDPD          | FMA4           | FMA4           | FMA4           | 5f           | 0b11       | rrr        | VV1 0x5F V66 W1 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_SE():r:qq:f64 REG3=YMM_B():r:qq:f64
 | VFMSUBADDPS          | FMA4           | FMA4           | FMA4           | 5e           | mm         | rrr        | VV1 0x5E V66 W0 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32 REG2=XMM_SE():r:dq:f32
 | VFMSUBADDPS          | FMA4           | FMA4           | FMA4           | 5e           | 0b11       | rrr        | VV1 0x5E V66 W0 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32 REG3=XMM_SE():r:dq:f32
 | VFMSUBADDPS          | FMA4           | FMA4           | FMA4           | 5e           | mm         | rrr        | VV1 0x5E V66 W1 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_SE():r:dq:f32 MEM0:r:dq:f32
 | VFMSUBADDPS          | FMA4           | FMA4           | FMA4           | 5e           | 0b11       | rrr        | VV1 0x5E V66 W1 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_SE():r:dq:f32 REG3=XMM_B():r:dq:f32
 | VFMSUBADDPS          | FMA4           | FMA4           | FMA4           | 5e           | mm         | rrr        | VV1 0x5E V66 W0 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32 REG2=YMM_SE():r:qq:f32
 | VFMSUBADDPS          | FMA4           | FMA4           | FMA4           | 5e           | 0b11       | rrr        | VV1 0x5E V66 W0 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32 REG3=YMM_SE():r:qq:f32
 | VFMSUBADDPS          | FMA4           | FMA4           | FMA4           | 5e           | mm         | rrr        | VV1 0x5E V66 W1 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_SE():r:qq:f32 MEM0:r:qq:f32
 | VFMSUBADDPS          | FMA4           | FMA4           | FMA4           | 5e           | 0b11       | rrr        | VV1 0x5E V66 W1 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_SE():r:qq:f32 REG3=YMM_B():r:qq:f32
 | VFMSUBPD             | FMA4           | FMA4           | FMA4           | 6d           | mm         | rrr        | VV1 0x6D V66 W0 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64 REG2=XMM_SE():r:dq:f64
 | VFMSUBPD             | FMA4           | FMA4           | FMA4           | 6d           | 0b11       | rrr        | VV1 0x6D V66 W0 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64 REG3=XMM_SE():r:dq:f64
 | VFMSUBPD             | FMA4           | FMA4           | FMA4           | 6d           | mm         | rrr        | VV1 0x6D V66 W1 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_SE():r:dq:f64 MEM0:r:dq:f64
 | VFMSUBPD             | FMA4           | FMA4           | FMA4           | 6d           | 0b11       | rrr        | VV1 0x6D V66 W1 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_SE():r:dq:f64 REG3=XMM_B():r:dq:f64
 | VFMSUBPD             | FMA4           | FMA4           | FMA4           | 6d           | mm         | rrr        | VV1 0x6D V66 W0 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64 REG2=YMM_SE():r:qq:f64
 | VFMSUBPD             | FMA4           | FMA4           | FMA4           | 6d           | 0b11       | rrr        | VV1 0x6D V66 W0 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64 REG3=YMM_SE():r:qq:f64
 | VFMSUBPD             | FMA4           | FMA4           | FMA4           | 6d           | mm         | rrr        | VV1 0x6D V66 W1 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_SE():r:qq:f64 MEM0:r:qq:f64
 | VFMSUBPD             | FMA4           | FMA4           | FMA4           | 6d           | 0b11       | rrr        | VV1 0x6D V66 W1 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_SE():r:qq:f64 REG3=YMM_B():r:qq:f64
 | VFMSUBPS             | FMA4           | FMA4           | FMA4           | 6c           | mm         | rrr        | VV1 0x6C V66 W0 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32 REG2=XMM_SE():r:dq:f32
 | VFMSUBPS             | FMA4           | FMA4           | FMA4           | 6c           | 0b11       | rrr        | VV1 0x6C V66 W0 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32 REG3=XMM_SE():r:dq:f32
 | VFMSUBPS             | FMA4           | FMA4           | FMA4           | 6c           | mm         | rrr        | VV1 0x6C V66 W1 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_SE():r:dq:f32 MEM0:r:dq:f32
 | VFMSUBPS             | FMA4           | FMA4           | FMA4           | 6c           | 0b11       | rrr        | VV1 0x6C V66 W1 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_SE():r:dq:f32 REG3=XMM_B():r:dq:f32
 | VFMSUBPS             | FMA4           | FMA4           | FMA4           | 6c           | mm         | rrr        | VV1 0x6C V66 W0 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32 REG2=YMM_SE():r:qq:f32
 | VFMSUBPS             | FMA4           | FMA4           | FMA4           | 6c           | 0b11       | rrr        | VV1 0x6C V66 W0 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32 REG3=YMM_SE():r:qq:f32
 | VFMSUBPS             | FMA4           | FMA4           | FMA4           | 6c           | mm         | rrr        | VV1 0x6C V66 W1 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_SE():r:qq:f32 MEM0:r:qq:f32
 | VFMSUBPS             | FMA4           | FMA4           | FMA4           | 6c           | 0b11       | rrr        | VV1 0x6C V66 W1 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_SE():r:qq:f32 REG3=YMM_B():r:qq:f32
 | VFMSUBSD             | FMA4           | FMA4           | FMA4           | 6f           | mm         | rrr        | VV1 0x6F V66 W0  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                             | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:q:f64 MEM0:r:q:f64 REG2=XMM_SE():r:q:f64
 | VFMSUBSD             | FMA4           | FMA4           | FMA4           | 6f           | 0b11       | rrr        | VV1 0x6F V66 W0  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                                    | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_B():r:q:f64 REG3=XMM_SE():r:q:f64
 | VFMSUBSD             | FMA4           | FMA4           | FMA4           | 6f           | mm         | rrr        | VV1 0x6F V66 W1  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                             | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_SE():r:q:f64 MEM0:r:q:f64
 | VFMSUBSD             | FMA4           | FMA4           | FMA4           | 6f           | 0b11       | rrr        | VV1 0x6F V66 W1  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                                    | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_SE():r:q:f64 REG3=XMM_B():r:q:f64
 | VFMSUBSS             | FMA4           | FMA4           | FMA4           | 6e           | mm         | rrr        | VV1 0x6E V66 W0  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                             | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:d:f32 MEM0:r:d:f32 REG2=XMM_SE():r:d:f32
 | VFMSUBSS             | FMA4           | FMA4           | FMA4           | 6e           | 0b11       | rrr        | VV1 0x6E V66 W0  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                                    | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_B():r:d:f32 REG3=XMM_SE():r:d:f32
 | VFMSUBSS             | FMA4           | FMA4           | FMA4           | 6e           | mm         | rrr        | VV1 0x6E V66 W1  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                             | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_SE():r:d:f32 MEM0:r:d:f32
 | VFMSUBSS             | FMA4           | FMA4           | FMA4           | 6e           | 0b11       | rrr        | VV1 0x6E V66 W1  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                                    | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_SE():r:d:f32 REG3=XMM_B():r:d:f32
 | VFNMADD132PD         | UFMA           | KNCE           |                | 9c           | mm         | rrr        | KVV 0x9C V0F38 V66  REXW=1 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT64()                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VFNMADD132PD         | UFMA           | KNCE           |                | 9c           | 0b11       | rrr        | KVV 0x9C V0F38 V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE64()                     | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=REGSWIZ
 | VFNMADD132PD         | UFMA           | KNCE           |                | 9c           | 0b11       | rrr        | KVV 0x9C V0F38 V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                   | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=ROUNDC:TXT=SAEC
 | VFNMADD132PD         | VFMA           | AVX512EVEX     | AVX512F_128    | 9c           | 0b11       | rrr        | EVV 0x9C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFNMADD132PD         | VFMA           | AVX512EVEX     | AVX512F_128    | 9c           | mm         | rrr        | EVV 0x9C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFNMADD132PD         | VFMA           | AVX512EVEX     | AVX512F_256    | 9c           | 0b11       | rrr        | EVV 0x9C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
 | VFNMADD132PD         | VFMA           | AVX512EVEX     | AVX512F_256    | 9c           | mm         | rrr        | EVV 0x9C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFNMADD132PD         | VFMA           | AVX512EVEX     | AVX512F_512    | 9c           | 0b11       | rrr        | EVV 0x9C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VFNMADD132PD         | VFMA           | AVX512EVEX     | AVX512F_512    | 9c           | 0b11       | rrr        | EVV 0x9C V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1     | REG0=ZMM_R3():rw:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VFNMADD132PD         | VFMA           | AVX512EVEX     | AVX512F_512    | 9c           | mm         | rrr        | EVV 0x9C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFNMADD132PD         | VFMA           | FMA            |                | 9c           | mm         | rrr        | VV1 0x9C VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
 | VFNMADD132PD         | VFMA           | FMA            |                | 9c           | 0b11       | rrr        | VV1 0x9C VL128 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
 | VFNMADD132PD         | VFMA           | FMA            |                | 9c           | mm         | rrr        | VV1 0x9C VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
 | VFNMADD132PD         | VFMA           | FMA            |                | 9c           | 0b11       | rrr        | VV1 0x9C VL256 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
 | VFNMADD132PS         | UFMA           | KNCE           |                | 9c           | mm         | rrr        | KVV 0x9C V0F38 V66  REXW=0 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT32()                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VFNMADD132PS         | UFMA           | KNCE           |                | 9c           | 0b11       | rrr        | KVV 0x9C V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VFNMADD132PS         | UFMA           | KNCE           |                | 9c           | 0b11       | rrr        | KVV 0x9C V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                   | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=ROUNDC:TXT=SAEC
 | VFNMADD132PS         | VFMA           | AVX512EVEX     | AVX512F_128    | 9c           | 0b11       | rrr        | EVV 0x9C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFNMADD132PS         | VFMA           | AVX512EVEX     | AVX512F_128    | 9c           | mm         | rrr        | EVV 0x9C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFNMADD132PS         | VFMA           | AVX512EVEX     | AVX512F_256    | 9c           | 0b11       | rrr        | EVV 0x9C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
 | VFNMADD132PS         | VFMA           | AVX512EVEX     | AVX512F_256    | 9c           | mm         | rrr        | EVV 0x9C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFNMADD132PS         | VFMA           | AVX512EVEX     | AVX512F_512    | 9c           | 0b11       | rrr        | EVV 0x9C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VFNMADD132PS         | VFMA           | AVX512EVEX     | AVX512F_512    | 9c           | 0b11       | rrr        | EVV 0x9C V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0     | REG0=ZMM_R3():rw:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VFNMADD132PS         | VFMA           | AVX512EVEX     | AVX512F_512    | 9c           | mm         | rrr        | EVV 0x9C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFNMADD132PS         | VFMA           | FMA            |                | 9c           | mm         | rrr        | VV1 0x9C VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
 | VFNMADD132PS         | VFMA           | FMA            |                | 9c           | 0b11       | rrr        | VV1 0x9C VL128 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
 | VFNMADD132PS         | VFMA           | FMA            |                | 9c           | mm         | rrr        | VV1 0x9C VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
 | VFNMADD132PS         | VFMA           | FMA            |                | 9c           | 0b11       | rrr        | VV1 0x9C VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
 | VFNMADD132SD         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | 9d           | 0b11       | rrr        | EVV 0x9D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                                       | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFNMADD132SD         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | 9d           | 0b11       | rrr        | EVV 0x9D V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1     | REG0=XMM_R3():rw:dq:f64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFNMADD132SD         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | 9d           | mm         | rrr        | EVV 0x9D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
 | VFNMADD132SD         | VFMA           | FMA            |                | 9d           | mm         | rrr        | VV1 0x9D V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                        | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 MEM0:r:q:f64
 | VFNMADD132SD         | VFMA           | FMA            |                | 9d           | 0b11       | rrr        | VV1 0x9D V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                               | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_B():r:q:f64
 | VFNMADD132SS         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | 9d           | 0b11       | rrr        | EVV 0x9D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                                       | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFNMADD132SS         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | 9d           | 0b11       | rrr        | EVV 0x9D V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W0     | REG0=XMM_R3():rw:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFNMADD132SS         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | 9d           | mm         | rrr        | EVV 0x9D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
 | VFNMADD132SS         | VFMA           | FMA            |                | 9d           | mm         | rrr        | VV1 0x9D  V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                       | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 MEM0:r:d:f32
 | VFNMADD132SS         | VFMA           | FMA            |                | 9d           | 0b11       | rrr        | VV1 0x9D  V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                              | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_B():r:d:f32
 | VFNMADD213PD         | UFMA           | KNCE           |                | ac           | mm         | rrr        | KVV 0xAC V0F38 V66  REXW=1 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT64()                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VFNMADD213PD         | UFMA           | KNCE           |                | ac           | 0b11       | rrr        | KVV 0xAC V0F38 V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE64()                     | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=REGSWIZ
 | VFNMADD213PD         | UFMA           | KNCE           |                | ac           | 0b11       | rrr        | KVV 0xAC V0F38 V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                   | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=ROUNDC:TXT=SAEC
 | VFNMADD213PD         | VFMA           | AVX512EVEX     | AVX512F_128    | ac           | 0b11       | rrr        | EVV 0xAC V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFNMADD213PD         | VFMA           | AVX512EVEX     | AVX512F_128    | ac           | mm         | rrr        | EVV 0xAC V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFNMADD213PD         | VFMA           | AVX512EVEX     | AVX512F_256    | ac           | 0b11       | rrr        | EVV 0xAC V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
 | VFNMADD213PD         | VFMA           | AVX512EVEX     | AVX512F_256    | ac           | mm         | rrr        | EVV 0xAC V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFNMADD213PD         | VFMA           | AVX512EVEX     | AVX512F_512    | ac           | 0b11       | rrr        | EVV 0xAC V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VFNMADD213PD         | VFMA           | AVX512EVEX     | AVX512F_512    | ac           | 0b11       | rrr        | EVV 0xAC V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1     | REG0=ZMM_R3():rw:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VFNMADD213PD         | VFMA           | AVX512EVEX     | AVX512F_512    | ac           | mm         | rrr        | EVV 0xAC V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFNMADD213PD         | VFMA           | FMA            |                | ac           | mm         | rrr        | VV1 0xAC VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64    MEM0:r:dq:f64
 | VFNMADD213PD         | VFMA           | FMA            |                | ac           | 0b11       | rrr        | VV1 0xAC VL128 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
 | VFNMADD213PD         | VFMA           | FMA            |                | ac           | mm         | rrr        | VV1 0xAC VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64    MEM0:r:qq:f64
 | VFNMADD213PD         | VFMA           | FMA            |                | ac           | 0b11       | rrr        | VV1 0xAC VL256 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
 | VFNMADD213PS         | UFMA           | KNCE           |                | ac           | mm         | rrr        | KVV 0xAC V0F38 V66  REXW=0 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT32()                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VFNMADD213PS         | UFMA           | KNCE           |                | ac           | 0b11       | rrr        | KVV 0xAC V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VFNMADD213PS         | UFMA           | KNCE           |                | ac           | 0b11       | rrr        | KVV 0xAC V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                   | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=ROUNDC:TXT=SAEC
 | VFNMADD213PS         | VFMA           | AVX512EVEX     | AVX512F_128    | ac           | 0b11       | rrr        | EVV 0xAC V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFNMADD213PS         | VFMA           | AVX512EVEX     | AVX512F_128    | ac           | mm         | rrr        | EVV 0xAC V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFNMADD213PS         | VFMA           | AVX512EVEX     | AVX512F_256    | ac           | 0b11       | rrr        | EVV 0xAC V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
 | VFNMADD213PS         | VFMA           | AVX512EVEX     | AVX512F_256    | ac           | mm         | rrr        | EVV 0xAC V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFNMADD213PS         | VFMA           | AVX512EVEX     | AVX512F_512    | ac           | 0b11       | rrr        | EVV 0xAC V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VFNMADD213PS         | VFMA           | AVX512EVEX     | AVX512F_512    | ac           | 0b11       | rrr        | EVV 0xAC V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0     | REG0=ZMM_R3():rw:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VFNMADD213PS         | VFMA           | AVX512EVEX     | AVX512F_512    | ac           | mm         | rrr        | EVV 0xAC V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFNMADD213PS         | VFMA           | FMA            |                | ac           | mm         | rrr        | VV1 0xAC VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
 | VFNMADD213PS         | VFMA           | FMA            |                | ac           | 0b11       | rrr        | VV1 0xAC VL128 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
 | VFNMADD213PS         | VFMA           | FMA            |                | ac           | mm         | rrr        | VV1 0xAC VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
 | VFNMADD213PS         | VFMA           | FMA            |                | ac           | 0b11       | rrr        | VV1 0xAC VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
 | VFNMADD213SD         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | ad           | 0b11       | rrr        | EVV 0xAD V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                                       | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFNMADD213SD         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | ad           | 0b11       | rrr        | EVV 0xAD V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1     | REG0=XMM_R3():rw:dq:f64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFNMADD213SD         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | ad           | mm         | rrr        | EVV 0xAD V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
 | VFNMADD213SD         | VFMA           | FMA            |                | ad           | mm         | rrr        | VV1 0xAD  V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                       | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64     MEM0:r:q:f64
 | VFNMADD213SD         | VFMA           | FMA            |                | ad           | 0b11       | rrr        | VV1 0xAD  V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                              | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_B():r:q:f64
 | VFNMADD213SS         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | ad           | 0b11       | rrr        | EVV 0xAD V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                                       | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFNMADD213SS         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | ad           | 0b11       | rrr        | EVV 0xAD V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W0     | REG0=XMM_R3():rw:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFNMADD213SS         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | ad           | mm         | rrr        | EVV 0xAD V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
 | VFNMADD213SS         | VFMA           | FMA            |                | ad           | mm         | rrr        | VV1 0xAD  V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                       | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32     MEM0:r:d:f32
 | VFNMADD213SS         | VFMA           | FMA            |                | ad           | 0b11       | rrr        | VV1 0xAD  V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                              | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_B():r:d:f32
 | VFNMADD231PD         | UFMA           | KNCE           |                | bc           | mm         | rrr        | KVV 0xBC V0F38 V66  REXW=1 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT64()                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VFNMADD231PD         | UFMA           | KNCE           |                | bc           | 0b11       | rrr        | KVV 0xBC V0F38 V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE64()                     | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=REGSWIZ
 | VFNMADD231PD         | UFMA           | KNCE           |                | bc           | 0b11       | rrr        | KVV 0xBC V0F38 V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                   | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=ROUNDC:TXT=SAEC
 | VFNMADD231PD         | VFMA           | AVX512EVEX     | AVX512F_128    | bc           | 0b11       | rrr        | EVV 0xBC V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFNMADD231PD         | VFMA           | AVX512EVEX     | AVX512F_128    | bc           | mm         | rrr        | EVV 0xBC V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFNMADD231PD         | VFMA           | AVX512EVEX     | AVX512F_256    | bc           | 0b11       | rrr        | EVV 0xBC V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
 | VFNMADD231PD         | VFMA           | AVX512EVEX     | AVX512F_256    | bc           | mm         | rrr        | EVV 0xBC V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFNMADD231PD         | VFMA           | AVX512EVEX     | AVX512F_512    | bc           | 0b11       | rrr        | EVV 0xBC V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VFNMADD231PD         | VFMA           | AVX512EVEX     | AVX512F_512    | bc           | 0b11       | rrr        | EVV 0xBC V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1     | REG0=ZMM_R3():rw:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VFNMADD231PD         | VFMA           | AVX512EVEX     | AVX512F_512    | bc           | mm         | rrr        | EVV 0xBC V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFNMADD231PD         | VFMA           | FMA            |                | bc           | mm         | rrr        | VV1 0xBC VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
 | VFNMADD231PD         | VFMA           | FMA            |                | bc           | 0b11       | rrr        | VV1 0xBC VL128 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
 | VFNMADD231PD         | VFMA           | FMA            |                | bc           | mm         | rrr        | VV1 0xBC VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
 | VFNMADD231PD         | VFMA           | FMA            |                | bc           | 0b11       | rrr        | VV1 0xBC VL256 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
 | VFNMADD231PS         | UFMA           | KNCE           |                | bc           | mm         | rrr        | KVV 0xBC V0F38 V66  REXW=0 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT32()                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VFNMADD231PS         | UFMA           | KNCE           |                | bc           | 0b11       | rrr        | KVV 0xBC V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VFNMADD231PS         | UFMA           | KNCE           |                | bc           | 0b11       | rrr        | KVV 0xBC V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                   | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=ROUNDC:TXT=SAEC
 | VFNMADD231PS         | VFMA           | AVX512EVEX     | AVX512F_128    | bc           | 0b11       | rrr        | EVV 0xBC V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFNMADD231PS         | VFMA           | AVX512EVEX     | AVX512F_128    | bc           | mm         | rrr        | EVV 0xBC V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFNMADD231PS         | VFMA           | AVX512EVEX     | AVX512F_256    | bc           | 0b11       | rrr        | EVV 0xBC V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
 | VFNMADD231PS         | VFMA           | AVX512EVEX     | AVX512F_256    | bc           | mm         | rrr        | EVV 0xBC V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFNMADD231PS         | VFMA           | AVX512EVEX     | AVX512F_512    | bc           | 0b11       | rrr        | EVV 0xBC V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VFNMADD231PS         | VFMA           | AVX512EVEX     | AVX512F_512    | bc           | 0b11       | rrr        | EVV 0xBC V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0     | REG0=ZMM_R3():rw:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VFNMADD231PS         | VFMA           | AVX512EVEX     | AVX512F_512    | bc           | mm         | rrr        | EVV 0xBC V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFNMADD231PS         | VFMA           | FMA            |                | bc           | mm         | rrr        | VV1 0xBC VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
 | VFNMADD231PS         | VFMA           | FMA            |                | bc           | 0b11       | rrr        | VV1 0xBC VL128 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
 | VFNMADD231PS         | VFMA           | FMA            |                | bc           | mm         | rrr        | VV1 0xBC VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
 | VFNMADD231PS         | VFMA           | FMA            |                | bc           | 0b11       | rrr        | VV1 0xBC VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
 | VFNMADD231SD         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | bd           | 0b11       | rrr        | EVV 0xBD V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                                       | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFNMADD231SD         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | bd           | 0b11       | rrr        | EVV 0xBD V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1     | REG0=XMM_R3():rw:dq:f64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFNMADD231SD         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | bd           | mm         | rrr        | EVV 0xBD V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
 | VFNMADD231SD         | VFMA           | FMA            |                | bd           | mm         | rrr        | VV1 0xBD V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                        | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 MEM0:r:q:f64
 | VFNMADD231SD         | VFMA           | FMA            |                | bd           | 0b11       | rrr        | VV1 0xBD V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                               | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_B():r:q:f64
 | VFNMADD231SS         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | bd           | 0b11       | rrr        | EVV 0xBD V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                                       | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFNMADD231SS         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | bd           | 0b11       | rrr        | EVV 0xBD V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W0     | REG0=XMM_R3():rw:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFNMADD231SS         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | bd           | mm         | rrr        | EVV 0xBD V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
 | VFNMADD231SS         | VFMA           | FMA            |                | bd           | mm         | rrr        | VV1 0xBD V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                        | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 MEM0:r:d:f32
 | VFNMADD231SS         | VFMA           | FMA            |                | bd           | 0b11       | rrr        | VV1 0xBD V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                               | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_B():r:d:f32
 | VFNMADDPD            | FMA4           | FMA4           | FMA4           | 79           | mm         | rrr        | VV1 0x79 V66 W0 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64 REG2=XMM_SE():r:dq:f64
 | VFNMADDPD            | FMA4           | FMA4           | FMA4           | 79           | 0b11       | rrr        | VV1 0x79 V66 W0 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64 REG3=XMM_SE():r:dq:f64
 | VFNMADDPD            | FMA4           | FMA4           | FMA4           | 79           | mm         | rrr        | VV1 0x79 V66 W1 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_SE():r:dq:f64 MEM0:r:dq:f64
 | VFNMADDPD            | FMA4           | FMA4           | FMA4           | 79           | 0b11       | rrr        | VV1 0x79 V66 W1 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_SE():r:dq:f64 REG3=XMM_B():r:dq:f64
 | VFNMADDPD            | FMA4           | FMA4           | FMA4           | 79           | mm         | rrr        | VV1 0x79 V66 W0 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64 REG2=YMM_SE():r:qq:f64
 | VFNMADDPD            | FMA4           | FMA4           | FMA4           | 79           | 0b11       | rrr        | VV1 0x79 V66 W0 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64 REG3=YMM_SE():r:qq:f64
 | VFNMADDPD            | FMA4           | FMA4           | FMA4           | 79           | mm         | rrr        | VV1 0x79 V66 W1 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_SE():r:qq:f64 MEM0:r:qq:f64
 | VFNMADDPD            | FMA4           | FMA4           | FMA4           | 79           | 0b11       | rrr        | VV1 0x79 V66 W1 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_SE():r:qq:f64 REG3=YMM_B():r:qq:f64
 | VFNMADDPS            | FMA4           | FMA4           | FMA4           | 78           | mm         | rrr        | VV1 0x78 V66 W0 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32 REG2=XMM_SE():r:dq:f32
 | VFNMADDPS            | FMA4           | FMA4           | FMA4           | 78           | 0b11       | rrr        | VV1 0x78 V66 W0 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32 REG3=XMM_SE():r:dq:f32
 | VFNMADDPS            | FMA4           | FMA4           | FMA4           | 78           | mm         | rrr        | VV1 0x78 V66 W1 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_SE():r:dq:f32 MEM0:r:dq:f32
 | VFNMADDPS            | FMA4           | FMA4           | FMA4           | 78           | 0b11       | rrr        | VV1 0x78 V66 W1 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_SE():r:dq:f32 REG3=XMM_B():r:dq:f32
 | VFNMADDPS            | FMA4           | FMA4           | FMA4           | 78           | mm         | rrr        | VV1 0x78 V66 W0 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32 REG2=YMM_SE():r:qq:f32
 | VFNMADDPS            | FMA4           | FMA4           | FMA4           | 78           | 0b11       | rrr        | VV1 0x78 V66 W0 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32 REG3=YMM_SE():r:qq:f32
 | VFNMADDPS            | FMA4           | FMA4           | FMA4           | 78           | mm         | rrr        | VV1 0x78 V66 W1 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_SE():r:qq:f32 MEM0:r:qq:f32
 | VFNMADDPS            | FMA4           | FMA4           | FMA4           | 78           | 0b11       | rrr        | VV1 0x78 V66 W1 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_SE():r:qq:f32 REG3=YMM_B():r:qq:f32
 | VFNMADDSD            | FMA4           | FMA4           | FMA4           | 7b           | mm         | rrr        | VV1 0x7B V66 W0  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                             | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:q:f64 MEM0:r:q:f64 REG2=XMM_SE():r:q:f64
 | VFNMADDSD            | FMA4           | FMA4           | FMA4           | 7b           | 0b11       | rrr        | VV1 0x7B V66 W0  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                                    | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_B():r:q:f64 REG3=XMM_SE():r:q:f64
 | VFNMADDSD            | FMA4           | FMA4           | FMA4           | 7b           | mm         | rrr        | VV1 0x7B V66 W1  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                             | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_SE():r:q:f64 MEM0:r:q:f64
 | VFNMADDSD            | FMA4           | FMA4           | FMA4           | 7b           | 0b11       | rrr        | VV1 0x7B V66 W1  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                                    | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_SE():r:q:f64 REG3=XMM_B():r:q:f64
 | VFNMADDSS            | FMA4           | FMA4           | FMA4           | 7a           | mm         | rrr        | VV1 0x7A V66 W0  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                             | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:d:f32 MEM0:r:d:f32 REG2=XMM_SE():r:d:f32
 | VFNMADDSS            | FMA4           | FMA4           | FMA4           | 7a           | 0b11       | rrr        | VV1 0x7A V66 W0  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                                    | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_B():r:d:f32 REG3=XMM_SE():r:d:f32
 | VFNMADDSS            | FMA4           | FMA4           | FMA4           | 7a           | mm         | rrr        | VV1 0x7A V66 W1  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                             | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_SE():r:d:f32 MEM0:r:d:f32
 | VFNMADDSS            | FMA4           | FMA4           | FMA4           | 7a           | 0b11       | rrr        | VV1 0x7A V66 W1  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                                    | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_SE():r:d:f32 REG3=XMM_B():r:d:f32
 | VFNMSUB132PD         | UFMA           | KNCE           |                | 9e           | mm         | rrr        | KVV 0x9E V0F38 V66  REXW=1 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT64()                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VFNMSUB132PD         | UFMA           | KNCE           |                | 9e           | 0b11       | rrr        | KVV 0x9E V0F38 V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE64()                     | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=REGSWIZ
 | VFNMSUB132PD         | UFMA           | KNCE           |                | 9e           | 0b11       | rrr        | KVV 0x9E V0F38 V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                   | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=ROUNDC:TXT=SAEC
 | VFNMSUB132PD         | VFMA           | AVX512EVEX     | AVX512F_128    | 9e           | 0b11       | rrr        | EVV 0x9E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFNMSUB132PD         | VFMA           | AVX512EVEX     | AVX512F_128    | 9e           | mm         | rrr        | EVV 0x9E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFNMSUB132PD         | VFMA           | AVX512EVEX     | AVX512F_256    | 9e           | 0b11       | rrr        | EVV 0x9E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
 | VFNMSUB132PD         | VFMA           | AVX512EVEX     | AVX512F_256    | 9e           | mm         | rrr        | EVV 0x9E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFNMSUB132PD         | VFMA           | AVX512EVEX     | AVX512F_512    | 9e           | 0b11       | rrr        | EVV 0x9E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VFNMSUB132PD         | VFMA           | AVX512EVEX     | AVX512F_512    | 9e           | 0b11       | rrr        | EVV 0x9E V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1     | REG0=ZMM_R3():rw:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VFNMSUB132PD         | VFMA           | AVX512EVEX     | AVX512F_512    | 9e           | mm         | rrr        | EVV 0x9E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFNMSUB132PD         | VFMA           | FMA            |                | 9e           | mm         | rrr        | VV1 0x9E VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
 | VFNMSUB132PD         | VFMA           | FMA            |                | 9e           | 0b11       | rrr        | VV1 0x9E VL128 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
 | VFNMSUB132PD         | VFMA           | FMA            |                | 9e           | mm         | rrr        | VV1 0x9E VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
 | VFNMSUB132PD         | VFMA           | FMA            |                | 9e           | 0b11       | rrr        | VV1 0x9E VL256 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
 | VFNMSUB132PS         | UFMA           | KNCE           |                | 9e           | mm         | rrr        | KVV 0x9E V0F38 V66  REXW=0 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT32()                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VFNMSUB132PS         | UFMA           | KNCE           |                | 9e           | 0b11       | rrr        | KVV 0x9E V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VFNMSUB132PS         | UFMA           | KNCE           |                | 9e           | 0b11       | rrr        | KVV 0x9E V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                   | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=ROUNDC:TXT=SAEC
 | VFNMSUB132PS         | VFMA           | AVX512EVEX     | AVX512F_128    | 9e           | 0b11       | rrr        | EVV 0x9E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFNMSUB132PS         | VFMA           | AVX512EVEX     | AVX512F_128    | 9e           | mm         | rrr        | EVV 0x9E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFNMSUB132PS         | VFMA           | AVX512EVEX     | AVX512F_256    | 9e           | 0b11       | rrr        | EVV 0x9E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
 | VFNMSUB132PS         | VFMA           | AVX512EVEX     | AVX512F_256    | 9e           | mm         | rrr        | EVV 0x9E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFNMSUB132PS         | VFMA           | AVX512EVEX     | AVX512F_512    | 9e           | 0b11       | rrr        | EVV 0x9E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VFNMSUB132PS         | VFMA           | AVX512EVEX     | AVX512F_512    | 9e           | 0b11       | rrr        | EVV 0x9E V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0     | REG0=ZMM_R3():rw:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VFNMSUB132PS         | VFMA           | AVX512EVEX     | AVX512F_512    | 9e           | mm         | rrr        | EVV 0x9E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFNMSUB132PS         | VFMA           | FMA            |                | 9e           | mm         | rrr        | VV1 0x9E VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
 | VFNMSUB132PS         | VFMA           | FMA            |                | 9e           | 0b11       | rrr        | VV1 0x9E VL128 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
 | VFNMSUB132PS         | VFMA           | FMA            |                | 9e           | mm         | rrr        | VV1 0x9E VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
 | VFNMSUB132PS         | VFMA           | FMA            |                | 9e           | 0b11       | rrr        | VV1 0x9E VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
 | VFNMSUB132SD         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | 9f           | 0b11       | rrr        | EVV 0x9F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                                       | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFNMSUB132SD         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | 9f           | 0b11       | rrr        | EVV 0x9F V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1     | REG0=XMM_R3():rw:dq:f64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFNMSUB132SD         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | 9f           | mm         | rrr        | EVV 0x9F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
 | VFNMSUB132SD         | VFMA           | FMA            |                | 9f           | mm         | rrr        | VV1 0x9F V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                        | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 MEM0:r:q:f64
 | VFNMSUB132SD         | VFMA           | FMA            |                | 9f           | 0b11       | rrr        | VV1 0x9F V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                               | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_B():r:q:f64
 | VFNMSUB132SS         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | 9f           | 0b11       | rrr        | EVV 0x9F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                                       | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFNMSUB132SS         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | 9f           | 0b11       | rrr        | EVV 0x9F V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W0     | REG0=XMM_R3():rw:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFNMSUB132SS         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | 9f           | mm         | rrr        | EVV 0x9F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
 | VFNMSUB132SS         | VFMA           | FMA            |                | 9f           | mm         | rrr        | VV1 0x9F  V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                       | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 MEM0:r:d:f32
 | VFNMSUB132SS         | VFMA           | FMA            |                | 9f           | 0b11       | rrr        | VV1 0x9F  V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                              | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_B():r:d:f32
 | VFNMSUB213PD         | UFMA           | KNCE           |                | ae           | mm         | rrr        | KVV 0xAE V0F38 V66  REXW=1 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT64()                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VFNMSUB213PD         | UFMA           | KNCE           |                | ae           | 0b11       | rrr        | KVV 0xAE V0F38 V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE64()                     | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=REGSWIZ
 | VFNMSUB213PD         | UFMA           | KNCE           |                | ae           | 0b11       | rrr        | KVV 0xAE V0F38 V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                   | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=ROUNDC:TXT=SAEC
 | VFNMSUB213PD         | VFMA           | AVX512EVEX     | AVX512F_128    | ae           | 0b11       | rrr        | EVV 0xAE V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFNMSUB213PD         | VFMA           | AVX512EVEX     | AVX512F_128    | ae           | mm         | rrr        | EVV 0xAE V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFNMSUB213PD         | VFMA           | AVX512EVEX     | AVX512F_256    | ae           | 0b11       | rrr        | EVV 0xAE V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
 | VFNMSUB213PD         | VFMA           | AVX512EVEX     | AVX512F_256    | ae           | mm         | rrr        | EVV 0xAE V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFNMSUB213PD         | VFMA           | AVX512EVEX     | AVX512F_512    | ae           | 0b11       | rrr        | EVV 0xAE V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VFNMSUB213PD         | VFMA           | AVX512EVEX     | AVX512F_512    | ae           | 0b11       | rrr        | EVV 0xAE V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1     | REG0=ZMM_R3():rw:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VFNMSUB213PD         | VFMA           | AVX512EVEX     | AVX512F_512    | ae           | mm         | rrr        | EVV 0xAE V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFNMSUB213PD         | VFMA           | FMA            |                | ae           | mm         | rrr        | VV1 0xAE VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64    MEM0:r:dq:f64
 | VFNMSUB213PD         | VFMA           | FMA            |                | ae           | 0b11       | rrr        | VV1 0xAE VL128 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
 | VFNMSUB213PD         | VFMA           | FMA            |                | ae           | mm         | rrr        | VV1 0xAE VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64    MEM0:r:qq:f64
 | VFNMSUB213PD         | VFMA           | FMA            |                | ae           | 0b11       | rrr        | VV1 0xAE VL256 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
 | VFNMSUB213PS         | UFMA           | KNCE           |                | ae           | mm         | rrr        | KVV 0xAE V0F38 V66  REXW=0 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT32()                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VFNMSUB213PS         | UFMA           | KNCE           |                | ae           | 0b11       | rrr        | KVV 0xAE V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VFNMSUB213PS         | UFMA           | KNCE           |                | ae           | 0b11       | rrr        | KVV 0xAE V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                   | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=ROUNDC:TXT=SAEC
 | VFNMSUB213PS         | VFMA           | AVX512EVEX     | AVX512F_128    | ae           | 0b11       | rrr        | EVV 0xAE V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFNMSUB213PS         | VFMA           | AVX512EVEX     | AVX512F_128    | ae           | mm         | rrr        | EVV 0xAE V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFNMSUB213PS         | VFMA           | AVX512EVEX     | AVX512F_256    | ae           | 0b11       | rrr        | EVV 0xAE V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
 | VFNMSUB213PS         | VFMA           | AVX512EVEX     | AVX512F_256    | ae           | mm         | rrr        | EVV 0xAE V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFNMSUB213PS         | VFMA           | AVX512EVEX     | AVX512F_512    | ae           | 0b11       | rrr        | EVV 0xAE V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VFNMSUB213PS         | VFMA           | AVX512EVEX     | AVX512F_512    | ae           | 0b11       | rrr        | EVV 0xAE V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0     | REG0=ZMM_R3():rw:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VFNMSUB213PS         | VFMA           | AVX512EVEX     | AVX512F_512    | ae           | mm         | rrr        | EVV 0xAE V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFNMSUB213PS         | VFMA           | FMA            |                | ae           | mm         | rrr        | VV1 0xAE VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
 | VFNMSUB213PS         | VFMA           | FMA            |                | ae           | 0b11       | rrr        | VV1 0xAE VL128 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
 | VFNMSUB213PS         | VFMA           | FMA            |                | ae           | mm         | rrr        | VV1 0xAE VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
 | VFNMSUB213PS         | VFMA           | FMA            |                | ae           | 0b11       | rrr        | VV1 0xAE VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
 | VFNMSUB213SD         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | af           | 0b11       | rrr        | EVV 0xAF V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                                       | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFNMSUB213SD         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | af           | 0b11       | rrr        | EVV 0xAF V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1     | REG0=XMM_R3():rw:dq:f64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFNMSUB213SD         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | af           | mm         | rrr        | EVV 0xAF V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
 | VFNMSUB213SD         | VFMA           | FMA            |                | af           | mm         | rrr        | VV1 0xAF  V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                       | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64     MEM0:r:q:f64
 | VFNMSUB213SD         | VFMA           | FMA            |                | af           | 0b11       | rrr        | VV1 0xAF  V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                              | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_B():r:q:f64
 | VFNMSUB213SS         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | af           | 0b11       | rrr        | EVV 0xAF V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                                       | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFNMSUB213SS         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | af           | 0b11       | rrr        | EVV 0xAF V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W0     | REG0=XMM_R3():rw:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFNMSUB213SS         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | af           | mm         | rrr        | EVV 0xAF V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
 | VFNMSUB213SS         | VFMA           | FMA            |                | af           | mm         | rrr        | VV1 0xAF  V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                       | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32     MEM0:r:d:f32
 | VFNMSUB213SS         | VFMA           | FMA            |                | af           | 0b11       | rrr        | VV1 0xAF  V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                              | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_B():r:d:f32
 | VFNMSUB231PD         | UFMA           | KNCE           |                | be           | mm         | rrr        | KVV 0xBE V0F38 V66  REXW=1 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT64()                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VFNMSUB231PD         | UFMA           | KNCE           |                | be           | 0b11       | rrr        | KVV 0xBE V0F38 V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE64()                     | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=REGSWIZ
 | VFNMSUB231PD         | UFMA           | KNCE           |                | be           | 0b11       | rrr        | KVV 0xBE V0F38 V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                   | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=ROUNDC:TXT=SAEC
 | VFNMSUB231PD         | VFMA           | AVX512EVEX     | AVX512F_128    | be           | 0b11       | rrr        | EVV 0xBE V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFNMSUB231PD         | VFMA           | AVX512EVEX     | AVX512F_128    | be           | mm         | rrr        | EVV 0xBE V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFNMSUB231PD         | VFMA           | AVX512EVEX     | AVX512F_256    | be           | 0b11       | rrr        | EVV 0xBE V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
 | VFNMSUB231PD         | VFMA           | AVX512EVEX     | AVX512F_256    | be           | mm         | rrr        | EVV 0xBE V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFNMSUB231PD         | VFMA           | AVX512EVEX     | AVX512F_512    | be           | 0b11       | rrr        | EVV 0xBE V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VFNMSUB231PD         | VFMA           | AVX512EVEX     | AVX512F_512    | be           | 0b11       | rrr        | EVV 0xBE V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1     | REG0=ZMM_R3():rw:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VFNMSUB231PD         | VFMA           | AVX512EVEX     | AVX512F_512    | be           | mm         | rrr        | EVV 0xBE V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VFNMSUB231PD         | VFMA           | FMA            |                | be           | mm         | rrr        | VV1 0xBE VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
 | VFNMSUB231PD         | VFMA           | FMA            |                | be           | 0b11       | rrr        | VV1 0xBE VL128 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
 | VFNMSUB231PD         | VFMA           | FMA            |                | be           | mm         | rrr        | VV1 0xBE VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
 | VFNMSUB231PD         | VFMA           | FMA            |                | be           | 0b11       | rrr        | VV1 0xBE VL256 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
 | VFNMSUB231PS         | UFMA           | KNCE           |                | be           | mm         | rrr        | KVV 0xBE V0F38 V66  REXW=0 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT32()                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VFNMSUB231PS         | UFMA           | KNCE           |                | be           | 0b11       | rrr        | KVV 0xBE V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VFNMSUB231PS         | UFMA           | KNCE           |                | be           | 0b11       | rrr        | KVV 0xBE V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                   | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=ROUNDC:TXT=SAEC
 | VFNMSUB231PS         | VFMA           | AVX512EVEX     | AVX512F_128    | be           | 0b11       | rrr        | EVV 0xBE V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFNMSUB231PS         | VFMA           | AVX512EVEX     | AVX512F_128    | be           | mm         | rrr        | EVV 0xBE V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFNMSUB231PS         | VFMA           | AVX512EVEX     | AVX512F_256    | be           | 0b11       | rrr        | EVV 0xBE V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
 | VFNMSUB231PS         | VFMA           | AVX512EVEX     | AVX512F_256    | be           | mm         | rrr        | EVV 0xBE V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFNMSUB231PS         | VFMA           | AVX512EVEX     | AVX512F_512    | be           | 0b11       | rrr        | EVV 0xBE V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VFNMSUB231PS         | VFMA           | AVX512EVEX     | AVX512F_512    | be           | 0b11       | rrr        | EVV 0xBE V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0     | REG0=ZMM_R3():rw:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VFNMSUB231PS         | VFMA           | AVX512EVEX     | AVX512F_512    | be           | mm         | rrr        | EVV 0xBE V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VFNMSUB231PS         | VFMA           | FMA            |                | be           | mm         | rrr        | VV1 0xBE VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
 | VFNMSUB231PS         | VFMA           | FMA            |                | be           | 0b11       | rrr        | VV1 0xBE VL128 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
 | VFNMSUB231PS         | VFMA           | FMA            |                | be           | mm         | rrr        | VV1 0xBE VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
 | VFNMSUB231PS         | VFMA           | FMA            |                | be           | 0b11       | rrr        | VV1 0xBE VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                         | REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
 | VFNMSUB231SD         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | bf           | 0b11       | rrr        | EVV 0xBF V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                                       | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFNMSUB231SD         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | bf           | 0b11       | rrr        | EVV 0xBF V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1     | REG0=XMM_R3():rw:dq:f64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VFNMSUB231SD         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | bf           | mm         | rrr        | EVV 0xBF V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
 | VFNMSUB231SD         | VFMA           | FMA            |                | bf           | mm         | rrr        | VV1 0xBF V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                        | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 MEM0:r:q:f64
 | VFNMSUB231SD         | VFMA           | FMA            |                | bf           | 0b11       | rrr        | VV1 0xBF V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                               | REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_B():r:q:f64
 | VFNMSUB231SS         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | bf           | 0b11       | rrr        | EVV 0xBF V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                                       | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFNMSUB231SS         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | bf           | 0b11       | rrr        | EVV 0xBF V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W0     | REG0=XMM_R3():rw:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VFNMSUB231SS         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | bf           | mm         | rrr        | EVV 0xBF V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
 | VFNMSUB231SS         | VFMA           | FMA            |                | bf           | mm         | rrr        | VV1 0xBF V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                        | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 MEM0:r:d:f32
 | VFNMSUB231SS         | VFMA           | FMA            |                | bf           | 0b11       | rrr        | VV1 0xBF V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                               | REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_B():r:d:f32
 | VFNMSUBPD            | FMA4           | FMA4           | FMA4           | 7d           | mm         | rrr        | VV1 0x7D V66 W0 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64 REG2=XMM_SE():r:dq:f64
 | VFNMSUBPD            | FMA4           | FMA4           | FMA4           | 7d           | 0b11       | rrr        | VV1 0x7D V66 W0 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64 REG3=XMM_SE():r:dq:f64
 | VFNMSUBPD            | FMA4           | FMA4           | FMA4           | 7d           | mm         | rrr        | VV1 0x7D V66 W1 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_SE():r:dq:f64 MEM0:r:dq:f64
 | VFNMSUBPD            | FMA4           | FMA4           | FMA4           | 7d           | 0b11       | rrr        | VV1 0x7D V66 W1 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_SE():r:dq:f64 REG3=XMM_B():r:dq:f64
 | VFNMSUBPD            | FMA4           | FMA4           | FMA4           | 7d           | mm         | rrr        | VV1 0x7D V66 W0 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64 REG2=YMM_SE():r:qq:f64
 | VFNMSUBPD            | FMA4           | FMA4           | FMA4           | 7d           | 0b11       | rrr        | VV1 0x7D V66 W0 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64 REG3=YMM_SE():r:qq:f64
 | VFNMSUBPD            | FMA4           | FMA4           | FMA4           | 7d           | mm         | rrr        | VV1 0x7D V66 W1 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_SE():r:qq:f64 MEM0:r:qq:f64
 | VFNMSUBPD            | FMA4           | FMA4           | FMA4           | 7d           | 0b11       | rrr        | VV1 0x7D V66 W1 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_SE():r:qq:f64 REG3=YMM_B():r:qq:f64
 | VFNMSUBPS            | FMA4           | FMA4           | FMA4           | 7c           | mm         | rrr        | VV1 0x7C V66 W0 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32 REG2=XMM_SE():r:dq:f32
 | VFNMSUBPS            | FMA4           | FMA4           | FMA4           | 7c           | 0b11       | rrr        | VV1 0x7C V66 W0 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32 REG3=XMM_SE():r:dq:f32
 | VFNMSUBPS            | FMA4           | FMA4           | FMA4           | 7c           | mm         | rrr        | VV1 0x7C V66 W1 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_SE():r:dq:f32 MEM0:r:dq:f32
 | VFNMSUBPS            | FMA4           | FMA4           | FMA4           | 7c           | 0b11       | rrr        | VV1 0x7C V66 W1 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_SE():r:dq:f32 REG3=XMM_B():r:dq:f32
 | VFNMSUBPS            | FMA4           | FMA4           | FMA4           | 7c           | mm         | rrr        | VV1 0x7C V66 W0 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32 REG2=YMM_SE():r:qq:f32
 | VFNMSUBPS            | FMA4           | FMA4           | FMA4           | 7c           | 0b11       | rrr        | VV1 0x7C V66 W0 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32 REG3=YMM_SE():r:qq:f32
 | VFNMSUBPS            | FMA4           | FMA4           | FMA4           | 7c           | mm         | rrr        | VV1 0x7C V66 W1 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_SE():r:qq:f32 MEM0:r:qq:f32
 | VFNMSUBPS            | FMA4           | FMA4           | FMA4           | 7c           | 0b11       | rrr        | VV1 0x7C V66 W1 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_SE():r:qq:f32 REG3=YMM_B():r:qq:f32
 | VFNMSUBSD            | FMA4           | FMA4           | FMA4           | 7f           | mm         | rrr        | VV1 0x7F V66 W0  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                             | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:q:f64 MEM0:r:q:f64 REG2=XMM_SE():r:q:f64
 | VFNMSUBSD            | FMA4           | FMA4           | FMA4           | 7f           | 0b11       | rrr        | VV1 0x7F V66 W0  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                                    | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_B():r:q:f64 REG3=XMM_SE():r:q:f64
 | VFNMSUBSD            | FMA4           | FMA4           | FMA4           | 7f           | mm         | rrr        | VV1 0x7F V66 W1  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                             | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_SE():r:q:f64 MEM0:r:q:f64
 | VFNMSUBSD            | FMA4           | FMA4           | FMA4           | 7f           | 0b11       | rrr        | VV1 0x7F V66 W1  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                                    | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_SE():r:q:f64 REG3=XMM_B():r:q:f64
 | VFNMSUBSS            | FMA4           | FMA4           | FMA4           | 7e           | mm         | rrr        | VV1 0x7E V66 W0  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                             | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:d:f32 MEM0:r:d:f32 REG2=XMM_SE():r:d:f32
 | VFNMSUBSS            | FMA4           | FMA4           | FMA4           | 7e           | 0b11       | rrr        | VV1 0x7E V66 W0  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                                    | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_B():r:d:f32 REG3=XMM_SE():r:d:f32
 | VFNMSUBSS            | FMA4           | FMA4           | FMA4           | 7e           | mm         | rrr        | VV1 0x7E V66 W1  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                             | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_SE():r:d:f32 MEM0:r:d:f32
 | VFNMSUBSS            | FMA4           | FMA4           | FMA4           | 7e           | 0b11       | rrr        | VV1 0x7E V66 W1  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                                    | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_SE():r:d:f32 REG3=XMM_B():r:d:f32
 | VFPCLASSPD           | AVX512         | AVX512EVEX     | AVX512DQ_128   | 66           | 0b11       | rrr        | EVV 0x66 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR  ZEROING=0 UIMM8()     | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_B3():r:dq:f64 IMM0:r:b
 | VFPCLASSPD           | AVX512         | AVX512EVEX     | AVX512DQ_128   | 66           | mm         | rrr        | EVV 0x66 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
 | VFPCLASSPD           | AVX512         | AVX512EVEX     | AVX512DQ_256   | 66           | 0b11       | rrr        | EVV 0x66 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR  ZEROING=0 UIMM8()     | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_B3():r:qq:f64 IMM0:r:b
 | VFPCLASSPD           | AVX512         | AVX512EVEX     | AVX512DQ_256   | 66           | mm         | rrr        | EVV 0x66 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
 | VFPCLASSPD           | AVX512         | AVX512EVEX     | AVX512DQ_512   | 66           | 0b11       | rrr        | EVV 0x66 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR  ZEROING=0 UIMM8()     | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_B3():r:zf64 IMM0:r:b
 | VFPCLASSPD           | AVX512         | AVX512EVEX     | AVX512DQ_512   | 66           | mm         | rrr        | EVV 0x66 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
 | VFPCLASSPS           | AVX512         | AVX512EVEX     | AVX512DQ_128   | 66           | 0b11       | rrr        | EVV 0x66 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR  ZEROING=0 UIMM8()     | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_B3():r:dq:f32 IMM0:r:b
 | VFPCLASSPS           | AVX512         | AVX512EVEX     | AVX512DQ_128   | 66           | mm         | rrr        | EVV 0x66 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
 | VFPCLASSPS           | AVX512         | AVX512EVEX     | AVX512DQ_256   | 66           | 0b11       | rrr        | EVV 0x66 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR  ZEROING=0 UIMM8()     | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_B3():r:qq:f32 IMM0:r:b
 | VFPCLASSPS           | AVX512         | AVX512EVEX     | AVX512DQ_256   | 66           | mm         | rrr        | EVV 0x66 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
 | VFPCLASSPS           | AVX512         | AVX512EVEX     | AVX512DQ_512   | 66           | 0b11       | rrr        | EVV 0x66 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR  ZEROING=0 UIMM8()     | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_B3():r:zf32 IMM0:r:b
 | VFPCLASSPS           | AVX512         | AVX512EVEX     | AVX512DQ_512   | 66           | mm         | rrr        | EVV 0x66 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
 | VFPCLASSSD           | AVX512         | AVX512EVEX     | AVX512DQ_SCALAR | 67           | 0b11       | rrr        | EVV 0x67 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1  NOEVSR  ZEROING=0 UIMM8()            | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_B3():r:dq:f64 IMM0:r:b
 | VFPCLASSSD           | AVX512         | AVX512EVEX     | AVX512DQ_SCALAR | 67           | mm         | rrr        | EVV 0x67 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1  NOEVSR  ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_SCALAR() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw MEM0:r:q:f64 IMM0:r:b
 | VFPCLASSSS           | AVX512         | AVX512EVEX     | AVX512DQ_SCALAR | 67           | 0b11       | rrr        | EVV 0x67 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0  NOEVSR  ZEROING=0 UIMM8()            | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_B3():r:dq:f32 IMM0:r:b
 | VFPCLASSSS           | AVX512         | AVX512EVEX     | AVX512DQ_SCALAR | 67           | mm         | rrr        | EVV 0x67 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0  NOEVSR  ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_SCALAR() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw MEM0:r:d:f32 IMM0:r:b
 | VFRCZPD              | XOP            | XOP            | XOP            | 81           | mm         | rrr        | XOPV 0x81 VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                           | REG0=XMM_R():w:dq:f64 MEM0:r:dq:f64
 | VFRCZPD              | XOP            | XOP            | XOP            | 81           | 0b11       | rrr        | XOPV 0x81 VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                  | REG0=XMM_R():w:dq:f64 REG1=XMM_B():r:dq:f64
 | VFRCZPD              | XOP            | XOP            | XOP            | 81           | mm         | rrr        | XOPV 0x81 VNP W0 VL256 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                           | REG0=YMM_R():w:qq:f64 MEM0:r:qq:f64
 | VFRCZPD              | XOP            | XOP            | XOP            | 81           | 0b11       | rrr        | XOPV 0x81 VNP W0 VL256 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                  | REG0=YMM_R():w:qq:f64 REG1=YMM_B():r:qq:f64
 | VFRCZPS              | XOP            | XOP            | XOP            | 80           | mm         | rrr        | XOPV 0x80 VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                           | REG0=XMM_R():w:dq:f32 MEM0:r:dq:f32
 | VFRCZPS              | XOP            | XOP            | XOP            | 80           | 0b11       | rrr        | XOPV 0x80 VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                  | REG0=XMM_R():w:dq:f32 REG1=XMM_B():r:dq:f32
 | VFRCZPS              | XOP            | XOP            | XOP            | 80           | mm         | rrr        | XOPV 0x80 VNP W0 VL256 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                           | REG0=YMM_R():w:qq:f32 MEM0:r:qq:f32
 | VFRCZPS              | XOP            | XOP            | XOP            | 80           | 0b11       | rrr        | XOPV 0x80 VNP W0 VL256 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                  | REG0=YMM_R():w:qq:f32 REG1=YMM_B():r:qq:f32
 | VFRCZSD              | XOP            | XOP            | XOP            | 83           | mm         | rrr        | XOPV 0x83 VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                           | REG0=XMM_R():w:dq:f64 MEM0:r:q:f64
 | VFRCZSD              | XOP            | XOP            | XOP            | 83           | 0b11       | rrr        | XOPV 0x83 VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                  | REG0=XMM_R():w:dq:f64 REG1=XMM_B():r:q:f64
 | VFRCZSS              | XOP            | XOP            | XOP            | 82           | mm         | rrr        | XOPV 0x82 VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                           | REG0=XMM_R():w:dq:f32 MEM0:r:d:f32
 | VFRCZSS              | XOP            | XOP            | XOP            | 82           | 0b11       | rrr        | XOPV 0x82 VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                  | REG0=XMM_R():w:dq:f32 REG1=XMM_B():r:d:f32
 | VGATHERDPD           | AVX2GATHER     | AVX2GATHER     |                | 92           | mm         | rrr        | VV1 0x92   VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] RM=4 VMODRM_XMM() eanot16              | REG0=YMM_R():crw:qq:f64   MEM0:r:q:f64 REG1=YMM_N():rw:qq:i64
 | VGATHERDPD           | AVX2GATHER     | AVX2GATHER     |                | 92           | mm         | rrr        | VV1 0x92   VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] RM=4 VMODRM_XMM() eanot16              | REG0=XMM_R():crw:dq:f64   MEM0:r:q:f64 REG1=XMM_N():rw:dq:i64
 | VGATHERDPD           | GATHER         | AVX512EVEX     | AVX512F_128    | 92           | mm         | rrr        | EVV 0x92 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL128  W1 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | REG0=XMM_R3():w:dq:f64 REG1=MASKNOT0():rw:mskw MEM0:r:q:f64
 | VGATHERDPD           | GATHER         | AVX512EVEX     | AVX512F_256    | 92           | mm         | rrr        | EVV 0x92 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL256  W1 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | REG0=YMM_R3():w:qq:f64 REG1=MASKNOT0():rw:mskw MEM0:r:q:f64
 | VGATHERDPD           | GATHER         | AVX512EVEX     | AVX512F_512    | 92           | mm         | rrr        | EVV 0x92 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL512  W1 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | REG0=ZMM_R3():w:zf64 REG1=MASKNOT0():rw:mskw MEM0:r:q:f64
 | VGATHERDPD           | KNC            | KNCE           | KNCE           | 92           | mm         | rrr        | KVV 0x92 V66 V0F38  W1 NO_SPARSE_EVSR MOD[mm] MOD!=3 REG[rrr] RM[0b100] KNC_VMODRM() UPCONVERT_FLT64_LOAD() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():rw:mskw  MEM0:r:zv:TXT=NT:TXT=CONVERT NELEM=1:SUPP
 | VGATHERDPS           | AVX2GATHER     | AVX2GATHER     |                | 92           | mm         | rrr        | VV1 0x92   VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] RM=4 VMODRM_YMM() eanot16              | REG0=YMM_R():crw:qq:f32   MEM0:r:d:f32 REG1=YMM_N():rw:qq:i32
 | VGATHERDPS           | AVX2GATHER     | AVX2GATHER     |                | 92           | mm         | rrr        | VV1 0x92   VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] RM=4 VMODRM_XMM() eanot16              | REG0=XMM_R():crw:dq:f32   MEM0:r:d:f32 REG1=XMM_N():rw:dq:i32
 | VGATHERDPS           | GATHER         | AVX512EVEX     | AVX512F_128    | 92           | mm         | rrr        | EVV 0x92 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL128  W0 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | REG0=XMM_R3():w:dq:f32 REG1=MASKNOT0():rw:mskw MEM0:r:d:f32
 | VGATHERDPS           | GATHER         | AVX512EVEX     | AVX512F_256    | 92           | mm         | rrr        | EVV 0x92 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL256  W0 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | REG0=YMM_R3():w:qq:f32 REG1=MASKNOT0():rw:mskw MEM0:r:d:f32
 | VGATHERDPS           | GATHER         | AVX512EVEX     | AVX512F_512    | 92           | mm         | rrr        | EVV 0x92 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL512  W0 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | REG0=ZMM_R3():w:zf32 REG1=MASKNOT0():rw:mskw MEM0:r:d:f32
 | VGATHERDPS           | KNC            | KNCE           | KNCE           | 92           | mm         | rrr        | KVV 0x92 V66 V0F38  W0 NO_SPARSE_EVSR MOD[mm] MOD!=3 REG[rrr] RM[0b100] KNC_VMODRM() UPCONVERT_FLT32_LOAD() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():rw:mskw  MEM0:r:zv:TXT=NT:TXT=CONVERT NELEM=1:SUPP
 | VGATHERPF0DPD        | GATHER         | AVX512EVEX     | AVX512PF_512   | c6           | mm         | 0b001      | EVV 0xC6 V66 V0F38 MOD[mm] MOD!=3 REG[0b001] RM[nnn] BCRC=0   VL512  W1 RM=4 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:r:q:f64 REG0=MASKNOT0():rw:mskw
 | VGATHERPF0DPS        | GATHER         | AVX512EVEX     | AVX512PF_512   | c6           | mm         | 0b001      | EVV 0xC6 V66 V0F38 MOD[mm] MOD!=3 REG[0b001] RM[nnn] BCRC=0   VL512  W0 RM=4 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:r:d:f32 REG0=MASKNOT0():rw:mskw
 | VGATHERPF0DPS        | PREFETCH       | KNCE           |                | c6           | mm         | 0b001      | KVV 0xC6 V0F38 V66  REXW=0 NO_SPARSE_EVSR MOD[mm] MOD!=3 REG[0b001] RM[0b100] KNC_VMODRM() UPCONVERT_FLT32_LOAD() | MEM0:r:zv:TXT=NT REG0=MASK1():rw:mskw
 | VGATHERPF0HINTDPD    | PREFETCH       | KNCE           | KNC_PF_HINT    | c6           | mm         | 0b000      | KVV 0xC6 V0F38 V66  REXW=1 NO_SPARSE_EVSR MOD[mm] MOD!=3 REG[0b000] RM[0b100] KNC_VMODRM() UPCONVERT_FLT32_LOAD() | MEM0:r:zv:TXT=NT REG0=MASK1():rw:mskw
 | VGATHERPF0HINTDPS    | PREFETCH       | KNCE           | KNC_PF_HINT    | c6           | mm         | 0b000      | KVV 0xC6 V0F38 V66  REXW=0 NO_SPARSE_EVSR MOD[mm] MOD!=3 REG[0b000] RM[0b100] KNC_VMODRM() UPCONVERT_FLT32_LOAD() | MEM0:r:zv:TXT=NT REG0=MASK1():rw:mskw
 | VGATHERPF0QPD        | GATHER         | AVX512EVEX     | AVX512PF_512   | c7           | mm         | 0b001      | EVV 0xC7 V66 V0F38 MOD[mm] MOD!=3 REG[0b001] RM[nnn] BCRC=0   VL512  W1 RM=4 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:r:q:f64 REG0=MASKNOT0():rw:mskw
 | VGATHERPF0QPS        | GATHER         | AVX512EVEX     | AVX512PF_512   | c7           | mm         | 0b001      | EVV 0xC7 V66 V0F38 MOD[mm] MOD!=3 REG[0b001] RM[nnn] BCRC=0   VL512  W0 RM=4 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:r:d:f32 REG0=MASKNOT0():rw:mskw
 | VGATHERPF1DPD        | GATHER         | AVX512EVEX     | AVX512PF_512   | c6           | mm         | 0b010      | EVV 0xC6 V66 V0F38 MOD[mm] MOD!=3 REG[0b010] RM[nnn] BCRC=0   VL512  W1 RM=4 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:r:q:f64 REG0=MASKNOT0():rw:mskw
 | VGATHERPF1DPS        | GATHER         | AVX512EVEX     | AVX512PF_512   | c6           | mm         | 0b010      | EVV 0xC6 V66 V0F38 MOD[mm] MOD!=3 REG[0b010] RM[nnn] BCRC=0   VL512  W0 RM=4 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:r:d:f32 REG0=MASKNOT0():rw:mskw
 | VGATHERPF1DPS        | PREFETCH       | KNCE           |                | c6           | mm         | 0b010      | KVV 0xC6 V0F38 V66  REXW=0 NO_SPARSE_EVSR MOD[mm] MOD!=3 REG[0b010] RM[0b100] KNC_VMODRM() UPCONVERT_FLT32_LOAD() | MEM0:r:zv:TXT=NT REG0=MASK1():rw:mskw
 | VGATHERPF1QPD        | GATHER         | AVX512EVEX     | AVX512PF_512   | c7           | mm         | 0b010      | EVV 0xC7 V66 V0F38 MOD[mm] MOD!=3 REG[0b010] RM[nnn] BCRC=0   VL512  W1 RM=4 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:r:q:f64 REG0=MASKNOT0():rw:mskw
 | VGATHERPF1QPS        | GATHER         | AVX512EVEX     | AVX512PF_512   | c7           | mm         | 0b010      | EVV 0xC7 V66 V0F38 MOD[mm] MOD!=3 REG[0b010] RM[nnn] BCRC=0   VL512  W0 RM=4 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:r:d:f32 REG0=MASKNOT0():rw:mskw
 | VGATHERQPD           | AVX2GATHER     | AVX2GATHER     |                | 93           | mm         | rrr        | VV1 0x93   VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] RM=4 VMODRM_YMM() eanot16              | REG0=YMM_R():crw:qq:f64   MEM0:r:q:f64 REG1=YMM_N():rw:qq:i64
 | VGATHERQPD           | AVX2GATHER     | AVX2GATHER     |                | 93           | mm         | rrr        | VV1 0x93   VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] RM=4 VMODRM_XMM() eanot16              | REG0=XMM_R():crw:dq:f64   MEM0:r:q:f64 REG1=XMM_N():rw:dq:i64
 | VGATHERQPD           | GATHER         | AVX512EVEX     | AVX512F_128    | 93           | mm         | rrr        | EVV 0x93 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL128  W1 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | REG0=XMM_R3():w:dq:f64 REG1=MASKNOT0():rw:mskw MEM0:r:q:f64
 | VGATHERQPD           | GATHER         | AVX512EVEX     | AVX512F_256    | 93           | mm         | rrr        | EVV 0x93 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL256  W1 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | REG0=YMM_R3():w:qq:f64 REG1=MASKNOT0():rw:mskw MEM0:r:q:f64
 | VGATHERQPD           | GATHER         | AVX512EVEX     | AVX512F_512    | 93           | mm         | rrr        | EVV 0x93 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL512  W1 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | REG0=ZMM_R3():w:zf64 REG1=MASKNOT0():rw:mskw MEM0:r:q:f64
 | VGATHERQPS           | AVX2GATHER     | AVX2GATHER     |                | 93           | mm         | rrr        | VV1 0x93   VL256 V66 V0F38   W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] RM=4 VMODRM_YMM() eanot16            | REG0=XMM_R():crw:dq:f32   MEM0:r:d:f32 REG1=XMM_N():rw:dq:i32
 | VGATHERQPS           | AVX2GATHER     | AVX2GATHER     |                | 93           | mm         | rrr        | VV1 0x93   VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] RM=4 VMODRM_XMM() eanot16              | REG0=XMM_R():crw:q:f32   MEM0:r:d:f32 REG1=XMM_N():rw:q:i32
 | VGATHERQPS           | GATHER         | AVX512EVEX     | AVX512F_128    | 93           | mm         | rrr        | EVV 0x93 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL128  W0 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | REG0=XMM_R3():w:dq:f32 REG1=MASKNOT0():rw:mskw MEM0:r:d:f32
 | VGATHERQPS           | GATHER         | AVX512EVEX     | AVX512F_256    | 93           | mm         | rrr        | EVV 0x93 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL256  W0 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | REG0=XMM_R3():w:dq:f32 REG1=MASKNOT0():rw:mskw MEM0:r:d:f32
 | VGATHERQPS           | GATHER         | AVX512EVEX     | AVX512F_512    | 93           | mm         | rrr        | EVV 0x93 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL512  W0 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | REG0=YMM_R3():w:qq:f32 REG1=MASKNOT0():rw:mskw MEM0:r:d:f32
 | VGETEXPPD            | AVX512         | AVX512EVEX     | AVX512F_128    | 42           | 0b11       | rrr        | EVV 0x42 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                        | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64
 | VGETEXPPD            | AVX512         | AVX512EVEX     | AVX512F_128    | 42           | mm         | rrr        | EVV 0x42 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VGETEXPPD            | AVX512         | AVX512EVEX     | AVX512F_256    | 42           | 0b11       | rrr        | EVV 0x42 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                        | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64
 | VGETEXPPD            | AVX512         | AVX512EVEX     | AVX512F_256    | 42           | mm         | rrr        | EVV 0x42 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VGETEXPPD            | AVX512         | AVX512EVEX     | AVX512F_512    | 42           | 0b11       | rrr        | EVV 0x42 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                        | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
 | VGETEXPPD            | AVX512         | AVX512EVEX     | AVX512F_512    | 42           | 0b11       | rrr        | EVV 0x42 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W1  NOEVSR      | REG0=ZMM_R3():w:zf64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
 | VGETEXPPD            | AVX512         | AVX512EVEX     | AVX512F_512    | 42           | mm         | rrr        | EVV 0x42 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VGETEXPPD            | KNC            | KNCE           |                | 42           | mm         | rrr        | KVV 0x42 V0F38 V66   W1  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT64()           | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VGETEXPPD            | KNC            | KNCE           |                | 42           | 0b11       | rrr        | KVV 0x42 V0F38 V66  W1  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE64()                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_B3():r:zf64:TXT=REGSWIZ
 | VGETEXPPD            | KNC            | KNCE           |                | 42           | 0b11       | rrr        | KVV 0x42 V0F38 V66  W1  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] NR=1 KNC_SAE()                      | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_B3():r:zf64:TXT=SAEC
 | VGETEXPPS            | AVX512         | AVX512EVEX     | AVX512F_128    | 42           | 0b11       | rrr        | EVV 0x42 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
 | VGETEXPPS            | AVX512         | AVX512EVEX     | AVX512F_128    | 42           | mm         | rrr        | EVV 0x42 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VGETEXPPS            | AVX512         | AVX512EVEX     | AVX512F_256    | 42           | 0b11       | rrr        | EVV 0x42 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
 | VGETEXPPS            | AVX512         | AVX512EVEX     | AVX512F_256    | 42           | mm         | rrr        | EVV 0x42 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VGETEXPPS            | AVX512         | AVX512EVEX     | AVX512F_512    | 42           | 0b11       | rrr        | EVV 0x42 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
 | VGETEXPPS            | AVX512         | AVX512EVEX     | AVX512F_512    | 42           | 0b11       | rrr        | EVV 0x42 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0  NOEVSR      | REG0=ZMM_R3():w:zf32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
 | VGETEXPPS            | AVX512         | AVX512EVEX     | AVX512F_512    | 42           | mm         | rrr        | EVV 0x42 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VGETEXPPS            | KNC            | KNCE           |                | 42           | mm         | rrr        | KVV 0x42 V0F38 V66   W0  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT32()           | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VGETEXPPS            | KNC            | KNCE           |                | 42           | 0b11       | rrr        | KVV 0x42 V0F38 V66  W0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VGETEXPPS            | KNC            | KNCE           |                | 42           | 0b11       | rrr        | KVV 0x42 V0F38 V66  W0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] NR=1 KNC_SAE()                      | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_B3():r:zf32:TXT=SAEC
 | VGETEXPSD            | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 43           | 0b11       | rrr        | EVV 0x43 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                                       | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VGETEXPSD            | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 43           | 0b11       | rrr        | EVV 0x43 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W1              | REG0=XMM_R3():w:dq:f64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VGETEXPSD            | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 43           | mm         | rrr        | EVV 0x43 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
 | VGETEXPSS            | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 43           | 0b11       | rrr        | EVV 0x43 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                                       | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VGETEXPSS            | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 43           | 0b11       | rrr        | EVV 0x43 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W0              | REG0=XMM_R3():w:dq:f32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VGETEXPSS            | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 43           | mm         | rrr        | EVV 0x43 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
 | VGETMANTPD           | AVX512         | AVX512EVEX     | AVX512F_128    | 26           | 0b11       | rrr        | EVV 0x26 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR UIMM8()                | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64 IMM0:r:b
 | VGETMANTPD           | AVX512         | AVX512EVEX     | AVX512F_128    | 26           | mm         | rrr        | EVV 0x26 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
 | VGETMANTPD           | AVX512         | AVX512EVEX     | AVX512F_256    | 26           | 0b11       | rrr        | EVV 0x26 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR UIMM8()                | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64 IMM0:r:b
 | VGETMANTPD           | AVX512         | AVX512EVEX     | AVX512F_256    | 26           | mm         | rrr        | EVV 0x26 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
 | VGETMANTPD           | AVX512         | AVX512EVEX     | AVX512F_512    | 26           | 0b11       | rrr        | EVV 0x26 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR UIMM8()                | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64 IMM0:r:b
 | VGETMANTPD           | AVX512         | AVX512EVEX     | AVX512F_512    | 26           | 0b11       | rrr        | EVV 0x26 V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W1  NOEVSR UIMM8() | REG0=ZMM_R3():w:zf64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64 IMM0:r:b
 | VGETMANTPD           | AVX512         | AVX512EVEX     | AVX512F_512    | 26           | mm         | rrr        | EVV 0x26 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
 | VGETMANTPD           | KNC            | KNCE           |                | 26           | mm         | rrr        | KVV 0x26 V0F3A V66  W1  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() UPCONVERT_FLT64()    | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT IMM0:r:b
 | VGETMANTPD           | KNC            | KNCE           |                | 26           | 0b11       | rrr        | KVV 0x26 V0F3A V66  W1  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] UIMM8() NR=1 KNC_SAE()              | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_B3():r:zf64:TXT=SAEC  IMM0:r:b
 | VGETMANTPD           | KNC            | KNCE           |                | 26           | 0b11       | rrr        | KVV 0x26 V0F3A V66  W1  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] UIMM8() NR=0 REG_SWIZZLE64()        | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_B3():r:zf64 IMM0:r:b
 | VGETMANTPS           | AVX512         | AVX512EVEX     | AVX512F_128    | 26           | 0b11       | rrr        | EVV 0x26 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR UIMM8()                | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32 IMM0:r:b
 | VGETMANTPS           | AVX512         | AVX512EVEX     | AVX512F_128    | 26           | mm         | rrr        | EVV 0x26 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
 | VGETMANTPS           | AVX512         | AVX512EVEX     | AVX512F_256    | 26           | 0b11       | rrr        | EVV 0x26 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR UIMM8()                | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32 IMM0:r:b
 | VGETMANTPS           | AVX512         | AVX512EVEX     | AVX512F_256    | 26           | mm         | rrr        | EVV 0x26 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
 | VGETMANTPS           | AVX512         | AVX512EVEX     | AVX512F_512    | 26           | 0b11       | rrr        | EVV 0x26 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR UIMM8()                | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32 IMM0:r:b
 | VGETMANTPS           | AVX512         | AVX512EVEX     | AVX512F_512    | 26           | 0b11       | rrr        | EVV 0x26 V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0  NOEVSR UIMM8() | REG0=ZMM_R3():w:zf32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32 IMM0:r:b
 | VGETMANTPS           | AVX512         | AVX512EVEX     | AVX512F_512    | 26           | mm         | rrr        | EVV 0x26 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
 | VGETMANTPS           | KNC            | KNCE           | KNCE           | 26           | mm         | rrr        | KVV 0x26 V0F3A V66  W0  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() UPCONVERT_FLT32()    | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw  MEM0:r:zv:TXT=CONVERT:TXT=NT IMM0:r:b
 | VGETMANTPS           | KNC            | KNCE           | KNCE           | 26           | 0b11       | rrr        | KVV 0x26 V0F3A V66  W0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] UIMM8() NR=1 KNC_SAE()              | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw  REG2=ZMM_B3():r:zf32:TXT=SAEC  IMM0:r:b
 | VGETMANTPS           | KNC            | KNCE           | KNCE           | 26           | 0b11       | rrr        | KVV 0x26 V0F3A V66  W0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] UIMM8() NR=0 REG_SWIZZLE32()        | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw  REG2=ZMM_B3():r:zf32:TXT=REGSWIZ IMM0:r:b
 | VGETMANTSD           | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 27           | 0b11       | rrr        | EVV 0x27 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1   UIMM8()                             | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64 IMM0:r:b
 | VGETMANTSD           | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 27           | 0b11       | rrr        | EVV 0x27 V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W1   UIMM8()    | REG0=XMM_R3():w:dq:f64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64 IMM0:r:b
 | VGETMANTSD           | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 27           | mm         | rrr        | EVV 0x27 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1   UIMM8()  ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64 IMM0:r:b
 | VGETMANTSS           | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 27           | 0b11       | rrr        | EVV 0x27 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0   UIMM8()                             | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32 IMM0:r:b
 | VGETMANTSS           | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 27           | 0b11       | rrr        | EVV 0x27 V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W0   UIMM8()    | REG0=XMM_R3():w:dq:f32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32 IMM0:r:b
 | VGETMANTSS           | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 27           | mm         | rrr        | EVV 0x27 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0   UIMM8()  ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32 IMM0:r:b
 | VGF2P8AFFINEINVQB    | GFNI           | AVX512EVEX     | AVX512_GFNI_128 | cf           | 0b11       | rrr        | EVV 0xCF V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1   UIMM8()                      | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u64 IMM0:r:b
 | VGF2P8AFFINEINVQB    | GFNI           | AVX512EVEX     | AVX512_GFNI_128 | cf           | mm         | rrr        | EVV 0xCF V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VGF2P8AFFINEINVQB    | GFNI           | AVX512EVEX     | AVX512_GFNI_256 | cf           | 0b11       | rrr        | EVV 0xCF V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1   UIMM8()                      | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u64 IMM0:r:b
 | VGF2P8AFFINEINVQB    | GFNI           | AVX512EVEX     | AVX512_GFNI_256 | cf           | mm         | rrr        | EVV 0xCF V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VGF2P8AFFINEINVQB    | GFNI           | AVX512EVEX     | AVX512_GFNI_512 | cf           | 0b11       | rrr        | EVV 0xCF V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1   UIMM8()                      | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu64 IMM0:r:b
 | VGF2P8AFFINEINVQB    | GFNI           | AVX512EVEX     | AVX512_GFNI_512 | cf           | mm         | rrr        | EVV 0xCF V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VGF2P8AFFINEINVQB    | GFNI           | GFNI           | AVX_GFNI       | cf           | 0b11       | rrr        | VV1 0xCF V66 V0F3A MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W1   UIMM8()                            | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:u64 IMM0:r:b
 | VGF2P8AFFINEINVQB    | GFNI           | GFNI           | AVX_GFNI       | cf           | mm         | rrr        | VV1 0xCF V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1   UIMM8()                     | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 MEM0:r:dq:u64 IMM0:r:b
 | VGF2P8AFFINEINVQB    | GFNI           | GFNI           | AVX_GFNI       | cf           | 0b11       | rrr        | VV1 0xCF V66 V0F3A MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W1   UIMM8()                            | REG0=YMM_R():w:qq:u8 REG1=YMM_N():r:qq:u8 REG2=YMM_B():r:qq:u64 IMM0:r:b
 | VGF2P8AFFINEINVQB    | GFNI           | GFNI           | AVX_GFNI       | cf           | mm         | rrr        | VV1 0xCF V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1   UIMM8()                     | REG0=YMM_R():w:qq:u8 REG1=YMM_N():r:qq:u8 MEM0:r:qq:u64 IMM0:r:b
 | VGF2P8AFFINEQB       | GFNI           | AVX512EVEX     | AVX512_GFNI_128 | ce           | 0b11       | rrr        | EVV 0xCE V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1   UIMM8()                      | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u64 IMM0:r:b
 | VGF2P8AFFINEQB       | GFNI           | AVX512EVEX     | AVX512_GFNI_128 | ce           | mm         | rrr        | EVV 0xCE V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VGF2P8AFFINEQB       | GFNI           | AVX512EVEX     | AVX512_GFNI_256 | ce           | 0b11       | rrr        | EVV 0xCE V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1   UIMM8()                      | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u64 IMM0:r:b
 | VGF2P8AFFINEQB       | GFNI           | AVX512EVEX     | AVX512_GFNI_256 | ce           | mm         | rrr        | EVV 0xCE V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VGF2P8AFFINEQB       | GFNI           | AVX512EVEX     | AVX512_GFNI_512 | ce           | 0b11       | rrr        | EVV 0xCE V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1   UIMM8()                      | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu64 IMM0:r:b
 | VGF2P8AFFINEQB       | GFNI           | AVX512EVEX     | AVX512_GFNI_512 | ce           | mm         | rrr        | EVV 0xCE V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VGF2P8AFFINEQB       | GFNI           | GFNI           | AVX_GFNI       | ce           | 0b11       | rrr        | VV1 0xCE V66 V0F3A MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W1   UIMM8()                            | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:u64 IMM0:r:b
 | VGF2P8AFFINEQB       | GFNI           | GFNI           | AVX_GFNI       | ce           | mm         | rrr        | VV1 0xCE V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1   UIMM8()                     | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 MEM0:r:dq:u64 IMM0:r:b
 | VGF2P8AFFINEQB       | GFNI           | GFNI           | AVX_GFNI       | ce           | 0b11       | rrr        | VV1 0xCE V66 V0F3A MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W1   UIMM8()                            | REG0=YMM_R():w:qq:u8 REG1=YMM_N():r:qq:u8 REG2=YMM_B():r:qq:u64 IMM0:r:b
 | VGF2P8AFFINEQB       | GFNI           | GFNI           | AVX_GFNI       | ce           | mm         | rrr        | VV1 0xCE V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1   UIMM8()                     | REG0=YMM_R():w:qq:u8 REG1=YMM_N():r:qq:u8 MEM0:r:qq:u64 IMM0:r:b
 | VGF2P8MULB           | GFNI           | AVX512EVEX     | AVX512_GFNI_128 | cf           | 0b11       | rrr        | EVV 0xCF V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
 | VGF2P8MULB           | GFNI           | AVX512EVEX     | AVX512_GFNI_128 | cf           | mm         | rrr        | EVV 0xCF V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0    ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
 | VGF2P8MULB           | GFNI           | AVX512EVEX     | AVX512_GFNI_256 | cf           | 0b11       | rrr        | EVV 0xCF V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
 | VGF2P8MULB           | GFNI           | AVX512EVEX     | AVX512_GFNI_256 | cf           | mm         | rrr        | EVV 0xCF V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0    ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
 | VGF2P8MULB           | GFNI           | AVX512EVEX     | AVX512_GFNI_512 | cf           | 0b11       | rrr        | EVV 0xCF V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
 | VGF2P8MULB           | GFNI           | AVX512EVEX     | AVX512_GFNI_512 | cf           | mm         | rrr        | EVV 0xCF V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0    ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
 | VGF2P8MULB           | GFNI           | GFNI           | AVX_GFNI       | cf           | 0b11       | rrr        | VV1 0xCF V66 V0F38 MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0                                      | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:u8
 | VGF2P8MULB           | GFNI           | GFNI           | AVX_GFNI       | cf           | mm         | rrr        | VV1 0xCF V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0                               | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 MEM0:r:dq:u8
 | VGF2P8MULB           | GFNI           | GFNI           | AVX_GFNI       | cf           | 0b11       | rrr        | VV1 0xCF V66 V0F38 MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W0                                      | REG0=YMM_R():w:qq:u8 REG1=YMM_N():r:qq:u8 REG2=YMM_B():r:qq:u8
 | VGF2P8MULB           | GFNI           | GFNI           | AVX_GFNI       | cf           | mm         | rrr        | VV1 0xCF V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0                               | REG0=YMM_R():w:qq:u8 REG1=YMM_N():r:qq:u8 MEM0:r:qq:u8
 | VGMAXABSPS           | KNC            | KNCE           |                | 51           | mm         | rrr        | KVV 0x51 V0F38 V66  REXW=0 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT32()                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VGMAXABSPS           | KNC            | KNCE           |                | 51           | 0b11       | rrr        | KVV 0x51 V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VGMAXABSPS           | KNC            | KNCE           |                | 51           | 0b11       | rrr        | KVV 0x51 V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 KNC_SAE()                           | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=SAEC
 | VGMAXPD              | KNC            | KNCE           |                | 53           | mm         | rrr        | KVV 0x53 V0F38 V66  REXW=1 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT64()                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VGMAXPD              | KNC            | KNCE           |                | 53           | 0b11       | rrr        | KVV 0x53 V0F38 V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE64()                     | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=REGSWIZ
 | VGMAXPD              | KNC            | KNCE           |                | 53           | 0b11       | rrr        | KVV 0x53 V0F38 V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 KNC_SAE()                           | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=SAEC
 | VGMAXPS              | KNC            | KNCE           |                | 53           | mm         | rrr        | KVV 0x53 V0F38 V66  REXW=0 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT32()                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VGMAXPS              | KNC            | KNCE           |                | 53           | 0b11       | rrr        | KVV 0x53 V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VGMAXPS              | KNC            | KNCE           |                | 53           | 0b11       | rrr        | KVV 0x53 V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                   | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=ROUNDC:TXT=SAEC
 | VGMINPD              | KNC            | KNCE           |                | 52           | mm         | rrr        | KVV 0x52 V0F38 V66  REXW=1 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT64()                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VGMINPD              | KNC            | KNCE           |                | 52           | 0b11       | rrr        | KVV 0x52 V0F38 V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE64()                     | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=REGSWIZ
 | VGMINPD              | KNC            | KNCE           |                | 52           | 0b11       | rrr        | KVV 0x52 V0F38 V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 KNC_SAE()                           | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=SAEC
 | VGMINPS              | KNC            | KNCE           |                | 52           | mm         | rrr        | KVV 0x52 V0F38 V66  REXW=0 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT32()                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VGMINPS              | KNC            | KNCE           |                | 52           | 0b11       | rrr        | KVV 0x52 V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VGMINPS              | KNC            | KNCE           |                | 52           | 0b11       | rrr        | KVV 0x52 V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                   | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=ROUNDC:TXT=SAEC
 | VHADDPD              | AVX            | AVX            |                | 7c           | mm         | rrr        | VV1 0x7C  VL128 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
 | VHADDPD              | AVX            | AVX            |                | 7c           | 0b11       | rrr        | VV1 0x7C  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
 | VHADDPD              | AVX            | AVX            |                | 7c           | mm         | rrr        | VV1 0x7C  VL256 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
 | VHADDPD              | AVX            | AVX            |                | 7c           | 0b11       | rrr        | VV1 0x7C  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
 | VHADDPS              | AVX            | AVX            |                | 7c           | mm         | rrr        | VV1 0x7C  VL128 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
 | VHADDPS              | AVX            | AVX            |                | 7c           | 0b11       | rrr        | VV1 0x7C  VL128 VF2 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
 | VHADDPS              | AVX            | AVX            |                | 7c           | mm         | rrr        | VV1 0x7C  VL256 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
 | VHADDPS              | AVX            | AVX            |                | 7c           | 0b11       | rrr        | VV1 0x7C  VL256 VF2 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
 | VHSUBPD              | AVX            | AVX            |                | 7d           | mm         | rrr        | VV1 0x7D  VL128 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
 | VHSUBPD              | AVX            | AVX            |                | 7d           | 0b11       | rrr        | VV1 0x7D  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
 | VHSUBPD              | AVX            | AVX            |                | 7d           | mm         | rrr        | VV1 0x7D  VL256 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
 | VHSUBPD              | AVX            | AVX            |                | 7d           | 0b11       | rrr        | VV1 0x7D  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
 | VHSUBPS              | AVX            | AVX            |                | 7d           | mm         | rrr        | VV1 0x7D  VL128 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
 | VHSUBPS              | AVX            | AVX            |                | 7d           | 0b11       | rrr        | VV1 0x7D  VL128 VF2 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
 | VHSUBPS              | AVX            | AVX            |                | 7d           | mm         | rrr        | VV1 0x7D  VL256 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
 | VHSUBPS              | AVX            | AVX            |                | 7d           | 0b11       | rrr        | VV1 0x7D  VL256 VF2 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
 | VINSERTF128          | AVX            | AVX            |                | 18           | mm         | rrr        | VV1 0x18  norexw_prefix VL256 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()              | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:dq:f64 IMM0:r:b EMX_BROADCAST_2TO4_64
 | VINSERTF128          | AVX            | AVX            |                | 18           | 0b11       | rrr        | VV1 0x18  norexw_prefix  VL256 V66 V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                    | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=XMM_B():r:dq:f64 IMM0:r:b EMX_BROADCAST_2TO4_64
 | VINSERTF32X4         | AVX512         | AVX512EVEX     | AVX512F_256    | 18           | 0b11       | rrr        | EVV 0x18 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0   UIMM8()                      | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=XMM_B3():r:dq:f32 IMM0:r:b
 | VINSERTF32X4         | AVX512         | AVX512EVEX     | AVX512F_256    | 18           | mm         | rrr        | EVV 0x18 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0   UIMM8()  ESIZE_32_BITS() NELEM_TUPLE4() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:dq:f32 IMM0:r:b
 | VINSERTF32X4         | AVX512         | AVX512EVEX     | AVX512F_512    | 18           | 0b11       | rrr        | EVV 0x18 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0   UIMM8()                      | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=XMM_B3():r:dq:f32 IMM0:r:b
 | VINSERTF32X4         | AVX512         | AVX512EVEX     | AVX512F_512    | 18           | mm         | rrr        | EVV 0x18 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0   UIMM8()  ESIZE_32_BITS() NELEM_TUPLE4() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:dq:f32 IMM0:r:b
 | VINSERTF32X8         | AVX512         | AVX512EVEX     | AVX512DQ_512   | 1a           | 0b11       | rrr        | EVV 0x1A V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0   UIMM8()                      | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=YMM_B3():r:qq:f32 IMM0:r:b
 | VINSERTF32X8         | AVX512         | AVX512EVEX     | AVX512DQ_512   | 1a           | mm         | rrr        | EVV 0x1A V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0   UIMM8()  ESIZE_32_BITS() NELEM_TUPLE8() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:qq:f32 IMM0:r:b
 | VINSERTF64X2         | AVX512         | AVX512EVEX     | AVX512DQ_256   | 18           | 0b11       | rrr        | EVV 0x18 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1   UIMM8()                      | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=XMM_B3():r:dq:f64 IMM0:r:b
 | VINSERTF64X2         | AVX512         | AVX512EVEX     | AVX512DQ_256   | 18           | mm         | rrr        | EVV 0x18 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1   UIMM8()  ESIZE_64_BITS() NELEM_TUPLE2() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:dq:f64 IMM0:r:b
 | VINSERTF64X2         | AVX512         | AVX512EVEX     | AVX512DQ_512   | 18           | 0b11       | rrr        | EVV 0x18 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1   UIMM8()                      | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=XMM_B3():r:dq:f64 IMM0:r:b
 | VINSERTF64X2         | AVX512         | AVX512EVEX     | AVX512DQ_512   | 18           | mm         | rrr        | EVV 0x18 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_TUPLE2() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:dq:f64 IMM0:r:b
 | VINSERTF64X4         | AVX512         | AVX512EVEX     | AVX512F_512    | 1a           | 0b11       | rrr        | EVV 0x1A V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1   UIMM8()                      | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=YMM_B3():r:qq:f64 IMM0:r:b
 | VINSERTF64X4         | AVX512         | AVX512EVEX     | AVX512F_512    | 1a           | mm         | rrr        | EVV 0x1A V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_TUPLE4() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:qq:f64 IMM0:r:b
 | VINSERTI128          | AVX2           | AVX2           |                | 38           | mm         | rrr        | VV1 0x38  VL256 V66 V0F3A W0  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                        | REG0=YMM_R():w:qq:u128 REG1=YMM_N():r:qq:u128 MEM0:r:dq:u128 IMM0:r:b
 | VINSERTI128          | AVX2           | AVX2           |                | 38           | 0b11       | rrr        | VV1 0x38  VL256 V66 V0F3A W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                | REG0=YMM_R():w:qq:u128 REG1=YMM_N():r:qq:u128 REG2=XMM_B():r:dq:u128 IMM0:r:b
 | VINSERTI32X4         | AVX512         | AVX512EVEX     | AVX512F_256    | 38           | 0b11       | rrr        | EVV 0x38 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0   UIMM8()                      | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=XMM_B3():r:dq:u32 IMM0:r:b
 | VINSERTI32X4         | AVX512         | AVX512EVEX     | AVX512F_256    | 38           | mm         | rrr        | EVV 0x38 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0   UIMM8()  ESIZE_32_BITS() NELEM_TUPLE4() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:dq:u32 IMM0:r:b
 | VINSERTI32X4         | AVX512         | AVX512EVEX     | AVX512F_512    | 38           | 0b11       | rrr        | EVV 0x38 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0   UIMM8()                      | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=XMM_B3():r:dq:u32 IMM0:r:b
 | VINSERTI32X4         | AVX512         | AVX512EVEX     | AVX512F_512    | 38           | mm         | rrr        | EVV 0x38 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0   UIMM8()  ESIZE_32_BITS() NELEM_TUPLE4() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:dq:u32 IMM0:r:b
 | VINSERTI32X8         | AVX512         | AVX512EVEX     | AVX512DQ_512   | 3a           | 0b11       | rrr        | EVV 0x3A V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0   UIMM8()                      | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=YMM_B3():r:qq:u32 IMM0:r:b
 | VINSERTI32X8         | AVX512         | AVX512EVEX     | AVX512DQ_512   | 3a           | mm         | rrr        | EVV 0x3A V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0   UIMM8()  ESIZE_32_BITS() NELEM_TUPLE8() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:qq:u32 IMM0:r:b
 | VINSERTI64X2         | AVX512         | AVX512EVEX     | AVX512DQ_256   | 38           | 0b11       | rrr        | EVV 0x38 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1   UIMM8()                      | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=XMM_B3():r:dq:u64 IMM0:r:b
 | VINSERTI64X2         | AVX512         | AVX512EVEX     | AVX512DQ_256   | 38           | mm         | rrr        | EVV 0x38 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1   UIMM8()  ESIZE_64_BITS() NELEM_TUPLE2() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:dq:u64 IMM0:r:b
 | VINSERTI64X2         | AVX512         | AVX512EVEX     | AVX512DQ_512   | 38           | 0b11       | rrr        | EVV 0x38 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1   UIMM8()                      | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=XMM_B3():r:dq:u64 IMM0:r:b
 | VINSERTI64X2         | AVX512         | AVX512EVEX     | AVX512DQ_512   | 38           | mm         | rrr        | EVV 0x38 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_TUPLE2() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:dq:u64 IMM0:r:b
 | VINSERTI64X4         | AVX512         | AVX512EVEX     | AVX512F_512    | 3a           | 0b11       | rrr        | EVV 0x3A V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1   UIMM8()                      | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=YMM_B3():r:qq:u64 IMM0:r:b
 | VINSERTI64X4         | AVX512         | AVX512EVEX     | AVX512F_512    | 3a           | mm         | rrr        | EVV 0x3A V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_TUPLE4() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:qq:u64 IMM0:r:b
 | VINSERTPS            | AVX            | AVX            |                | 21           | mm         | rrr        | VV1 0x21  VL128 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                            | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:d:f32 IMM0:r:b
 | VINSERTPS            | AVX            | AVX            |                | 21           | 0b11       | rrr        | VV1 0x21  VL128 V66 V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                   | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32 IMM0:r:b
 | VINSERTPS            | AVX512         | AVX512EVEX     | AVX512F_128N   | 21           | 0b11       | rrr        | EVV 0x21 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0    ZEROING=0 MASK=0 UIMM8()    | REG0=XMM_R3():w:dq:f32 REG1=XMM_N3():r:dq:f32 REG2=XMM_B3():r:dq:f32 IMM0:r:b
 | VINSERTPS            | AVX512         | AVX512EVEX     | AVX512F_128N   | 21           | mm         | rrr        | EVV 0x21 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0    ZEROING=0 MASK=0 UIMM8()  ESIZE_32_BITS() NELEM_TUPLE1() | REG0=XMM_R3():w:dq:f32 REG1=XMM_N3():r:dq:f32 MEM0:r:d:f32 IMM0:r:b
 | VLDDQU               | AVX            | AVX            |                | f0           | mm         | rrr        | VV1 0xF0  VL128 VF2 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq  MEM0:r:dq
 | VLDDQU               | AVX            | AVX            |                | f0           | mm         | rrr        | VV1 0xF0  VL256 VF2 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=YMM_R():w:qq MEM0:r:qq
 | VLDMXCSR             | AVX            | AVX            |                | ae           | mm         | 0b010      | VV1 0xAE VL128 VNP V0F NOVSR MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM()                               | MEM0:r:d REG0=XED_REG_MXCSR:w:SUPP
 | VLOADUNPACKHD        | DATAXFER       | KNCE           |                | d4           | mm         | rrr        | KVV 0xD4 V0F38 VNP W0  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM()  UPCONVERT_INT32_LOAD()       | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT  NO_SCALE_DISP8=1
 | VLOADUNPACKHPD       | DATAXFER       | KNCE           |                | d5           | mm         | rrr        | KVV 0xD5 V0F38 VNP W1  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM()  UPCONVERT_FLT64_LOAD()       | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw MEM0:r:zv:TXT=NT   NO_SCALE_DISP8=1
 | VLOADUNPACKHPS       | DATAXFER       | KNCE           |                | d5           | mm         | rrr        | KVV 0xD5 V0F38 VNP W0  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM()  UPCONVERT_FLT32_LOAD()       | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT   NO_SCALE_DISP8=1
 | VLOADUNPACKHQ        | DATAXFER       | KNCE           |                | d4           | mm         | rrr        | KVV 0xD4 V0F38 VNP W1  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM()  UPCONVERT_INT64_LOAD()       | REG0=ZMM_R3():rw:zq REG1=MASK1():r:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT   NO_SCALE_DISP8=1
 | VLOADUNPACKLD        | DATAXFER       | KNCE           |                | d0           | mm         | rrr        | KVV 0xD0 V0F38 VNP W0  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM()  UPCONVERT_INT32_LOAD()       | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT  NO_SCALE_DISP8=1
 | VLOADUNPACKLPD       | DATAXFER       | KNCE           |                | d1           | mm         | rrr        | KVV 0xD1 V0F38 VNP W1  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM()  UPCONVERT_FLT64_LOAD()       | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw MEM0:r:zv:TXT=NT   NO_SCALE_DISP8=1
 | VLOADUNPACKLPS       | DATAXFER       | KNCE           |                | d1           | mm         | rrr        | KVV 0xD1 V0F38 VNP W0  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM()  UPCONVERT_FLT32_LOAD()       | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT   NO_SCALE_DISP8=1
 | VLOADUNPACKLQ        | DATAXFER       | KNCE           |                | d0           | mm         | rrr        | KVV 0xD0 V0F38 VNP W1  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM()  UPCONVERT_INT64_LOAD()       | REG0=ZMM_R3():rw:zq REG1=MASK1():r:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT   NO_SCALE_DISP8=1
 | VLOG2PS              | KNC            | KNCE           | KNCE           | c9           | mm         | rrr        | KVV 0xC9 V0F38 V66  W0  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() NOSWIZF32()                  | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  MEM0:r:zd:TXT=NT
 | VLOG2PS              | KNC            | KNCE           | KNCE           | c9           | 0b11       | rrr        | KVV 0xC9 V0F38 V66  W0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] NR=1 KNC_SAE()                      | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw REG2=ZMM_B3():r:zd:TXT=SAEC
 | VLOG2PS              | KNC            | KNCE           | KNCE           | c9           | 0b11       | rrr        | KVV 0xC9 V0F38 V66  W0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] NR=0 SWIZ=0                         | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw REG2=ZMM_B3():r:zd
 | VMASKMOVDQU          | AVX            | AVX            |                | f7           | 0b11       | rrr        | VV1 0xF7 V0F V66 VL128  NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():r:dq:u8 REG1=XMM_B():r:dq:u8 MEM0:w:SUPP:dq:u8 BASE0=ArDI():r:SUPP SEG0=FINAL_DSEG():r:SUPP
 | VMASKMOVPD           | AVX            | AVX            |                | 2d           | mm         | rrr        | VV1 0x2D  V66 VL128 V0F38  norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                     | REG0=XMM_R():w:dq:f64   REG1=XMM_N():r:dq:u64 MEM0:r:dq:f64
 | VMASKMOVPD           | AVX            | AVX            |                | 2d           | mm         | rrr        | VV1 0x2D  V66 VL256 V0F38 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                      | REG0=YMM_R():w:qq:f64   REG1=YMM_N():r:qq:u64 MEM0:r:qq:f64
 | VMASKMOVPD           | AVX            | AVX            |                | 2f           | mm         | rrr        | VV1 0x2F   V66 V0F38 VL128 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                     | MEM0:w:dq:f64  REG0=XMM_N():r:dq:u64  REG1=XMM_R():r:dq:f64
 | VMASKMOVPD           | AVX            | AVX            |                | 2f           | mm         | rrr        | VV1 0x2F   V66 V0F38 VL256 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                     | MEM0:w:qq:f64  REG0=YMM_N():r:qq:u64   REG1=YMM_R():r:qq:f64
 | VMASKMOVPS           | AVX            | AVX            |                | 2c           | mm         | rrr        | VV1 0x2C V66 VL128 V0F38 norexw_prefix  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                      | REG0=XMM_R():w:dq:f32   REG1=XMM_N():r:dq MEM0:r:dq:f32
 | VMASKMOVPS           | AVX            | AVX            |                | 2c           | mm         | rrr        | VV1 0x2C V66 VL256 V0F38    norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                    | REG0=YMM_R():w:qq:f32   REG1=YMM_N():r:qq MEM0:r:qq:f32
 | VMASKMOVPS           | AVX            | AVX            |                | 2e           | mm         | rrr        | VV1 0x2E V66 V0F38 VL128  norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                      | MEM0:w:dq:f32  REG0=XMM_N():r:dq   REG1=XMM_R():r:dq:f32
 | VMASKMOVPS           | AVX            | AVX            |                | 2e           | mm         | rrr        | VV1 0x2E V66 V0F38 VL256 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                       | MEM0:w:qq:f32   REG0=YMM_N():r:qq  REG1=YMM_R():r:qq:f32
 | VMAXPD               | AVX            | AVX            |                | 5f           | mm         | rrr        | VV1 0x5F  V66 V0F VL128 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
 | VMAXPD               | AVX            | AVX            |                | 5f           | 0b11       | rrr        | VV1 0x5F  V66 V0F VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
 | VMAXPD               | AVX            | AVX            |                | 5f           | mm         | rrr        | VV1 0x5F  V66 V0F VL256 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
 | VMAXPD               | AVX            | AVX            |                | 5f           | 0b11       | rrr        | VV1 0x5F  V66 V0F VL256 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
 | VMAXPD               | AVX512         | AVX512EVEX     | AVX512F_128    | 5f           | 0b11       | rrr        | EVV 0x5F V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                  | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VMAXPD               | AVX512         | AVX512EVEX     | AVX512F_128    | 5f           | mm         | rrr        | EVV 0x5F V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VMAXPD               | AVX512         | AVX512EVEX     | AVX512F_256    | 5f           | 0b11       | rrr        | EVV 0x5F V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                  | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
 | VMAXPD               | AVX512         | AVX512EVEX     | AVX512F_256    | 5f           | mm         | rrr        | EVV 0x5F V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VMAXPD               | AVX512         | AVX512EVEX     | AVX512F_512    | 5f           | 0b11       | rrr        | EVV 0x5F V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                  | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VMAXPD               | AVX512         | AVX512EVEX     | AVX512F_512    | 5f           | 0b11       | rrr        | EVV 0x5F V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W1                | REG0=ZMM_R3():w:zf64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VMAXPD               | AVX512         | AVX512EVEX     | AVX512F_512    | 5f           | mm         | rrr        | EVV 0x5F V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VMAXPS               | AVX            | AVX            |                | 5f           | mm         | rrr        | VV1 0x5F  VNP V0F VL128 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
 | VMAXPS               | AVX            | AVX            |                | 5f           | 0b11       | rrr        | VV1 0x5F  VNP V0F VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
 | VMAXPS               | AVX            | AVX            |                | 5f           | mm         | rrr        | VV1 0x5F  VNP V0F VL256 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
 | VMAXPS               | AVX            | AVX            |                | 5f           | 0b11       | rrr        | VV1 0x5F  VNP V0F VL256 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
 | VMAXPS               | AVX512         | AVX512EVEX     | AVX512F_128    | 5f           | 0b11       | rrr        | EVV 0x5F VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                  | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VMAXPS               | AVX512         | AVX512EVEX     | AVX512F_128    | 5f           | mm         | rrr        | EVV 0x5F VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VMAXPS               | AVX512         | AVX512EVEX     | AVX512F_256    | 5f           | 0b11       | rrr        | EVV 0x5F VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                  | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
 | VMAXPS               | AVX512         | AVX512EVEX     | AVX512F_256    | 5f           | mm         | rrr        | EVV 0x5F VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VMAXPS               | AVX512         | AVX512EVEX     | AVX512F_512    | 5f           | 0b11       | rrr        | EVV 0x5F VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                  | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VMAXPS               | AVX512         | AVX512EVEX     | AVX512F_512    | 5f           | 0b11       | rrr        | EVV 0x5F VNP V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0                | REG0=ZMM_R3():w:zf32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VMAXPS               | AVX512         | AVX512EVEX     | AVX512F_512    | 5f           | mm         | rrr        | EVV 0x5F VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VMAXSD               | AVX            | AVX            |                | 5f           | mm         | rrr        | VV1 0x5F  VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                            | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:q:f64
 | VMAXSD               | AVX            | AVX            |                | 5f           | 0b11       | rrr        | VV1 0x5F  VF2 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                   | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:q:f64
 | VMAXSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 5f           | 0b11       | rrr        | EVV 0x5F VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                                         | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VMAXSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 5f           | 0b11       | rrr        | EVV 0x5F VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W1                | REG0=XMM_R3():w:dq:f64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VMAXSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 5f           | mm         | rrr        | EVV 0x5F VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
 | VMAXSS               | AVX            | AVX            |                | 5f           | mm         | rrr        | VV1 0x5F  VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                            | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:d:f32
 | VMAXSS               | AVX            | AVX            |                | 5f           | 0b11       | rrr        | VV1 0x5F  VF3 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                   | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:d:f32
 | VMAXSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 5f           | 0b11       | rrr        | EVV 0x5F VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                                         | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VMAXSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 5f           | 0b11       | rrr        | EVV 0x5F VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W0                | REG0=XMM_R3():w:dq:f32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VMAXSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 5f           | mm         | rrr        | EVV 0x5F VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
 | VMCALL               | VTX            | VTX            |                | 0f 01        | 0b11       | 0b000      | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b000] RM[0b001] no_refining_prefix                                    | 
 | VMCLEAR              | VTX            | VTX            |                | 0f c7        | mm         | 0b110      | 0x0F 0xC7  MOD[mm] MOD!=3 REG[0b110] RM[nnn] osz_refining_prefix REFINING66() MODRM()                | MEM0:r:q
 | VMFUNC               | VTX            | VMFUNC         | VMFUNC         | 0f 01        | 0b11       | 0b010      | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b010] RM[0b100] no_refining_prefix                                    | REG0=XED_REG_EAX:r:SUPP
 | VMINPD               | AVX            | AVX            |                | 5d           | mm         | rrr        | VV1 0x5D  V66 V0F VL128 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
 | VMINPD               | AVX            | AVX            |                | 5d           | 0b11       | rrr        | VV1 0x5D  V66 V0F VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
 | VMINPD               | AVX            | AVX            |                | 5d           | mm         | rrr        | VV1 0x5D  V66 V0F VL256 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
 | VMINPD               | AVX            | AVX            |                | 5d           | 0b11       | rrr        | VV1 0x5D  V66 V0F VL256 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
 | VMINPD               | AVX512         | AVX512EVEX     | AVX512F_128    | 5d           | 0b11       | rrr        | EVV 0x5D V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                  | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VMINPD               | AVX512         | AVX512EVEX     | AVX512F_128    | 5d           | mm         | rrr        | EVV 0x5D V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VMINPD               | AVX512         | AVX512EVEX     | AVX512F_256    | 5d           | 0b11       | rrr        | EVV 0x5D V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                  | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
 | VMINPD               | AVX512         | AVX512EVEX     | AVX512F_256    | 5d           | mm         | rrr        | EVV 0x5D V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VMINPD               | AVX512         | AVX512EVEX     | AVX512F_512    | 5d           | 0b11       | rrr        | EVV 0x5D V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                  | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VMINPD               | AVX512         | AVX512EVEX     | AVX512F_512    | 5d           | 0b11       | rrr        | EVV 0x5D V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W1                | REG0=ZMM_R3():w:zf64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VMINPD               | AVX512         | AVX512EVEX     | AVX512F_512    | 5d           | mm         | rrr        | EVV 0x5D V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VMINPS               | AVX            | AVX            |                | 5d           | mm         | rrr        | VV1 0x5D  VNP V0F VL128 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
 | VMINPS               | AVX            | AVX            |                | 5d           | 0b11       | rrr        | VV1 0x5D  VNP V0F VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
 | VMINPS               | AVX            | AVX            |                | 5d           | mm         | rrr        | VV1 0x5D  VNP V0F VL256 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
 | VMINPS               | AVX            | AVX            |                | 5d           | 0b11       | rrr        | VV1 0x5D  VNP V0F VL256 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
 | VMINPS               | AVX512         | AVX512EVEX     | AVX512F_128    | 5d           | 0b11       | rrr        | EVV 0x5D VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                  | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VMINPS               | AVX512         | AVX512EVEX     | AVX512F_128    | 5d           | mm         | rrr        | EVV 0x5D VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VMINPS               | AVX512         | AVX512EVEX     | AVX512F_256    | 5d           | 0b11       | rrr        | EVV 0x5D VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                  | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
 | VMINPS               | AVX512         | AVX512EVEX     | AVX512F_256    | 5d           | mm         | rrr        | EVV 0x5D VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VMINPS               | AVX512         | AVX512EVEX     | AVX512F_512    | 5d           | 0b11       | rrr        | EVV 0x5D VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                  | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VMINPS               | AVX512         | AVX512EVEX     | AVX512F_512    | 5d           | 0b11       | rrr        | EVV 0x5D VNP V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0                | REG0=ZMM_R3():w:zf32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VMINPS               | AVX512         | AVX512EVEX     | AVX512F_512    | 5d           | mm         | rrr        | EVV 0x5D VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VMINSD               | AVX            | AVX            |                | 5d           | mm         | rrr        | VV1 0x5D  VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                            | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:q:f64
 | VMINSD               | AVX            | AVX            |                | 5d           | 0b11       | rrr        | VV1 0x5D  VF2 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                   | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:q:f64
 | VMINSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 5d           | 0b11       | rrr        | EVV 0x5D VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                                         | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VMINSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 5d           | 0b11       | rrr        | EVV 0x5D VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W1                | REG0=XMM_R3():w:dq:f64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VMINSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 5d           | mm         | rrr        | EVV 0x5D VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
 | VMINSS               | AVX            | AVX            |                | 5d           | mm         | rrr        | VV1 0x5D  VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                            | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:d:f32
 | VMINSS               | AVX            | AVX            |                | 5d           | 0b11       | rrr        | VV1 0x5D  VF3 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                   | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:d:f32
 | VMINSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 5d           | 0b11       | rrr        | EVV 0x5D VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                                         | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VMINSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 5d           | 0b11       | rrr        | EVV 0x5D VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W0                | REG0=XMM_R3():w:dq:f32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VMINSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 5d           | mm         | rrr        | EVV 0x5D VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
 | VMLAUNCH             | VTX            | VTX            |                | 0f 01        | 0b11       | 0b000      | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b000] RM[0b010] no_refining_prefix                                    | 
 | VMLOAD               | SYSTEM         | SVM            |                | 0f 01        | 0b11       | 0b011      | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b011] RM[0b010]                                                       | REG0=ArAX():r:IMPL
 | VMMCALL              | SYSTEM         | SVM            |                | 0f 01        | 0b11       | 0b011      | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b011] RM[0b001]                                                       | 
 | VMOVAPD              | DATAXFER       | AVX            |                | 28           | mm         | rrr        | VV1 0x28  VL128 V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:f64  MEM0:r:dq:f64
 | VMOVAPD              | DATAXFER       | AVX            |                | 28           | 0b11       | rrr        | VV1 0x28  VL128 V66 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:f64  REG1=XMM_B():r:dq:f64
 | VMOVAPD              | DATAXFER       | AVX            |                | 29           | mm         | rrr        | VV1 0x29  VL128 V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:dq:f64 REG0=XMM_R():r:dq:f64
 | VMOVAPD              | DATAXFER       | AVX            |                | 29           | 0b11       | rrr        | VV1 0x29  VL128 V66 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_B():w:dq:f64 REG1=XMM_R():r:dq:f64
 | VMOVAPD              | DATAXFER       | AVX            |                | 28           | mm         | rrr        | VV1 0x28  VL256 V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=YMM_R():w:qq:f64  MEM0:r:qq:f64
 | VMOVAPD              | DATAXFER       | AVX            |                | 28           | 0b11       | rrr        | VV1 0x28  VL256 V66 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=YMM_R():w:qq:f64  REG1=YMM_B():r:qq:f64
 | VMOVAPD              | DATAXFER       | AVX            |                | 29           | mm         | rrr        | VV1 0x29  VL256 V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:qq:f64 REG0=YMM_R():r:qq:f64
 | VMOVAPD              | DATAXFER       | AVX            |                | 29           | 0b11       | rrr        | VV1 0x29  VL256 V66 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=YMM_B():w:qq:f64 REG1=YMM_R():r:qq:f64
 | VMOVAPD              | DATAXFER       | AVX512EVEX     | AVX512F_128    | 28           | 0b11       | rrr        | EVV 0x28 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                          | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64
 | VMOVAPD              | DATAXFER       | AVX512EVEX     | AVX512F_128    | 28           | mm         | rrr        | EVV 0x28 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:f64
 | VMOVAPD              | DATAXFER       | AVX512EVEX     | AVX512F_128    | 29           | 0b11       | rrr        | EVV 0x29 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                          | REG0=XMM_B3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:f64
 | VMOVAPD              | DATAXFER       | AVX512EVEX     | AVX512F_128    | 29           | mm         | rrr        | EVV 0x29 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_FULLMEM() | MEM0:w:dq:f64 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:f64
 | VMOVAPD              | DATAXFER       | AVX512EVEX     | AVX512F_256    | 28           | 0b11       | rrr        | EVV 0x28 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                          | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64
 | VMOVAPD              | DATAXFER       | AVX512EVEX     | AVX512F_256    | 28           | mm         | rrr        | EVV 0x28 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:f64
 | VMOVAPD              | DATAXFER       | AVX512EVEX     | AVX512F_256    | 29           | 0b11       | rrr        | EVV 0x29 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                          | REG0=YMM_B3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:f64
 | VMOVAPD              | DATAXFER       | AVX512EVEX     | AVX512F_256    | 29           | mm         | rrr        | EVV 0x29 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_FULLMEM() | MEM0:w:qq:f64 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:f64
 | VMOVAPD              | DATAXFER       | AVX512EVEX     | AVX512F_512    | 28           | 0b11       | rrr        | EVV 0x28 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                          | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
 | VMOVAPD              | DATAXFER       | AVX512EVEX     | AVX512F_512    | 28           | mm         | rrr        | EVV 0x28 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:f64
 | VMOVAPD              | DATAXFER       | AVX512EVEX     | AVX512F_512    | 29           | 0b11       | rrr        | EVV 0x29 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                          | REG0=ZMM_B3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zf64
 | VMOVAPD              | DATAXFER       | AVX512EVEX     | AVX512F_512    | 29           | mm         | rrr        | EVV 0x29 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_FULLMEM() | MEM0:w:zd:f64 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zf64
 | VMOVAPD              | DATAXFER       | KNCE           |                | 28           | mm         | rrr        | KVV 0x28 V0F V66  REXW=1 NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT64_LOAD()      | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VMOVAPD              | DATAXFER       | KNCE           |                | 28           | 0b11       | rrr        | KVV 0x28 V0F V66  REXW=1 NOEVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]  NR=0 REG_SWIZZLE64()               | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_B3():r:zf64:TXT=REGSWIZ
 | VMOVAPD              | DATAXFER       | KNCE           |                | 28           | 0b11       | rrr        | KVV 0x28 V0F V66  REXW=1 NOEVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]  NR=1 SWIZ=0                        | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_B3():r:zf64
 | VMOVAPD              | DATAXFER       | KNCE           |                | 29           | mm         | rrr        | KVV 0x29 V0F V66  REXW=1 NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM()  DNCONVERT_FLT64()          | MEM0:w:zf64:TXT=NT  REG0=MASK1():r:mskw REG1=ZMM_R3():r:zf64
 | VMOVAPS              | DATAXFER       | AVX            |                | 28           | mm         | rrr        | VV1 0x28  VL128 VNP V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:f32  MEM0:r:dq:f32
 | VMOVAPS              | DATAXFER       | AVX            |                | 28           | 0b11       | rrr        | VV1 0x28  VL128 VNP V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:f32  REG1=XMM_B():r:dq:f32
 | VMOVAPS              | DATAXFER       | AVX            |                | 29           | mm         | rrr        | VV1 0x29  VL128 VNP V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:dq:f32 REG0=XMM_R():r:dq:f32
 | VMOVAPS              | DATAXFER       | AVX            |                | 29           | 0b11       | rrr        | VV1 0x29  VL128 VNP V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_B():w:dq:f32 REG1=XMM_R():r:dq:f32
 | VMOVAPS              | DATAXFER       | AVX            |                | 28           | mm         | rrr        | VV1 0x28  VL256 VNP V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=YMM_R():w:qq:f32  MEM0:r:qq:f32
 | VMOVAPS              | DATAXFER       | AVX            |                | 28           | 0b11       | rrr        | VV1 0x28  VL256 VNP V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=YMM_R():w:qq:f32  REG1=YMM_B():r:qq:f32
 | VMOVAPS              | DATAXFER       | AVX            |                | 29           | mm         | rrr        | VV1 0x29  VL256 VNP V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:qq:f32 REG0=YMM_R():r:qq:f32
 | VMOVAPS              | DATAXFER       | AVX            |                | 29           | 0b11       | rrr        | VV1 0x29  VL256 VNP V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=YMM_B():w:qq:f32 REG1=YMM_R():r:qq:f32
 | VMOVAPS              | DATAXFER       | AVX512EVEX     | AVX512F_128    | 28           | 0b11       | rrr        | EVV 0x28 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                          | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
 | VMOVAPS              | DATAXFER       | AVX512EVEX     | AVX512F_128    | 28           | mm         | rrr        | EVV 0x28 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:f32
 | VMOVAPS              | DATAXFER       | AVX512EVEX     | AVX512F_128    | 29           | 0b11       | rrr        | EVV 0x29 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                          | REG0=XMM_B3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:f32
 | VMOVAPS              | DATAXFER       | AVX512EVEX     | AVX512F_128    | 29           | mm         | rrr        | EVV 0x29 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_FULLMEM() | MEM0:w:dq:f32 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:f32
 | VMOVAPS              | DATAXFER       | AVX512EVEX     | AVX512F_256    | 28           | 0b11       | rrr        | EVV 0x28 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                          | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
 | VMOVAPS              | DATAXFER       | AVX512EVEX     | AVX512F_256    | 28           | mm         | rrr        | EVV 0x28 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:f32
 | VMOVAPS              | DATAXFER       | AVX512EVEX     | AVX512F_256    | 29           | 0b11       | rrr        | EVV 0x29 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                          | REG0=YMM_B3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:f32
 | VMOVAPS              | DATAXFER       | AVX512EVEX     | AVX512F_256    | 29           | mm         | rrr        | EVV 0x29 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_FULLMEM() | MEM0:w:qq:f32 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:f32
 | VMOVAPS              | DATAXFER       | AVX512EVEX     | AVX512F_512    | 28           | 0b11       | rrr        | EVV 0x28 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                          | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
 | VMOVAPS              | DATAXFER       | AVX512EVEX     | AVX512F_512    | 28           | mm         | rrr        | EVV 0x28 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:f32
 | VMOVAPS              | DATAXFER       | AVX512EVEX     | AVX512F_512    | 29           | 0b11       | rrr        | EVV 0x29 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                          | REG0=ZMM_B3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zf32
 | VMOVAPS              | DATAXFER       | AVX512EVEX     | AVX512F_512    | 29           | mm         | rrr        | EVV 0x29 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_FULLMEM() | MEM0:w:zd:f32 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zf32
 | VMOVAPS              | DATAXFER       | KNCE           |                | 28           | mm         | rrr        | KVV 0x28 V0F VNP  REXW=0 NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT32_LOAD()      | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw  MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VMOVAPS              | DATAXFER       | KNCE           |                | 28           | 0b11       | rrr        | KVV 0x28 V0F VNP  REXW=0 NOEVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw  REG2=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VMOVAPS              | DATAXFER       | KNCE           |                | 28           | 0b11       | rrr        | KVV 0x28 V0F VNP  REXW=0 NOEVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 SWIZ=0                         | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw  REG2=ZMM_B3():r:zf32
 | VMOVAPS              | DATAXFER       | KNCE           |                | 29           | mm         | rrr        | KVV 0x29 V0F VNP  REXW=0 NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() DNCONVERT_FLT32()           | MEM0:w:zv:TXT=CONVERT:TXT=NT  REG0=MASK1():r:mskw   REG1=ZMM_R3():r:zf32
 | VMOVD                | DATAXFER       | AVX            |                | 6e           | mm         | rrr        | VV1 0x6E  VL128 V66 V0F not64 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                          | REG0=XMM_R():w:dq  MEM0:r:d
 | VMOVD                | DATAXFER       | AVX            |                | 6e           | 0b11       | rrr        | VV1 0x6E  VL128 V66 V0F not64  NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                | REG0=XMM_R():w:dq  REG1=GPR32_B():r:d
 | VMOVD                | DATAXFER       | AVX            |                | 7e           | mm         | rrr        | VV1 0x7E  VL128 V66 V0F not64  NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                         | MEM0:w:d           REG0=XMM_R():r:d
 | VMOVD                | DATAXFER       | AVX            |                | 7e           | 0b11       | rrr        | VV1 0x7E  VL128 V66 V0F not64  NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                | REG0=GPR32_B():w:d REG1=XMM_R():r:d
 | VMOVD                | DATAXFER       | AVX            |                | 6e           | mm         | rrr        | VV1 0x6E  VL128 V66 V0F mode64 norexw_prefix NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()           | REG0=XMM_R():w:dq  MEM0:r:d
 | VMOVD                | DATAXFER       | AVX            |                | 6e           | 0b11       | rrr        | VV1 0x6E  VL128 V66 V0F mode64 norexw_prefix NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                  | REG0=XMM_R():w:dq  REG1=GPR32_B():r:d
 | VMOVD                | DATAXFER       | AVX            |                | 7e           | mm         | rrr        | VV1 0x7E  VL128 V66 V0F mode64 norexw_prefix NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()           | MEM0:w:d           REG0=XMM_R():r:d
 | VMOVD                | DATAXFER       | AVX            |                | 7e           | 0b11       | rrr        | VV1 0x7E  VL128 V66 V0F mode64 norexw_prefix NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                  | REG0=GPR32_B():w:d REG1=XMM_R():r:d
 | VMOVD                | DATAXFER       | AVX512EVEX     | AVX512F_128N   | 6e           | 0b11       | rrr        | EVV 0x6E V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  not64  NOEVSR  ZEROING=0 MASK=0     | REG0=XMM_R3():w:dq:u32 REG1=GPR32_B():r:d:u32
 | VMOVD                | DATAXFER       | AVX512EVEX     | AVX512F_128N   | 6e           | 0b11       | rrr        | EVV 0x6E V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  mode64 W0  NOEVSR  ZEROING=0 MASK=0 | REG0=XMM_R3():w:dq:u32 REG1=GPR32_B():r:d:u32
 | VMOVD                | DATAXFER       | AVX512EVEX     | AVX512F_128N   | 6e           | mm         | rrr        | EVV 0x6E V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  not64  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_READER() | REG0=XMM_R3():w:dq:u32 MEM0:r:d:u32
 | VMOVD                | DATAXFER       | AVX512EVEX     | AVX512F_128N   | 6e           | mm         | rrr        | EVV 0x6E V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  mode64 W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_READER() | REG0=XMM_R3():w:dq:u32 MEM0:r:d:u32
 | VMOVD                | DATAXFER       | AVX512EVEX     | AVX512F_128N   | 7e           | 0b11       | rrr        | EVV 0x7E V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  not64  NOEVSR  ZEROING=0 MASK=0     | REG0=GPR32_B():w:d:u32 REG1=XMM_R3():r:dq:u32
 | VMOVD                | DATAXFER       | AVX512EVEX     | AVX512F_128N   | 7e           | 0b11       | rrr        | EVV 0x7E V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  mode64 W0  NOEVSR  ZEROING=0 MASK=0 | REG0=GPR32_B():w:d:u32 REG1=XMM_R3():r:dq:u32
 | VMOVD                | DATAXFER       | AVX512EVEX     | AVX512F_128N   | 7e           | mm         | rrr        | EVV 0x7E V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  not64  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_WRITER_STORE() | MEM0:w:d:u32 REG0=XMM_R3():r:dq:u32
 | VMOVD                | DATAXFER       | AVX512EVEX     | AVX512F_128N   | 7e           | mm         | rrr        | EVV 0x7E V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  mode64 W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_WRITER_STORE() | MEM0:w:d:u32 REG0=XMM_R3():r:dq:u32
 | VMOVDDUP             | DATAXFER       | AVX            |                | 12           | mm         | rrr        | VV1 0x12  VL128 VF2 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:f64  MEM0:r:q:f64
 | VMOVDDUP             | DATAXFER       | AVX            |                | 12           | 0b11       | rrr        | VV1 0x12  VL128 VF2 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:f64  REG1=XMM_B():r:dq:f64
 | VMOVDDUP             | DATAXFER       | AVX            |                | 12           | mm         | rrr        | VV1 0x12  VL256 VF2 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=YMM_R():w:qq:f64  MEM0:r:qq:f64
 | VMOVDDUP             | DATAXFER       | AVX            |                | 12           | 0b11       | rrr        | VV1 0x12  VL256 VF2 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=YMM_R():w:qq:f64  REG1=YMM_B():r:qq:f64
 | VMOVDDUP             | DATAXFER       | AVX512EVEX     | AVX512F_128    | 12           | 0b11       | rrr        | EVV 0x12 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                          | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64
 | VMOVDDUP             | DATAXFER       | AVX512EVEX     | AVX512F_128    | 12           | mm         | rrr        | EVV 0x12 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_MOVDDUP() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:f64
 | VMOVDDUP             | DATAXFER       | AVX512EVEX     | AVX512F_256    | 12           | 0b11       | rrr        | EVV 0x12 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                          | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64
 | VMOVDDUP             | DATAXFER       | AVX512EVEX     | AVX512F_256    | 12           | mm         | rrr        | EVV 0x12 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_MOVDDUP() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:f64
 | VMOVDDUP             | DATAXFER       | AVX512EVEX     | AVX512F_512    | 12           | 0b11       | rrr        | EVV 0x12 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                          | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
 | VMOVDDUP             | DATAXFER       | AVX512EVEX     | AVX512F_512    | 12           | mm         | rrr        | EVV 0x12 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_MOVDDUP() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:f64
 | VMOVDQA              | DATAXFER       | AVX            |                | 6f           | mm         | rrr        | VV1 0x6F  VL128 V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq  MEM0:r:dq
 | VMOVDQA              | DATAXFER       | AVX            |                | 6f           | 0b11       | rrr        | VV1 0x6F  VL128 V66 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq  REG1=XMM_B():r:dq
 | VMOVDQA              | DATAXFER       | AVX            |                | 7f           | mm         | rrr        | VV1 0x7F  VL128 V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:dq REG0=XMM_R():r:dq
 | VMOVDQA              | DATAXFER       | AVX            |                | 7f           | 0b11       | rrr        | VV1 0x7F  VL128 V66 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_B():w:dq REG1=XMM_R():r:dq
 | VMOVDQA              | DATAXFER       | AVX            |                | 6f           | mm         | rrr        | VV1 0x6F  VL256 V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=YMM_R():w:qq  MEM0:r:qq
 | VMOVDQA              | DATAXFER       | AVX            |                | 6f           | 0b11       | rrr        | VV1 0x6F  VL256 V66 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=YMM_R():w:qq  REG1=YMM_B():r:qq
 | VMOVDQA              | DATAXFER       | AVX            |                | 7f           | mm         | rrr        | VV1 0x7F  VL256 V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:qq REG0=YMM_R():r:qq
 | VMOVDQA              | DATAXFER       | AVX            |                | 7f           | 0b11       | rrr        | VV1 0x7F  VL256 V66 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=YMM_B():w:qq REG1=YMM_R():r:qq
 | VMOVDQA32            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 6f           | 0b11       | rrr        | EVV 0x6F V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                          | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32
 | VMOVDQA32            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 6f           | mm         | rrr        | EVV 0x6F V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u32
 | VMOVDQA32            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 7f           | 0b11       | rrr        | EVV 0x7F V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                          | REG0=XMM_B3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u32
 | VMOVDQA32            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 7f           | mm         | rrr        | EVV 0x7F V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_FULLMEM() | MEM0:w:dq:u32 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u32
 | VMOVDQA32            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 6f           | 0b11       | rrr        | EVV 0x6F V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                          | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u32
 | VMOVDQA32            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 6f           | mm         | rrr        | EVV 0x6F V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u32
 | VMOVDQA32            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 7f           | 0b11       | rrr        | EVV 0x7F V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                          | REG0=YMM_B3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u32
 | VMOVDQA32            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 7f           | mm         | rrr        | EVV 0x7F V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_FULLMEM() | MEM0:w:qq:u32 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u32
 | VMOVDQA32            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 6f           | 0b11       | rrr        | EVV 0x6F V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                          | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu32
 | VMOVDQA32            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 6f           | mm         | rrr        | EVV 0x6F V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:u32
 | VMOVDQA32            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 7f           | 0b11       | rrr        | EVV 0x7F V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                          | REG0=ZMM_B3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu32
 | VMOVDQA32            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 7f           | mm         | rrr        | EVV 0x7F V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_FULLMEM() | MEM0:w:zd:u32 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu32
 | VMOVDQA32            | DATAXFER       | KNCE           |                | 6f           | mm         | rrr        | KVV 0x6F  V0F V66  REXW=0 NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT32_LOAD()     | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VMOVDQA32            | DATAXFER       | KNCE           |                | 6f           | 0b11       | rrr        | KVV 0x6F  V0F V66  REXW=0 NOEVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()               | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  REG2=ZMM_B3():r:zd:TXT=REGSWIZ
 | VMOVDQA32            | DATAXFER       | KNCE           |                | 6f           | 0b11       | rrr        | KVV 0x6F  V0F V66  REXW=0 NOEVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 SWIZ=0                        | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  REG2=ZMM_B3():r:zd
 | VMOVDQA32            | DATAXFER       | KNCE           |                | 7f           | mm         | rrr        | KVV 0x7F  V0F V66  REXW=0 NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() DNCONVERT_INT32()          | MEM0:w:zv:TXT=NT:TXT=CONVERT  REG0=MASK1():r:mskw   REG1=ZMM_R3():r:zd
 | VMOVDQA64            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 6f           | 0b11       | rrr        | EVV 0x6F V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                          | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u64
 | VMOVDQA64            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 6f           | mm         | rrr        | EVV 0x6F V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u64
 | VMOVDQA64            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 7f           | 0b11       | rrr        | EVV 0x7F V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                          | REG0=XMM_B3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u64
 | VMOVDQA64            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 7f           | mm         | rrr        | EVV 0x7F V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_FULLMEM() | MEM0:w:dq:u64 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u64
 | VMOVDQA64            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 6f           | 0b11       | rrr        | EVV 0x6F V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                          | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u64
 | VMOVDQA64            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 6f           | mm         | rrr        | EVV 0x6F V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u64
 | VMOVDQA64            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 7f           | 0b11       | rrr        | EVV 0x7F V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                          | REG0=YMM_B3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u64
 | VMOVDQA64            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 7f           | mm         | rrr        | EVV 0x7F V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_FULLMEM() | MEM0:w:qq:u64 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u64
 | VMOVDQA64            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 6f           | 0b11       | rrr        | EVV 0x6F V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                          | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu64
 | VMOVDQA64            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 6f           | mm         | rrr        | EVV 0x6F V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:u64
 | VMOVDQA64            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 7f           | 0b11       | rrr        | EVV 0x7F V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                          | REG0=ZMM_B3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu64
 | VMOVDQA64            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 7f           | mm         | rrr        | EVV 0x7F V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_FULLMEM() | MEM0:w:zd:u64 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu64
 | VMOVDQA64            | DATAXFER       | KNCE           |                | 6f           | mm         | rrr        | KVV 0x6F  V0F V66  REXW=1 NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT64_LOAD()     | REG0=ZMM_R3():rw:zq REG1=MASK1():r:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VMOVDQA64            | DATAXFER       | KNCE           |                | 6f           | 0b11       | rrr        | KVV 0x6F  V0F V66  REXW=1 NOEVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]  NR=0 REG_SWIZZLE64()              | REG0=ZMM_R3():rw:zq REG1=MASK1():r:mskw REG2=ZMM_B3():r:zq:TXT=REGSWIZ
 | VMOVDQA64            | DATAXFER       | KNCE           |                | 6f           | 0b11       | rrr        | KVV 0x6F  V0F V66  REXW=1 NOEVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]  NR=1 SWIZ=0                       | REG0=ZMM_R3():rw:zq REG1=MASK1():r:mskw REG2=ZMM_B3():r:zq
 | VMOVDQA64            | DATAXFER       | KNCE           |                | 7f           | mm         | rrr        | KVV 0x7F  V0F V66  REXW=1 NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM()  DNCONVERT_INT64()         | MEM0:w:zq:TXT=NT  REG0=MASK1():r:mskw REG1=ZMM_R3():r:zq
 | VMOVDQU              | DATAXFER       | AVX            |                | 6f           | mm         | rrr        | VV1 0x6F  VL128 VF3 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq  MEM0:r:dq
 | VMOVDQU              | DATAXFER       | AVX            |                | 6f           | 0b11       | rrr        | VV1 0x6F  VL128 VF3 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq  REG1=XMM_B():r:dq
 | VMOVDQU              | DATAXFER       | AVX            |                | 6f           | mm         | rrr        | VV1 0x6F  VL256 VF3 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=YMM_R():w:qq  MEM0:r:qq
 | VMOVDQU              | DATAXFER       | AVX            |                | 6f           | 0b11       | rrr        | VV1 0x6F  VL256 VF3 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=YMM_R():w:qq  REG1=YMM_B():r:qq
 | VMOVDQU              | DATAXFER       | AVX            |                | 7f           | mm         | rrr        | VV1 0x7F  VL128 VF3 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:dq REG0=XMM_R():r:dq
 | VMOVDQU              | DATAXFER       | AVX            |                | 7f           | 0b11       | rrr        | VV1 0x7F  VL128 VF3 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_B():w:dq REG1=XMM_R():r:dq
 | VMOVDQU              | DATAXFER       | AVX            |                | 7f           | mm         | rrr        | VV1 0x7F  VL256 VF3 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:qq REG0=YMM_R():r:qq
 | VMOVDQU              | DATAXFER       | AVX            |                | 7f           | 0b11       | rrr        | VV1 0x7F  VL256 VF3 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=YMM_B():w:qq REG1=YMM_R():r:qq
 | VMOVDQU16            | DATAXFER       | AVX512EVEX     | AVX512BW_128   | 6f           | 0b11       | rrr        | EVV 0x6F VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                          | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u16
 | VMOVDQU16            | DATAXFER       | AVX512EVEX     | AVX512BW_128   | 6f           | mm         | rrr        | EVV 0x6F VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u16
 | VMOVDQU16            | DATAXFER       | AVX512EVEX     | AVX512BW_128   | 7f           | 0b11       | rrr        | EVV 0x7F VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                          | REG0=XMM_B3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u16
 | VMOVDQU16            | DATAXFER       | AVX512EVEX     | AVX512BW_128   | 7f           | mm         | rrr        | EVV 0x7F VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_FULLMEM() | MEM0:w:dq:u16 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u16
 | VMOVDQU16            | DATAXFER       | AVX512EVEX     | AVX512BW_256   | 6f           | 0b11       | rrr        | EVV 0x6F VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                          | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u16
 | VMOVDQU16            | DATAXFER       | AVX512EVEX     | AVX512BW_256   | 6f           | mm         | rrr        | EVV 0x6F VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u16
 | VMOVDQU16            | DATAXFER       | AVX512EVEX     | AVX512BW_256   | 7f           | 0b11       | rrr        | EVV 0x7F VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                          | REG0=YMM_B3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u16
 | VMOVDQU16            | DATAXFER       | AVX512EVEX     | AVX512BW_256   | 7f           | mm         | rrr        | EVV 0x7F VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_FULLMEM() | MEM0:w:qq:u16 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u16
 | VMOVDQU16            | DATAXFER       | AVX512EVEX     | AVX512BW_512   | 6f           | 0b11       | rrr        | EVV 0x6F VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                          | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu16
 | VMOVDQU16            | DATAXFER       | AVX512EVEX     | AVX512BW_512   | 6f           | mm         | rrr        | EVV 0x6F VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:u16
 | VMOVDQU16            | DATAXFER       | AVX512EVEX     | AVX512BW_512   | 7f           | 0b11       | rrr        | EVV 0x7F VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                          | REG0=ZMM_B3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu16
 | VMOVDQU16            | DATAXFER       | AVX512EVEX     | AVX512BW_512   | 7f           | mm         | rrr        | EVV 0x7F VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_FULLMEM() | MEM0:w:zd:u16 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu16
 | VMOVDQU32            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 6f           | 0b11       | rrr        | EVV 0x6F VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                          | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32
 | VMOVDQU32            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 6f           | mm         | rrr        | EVV 0x6F VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u32
 | VMOVDQU32            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 7f           | 0b11       | rrr        | EVV 0x7F VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                          | REG0=XMM_B3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u32
 | VMOVDQU32            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 7f           | mm         | rrr        | EVV 0x7F VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_FULLMEM() | MEM0:w:dq:u32 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u32
 | VMOVDQU32            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 6f           | 0b11       | rrr        | EVV 0x6F VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                          | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u32
 | VMOVDQU32            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 6f           | mm         | rrr        | EVV 0x6F VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u32
 | VMOVDQU32            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 7f           | 0b11       | rrr        | EVV 0x7F VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                          | REG0=YMM_B3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u32
 | VMOVDQU32            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 7f           | mm         | rrr        | EVV 0x7F VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_FULLMEM() | MEM0:w:qq:u32 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u32
 | VMOVDQU32            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 6f           | 0b11       | rrr        | EVV 0x6F VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                          | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu32
 | VMOVDQU32            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 6f           | mm         | rrr        | EVV 0x6F VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:u32
 | VMOVDQU32            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 7f           | 0b11       | rrr        | EVV 0x7F VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                          | REG0=ZMM_B3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu32
 | VMOVDQU32            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 7f           | mm         | rrr        | EVV 0x7F VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_FULLMEM() | MEM0:w:zd:u32 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu32
 | VMOVDQU64            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 6f           | 0b11       | rrr        | EVV 0x6F VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                          | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u64
 | VMOVDQU64            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 6f           | mm         | rrr        | EVV 0x6F VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u64
 | VMOVDQU64            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 7f           | 0b11       | rrr        | EVV 0x7F VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                          | REG0=XMM_B3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u64
 | VMOVDQU64            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 7f           | mm         | rrr        | EVV 0x7F VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_FULLMEM() | MEM0:w:dq:u64 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u64
 | VMOVDQU64            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 6f           | 0b11       | rrr        | EVV 0x6F VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                          | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u64
 | VMOVDQU64            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 6f           | mm         | rrr        | EVV 0x6F VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u64
 | VMOVDQU64            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 7f           | 0b11       | rrr        | EVV 0x7F VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                          | REG0=YMM_B3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u64
 | VMOVDQU64            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 7f           | mm         | rrr        | EVV 0x7F VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_FULLMEM() | MEM0:w:qq:u64 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u64
 | VMOVDQU64            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 6f           | 0b11       | rrr        | EVV 0x6F VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                          | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu64
 | VMOVDQU64            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 6f           | mm         | rrr        | EVV 0x6F VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:u64
 | VMOVDQU64            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 7f           | 0b11       | rrr        | EVV 0x7F VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                          | REG0=ZMM_B3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu64
 | VMOVDQU64            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 7f           | mm         | rrr        | EVV 0x7F VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_FULLMEM() | MEM0:w:zd:u64 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu64
 | VMOVDQU8             | DATAXFER       | AVX512EVEX     | AVX512BW_128   | 6f           | 0b11       | rrr        | EVV 0x6F VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                          | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u8
 | VMOVDQU8             | DATAXFER       | AVX512EVEX     | AVX512BW_128   | 6f           | mm         | rrr        | EVV 0x6F VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u8
 | VMOVDQU8             | DATAXFER       | AVX512EVEX     | AVX512BW_128   | 7f           | 0b11       | rrr        | EVV 0x7F VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                          | REG0=XMM_B3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u8
 | VMOVDQU8             | DATAXFER       | AVX512EVEX     | AVX512BW_128   | 7f           | mm         | rrr        | EVV 0x7F VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_FULLMEM() | MEM0:w:dq:u8 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u8
 | VMOVDQU8             | DATAXFER       | AVX512EVEX     | AVX512BW_256   | 6f           | 0b11       | rrr        | EVV 0x6F VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                          | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u8
 | VMOVDQU8             | DATAXFER       | AVX512EVEX     | AVX512BW_256   | 6f           | mm         | rrr        | EVV 0x6F VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u8
 | VMOVDQU8             | DATAXFER       | AVX512EVEX     | AVX512BW_256   | 7f           | 0b11       | rrr        | EVV 0x7F VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                          | REG0=YMM_B3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u8
 | VMOVDQU8             | DATAXFER       | AVX512EVEX     | AVX512BW_256   | 7f           | mm         | rrr        | EVV 0x7F VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_FULLMEM() | MEM0:w:qq:u8 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u8
 | VMOVDQU8             | DATAXFER       | AVX512EVEX     | AVX512BW_512   | 6f           | 0b11       | rrr        | EVV 0x6F VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                          | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu8
 | VMOVDQU8             | DATAXFER       | AVX512EVEX     | AVX512BW_512   | 6f           | mm         | rrr        | EVV 0x6F VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:u8
 | VMOVDQU8             | DATAXFER       | AVX512EVEX     | AVX512BW_512   | 7f           | 0b11       | rrr        | EVV 0x7F VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                          | REG0=ZMM_B3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu8
 | VMOVDQU8             | DATAXFER       | AVX512EVEX     | AVX512BW_512   | 7f           | mm         | rrr        | EVV 0x7F VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_FULLMEM() | MEM0:w:zd:u8 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu8
 | VMOVHLPS             | DATAXFER       | AVX            |                | 12           | 0b11       | rrr        | VV1 0x12  VL128 VNP V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
 | VMOVHLPS             | DATAXFER       | AVX512EVEX     | AVX512F_128N   | 12           | 0b11       | rrr        | EVV 0x12 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0    ZEROING=0 MASK=0              | REG0=XMM_R3():w:dq:f32 REG1=XMM_N3():r:dq:f32 REG2=XMM_B3():r:dq:f32
 | VMOVHPD              | DATAXFER       | AVX            |                | 16           | mm         | rrr        | VV1 0x16  VL128 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq:f64   REG1=XMM_N():r:q:f64   MEM0:r:q:f64
 | VMOVHPD              | DATAXFER       | AVX            |                | 17           | mm         | rrr        | VV1 0x17  VL128 V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:q:f64            REG0=XMM_R():r:dq:f64
 | VMOVHPD              | DATAXFER       | AVX512EVEX     | AVX512F_128N   | 16           | mm         | rrr        | EVV 0x16 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128 W1    ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=XMM_N3():r:q:f64 MEM0:r:q:f64
 | VMOVHPD              | DATAXFER       | AVX512EVEX     | AVX512F_128N   | 17           | mm         | rrr        | EVV 0x17 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM() VL128  W1  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_SCALAR() | MEM0:w:q:f64 REG0=XMM_R3():r:dq:f64
 | VMOVHPS              | DATAXFER       | AVX            |                | 16           | mm         | rrr        | VV1 0x16  VL128 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq:f32   REG1=XMM_N():r:q:f32   MEM0:r:q:f32
 | VMOVHPS              | DATAXFER       | AVX            |                | 17           | mm         | rrr        | VV1 0x17  VL128 VNP V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:q:f32            REG0=XMM_R():r:dq:f32
 | VMOVHPS              | DATAXFER       | AVX512EVEX     | AVX512F_128N   | 16           | mm         | rrr        | EVV 0x16 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0    ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_TUPLE2() | REG0=XMM_R3():w:dq:f32 REG1=XMM_N3():r:q:f32 MEM0:r:q:f32
 | VMOVHPS              | DATAXFER       | AVX512EVEX     | AVX512F_128N   | 17           | mm         | rrr        | EVV 0x17 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_TUPLE2() | MEM0:w:q:f32 REG0=XMM_R3():r:dq:f32
 | VMOVLHPS             | DATAXFER       | AVX            |                | 16           | 0b11       | rrr        | VV1 0x16  VL128 VNP V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:q:f32 REG2=XMM_B():r:q:f32
 | VMOVLHPS             | DATAXFER       | AVX512EVEX     | AVX512F_128N   | 16           | 0b11       | rrr        | EVV 0x16 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0    ZEROING=0 MASK=0              | REG0=XMM_R3():w:dq:f32 REG1=XMM_N3():r:q:f32 REG2=XMM_B3():r:q:f32
 | VMOVLPD              | DATAXFER       | AVX            |                | 12           | mm         | rrr        | VV1 0x12  VL128 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq:f64   REG1=XMM_N():r:dq:f64   MEM0:r:q:f64
 | VMOVLPD              | DATAXFER       | AVX            |                | 13           | mm         | rrr        | VV1 0x13  VL128 V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:q:f64            REG0=XMM_R():r:q:f64
 | VMOVLPD              | DATAXFER       | AVX512EVEX     | AVX512F_128N   | 12           | mm         | rrr        | EVV 0x12 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM() VL128  W1    ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=XMM_N3():r:dq:f64 MEM0:r:q:f64
 | VMOVLPD              | DATAXFER       | AVX512EVEX     | AVX512F_128N   | 13           | mm         | rrr        | EVV 0x13 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM() VL128 W1  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_SCALAR() | MEM0:w:q:f64 REG0=XMM_R3():r:q:f64
 | VMOVLPS              | DATAXFER       | AVX            |                | 12           | mm         | rrr        | VV1 0x12  VL128 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq:f32   REG1=XMM_N():r:dq:f32   MEM0:r:q:f32
 | VMOVLPS              | DATAXFER       | AVX            |                | 13           | mm         | rrr        | VV1 0x13  VL128 VNP V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:q:f32            REG0=XMM_R():r:q:f32
 | VMOVLPS              | DATAXFER       | AVX512EVEX     | AVX512F_128N   | 12           | mm         | rrr        | EVV 0x12 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0    ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_TUPLE2() | REG0=XMM_R3():w:dq:f32 REG1=XMM_N3():r:dq:f32 MEM0:r:q:f32
 | VMOVLPS              | DATAXFER       | AVX512EVEX     | AVX512F_128N   | 13           | mm         | rrr        | EVV 0x13 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_TUPLE2() | MEM0:w:q:f32 REG0=XMM_R3():r:q:f32
 | VMOVMSKPD            | DATAXFER       | AVX            |                | 50           | 0b11       | rrr        | VV1 0x50  VL128 V66 V0F  NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=GPR32_R():w:d   REG1=XMM_B():r:dq:f64
 | VMOVMSKPD            | DATAXFER       | AVX            |                | 50           | 0b11       | rrr        | VV1 0x50  VL256 V66 V0F  NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=GPR32_R():w:d   REG1=YMM_B():r:qq:f64
 | VMOVMSKPS            | DATAXFER       | AVX            |                | 50           | 0b11       | rrr        | VV1 0x50  VL128 VNP V0F  NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=GPR32_R():w:d   REG1=XMM_B():r:dq:f32
 | VMOVMSKPS            | DATAXFER       | AVX            |                | 50           | 0b11       | rrr        | VV1 0x50  VL256 VNP V0F  NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=GPR32_R():w:d   REG1=YMM_B():r:qq:f32
 | VMOVNRAPD            | DATAXFER       | KNCE           | KNCSTREAM      | 29           | mm         | rrr        | KVV 0x29 V0F VF3  REXW=1 NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] NR=0 MODRM()  DNCONVERT_FLT64()     | MEM0:rw:zf64:TXT=NT  REG0=MASK1():r:mskw REG1=ZMM_R3():r:zf64
 | VMOVNRAPS            | DATAXFER       | KNCE           | KNCSTREAM      | 29           | mm         | rrr        | KVV 0x29 V0F VF2  REXW=0 NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] NR=0 MODRM() DNCONVERT_FLT32()      | MEM0:rw:zv:TXT=CONVERT:TXT=NT  REG0=MASK1():r:mskw   REG1=ZMM_R3():r:zf32
 | VMOVNRNGOAPD         | DATAXFER       | KNCE           | KNCSTREAM      | 29           | mm         | rrr        | KVV 0x29 V0F VF3  REXW=1 NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] NR=1 MODRM()  DNCONVERT_FLT64()     | MEM0:rw:zf64:TXT=NT  REG0=MASK1():r:mskw REG1=ZMM_R3():r:zf64
 | VMOVNRNGOAPS         | DATAXFER       | KNCE           | KNCSTREAM      | 29           | mm         | rrr        | KVV 0x29 V0F VF2  REXW=0 NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] NR=1 MODRM() DNCONVERT_FLT32()      | MEM0:rw:zv:TXT=CONVERT:TXT=NT  REG0=MASK1():r:mskw   REG1=ZMM_R3():r:zf32
 | VMOVNTDQ             | DATAXFER       | AVX            |                | e7           | mm         | rrr        | VV1 0xE7  V66 V0F VL128 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:dq:i32  REG0=XMM_R():r:dq:i32
 | VMOVNTDQ             | DATAXFER       | AVX            |                | e7           | mm         | rrr        | VV1 0xE7  V66 V0F VL256 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:qq:i32  REG0=YMM_R():r:qq:i32
 | VMOVNTDQ             | DATAXFER       | AVX512EVEX     | AVX512F_128    | e7           | mm         | rrr        | EVV 0xE7 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_FULLMEM() | MEM0:w:dq:u32 REG0=XMM_R3():r:dq:u32
 | VMOVNTDQ             | DATAXFER       | AVX512EVEX     | AVX512F_256    | e7           | mm         | rrr        | EVV 0xE7 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_FULLMEM() | MEM0:w:qq:u32 REG0=YMM_R3():r:qq:u32
 | VMOVNTDQ             | DATAXFER       | AVX512EVEX     | AVX512F_512    | e7           | mm         | rrr        | EVV 0xE7 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_FULLMEM() | MEM0:w:zd:u32 REG0=ZMM_R3():r:zu32
 | VMOVNTDQA            | DATAXFER       | AVX            |                | 2a           | mm         | rrr        | VV1 0x2A  V66 V0F38 VL128 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                              | REG0=XMM_R():w:dq MEM0:r:dq
 | VMOVNTDQA            | DATAXFER       | AVX2           |                | 2a           | mm         | rrr        | VV1 0x2A  V66 V0F38 VL256 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                              | REG0=YMM_R():w:qq MEM0:r:qq
 | VMOVNTDQA            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 2a           | mm         | rrr        | EVV 0x2A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u32 MEM0:r:dq:u32
 | VMOVNTDQA            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 2a           | mm         | rrr        | EVV 0x2A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u32 MEM0:r:qq:u32
 | VMOVNTDQA            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 2a           | mm         | rrr        | EVV 0x2A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu32 MEM0:r:zd:u32
 | VMOVNTPD             | DATAXFER       | AVX            |                | 2b           | mm         | rrr        | VV1 0x2B  V66 V0F VL128 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:dq:f64  REG0=XMM_R():r:dq:f64
 | VMOVNTPD             | DATAXFER       | AVX            |                | 2b           | mm         | rrr        | VV1 0x2B  V66 V0F VL256 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:qq:f64  REG0=YMM_R():r:qq:f64
 | VMOVNTPD             | DATAXFER       | AVX512EVEX     | AVX512F_128    | 2b           | mm         | rrr        | EVV 0x2B V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_FULLMEM() | MEM0:w:dq:f64 REG0=XMM_R3():r:dq:f64
 | VMOVNTPD             | DATAXFER       | AVX512EVEX     | AVX512F_256    | 2b           | mm         | rrr        | EVV 0x2B V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_FULLMEM() | MEM0:w:qq:f64 REG0=YMM_R3():r:qq:f64
 | VMOVNTPD             | DATAXFER       | AVX512EVEX     | AVX512F_512    | 2b           | mm         | rrr        | EVV 0x2B V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_FULLMEM() | MEM0:w:zd:f64 REG0=ZMM_R3():r:zf64
 | VMOVNTPS             | DATAXFER       | AVX            |                | 2b           | mm         | rrr        | VV1 0x2B  VNP V0F VL128 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:dq:f32  REG0=XMM_R():r:dq:f32
 | VMOVNTPS             | DATAXFER       | AVX            |                | 2b           | mm         | rrr        | VV1 0x2B  VNP V0F VL256 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:qq:f32  REG0=YMM_R():r:qq:f32
 | VMOVNTPS             | DATAXFER       | AVX512EVEX     | AVX512F_128    | 2b           | mm         | rrr        | EVV 0x2B VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_FULLMEM() | MEM0:w:dq:f32 REG0=XMM_R3():r:dq:f32
 | VMOVNTPS             | DATAXFER       | AVX512EVEX     | AVX512F_256    | 2b           | mm         | rrr        | EVV 0x2B VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_FULLMEM() | MEM0:w:qq:f32 REG0=YMM_R3():r:qq:f32
 | VMOVNTPS             | DATAXFER       | AVX512EVEX     | AVX512F_512    | 2b           | mm         | rrr        | EVV 0x2B VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_FULLMEM() | MEM0:w:zd:f32 REG0=ZMM_R3():r:zf32
 | VMOVQ                | DATAXFER       | AVX            |                | 6e           | mm         | rrr        | VV1 0x6E  VL128 V66 V0F mode64 rexw_prefix NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()             | REG0=XMM_R():w:dq  MEM0:r:q
 | VMOVQ                | DATAXFER       | AVX            |                | 6e           | 0b11       | rrr        | VV1 0x6E  VL128 V66 V0F mode64 rexw_prefix NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=XMM_R():w:dq  REG1=GPR64_B():r:q
 | VMOVQ                | DATAXFER       | AVX            |                | 7e           | mm         | rrr        | VV1 0x7E  VL128 V66 V0F mode64 rexw_prefix NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()             | MEM0:w:q           REG0=XMM_R():r:q
 | VMOVQ                | DATAXFER       | AVX            |                | 7e           | 0b11       | rrr        | VV1 0x7E  VL128 V66 V0F mode64 rexw_prefix NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=GPR64_B():w:q REG1=XMM_R():r:q
 | VMOVQ                | DATAXFER       | AVX            |                | 7e           | mm         | rrr        | VV1 0x7E  VL128 VF3 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq   MEM0:r:q
 | VMOVQ                | DATAXFER       | AVX            |                | 7e           | 0b11       | rrr        | VV1 0x7E  VL128 VF3 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq   REG1=XMM_B():r:q
 | VMOVQ                | DATAXFER       | AVX            |                | d6           | mm         | rrr        | VV1 0xD6  VL128 V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:q   REG0=XMM_R():r:q
 | VMOVQ                | DATAXFER       | AVX            |                | d6           | 0b11       | rrr        | VV1 0xD6  VL128 V66 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_B():w:dq  REG1=XMM_R():r:q
 | VMOVQ                | DATAXFER       | AVX512EVEX     | AVX512F_128N   | 6e           | 0b11       | rrr        | EVV 0x6E V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  mode64  NOEVSR  ZEROING=0 MASK=0 | REG0=XMM_R3():w:dq:u64 REG1=GPR64_B():r:q:u64
 | VMOVQ                | DATAXFER       | AVX512EVEX     | AVX512F_128N   | 6e           | mm         | rrr        | EVV 0x6E V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  mode64  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_GPR_READER() | REG0=XMM_R3():w:dq:u64 MEM0:r:q:u64
 | VMOVQ                | DATAXFER       | AVX512EVEX     | AVX512F_128N   | 7e           | 0b11       | rrr        | EVV 0x7E V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  mode64  NOEVSR  ZEROING=0 MASK=0 | REG0=GPR64_B():w:q:u64 REG1=XMM_R3():r:dq:u64
 | VMOVQ                | DATAXFER       | AVX512EVEX     | AVX512F_128N   | 7e           | mm         | rrr        | EVV 0x7E V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  mode64  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_GPR_WRITER_STORE() | MEM0:w:q:u64 REG0=XMM_R3():r:dq:u64
 | VMOVQ                | DATAXFER       | AVX512EVEX     | AVX512F_128N   | 7e           | 0b11       | rrr        | EVV 0x7E VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn] VL128 W1  NOEVSR  ZEROING=0 MASK=0          | REG0=XMM_R3():w:dq:u64 REG1=XMM_B3():r:dq:u64
 | VMOVQ                | DATAXFER       | AVX512EVEX     | AVX512F_128N   | 7e           | mm         | rrr        | EVV 0x7E VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128 W1  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:u64 MEM0:r:q:u64
 | VMOVQ                | DATAXFER       | AVX512EVEX     | AVX512F_128N   | d6           | 0b11       | rrr        | EVV 0xD6 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128 W1  NOEVSR  ZEROING=0 MASK=0         | REG0=XMM_B3():w:dq:u64 REG1=XMM_R3():r:dq:u64
 | VMOVQ                | DATAXFER       | AVX512EVEX     | AVX512F_128N   | d6           | mm         | rrr        | EVV 0xD6 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128 W1  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_SCALAR() | MEM0:w:q:u64 REG0=XMM_R3():r:dq:u64
 | VMOVSD               | DATAXFER       | AVX            |                | 10           | mm         | rrr        | VV1 0x10  VF2 V0F MOD[mm] MOD!=3  NOVSR REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:f64   MEM0:r:q:f64
 | VMOVSD               | DATAXFER       | AVX            |                | 10           | 0b11       | rrr        | VV1 0x10  VF2 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                   | REG0=XMM_R():w:dq:f64  REG1=XMM_N():r:dq:f64    REG2=XMM_B():r:q:f64
 | VMOVSD               | DATAXFER       | AVX            |                | 11           | mm         | rrr        | VV1 0x11  VF2 V0F MOD[mm] MOD!=3 NOVSR REG[rrr] RM[nnn] MODRM()                                      | MEM0:w:q:f64           REG0=XMM_R():r:q:f64
 | VMOVSD               | DATAXFER       | AVX            |                | 11           | 0b11       | rrr        | VV1 0x11  VF2 V0F  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                  | REG0=XMM_B():w:dq:f64   REG1=XMM_N():r:dq:f64  REG2=XMM_R():r:q:f64
 | VMOVSD               | DATAXFER       | AVX512EVEX     | AVX512F_SCALAR | 10           | mm         | rrr        | EVV 0x10 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1  NOEVSR  ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:f64
 | VMOVSD               | DATAXFER       | AVX512EVEX     | AVX512F_SCALAR | 11           | mm         | rrr        | EVV 0x11 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_SCALAR() | MEM0:w:q:f64 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:f64
 | VMOVSD               | DATAXFER       | AVX512EVEX     | AVX512F_SCALAR | 10           | 0b11       | rrr        | EVV 0x10 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                                         | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VMOVSD               | DATAXFER       | AVX512EVEX     | AVX512F_SCALAR | 11           | 0b11       | rrr        | EVV 0x11 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                                         | REG0=XMM_B3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_R3():r:dq:f64
 | VMOVSHDUP            | DATAXFER       | AVX            |                | 16           | mm         | rrr        | VV1 0x16  VL128 VF3 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:f32 MEM0:r:dq:f32
 | VMOVSHDUP            | DATAXFER       | AVX            |                | 16           | 0b11       | rrr        | VV1 0x16  VL128 VF3 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:f32 REG1=XMM_B():r:dq:f32
 | VMOVSHDUP            | DATAXFER       | AVX            |                | 16           | mm         | rrr        | VV1 0x16  VL256 VF3 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=YMM_R():w:qq:f32 MEM0:r:qq:f32
 | VMOVSHDUP            | DATAXFER       | AVX            |                | 16           | 0b11       | rrr        | VV1 0x16  VL256 VF3 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=YMM_R():w:qq:f32 REG1=YMM_B():r:qq:f32
 | VMOVSHDUP            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 16           | 0b11       | rrr        | EVV 0x16 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                          | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
 | VMOVSHDUP            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 16           | mm         | rrr        | EVV 0x16 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:f32
 | VMOVSHDUP            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 16           | 0b11       | rrr        | EVV 0x16 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                          | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
 | VMOVSHDUP            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 16           | mm         | rrr        | EVV 0x16 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:f32
 | VMOVSHDUP            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 16           | 0b11       | rrr        | EVV 0x16 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                          | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
 | VMOVSHDUP            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 16           | mm         | rrr        | EVV 0x16 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:f32
 | VMOVSLDUP            | DATAXFER       | AVX            |                | 12           | mm         | rrr        | VV1 0x12  VL128 VF3 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:f32 MEM0:r:dq:f32
 | VMOVSLDUP            | DATAXFER       | AVX            |                | 12           | 0b11       | rrr        | VV1 0x12  VL128 VF3 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:f32 REG1=XMM_B():r:dq:f32
 | VMOVSLDUP            | DATAXFER       | AVX            |                | 12           | mm         | rrr        | VV1 0x12  VL256 VF3 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=YMM_R():w:qq:f32 MEM0:r:qq:f32
 | VMOVSLDUP            | DATAXFER       | AVX            |                | 12           | 0b11       | rrr        | VV1 0x12  VL256 VF3 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=YMM_R():w:qq:f32 REG1=YMM_B():r:qq:f32
 | VMOVSLDUP            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 12           | 0b11       | rrr        | EVV 0x12 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                          | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
 | VMOVSLDUP            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 12           | mm         | rrr        | EVV 0x12 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:f32
 | VMOVSLDUP            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 12           | 0b11       | rrr        | EVV 0x12 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                          | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
 | VMOVSLDUP            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 12           | mm         | rrr        | EVV 0x12 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:f32
 | VMOVSLDUP            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 12           | 0b11       | rrr        | EVV 0x12 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                          | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
 | VMOVSLDUP            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 12           | mm         | rrr        | EVV 0x12 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:f32
 | VMOVSS               | DATAXFER       | AVX            |                | 10           | mm         | rrr        | VV1 0x10  VF3 V0F MOD[mm] MOD!=3  NOVSR REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:f32  MEM0:r:d:f32
 | VMOVSS               | DATAXFER       | AVX            |                | 10           | 0b11       | rrr        | VV1 0x10  VF3 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                   | REG0=XMM_R():w:dq:f32  REG1=XMM_N():r:dq:f32    REG2=XMM_B():r:d:f32
 | VMOVSS               | DATAXFER       | AVX            |                | 11           | mm         | rrr        | VV1 0x11  VF3 V0F  MOD[mm] MOD!=3 NOVSR  REG[rrr] RM[nnn] MODRM()                                    | MEM0:w:d:f32          REG0=XMM_R():r:d:f32
 | VMOVSS               | DATAXFER       | AVX            |                | 11           | 0b11       | rrr        | VV1 0x11  VF3 V0F  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                  | REG0=XMM_B():w:dq:f32   REG1=XMM_N():r:dq:f32   REG2=XMM_R():r:d:f32
 | VMOVSS               | DATAXFER       | AVX512EVEX     | AVX512F_SCALAR | 10           | mm         | rrr        | EVV 0x10 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0  NOEVSR  ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:d:f32
 | VMOVSS               | DATAXFER       | AVX512EVEX     | AVX512F_SCALAR | 11           | mm         | rrr        | EVV 0x11 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_SCALAR() | MEM0:w:d:f32 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:f32
 | VMOVSS               | DATAXFER       | AVX512EVEX     | AVX512F_SCALAR | 10           | 0b11       | rrr        | EVV 0x10 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                                         | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VMOVSS               | DATAXFER       | AVX512EVEX     | AVX512F_SCALAR | 11           | 0b11       | rrr        | EVV 0x11 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                                         | REG0=XMM_B3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_R3():r:dq:f32
 | VMOVUPD              | DATAXFER       | AVX            |                | 10           | mm         | rrr        | VV1 0x10  V66 VL128 V0F NOVSR  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=XMM_R():w:dq:f64   MEM0:r:dq:f64
 | VMOVUPD              | DATAXFER       | AVX            |                | 10           | 0b11       | rrr        | VV1 0x10  V66 VL128 V0F NOVSR  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=XMM_R():w:dq:f64   REG1=XMM_B():r:dq:f64
 | VMOVUPD              | DATAXFER       | AVX            |                | 11           | mm         | rrr        | VV1 0x11  V66 VL128 V0F NOVSR  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | MEM0:w:dq:f64           REG0=XMM_R():r:dq:f64
 | VMOVUPD              | DATAXFER       | AVX            |                | 11           | 0b11       | rrr        | VV1 0x11  V66 VL128 V0F NOVSR  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=XMM_B():w:dq:f64   REG1=XMM_R():r:dq:f64
 | VMOVUPD              | DATAXFER       | AVX            |                | 10           | mm         | rrr        | VV1 0x10  V66 VL256 V0F NOVSR  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=YMM_R():w:qq:f64      MEM0:r:qq:f64
 | VMOVUPD              | DATAXFER       | AVX            |                | 10           | 0b11       | rrr        | VV1 0x10  V66 VL256 V0F NOVSR  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=YMM_R():w:qq:f64      REG1=YMM_B():r:qq:f64
 | VMOVUPD              | DATAXFER       | AVX            |                | 11           | mm         | rrr        | VV1 0x11  V66 VL256 V0F NOVSR  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | MEM0:w:qq:f64              REG0=YMM_R():r:qq:f64
 | VMOVUPD              | DATAXFER       | AVX            |                | 11           | 0b11       | rrr        | VV1 0x11  V66 VL256 V0F NOVSR  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=YMM_B():w:qq:f64      REG1=YMM_R():r:qq:f64
 | VMOVUPD              | DATAXFER       | AVX512EVEX     | AVX512F_128    | 10           | 0b11       | rrr        | EVV 0x10 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                          | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64
 | VMOVUPD              | DATAXFER       | AVX512EVEX     | AVX512F_128    | 10           | mm         | rrr        | EVV 0x10 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:f64
 | VMOVUPD              | DATAXFER       | AVX512EVEX     | AVX512F_128    | 11           | 0b11       | rrr        | EVV 0x11 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                          | REG0=XMM_B3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:f64
 | VMOVUPD              | DATAXFER       | AVX512EVEX     | AVX512F_128    | 11           | mm         | rrr        | EVV 0x11 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_FULLMEM() | MEM0:w:dq:f64 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:f64
 | VMOVUPD              | DATAXFER       | AVX512EVEX     | AVX512F_256    | 10           | 0b11       | rrr        | EVV 0x10 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                          | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64
 | VMOVUPD              | DATAXFER       | AVX512EVEX     | AVX512F_256    | 10           | mm         | rrr        | EVV 0x10 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:f64
 | VMOVUPD              | DATAXFER       | AVX512EVEX     | AVX512F_256    | 11           | 0b11       | rrr        | EVV 0x11 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                          | REG0=YMM_B3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:f64
 | VMOVUPD              | DATAXFER       | AVX512EVEX     | AVX512F_256    | 11           | mm         | rrr        | EVV 0x11 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_FULLMEM() | MEM0:w:qq:f64 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:f64
 | VMOVUPD              | DATAXFER       | AVX512EVEX     | AVX512F_512    | 10           | 0b11       | rrr        | EVV 0x10 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                          | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
 | VMOVUPD              | DATAXFER       | AVX512EVEX     | AVX512F_512    | 10           | mm         | rrr        | EVV 0x10 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:f64
 | VMOVUPD              | DATAXFER       | AVX512EVEX     | AVX512F_512    | 11           | 0b11       | rrr        | EVV 0x11 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                          | REG0=ZMM_B3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zf64
 | VMOVUPD              | DATAXFER       | AVX512EVEX     | AVX512F_512    | 11           | mm         | rrr        | EVV 0x11 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_FULLMEM() | MEM0:w:zd:f64 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zf64
 | VMOVUPS              | DATAXFER       | AVX            |                | 10           | mm         | rrr        | VV1 0x10  VNP VL128 V0F NOVSR  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=XMM_R():w:dq:f32   MEM0:r:dq:f32
 | VMOVUPS              | DATAXFER       | AVX            |                | 10           | 0b11       | rrr        | VV1 0x10  VNP VL128 V0F NOVSR  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=XMM_R():w:dq:f32   REG1=XMM_B():r:dq:f32
 | VMOVUPS              | DATAXFER       | AVX            |                | 11           | mm         | rrr        | VV1 0x11  VNP VL128 V0F NOVSR  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | MEM0:w:dq:f32           REG0=XMM_R():r:dq:f32
 | VMOVUPS              | DATAXFER       | AVX            |                | 11           | 0b11       | rrr        | VV1 0x11  VNP VL128 V0F NOVSR  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=XMM_B():w:dq:f32   REG1=XMM_R():r:dq:f32
 | VMOVUPS              | DATAXFER       | AVX            |                | 10           | mm         | rrr        | VV1 0x10  VNP VL256 V0F NOVSR  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=YMM_R():w:qq:f32      MEM0:r:qq:f32
 | VMOVUPS              | DATAXFER       | AVX            |                | 10           | 0b11       | rrr        | VV1 0x10  VNP VL256 V0F NOVSR  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=YMM_R():w:qq:f32      REG1=YMM_B():r:qq:f32
 | VMOVUPS              | DATAXFER       | AVX            |                | 11           | mm         | rrr        | VV1 0x11  VNP VL256 V0F NOVSR  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | MEM0:w:qq:f32              REG0=YMM_R():r:qq:f32
 | VMOVUPS              | DATAXFER       | AVX            |                | 11           | 0b11       | rrr        | VV1 0x11  VNP VL256 V0F NOVSR  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=YMM_B():w:qq:f32      REG1=YMM_R():r:qq:f32
 | VMOVUPS              | DATAXFER       | AVX512EVEX     | AVX512F_128    | 10           | 0b11       | rrr        | EVV 0x10 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                          | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
 | VMOVUPS              | DATAXFER       | AVX512EVEX     | AVX512F_128    | 10           | mm         | rrr        | EVV 0x10 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:f32
 | VMOVUPS              | DATAXFER       | AVX512EVEX     | AVX512F_128    | 11           | 0b11       | rrr        | EVV 0x11 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                          | REG0=XMM_B3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:f32
 | VMOVUPS              | DATAXFER       | AVX512EVEX     | AVX512F_128    | 11           | mm         | rrr        | EVV 0x11 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_FULLMEM() | MEM0:w:dq:f32 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:f32
 | VMOVUPS              | DATAXFER       | AVX512EVEX     | AVX512F_256    | 10           | 0b11       | rrr        | EVV 0x10 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                          | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
 | VMOVUPS              | DATAXFER       | AVX512EVEX     | AVX512F_256    | 10           | mm         | rrr        | EVV 0x10 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:f32
 | VMOVUPS              | DATAXFER       | AVX512EVEX     | AVX512F_256    | 11           | 0b11       | rrr        | EVV 0x11 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                          | REG0=YMM_B3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:f32
 | VMOVUPS              | DATAXFER       | AVX512EVEX     | AVX512F_256    | 11           | mm         | rrr        | EVV 0x11 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_FULLMEM() | MEM0:w:qq:f32 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:f32
 | VMOVUPS              | DATAXFER       | AVX512EVEX     | AVX512F_512    | 10           | 0b11       | rrr        | EVV 0x10 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                          | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
 | VMOVUPS              | DATAXFER       | AVX512EVEX     | AVX512F_512    | 10           | mm         | rrr        | EVV 0x10 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:f32
 | VMOVUPS              | DATAXFER       | AVX512EVEX     | AVX512F_512    | 11           | 0b11       | rrr        | EVV 0x11 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                          | REG0=ZMM_B3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zf32
 | VMOVUPS              | DATAXFER       | AVX512EVEX     | AVX512F_512    | 11           | mm         | rrr        | EVV 0x11 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_FULLMEM() | MEM0:w:zd:f32 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zf32
 | VMPSADBW             | AVX            | AVX            |                | 42           | mm         | rrr        | VV1 0x42  VL128 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                            | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u8 MEM0:r:dq:u8 IMM0:r:b
 | VMPSADBW             | AVX            | AVX            |                | 42           | 0b11       | rrr        | VV1 0x42  VL128 V66 V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                   | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:u8 IMM0:r:b
 | VMPSADBW             | AVX2           | AVX2           |                | 42           | mm         | rrr        | VV1 0x42  VL256 V66 V0F3A  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                           | REG0=YMM_R():w:qq:u16 REG1=YMM_N():r:qq:u8 MEM0:r:qq:u8 IMM0:r:b
 | VMPSADBW             | AVX2           | AVX2           |                | 42           | 0b11       | rrr        | VV1 0x42  VL256 V66 V0F3A  MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                  | REG0=YMM_R():w:qq:u16 REG1=YMM_N():r:qq:u8 REG2=YMM_B():r:qq:u8 IMM0:r:b
 | VMPTRLD              | VTX            | VTX            |                | 0f c7        | mm         | 0b110      | 0x0F 0xC7  MOD[mm] MOD!=3 REG[0b110] RM[nnn] no_refining_prefix  MODRM()                             | MEM0:r:q
 | VMPTRST              | VTX            | VTX            |                | 0f c7        | mm         | 0b111      | 0x0F 0xC7  MOD[mm] MOD!=3 REG[0b111] RM[nnn] no_refining_prefix MODRM()                              | MEM0:w:q
 | VMREAD               | VTX            | VTX            |                | 0f 78        | mm         | rrr        | 0x0F 0x78 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] mode64   MODRM() CR_WIDTH()             | MEM0:w:q REG0=GPR64_R():r
 | VMREAD               | VTX            | VTX            |                | 0f 78        | 0b11       | rrr        | 0x0F 0x78 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn] mode64 CR_WIDTH()                      | REG0=GPR64_B():w REG1=GPR64_R():r
 | VMREAD               | VTX            | VTX            |                | 0f 78        | mm         | rrr        | 0x0F 0x78 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] not64  MODRM() CR_WIDTH()               | MEM0:w:d REG0=GPR32_R():r
 | VMREAD               | VTX            | VTX            |                | 0f 78        | 0b11       | rrr        | 0x0F 0x78 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn] not64 CR_WIDTH()                       | REG0=GPR32_B():w REG1=GPR32_R():r
 | VMRESUME             | VTX            | VTX            |                | 0f 01        | 0b11       | 0b000      | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b000] RM[0b011] no_refining_prefix                                    | 
 | VMRUN                | SYSTEM         | SVM            |                | 0f 01        | 0b11       | 0b011      | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b011] RM[0b000]                                                       | REG0=ArAX():r:IMPL
 | VMSAVE               | SYSTEM         | SVM            |                | 0f 01        | 0b11       | 0b011      | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b011] RM[0b011]                                                       | 
 | VMULPD               | AVX            | AVX            |                | 59           | mm         | rrr        | VV1 0x59  VL128 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
 | VMULPD               | AVX            | AVX            |                | 59           | 0b11       | rrr        | VV1 0x59  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
 | VMULPD               | AVX            | AVX            |                | 59           | mm         | rrr        | VV1 0x59  VL256 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
 | VMULPD               | AVX            | AVX            |                | 59           | 0b11       | rrr        | VV1 0x59  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
 | VMULPD               | AVX512         | AVX512EVEX     | AVX512F_128    | 59           | 0b11       | rrr        | EVV 0x59 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                  | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VMULPD               | AVX512         | AVX512EVEX     | AVX512F_128    | 59           | mm         | rrr        | EVV 0x59 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VMULPD               | AVX512         | AVX512EVEX     | AVX512F_256    | 59           | 0b11       | rrr        | EVV 0x59 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                  | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
 | VMULPD               | AVX512         | AVX512EVEX     | AVX512F_256    | 59           | mm         | rrr        | EVV 0x59 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VMULPD               | AVX512         | AVX512EVEX     | AVX512F_512    | 59           | 0b11       | rrr        | EVV 0x59 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                  | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VMULPD               | AVX512         | AVX512EVEX     | AVX512F_512    | 59           | 0b11       | rrr        | EVV 0x59 V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1       | REG0=ZMM_R3():w:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VMULPD               | AVX512         | AVX512EVEX     | AVX512F_512    | 59           | mm         | rrr        | EVV 0x59 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VMULPD               | KNC            | KNCE           |                | 59           | mm         | rrr        | KVV 0x59 V0F V66  REXW=1 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT64()                  | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VMULPD               | KNC            | KNCE           |                | 59           | 0b11       | rrr        | KVV 0x59 V0F V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE64()                       | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=REGSWIZ
 | VMULPD               | KNC            | KNCE           |                | 59           | 0b11       | rrr        | KVV 0x59 V0F V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                     | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=ROUNDC:TXT=SAEC
 | VMULPS               | AVX            | AVX            |                | 59           | mm         | rrr        | VV1 0x59  VL128 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
 | VMULPS               | AVX            | AVX            |                | 59           | 0b11       | rrr        | VV1 0x59  VL128 VNP V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
 | VMULPS               | AVX            | AVX            |                | 59           | mm         | rrr        | VV1 0x59  VL256 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
 | VMULPS               | AVX            | AVX            |                | 59           | 0b11       | rrr        | VV1 0x59  VL256 VNP V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
 | VMULPS               | AVX512         | AVX512EVEX     | AVX512F_128    | 59           | 0b11       | rrr        | EVV 0x59 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                  | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VMULPS               | AVX512         | AVX512EVEX     | AVX512F_128    | 59           | mm         | rrr        | EVV 0x59 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VMULPS               | AVX512         | AVX512EVEX     | AVX512F_256    | 59           | 0b11       | rrr        | EVV 0x59 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                  | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
 | VMULPS               | AVX512         | AVX512EVEX     | AVX512F_256    | 59           | mm         | rrr        | EVV 0x59 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VMULPS               | AVX512         | AVX512EVEX     | AVX512F_512    | 59           | 0b11       | rrr        | EVV 0x59 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                  | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VMULPS               | AVX512         | AVX512EVEX     | AVX512F_512    | 59           | 0b11       | rrr        | EVV 0x59 VNP V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0       | REG0=ZMM_R3():w:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VMULPS               | AVX512         | AVX512EVEX     | AVX512F_512    | 59           | mm         | rrr        | EVV 0x59 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VMULPS               | KNC            | KNCE           |                | 59           | mm         | rrr        | KVV 0x59 V0F VNP  REXW=0 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT32()                  | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VMULPS               | KNC            | KNCE           |                | 59           | 0b11       | rrr        | KVV 0x59 V0F VNP  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                       | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VMULPS               | KNC            | KNCE           |                | 59           | 0b11       | rrr        | KVV 0x59 V0F VNP  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=ROUNDC:TXT=SAEC
 | VMULSD               | AVX            | AVX            |                | 59           | mm         | rrr        | VV1 0x59  VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                            | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:q:f64
 | VMULSD               | AVX            | AVX            |                | 59           | 0b11       | rrr        | VV1 0x59  VF2 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                   | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:q:f64
 | VMULSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 59           | 0b11       | rrr        | EVV 0x59 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                                         | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VMULSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 59           | 0b11       | rrr        | EVV 0x59 VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1       | REG0=XMM_R3():w:dq:f64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VMULSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 59           | mm         | rrr        | EVV 0x59 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
 | VMULSS               | AVX            | AVX            |                | 59           | mm         | rrr        | VV1 0x59  VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                            | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:d:f32
 | VMULSS               | AVX            | AVX            |                | 59           | 0b11       | rrr        | VV1 0x59  VF3 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                   | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:d:f32
 | VMULSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 59           | 0b11       | rrr        | EVV 0x59 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                                         | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VMULSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 59           | 0b11       | rrr        | EVV 0x59 VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W0       | REG0=XMM_R3():w:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VMULSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 59           | mm         | rrr        | EVV 0x59 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
 | VMWRITE              | VTX            | VTX            |                | 0f 79        | mm         | rrr        | 0x0F 0x79 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] mode64  MODRM() CR_WIDTH()              | REG0=GPR64_R():r MEM0:r:q
 | VMWRITE              | VTX            | VTX            |                | 0f 79        | 0b11       | rrr        | 0x0F 0x79 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn] mode64 CR_WIDTH()                      | REG0=GPR64_R():r REG1=GPR64_B():r
 | VMWRITE              | VTX            | VTX            |                | 0f 79        | mm         | rrr        | 0x0F 0x79 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] not64  MODRM() CR_WIDTH()               | REG0=GPR32_R():r MEM0:r:d
 | VMWRITE              | VTX            | VTX            |                | 0f 79        | 0b11       | rrr        | 0x0F 0x79 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn] not64  CR_WIDTH()                      | REG0=GPR32_R():r REG1=GPR32_B():r
 | VMXOFF               | VTX            | VTX            |                | 0f 01        | 0b11       | 0b000      | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b000] RM[0b100] no_refining_prefix                                    | 
 | VMXON                | VTX            | VTX            |                | 0f c7        | mm         | 0b110      | 0x0F 0xC7  MOD[mm] MOD!=3 REG[0b110] RM[nnn] f3_refining_prefix IGNORE66() MODRM()                   | MEM0:r:q
 | VORPD                | LOGICAL_FP     | AVX            |                | 56           | mm         | rrr        | VV1 0x56  VL128 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 MEM0:r:dq:u64
 | VORPD                | LOGICAL_FP     | AVX            |                | 56           | 0b11       | rrr        | VV1 0x56  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 REG2=XMM_B():r:dq:u64
 | VORPD                | LOGICAL_FP     | AVX            |                | 56           | mm         | rrr        | VV1 0x56  VL256 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=YMM_R():w:qq:u64 REG1=YMM_N():r:qq:u64 MEM0:r:qq:u64
 | VORPD                | LOGICAL_FP     | AVX            |                | 56           | 0b11       | rrr        | VV1 0x56  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:u64 REG1=YMM_N():r:qq:u64 REG2=YMM_B():r:qq:u64
 | VORPD                | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_128   | 56           | 0b11       | rrr        | EVV 0x56 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                  | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
 | VORPD                | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_128   | 56           | mm         | rrr        | EVV 0x56 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VORPD                | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_256   | 56           | 0b11       | rrr        | EVV 0x56 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                  | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
 | VORPD                | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_256   | 56           | mm         | rrr        | EVV 0x56 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VORPD                | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_512   | 56           | 0b11       | rrr        | EVV 0x56 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                  | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
 | VORPD                | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_512   | 56           | mm         | rrr        | EVV 0x56 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VORPS                | LOGICAL_FP     | AVX            |                | 56           | mm         | rrr        | VV1 0x56  VNP V0F VL128 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 MEM0:r:dq:u32
 | VORPS                | LOGICAL_FP     | AVX            |                | 56           | 0b11       | rrr        | VV1 0x56  VNP V0F VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 REG2=XMM_B():r:dq:u32
 | VORPS                | LOGICAL_FP     | AVX            |                | 56           | mm         | rrr        | VV1 0x56  VNP V0F VL256 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=YMM_R():w:qq:u32 REG1=YMM_N():r:qq:u32 MEM0:r:qq:u32
 | VORPS                | LOGICAL_FP     | AVX            |                | 56           | 0b11       | rrr        | VV1 0x56  VNP V0F VL256 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:u32 REG1=YMM_N():r:qq:u32 REG2=YMM_B():r:qq:u32
 | VORPS                | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_128   | 56           | 0b11       | rrr        | EVV 0x56 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                  | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
 | VORPS                | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_128   | 56           | mm         | rrr        | EVV 0x56 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VORPS                | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_256   | 56           | 0b11       | rrr        | EVV 0x56 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                  | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
 | VORPS                | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_256   | 56           | mm         | rrr        | EVV 0x56 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VORPS                | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_512   | 56           | 0b11       | rrr        | EVV 0x56 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                  | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
 | VORPS                | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_512   | 56           | mm         | rrr        | EVV 0x56 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VP2INTERSECTD        | AVX512_VP2INTERSECT | AVX512EVEX     | AVX512_VP2INTERSECT_128 | 68           | 0b11       | rrr        | EVV 0x68 VF2 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0    ZEROING=0 MASK=0            | REG0=MASK_R():w:mskw:MULTIDEST2 REG1=XMM_N3():r:dq:u32 REG2=XMM_B3():r:dq:u32
 | VP2INTERSECTD        | AVX512_VP2INTERSECT | AVX512EVEX     | AVX512_VP2INTERSECT_128 | 68           | mm         | rrr        | EVV 0x68 VF2 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw:MULTIDEST2 REG1=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VP2INTERSECTD        | AVX512_VP2INTERSECT | AVX512EVEX     | AVX512_VP2INTERSECT_256 | 68           | 0b11       | rrr        | EVV 0x68 VF2 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0    ZEROING=0 MASK=0            | REG0=MASK_R():w:mskw:MULTIDEST2 REG1=YMM_N3():r:qq:u32 REG2=YMM_B3():r:qq:u32
 | VP2INTERSECTD        | AVX512_VP2INTERSECT | AVX512EVEX     | AVX512_VP2INTERSECT_256 | 68           | mm         | rrr        | EVV 0x68 VF2 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw:MULTIDEST2 REG1=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VP2INTERSECTD        | AVX512_VP2INTERSECT | AVX512EVEX     | AVX512_VP2INTERSECT_512 | 68           | 0b11       | rrr        | EVV 0x68 VF2 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0    ZEROING=0 MASK=0            | REG0=MASK_R():w:mskw:MULTIDEST2 REG1=ZMM_N3():r:zu32 REG2=ZMM_B3():r:zu32
 | VP2INTERSECTD        | AVX512_VP2INTERSECT | AVX512EVEX     | AVX512_VP2INTERSECT_512 | 68           | mm         | rrr        | EVV 0x68 VF2 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw:MULTIDEST2 REG1=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VP2INTERSECTQ        | AVX512_VP2INTERSECT | AVX512EVEX     | AVX512_VP2INTERSECT_128 | 68           | 0b11       | rrr        | EVV 0x68 VF2 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1    ZEROING=0 MASK=0            | REG0=MASK_R():w:mskw:MULTIDEST2 REG1=XMM_N3():r:dq:u64 REG2=XMM_B3():r:dq:u64
 | VP2INTERSECTQ        | AVX512_VP2INTERSECT | AVX512EVEX     | AVX512_VP2INTERSECT_128 | 68           | mm         | rrr        | EVV 0x68 VF2 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw:MULTIDEST2 REG1=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VP2INTERSECTQ        | AVX512_VP2INTERSECT | AVX512EVEX     | AVX512_VP2INTERSECT_256 | 68           | 0b11       | rrr        | EVV 0x68 VF2 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1    ZEROING=0 MASK=0            | REG0=MASK_R():w:mskw:MULTIDEST2 REG1=YMM_N3():r:qq:u64 REG2=YMM_B3():r:qq:u64
 | VP2INTERSECTQ        | AVX512_VP2INTERSECT | AVX512EVEX     | AVX512_VP2INTERSECT_256 | 68           | mm         | rrr        | EVV 0x68 VF2 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw:MULTIDEST2 REG1=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VP2INTERSECTQ        | AVX512_VP2INTERSECT | AVX512EVEX     | AVX512_VP2INTERSECT_512 | 68           | 0b11       | rrr        | EVV 0x68 VF2 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1    ZEROING=0 MASK=0            | REG0=MASK_R():w:mskw:MULTIDEST2 REG1=ZMM_N3():r:zu64 REG2=ZMM_B3():r:zu64
 | VP2INTERSECTQ        | AVX512_VP2INTERSECT | AVX512EVEX     | AVX512_VP2INTERSECT_512 | 68           | mm         | rrr        | EVV 0x68 VF2 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw:MULTIDEST2 REG1=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VP4DPWSSD            | AVX512_4VNNIW  | AVX512EVEX     | AVX512_4VNNIW_512 | 52           | mm         | rrr        | EVV 0x52 VF2 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() BCRC=0  VL512  W0    ESIZE_32_BITS() NELEM_TUPLE1_4X() | REG0=ZMM_R3():rw:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16:MULTISOURCE4 MEM0:r:dq:u32
 | VP4DPWSSDS           | AVX512_4VNNIW  | AVX512EVEX     | AVX512_4VNNIW_512 | 53           | mm         | rrr        | EVV 0x53 VF2 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() BCRC=0  VL512  W0    ESIZE_32_BITS() NELEM_TUPLE1_4X() | REG0=ZMM_R3():rw:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16:MULTISOURCE4 MEM0:r:dq:u32
 | VPABSB               | AVX            | AVX            |                | 1c           | mm         | rrr        | VV1 0x1C   V66 V0F38 VL128 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                             | REG0=XMM_R():w:dq:u8 MEM0:r:dq:i8
 | VPABSB               | AVX            | AVX            |                | 1c           | 0b11       | rrr        | VV1 0x1C  V66 V0F38 VL128 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=XMM_R():w:dq:u8  REG1=XMM_B():r:dq:i8
 | VPABSB               | AVX2           | AVX2           |                | 1c           | mm         | rrr        | VV1 0x1C   VL256 V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                             | REG0=YMM_R():w:qq:u8 MEM0:r:qq:i8
 | VPABSB               | AVX2           | AVX2           |                | 1c           | 0b11       | rrr        | VV1 0x1C   VL256 V66 V0F38 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                    | REG0=YMM_R():w:qq:u8  REG1=YMM_B():r:qq:i8
 | VPABSB               | AVX512         | AVX512EVEX     | AVX512BW_128   | 1c           | 0b11       | rrr        | EVV 0x1C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR                          | REG0=XMM_R3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i8
 | VPABSB               | AVX512         | AVX512EVEX     | AVX512BW_128   | 1c           | mm         | rrr        | EVV 0x1C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128    NOEVSR  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:i8
 | VPABSB               | AVX512         | AVX512EVEX     | AVX512BW_256   | 1c           | 0b11       | rrr        | EVV 0x1C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256    NOEVSR                          | REG0=YMM_R3():w:qq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:i8
 | VPABSB               | AVX512         | AVX512EVEX     | AVX512BW_256   | 1c           | mm         | rrr        | EVV 0x1C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256    NOEVSR  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:i8
 | VPABSB               | AVX512         | AVX512EVEX     | AVX512BW_512   | 1c           | 0b11       | rrr        | EVV 0x1C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512    NOEVSR                          | REG0=ZMM_R3():w:zi8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zi8
 | VPABSB               | AVX512         | AVX512EVEX     | AVX512BW_512   | 1c           | mm         | rrr        | EVV 0x1C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512    NOEVSR  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zi8 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:i8
 | VPABSD               | AVX            | AVX            |                | 1e           | mm         | rrr        | VV1 0x1E   V66 V0F38 VL128 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                             | REG0=XMM_R():w:dq:u32 MEM0:r:dq:i32
 | VPABSD               | AVX            | AVX            |                | 1e           | 0b11       | rrr        | VV1 0x1E  V66 V0F38 VL128 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=XMM_R():w:dq:u32  REG1=XMM_B():r:dq:i32
 | VPABSD               | AVX2           | AVX2           |                | 1e           | mm         | rrr        | VV1 0x1E   VL256 V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                             | REG0=YMM_R():w:qq:u32 MEM0:r:qq:i32
 | VPABSD               | AVX2           | AVX2           |                | 1e           | 0b11       | rrr        | VV1 0x1E   VL256 V66 V0F38 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                    | REG0=YMM_R():w:qq:u32  REG1=YMM_B():r:qq:i32
 | VPABSD               | AVX512         | AVX512EVEX     | AVX512F_128    | 1e           | 0b11       | rrr        | EVV 0x1E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i32
 | VPABSD               | AVX512         | AVX512EVEX     | AVX512F_128    | 1e           | mm         | rrr        | EVV 0x1E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:i32:TXT=BCASTSTR
 | VPABSD               | AVX512         | AVX512EVEX     | AVX512F_256    | 1e           | 0b11       | rrr        | EVV 0x1E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:i32
 | VPABSD               | AVX512         | AVX512EVEX     | AVX512F_256    | 1e           | mm         | rrr        | EVV 0x1E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:i32:TXT=BCASTSTR
 | VPABSD               | AVX512         | AVX512EVEX     | AVX512F_512    | 1e           | 0b11       | rrr        | EVV 0x1E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zi32
 | VPABSD               | AVX512         | AVX512EVEX     | AVX512F_512    | 1e           | mm         | rrr        | EVV 0x1E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:i32:TXT=BCASTSTR
 | VPABSQ               | AVX512         | AVX512EVEX     | AVX512F_128    | 1f           | 0b11       | rrr        | EVV 0x1F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                        | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i64
 | VPABSQ               | AVX512         | AVX512EVEX     | AVX512F_128    | 1f           | mm         | rrr        | EVV 0x1F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:i64:TXT=BCASTSTR
 | VPABSQ               | AVX512         | AVX512EVEX     | AVX512F_256    | 1f           | 0b11       | rrr        | EVV 0x1F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                        | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:i64
 | VPABSQ               | AVX512         | AVX512EVEX     | AVX512F_256    | 1f           | mm         | rrr        | EVV 0x1F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:i64:TXT=BCASTSTR
 | VPABSQ               | AVX512         | AVX512EVEX     | AVX512F_512    | 1f           | 0b11       | rrr        | EVV 0x1F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                        | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zi64
 | VPABSQ               | AVX512         | AVX512EVEX     | AVX512F_512    | 1f           | mm         | rrr        | EVV 0x1F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:i64:TXT=BCASTSTR
 | VPABSW               | AVX            | AVX            |                | 1d           | mm         | rrr        | VV1 0x1D   V66 V0F38 VL128 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                             | REG0=XMM_R():w:dq:u16 MEM0:r:dq:i16
 | VPABSW               | AVX            | AVX            |                | 1d           | 0b11       | rrr        | VV1 0x1D  V66 V0F38 VL128 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=XMM_R():w:dq:u16  REG1=XMM_B():r:dq:i16
 | VPABSW               | AVX2           | AVX2           |                | 1d           | mm         | rrr        | VV1 0x1D   VL256 V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                             | REG0=YMM_R():w:qq:u16 MEM0:r:qq:i16
 | VPABSW               | AVX2           | AVX2           |                | 1d           | 0b11       | rrr        | VV1 0x1D   VL256 V66 V0F38 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                    | REG0=YMM_R():w:qq:u16  REG1=YMM_B():r:qq:i16
 | VPABSW               | AVX512         | AVX512EVEX     | AVX512BW_128   | 1d           | 0b11       | rrr        | EVV 0x1D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR                          | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i16
 | VPABSW               | AVX512         | AVX512EVEX     | AVX512BW_128   | 1d           | mm         | rrr        | EVV 0x1D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128    NOEVSR  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:i16
 | VPABSW               | AVX512         | AVX512EVEX     | AVX512BW_256   | 1d           | 0b11       | rrr        | EVV 0x1D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256    NOEVSR                          | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:i16
 | VPABSW               | AVX512         | AVX512EVEX     | AVX512BW_256   | 1d           | mm         | rrr        | EVV 0x1D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256    NOEVSR  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:i16
 | VPABSW               | AVX512         | AVX512EVEX     | AVX512BW_512   | 1d           | 0b11       | rrr        | EVV 0x1D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512    NOEVSR                          | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zi16
 | VPABSW               | AVX512         | AVX512EVEX     | AVX512BW_512   | 1d           | mm         | rrr        | EVV 0x1D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512    NOEVSR  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:i16
 | VPACKSSDW            | AVX            | AVX            |                | 6b           | mm         | rrr        | VV1 0x6B  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32
 | VPACKSSDW            | AVX            | AVX            |                | 6b           | 0b11       | rrr        | VV1 0x6B  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32
 | VPACKSSDW            | AVX2           | AVX2           |                | 6b           | mm         | rrr        | VV1 0x6B  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:i16 REG1=YMM_N():r:qq:i32 MEM0:r:qq:i32
 | VPACKSSDW            | AVX2           | AVX2           |                | 6b           | 0b11       | rrr        | VV1 0x6B  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:i16 REG1=YMM_N():r:qq:i32 REG2=YMM_B():r:qq:i32
 | VPACKSSDW            | AVX512         | AVX512EVEX     | AVX512BW_128   | 6b           | 0b11       | rrr        | EVV 0x6B V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                  | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i32 REG3=XMM_B3():r:dq:i32
 | VPACKSSDW            | AVX512         | AVX512EVEX     | AVX512BW_128   | 6b           | mm         | rrr        | EVV 0x6B V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i32 MEM0:r:vv:i32:TXT=BCASTSTR
 | VPACKSSDW            | AVX512         | AVX512EVEX     | AVX512BW_256   | 6b           | 0b11       | rrr        | EVV 0x6B V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                  | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i32 REG3=YMM_B3():r:qq:i32
 | VPACKSSDW            | AVX512         | AVX512EVEX     | AVX512BW_256   | 6b           | mm         | rrr        | EVV 0x6B V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i32 MEM0:r:vv:i32:TXT=BCASTSTR
 | VPACKSSDW            | AVX512         | AVX512EVEX     | AVX512BW_512   | 6b           | 0b11       | rrr        | EVV 0x6B V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                  | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi32 REG3=ZMM_B3():r:zi32
 | VPACKSSDW            | AVX512         | AVX512EVEX     | AVX512BW_512   | 6b           | mm         | rrr        | EVV 0x6B V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi32 MEM0:r:vv:i32:TXT=BCASTSTR
 | VPACKSSWB            | AVX            | AVX            |                | 63           | mm         | rrr        | VV1 0x63  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
 | VPACKSSWB            | AVX            | AVX            |                | 63           | 0b11       | rrr        | VV1 0x63  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
 | VPACKSSWB            | AVX2           | AVX2           |                | 63           | mm         | rrr        | VV1 0x63  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:i8 REG1=YMM_N():r:qq:i16 MEM0:r:qq:i16
 | VPACKSSWB            | AVX2           | AVX2           |                | 63           | 0b11       | rrr        | VV1 0x63  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:i8 REG1=YMM_N():r:qq:i16 REG2=YMM_B():r:qq:i16
 | VPACKSSWB            | AVX512         | AVX512EVEX     | AVX512BW_128   | 63           | 0b11       | rrr        | EVV 0x63 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                      | REG0=XMM_R3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 REG3=XMM_B3():r:dq:i16
 | VPACKSSWB            | AVX512         | AVX512EVEX     | AVX512BW_128   | 63           | mm         | rrr        | EVV 0x63 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 MEM0:r:dq:i16
 | VPACKSSWB            | AVX512         | AVX512EVEX     | AVX512BW_256   | 63           | 0b11       | rrr        | EVV 0x63 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                      | REG0=YMM_R3():w:qq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 REG3=YMM_B3():r:qq:i16
 | VPACKSSWB            | AVX512         | AVX512EVEX     | AVX512BW_256   | 63           | mm         | rrr        | EVV 0x63 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 MEM0:r:qq:i16
 | VPACKSSWB            | AVX512         | AVX512EVEX     | AVX512BW_512   | 63           | 0b11       | rrr        | EVV 0x63 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                      | REG0=ZMM_R3():w:zi8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 REG3=ZMM_B3():r:zi16
 | VPACKSSWB            | AVX512         | AVX512EVEX     | AVX512BW_512   | 63           | mm         | rrr        | EVV 0x63 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zi8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 MEM0:r:zd:i16
 | VPACKSTOREHD         | DATAXFER       | KNCE           | KNCE           | d4           | mm         | rrr        | KVV 0xD4 V66 V0F38  W0 NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() DNCONVERT_INT32()             | MEM0:w:zv:TXT=CONVERT:TXT=NT  REG0=MASK1():r:mskw   REG1=ZMM_R3():r:zd NO_SCALE_DISP8=1
 | VPACKSTOREHPD        | DATAXFER       | KNCE           | KNCE           | d5           | mm         | rrr        | KVV 0xD5 V66 V0F38  W1 NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() DNCONVERT_FLT64()             | MEM0:w:zv:TXT=CONVERT:TXT=NT  REG0=MASK1():r:mskw   REG1=ZMM_R3():r:zf64 NO_SCALE_DISP8=1
 | VPACKSTOREHPS        | DATAXFER       | KNCE           | KNCE           | d5           | mm         | rrr        | KVV 0xD5 V66 V0F38  W0 NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() DNCONVERT_FLT32()             | MEM0:w:zv:TXT=CONVERT:TXT=NT  REG0=MASK1():r:mskw   REG1=ZMM_R3():r:zf32 NO_SCALE_DISP8=1
 | VPACKSTOREHQ         | DATAXFER       | KNCE           | KNCE           | d4           | mm         | rrr        | KVV 0xD4 V66 V0F38  W1 NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() DNCONVERT_INT64()             | MEM0:w:zv:TXT=CONVERT:TXT=NT  REG0=MASK1():r:mskw   REG1=ZMM_R3():r:zq NO_SCALE_DISP8=1
 | VPACKSTORELD         | DATAXFER       | KNCE           | KNCE           | d0           | mm         | rrr        | KVV 0xD0 V66 V0F38  W0 NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() DNCONVERT_INT32()             | MEM0:w:zv:TXT=CONVERT:TXT=NT  REG0=MASK1():r:mskw   REG1=ZMM_R3():r:zd NO_SCALE_DISP8=1
 | VPACKSTORELPD        | DATAXFER       | KNCE           | KNCE           | d1           | mm         | rrr        | KVV 0xD1 V66 V0F38  W1 NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() DNCONVERT_FLT64()             | MEM0:w:zv:TXT=CONVERT:TXT=NT  REG0=MASK1():r:mskw   REG1=ZMM_R3():r:zf64 NO_SCALE_DISP8=1
 | VPACKSTORELPS        | DATAXFER       | KNCE           | KNCE           | d1           | mm         | rrr        | KVV 0xD1 V66 V0F38  W0 NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() DNCONVERT_FLT32()             | MEM0:w:zv:TXT=CONVERT:TXT=NT  REG0=MASK1():r:mskw   REG1=ZMM_R3():r:zf32 NO_SCALE_DISP8=1
 | VPACKSTORELQ         | DATAXFER       | KNCE           | KNCE           | d0           | mm         | rrr        | KVV 0xD0 V66 V0F38  W1 NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() DNCONVERT_INT64()             | MEM0:w:zv:TXT=CONVERT:TXT=NT  REG0=MASK1():r:mskw   REG1=ZMM_R3():r:zq NO_SCALE_DISP8=1
 | VPACKUSDW            | AVX            | AVX            |                | 2b           | mm         | rrr        | VV1 0x2B  V66 V0F38 VL128  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32
 | VPACKUSDW            | AVX            | AVX            |                | 2b           | 0b11       | rrr        | VV1 0x2B  V66 V0F38 VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32
 | VPACKUSDW            | AVX2           | AVX2           |                | 2b           | mm         | rrr        | VV1 0x2B  V66 V0F38 VL256  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=YMM_R():w:qq:u16 REG1=YMM_N():r:qq:i32 MEM0:r:qq:i32
 | VPACKUSDW            | AVX2           | AVX2           |                | 2b           | 0b11       | rrr        | VV1 0x2B  V66 V0F38 VL256 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=YMM_R():w:qq:u16 REG1=YMM_N():r:qq:i32 REG2=YMM_B():r:qq:i32
 | VPACKUSDW            | AVX512         | AVX512EVEX     | AVX512BW_128   | 2b           | 0b11       | rrr        | EVV 0x2B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
 | VPACKUSDW            | AVX512         | AVX512EVEX     | AVX512BW_128   | 2b           | mm         | rrr        | EVV 0x2B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPACKUSDW            | AVX512         | AVX512EVEX     | AVX512BW_256   | 2b           | 0b11       | rrr        | EVV 0x2B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
 | VPACKUSDW            | AVX512         | AVX512EVEX     | AVX512BW_256   | 2b           | mm         | rrr        | EVV 0x2B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPACKUSDW            | AVX512         | AVX512EVEX     | AVX512BW_512   | 2b           | 0b11       | rrr        | EVV 0x2B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
 | VPACKUSDW            | AVX512         | AVX512EVEX     | AVX512BW_512   | 2b           | mm         | rrr        | EVV 0x2B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPACKUSWB            | AVX            | AVX            |                | 67           | mm         | rrr        | VV1 0x67  V66 V0F VL128  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
 | VPACKUSWB            | AVX            | AVX            |                | 67           | 0b11       | rrr        | VV1 0x67  V66 V0F VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
 | VPACKUSWB            | AVX2           | AVX2           |                | 67           | mm         | rrr        | VV1 0x67  V66 V0F VL256  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u8 REG1=YMM_N():r:qq:i16 MEM0:r:qq:i16
 | VPACKUSWB            | AVX2           | AVX2           |                | 67           | 0b11       | rrr        | VV1 0x67  V66 V0F VL256 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:u8 REG1=YMM_N():r:qq:i16 REG2=YMM_B():r:qq:i16
 | VPACKUSWB            | AVX512         | AVX512EVEX     | AVX512BW_128   | 67           | 0b11       | rrr        | EVV 0x67 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                      | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
 | VPACKUSWB            | AVX512         | AVX512EVEX     | AVX512BW_128   | 67           | mm         | rrr        | EVV 0x67 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
 | VPACKUSWB            | AVX512         | AVX512EVEX     | AVX512BW_256   | 67           | 0b11       | rrr        | EVV 0x67 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                      | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
 | VPACKUSWB            | AVX512         | AVX512EVEX     | AVX512BW_256   | 67           | mm         | rrr        | EVV 0x67 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
 | VPACKUSWB            | AVX512         | AVX512EVEX     | AVX512BW_512   | 67           | 0b11       | rrr        | EVV 0x67 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                      | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
 | VPACKUSWB            | AVX512         | AVX512EVEX     | AVX512BW_512   | 67           | mm         | rrr        | EVV 0x67 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
 | VPADCD               | KNC            | KNCE           |                | 5c           | mm         | rrr        | KVV 0x5C  V0F38 V66  W0  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT32()                  | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  REG2=MASK_N():w:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VPADCD               | KNC            | KNCE           |                | 5c           | 0b11       | rrr        | KVV 0x5C  V0F38 V66  W0  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                       | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  REG2=MASK_N():w:mskw  REG3=ZMM_B3():r:zd:TXT=REGSWIZ
 | VPADCD               | KNC            | KNCE           |                | 5c           | 0b11       | rrr        | KVV 0x5C  V0F38 V66  W0  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 SWIZ=0                                | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  REG2=MASK_N():w:mskw  REG3=ZMM_B3():r:zd
 | VPADDB               | AVX            | AVX            |                | fc           | mm         | rrr        | VV1 0xFC  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 MEM0:r:dq:i8
 | VPADDB               | AVX            | AVX            |                | fc           | 0b11       | rrr        | VV1 0xFC  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 REG2=XMM_B():r:dq:i8
 | VPADDB               | AVX2           | AVX2           |                | fc           | mm         | rrr        | VV1 0xFC  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:i8 REG1=YMM_N():r:qq:i8 MEM0:r:qq:i8
 | VPADDB               | AVX2           | AVX2           |                | fc           | 0b11       | rrr        | VV1 0xFC  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:i8 REG1=YMM_N():r:qq:i8 REG2=YMM_B():r:qq:i8
 | VPADDB               | AVX512         | AVX512EVEX     | AVX512BW_128   | fc           | 0b11       | rrr        | EVV 0xFC V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                      | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
 | VPADDB               | AVX512         | AVX512EVEX     | AVX512BW_128   | fc           | mm         | rrr        | EVV 0xFC V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
 | VPADDB               | AVX512         | AVX512EVEX     | AVX512BW_256   | fc           | 0b11       | rrr        | EVV 0xFC V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                      | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
 | VPADDB               | AVX512         | AVX512EVEX     | AVX512BW_256   | fc           | mm         | rrr        | EVV 0xFC V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
 | VPADDB               | AVX512         | AVX512EVEX     | AVX512BW_512   | fc           | 0b11       | rrr        | EVV 0xFC V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                      | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
 | VPADDB               | AVX512         | AVX512EVEX     | AVX512BW_512   | fc           | mm         | rrr        | EVV 0xFC V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
 | VPADDD               | AVX            | AVX            |                | fe           | mm         | rrr        | VV1 0xFE  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32
 | VPADDD               | AVX            | AVX            |                | fe           | 0b11       | rrr        | VV1 0xFE  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32
 | VPADDD               | AVX2           | AVX2           |                | fe           | mm         | rrr        | VV1 0xFE  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:i32 REG1=YMM_N():r:qq:i32 MEM0:r:qq:i32
 | VPADDD               | AVX2           | AVX2           |                | fe           | 0b11       | rrr        | VV1 0xFE  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:i32 REG1=YMM_N():r:qq:i32 REG2=YMM_B():r:qq:i32
 | VPADDD               | AVX512         | AVX512EVEX     | AVX512F_128    | fe           | 0b11       | rrr        | EVV 0xFE V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                  | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
 | VPADDD               | AVX512         | AVX512EVEX     | AVX512F_128    | fe           | mm         | rrr        | EVV 0xFE V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPADDD               | AVX512         | AVX512EVEX     | AVX512F_256    | fe           | 0b11       | rrr        | EVV 0xFE V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                  | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
 | VPADDD               | AVX512         | AVX512EVEX     | AVX512F_256    | fe           | mm         | rrr        | EVV 0xFE V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPADDD               | AVX512         | AVX512EVEX     | AVX512F_512    | fe           | 0b11       | rrr        | EVV 0xFE V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                  | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
 | VPADDD               | AVX512         | AVX512EVEX     | AVX512F_512    | fe           | mm         | rrr        | EVV 0xFE V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPADDD               | KNC            | KNCE           |                | fe           | mm         | rrr        | KVV 0xFE V0F REXW=0 V66  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT32()                  | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VPADDD               | KNC            | KNCE           |                | fe           | 0b11       | rrr        | KVV 0xFE V0F REXW=0 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                       | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VPADDD               | KNC            | KNCE           |                | fe           | 0b11       | rrr        | KVV 0xFE V0F REXW=0 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1                                       | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VPADDQ               | AVX            | AVX            |                | d4           | mm         | rrr        | VV1 0xD4  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:i64 REG1=XMM_N():r:dq:i64 MEM0:r:dq:i64
 | VPADDQ               | AVX            | AVX            |                | d4           | 0b11       | rrr        | VV1 0xD4  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:i64 REG1=XMM_N():r:dq:i64 REG2=XMM_B():r:dq:i64
 | VPADDQ               | AVX2           | AVX2           |                | d4           | mm         | rrr        | VV1 0xD4  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:i64 REG1=YMM_N():r:qq:i64 MEM0:r:qq:i64
 | VPADDQ               | AVX2           | AVX2           |                | d4           | 0b11       | rrr        | VV1 0xD4  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:i64 REG1=YMM_N():r:qq:i64 REG2=YMM_B():r:qq:i64
 | VPADDQ               | AVX512         | AVX512EVEX     | AVX512F_128    | d4           | 0b11       | rrr        | EVV 0xD4 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                  | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
 | VPADDQ               | AVX512         | AVX512EVEX     | AVX512F_128    | d4           | mm         | rrr        | EVV 0xD4 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPADDQ               | AVX512         | AVX512EVEX     | AVX512F_256    | d4           | 0b11       | rrr        | EVV 0xD4 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                  | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
 | VPADDQ               | AVX512         | AVX512EVEX     | AVX512F_256    | d4           | mm         | rrr        | EVV 0xD4 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPADDQ               | AVX512         | AVX512EVEX     | AVX512F_512    | d4           | 0b11       | rrr        | EVV 0xD4 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                  | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
 | VPADDQ               | AVX512         | AVX512EVEX     | AVX512F_512    | d4           | mm         | rrr        | EVV 0xD4 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPADDSB              | AVX            | AVX            |                | ec           | mm         | rrr        | VV1 0xEC  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 MEM0:r:dq:i8
 | VPADDSB              | AVX            | AVX            |                | ec           | 0b11       | rrr        | VV1 0xEC  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 REG2=XMM_B():r:dq:i8
 | VPADDSB              | AVX2           | AVX2           |                | ec           | mm         | rrr        | VV1 0xEC  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:i8 REG1=YMM_N():r:qq:i8 MEM0:r:qq:i8
 | VPADDSB              | AVX2           | AVX2           |                | ec           | 0b11       | rrr        | VV1 0xEC  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:i8 REG1=YMM_N():r:qq:i8 REG2=YMM_B():r:qq:i8
 | VPADDSB              | AVX512         | AVX512EVEX     | AVX512BW_128   | ec           | 0b11       | rrr        | EVV 0xEC V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                      | REG0=XMM_R3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i8 REG3=XMM_B3():r:dq:i8
 | VPADDSB              | AVX512         | AVX512EVEX     | AVX512BW_128   | ec           | mm         | rrr        | EVV 0xEC V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i8 MEM0:r:dq:i8
 | VPADDSB              | AVX512         | AVX512EVEX     | AVX512BW_256   | ec           | 0b11       | rrr        | EVV 0xEC V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                      | REG0=YMM_R3():w:qq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i8 REG3=YMM_B3():r:qq:i8
 | VPADDSB              | AVX512         | AVX512EVEX     | AVX512BW_256   | ec           | mm         | rrr        | EVV 0xEC V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i8 MEM0:r:qq:i8
 | VPADDSB              | AVX512         | AVX512EVEX     | AVX512BW_512   | ec           | 0b11       | rrr        | EVV 0xEC V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                      | REG0=ZMM_R3():w:zi8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi8 REG3=ZMM_B3():r:zi8
 | VPADDSB              | AVX512         | AVX512EVEX     | AVX512BW_512   | ec           | mm         | rrr        | EVV 0xEC V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zi8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi8 MEM0:r:zd:i8
 | VPADDSETCD           | KNC            | KNCE           |                | 5d           | mm         | rrr        | KVV 0x5D  V0F38 V66  W0  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT32()                  | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  REG2=MASK_N():w:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VPADDSETCD           | KNC            | KNCE           |                | 5d           | 0b11       | rrr        | KVV 0x5D  V0F38 V66  W0  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                       | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  REG2=MASK_N():w:mskw  REG3=ZMM_B3():r:zd:TXT=REGSWIZ
 | VPADDSETCD           | KNC            | KNCE           |                | 5d           | 0b11       | rrr        | KVV 0x5D  V0F38 V66  W0  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 SWIZ=0                                | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  REG2=MASK_N():w:mskw  REG3=ZMM_B3():r:zd
 | VPADDSETSD           | KNC            | KNCE           |                | cd           | mm         | rrr        | KVV 0xCD V0F38 REXW=0 V66  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT32()                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():rw:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VPADDSETSD           | KNC            | KNCE           |                | cd           | 0b11       | rrr        | KVV 0xCD V0F38 REXW=0 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():rw:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VPADDSETSD           | KNC            | KNCE           |                | cd           | 0b11       | rrr        | KVV 0xCD V0F38 REXW=0 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1                                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():rw:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VPADDSW              | AVX            | AVX            |                | ed           | mm         | rrr        | VV1 0xED  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
 | VPADDSW              | AVX            | AVX            |                | ed           | 0b11       | rrr        | VV1 0xED  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
 | VPADDSW              | AVX2           | AVX2           |                | ed           | mm         | rrr        | VV1 0xED  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:i16 REG1=YMM_N():r:qq:i16 MEM0:r:qq:i16
 | VPADDSW              | AVX2           | AVX2           |                | ed           | 0b11       | rrr        | VV1 0xED  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:i16 REG1=YMM_N():r:qq:i16 REG2=YMM_B():r:qq:i16
 | VPADDSW              | AVX512         | AVX512EVEX     | AVX512BW_128   | ed           | 0b11       | rrr        | EVV 0xED V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                      | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 REG3=XMM_B3():r:dq:i16
 | VPADDSW              | AVX512         | AVX512EVEX     | AVX512BW_128   | ed           | mm         | rrr        | EVV 0xED V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 MEM0:r:dq:i16
 | VPADDSW              | AVX512         | AVX512EVEX     | AVX512BW_256   | ed           | 0b11       | rrr        | EVV 0xED V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                      | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 REG3=YMM_B3():r:qq:i16
 | VPADDSW              | AVX512         | AVX512EVEX     | AVX512BW_256   | ed           | mm         | rrr        | EVV 0xED V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 MEM0:r:qq:i16
 | VPADDSW              | AVX512         | AVX512EVEX     | AVX512BW_512   | ed           | 0b11       | rrr        | EVV 0xED V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                      | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 REG3=ZMM_B3():r:zi16
 | VPADDSW              | AVX512         | AVX512EVEX     | AVX512BW_512   | ed           | mm         | rrr        | EVV 0xED V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 MEM0:r:zd:i16
 | VPADDUSB             | AVX            | AVX            |                | dc           | mm         | rrr        | VV1 0xDC  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 MEM0:r:dq:u8
 | VPADDUSB             | AVX            | AVX            |                | dc           | 0b11       | rrr        | VV1 0xDC  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:u8
 | VPADDUSB             | AVX2           | AVX2           |                | dc           | mm         | rrr        | VV1 0xDC  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u8 REG1=YMM_N():r:qq:u8 MEM0:r:qq:u8
 | VPADDUSB             | AVX2           | AVX2           |                | dc           | 0b11       | rrr        | VV1 0xDC  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:u8 REG1=YMM_N():r:qq:u8 REG2=YMM_B():r:qq:u8
 | VPADDUSB             | AVX512         | AVX512EVEX     | AVX512BW_128   | dc           | 0b11       | rrr        | EVV 0xDC V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                      | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
 | VPADDUSB             | AVX512         | AVX512EVEX     | AVX512BW_128   | dc           | mm         | rrr        | EVV 0xDC V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
 | VPADDUSB             | AVX512         | AVX512EVEX     | AVX512BW_256   | dc           | 0b11       | rrr        | EVV 0xDC V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                      | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
 | VPADDUSB             | AVX512         | AVX512EVEX     | AVX512BW_256   | dc           | mm         | rrr        | EVV 0xDC V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
 | VPADDUSB             | AVX512         | AVX512EVEX     | AVX512BW_512   | dc           | 0b11       | rrr        | EVV 0xDC V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                      | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
 | VPADDUSB             | AVX512         | AVX512EVEX     | AVX512BW_512   | dc           | mm         | rrr        | EVV 0xDC V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
 | VPADDUSW             | AVX            | AVX            |                | dd           | mm         | rrr        | VV1 0xDD  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 MEM0:r:dq:u16
 | VPADDUSW             | AVX            | AVX            |                | dd           | 0b11       | rrr        | VV1 0xDD  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 REG2=XMM_B():r:dq:u16
 | VPADDUSW             | AVX2           | AVX2           |                | dd           | mm         | rrr        | VV1 0xDD  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u16 REG1=YMM_N():r:qq:u16 MEM0:r:qq:u16
 | VPADDUSW             | AVX2           | AVX2           |                | dd           | 0b11       | rrr        | VV1 0xDD  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:u16 REG1=YMM_N():r:qq:u16 REG2=YMM_B():r:qq:u16
 | VPADDUSW             | AVX512         | AVX512EVEX     | AVX512BW_128   | dd           | 0b11       | rrr        | EVV 0xDD V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                      | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
 | VPADDUSW             | AVX512         | AVX512EVEX     | AVX512BW_128   | dd           | mm         | rrr        | EVV 0xDD V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
 | VPADDUSW             | AVX512         | AVX512EVEX     | AVX512BW_256   | dd           | 0b11       | rrr        | EVV 0xDD V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                      | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
 | VPADDUSW             | AVX512         | AVX512EVEX     | AVX512BW_256   | dd           | mm         | rrr        | EVV 0xDD V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
 | VPADDUSW             | AVX512         | AVX512EVEX     | AVX512BW_512   | dd           | 0b11       | rrr        | EVV 0xDD V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                      | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
 | VPADDUSW             | AVX512         | AVX512EVEX     | AVX512BW_512   | dd           | mm         | rrr        | EVV 0xDD V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
 | VPADDW               | AVX            | AVX            |                | fd           | mm         | rrr        | VV1 0xFD  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
 | VPADDW               | AVX            | AVX            |                | fd           | 0b11       | rrr        | VV1 0xFD  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
 | VPADDW               | AVX2           | AVX2           |                | fd           | mm         | rrr        | VV1 0xFD  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:i16 REG1=YMM_N():r:qq:i16 MEM0:r:qq:i16
 | VPADDW               | AVX2           | AVX2           |                | fd           | 0b11       | rrr        | VV1 0xFD  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:i16 REG1=YMM_N():r:qq:i16 REG2=YMM_B():r:qq:i16
 | VPADDW               | AVX512         | AVX512EVEX     | AVX512BW_128   | fd           | 0b11       | rrr        | EVV 0xFD V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                      | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
 | VPADDW               | AVX512         | AVX512EVEX     | AVX512BW_128   | fd           | mm         | rrr        | EVV 0xFD V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
 | VPADDW               | AVX512         | AVX512EVEX     | AVX512BW_256   | fd           | 0b11       | rrr        | EVV 0xFD V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                      | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
 | VPADDW               | AVX512         | AVX512EVEX     | AVX512BW_256   | fd           | mm         | rrr        | EVV 0xFD V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
 | VPADDW               | AVX512         | AVX512EVEX     | AVX512BW_512   | fd           | 0b11       | rrr        | EVV 0xFD V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                      | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
 | VPADDW               | AVX512         | AVX512EVEX     | AVX512BW_512   | fd           | mm         | rrr        | EVV 0xFD V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
 | VPALIGNR             | AVX            | AVX            |                | 0f           | mm         | rrr        | VV1 0x0F  VL128 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                            | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 MEM0:r:dq:u8 IMM0:r:b
 | VPALIGNR             | AVX            | AVX            |                | 0f           | 0b11       | rrr        | VV1 0x0F  VL128 V66 V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                   | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:u8 IMM0:r:b
 | VPALIGNR             | AVX2           | AVX2           |                | 0f           | mm         | rrr        | VV1 0x0F  VL256 V66 V0F3A  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                           | REG0=YMM_R():w:qq:u8 REG1=YMM_N():r:qq:u8 MEM0:r:qq:u8 IMM0:r:b
 | VPALIGNR             | AVX2           | AVX2           |                | 0f           | 0b11       | rrr        | VV1 0x0F  VL256 V66 V0F3A  MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                  | REG0=YMM_R():w:qq:u8 REG1=YMM_N():r:qq:u8 REG2=YMM_B():r:qq:u8 IMM0:r:b
 | VPALIGNR             | AVX512         | AVX512EVEX     | AVX512BW_128   | 0f           | 0b11       | rrr        | EVV 0x0F V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128     UIMM8()                        | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8 IMM0:r:b
 | VPALIGNR             | AVX512         | AVX512EVEX     | AVX512BW_128   | 0f           | mm         | rrr        | EVV 0x0F V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128     UIMM8()  ESIZE_64_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8 IMM0:r:b
 | VPALIGNR             | AVX512         | AVX512EVEX     | AVX512BW_256   | 0f           | 0b11       | rrr        | EVV 0x0F V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256     UIMM8()                        | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8 IMM0:r:b
 | VPALIGNR             | AVX512         | AVX512EVEX     | AVX512BW_256   | 0f           | mm         | rrr        | EVV 0x0F V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256     UIMM8()  ESIZE_64_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8 IMM0:r:b
 | VPALIGNR             | AVX512         | AVX512EVEX     | AVX512BW_512   | 0f           | 0b11       | rrr        | EVV 0x0F V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512     UIMM8()                        | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8 IMM0:r:b
 | VPALIGNR             | AVX512         | AVX512EVEX     | AVX512BW_512   | 0f           | mm         | rrr        | EVV 0x0F V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512     UIMM8()  ESIZE_64_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8 IMM0:r:b
 | VPAND                | LOGICAL        | AVX            |                | db           | mm         | rrr        | VV1 0xDB  VL128 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq:u128 REG1=XMM_N():r:dq:u128 MEM0:r:dq:u128
 | VPAND                | LOGICAL        | AVX            |                | db           | 0b11       | rrr        | VV1 0xDB  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u128 REG1=XMM_N():r:dq:u128 REG2=XMM_B():r:dq:u128
 | VPAND                | LOGICAL        | AVX2           |                | db           | mm         | rrr        | VV1 0xDB  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u256 REG1=YMM_N():r:qq:u256 MEM0:r:qq:u256
 | VPAND                | LOGICAL        | AVX2           |                | db           | 0b11       | rrr        | VV1 0xDB   VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=YMM_R():w:qq:u256 REG1=YMM_N():r:qq:u256 REG2=YMM_B():r:qq:u256
 | VPANDD               | KNC            | KNCE           |                | db           | mm         | rrr        | KVV 0xDB V0F REXW=0 V66  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT32()                  | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VPANDD               | KNC            | KNCE           |                | db           | 0b11       | rrr        | KVV 0xDB V0F REXW=0 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                       | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VPANDD               | KNC            | KNCE           |                | db           | 0b11       | rrr        | KVV 0xDB V0F REXW=0 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1                                       | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VPANDD               | LOGICAL        | AVX512EVEX     | AVX512F_128    | db           | 0b11       | rrr        | EVV 0xDB V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                  | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
 | VPANDD               | LOGICAL        | AVX512EVEX     | AVX512F_128    | db           | mm         | rrr        | EVV 0xDB V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPANDD               | LOGICAL        | AVX512EVEX     | AVX512F_256    | db           | 0b11       | rrr        | EVV 0xDB V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                  | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
 | VPANDD               | LOGICAL        | AVX512EVEX     | AVX512F_256    | db           | mm         | rrr        | EVV 0xDB V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPANDD               | LOGICAL        | AVX512EVEX     | AVX512F_512    | db           | 0b11       | rrr        | EVV 0xDB V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                  | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
 | VPANDD               | LOGICAL        | AVX512EVEX     | AVX512F_512    | db           | mm         | rrr        | EVV 0xDB V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPANDN               | LOGICAL        | AVX            |                | df           | mm         | rrr        | VV1 0xDF  VL128 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq:u128 REG1=XMM_N():r:dq:u128 MEM0:r:dq:u128
 | VPANDN               | LOGICAL        | AVX            |                | df           | 0b11       | rrr        | VV1 0xDF  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u128 REG1=XMM_N():r:dq:u128 REG2=XMM_B():r:dq:u128
 | VPANDN               | LOGICAL        | AVX2           |                | df           | mm         | rrr        | VV1 0xDF  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u256 REG1=YMM_N():r:qq:u256 MEM0:r:qq:u256
 | VPANDN               | LOGICAL        | AVX2           |                | df           | 0b11       | rrr        | VV1 0xDF   VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=YMM_R():w:qq:u256 REG1=YMM_N():r:qq:u256 REG2=YMM_B():r:qq:u256
 | VPANDND              | KNC            | KNCE           |                | df           | mm         | rrr        | KVV 0xDF V0F REXW=0 V66  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT32()                  | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VPANDND              | KNC            | KNCE           |                | df           | 0b11       | rrr        | KVV 0xDF V0F REXW=0 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                       | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VPANDND              | KNC            | KNCE           |                | df           | 0b11       | rrr        | KVV 0xDF V0F REXW=0 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1                                       | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VPANDND              | LOGICAL        | AVX512EVEX     | AVX512F_128    | df           | 0b11       | rrr        | EVV 0xDF V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                  | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
 | VPANDND              | LOGICAL        | AVX512EVEX     | AVX512F_128    | df           | mm         | rrr        | EVV 0xDF V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPANDND              | LOGICAL        | AVX512EVEX     | AVX512F_256    | df           | 0b11       | rrr        | EVV 0xDF V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                  | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
 | VPANDND              | LOGICAL        | AVX512EVEX     | AVX512F_256    | df           | mm         | rrr        | EVV 0xDF V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPANDND              | LOGICAL        | AVX512EVEX     | AVX512F_512    | df           | 0b11       | rrr        | EVV 0xDF V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                  | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
 | VPANDND              | LOGICAL        | AVX512EVEX     | AVX512F_512    | df           | mm         | rrr        | EVV 0xDF V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPANDNQ              | KNC            | KNCE           |                | df           | mm         | rrr        | KVV 0xDF V0F REXW=1 V66  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT64()                  | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VPANDNQ              | KNC            | KNCE           |                | df           | 0b11       | rrr        | KVV 0xDF V0F REXW=1 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE64()                       | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=REGSWIZ
 | VPANDNQ              | KNC            | KNCE           |                | df           | 0b11       | rrr        | KVV 0xDF V0F REXW=1 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1                                       | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VPANDNQ              | LOGICAL        | AVX512EVEX     | AVX512F_128    | df           | 0b11       | rrr        | EVV 0xDF V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                  | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
 | VPANDNQ              | LOGICAL        | AVX512EVEX     | AVX512F_128    | df           | mm         | rrr        | EVV 0xDF V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPANDNQ              | LOGICAL        | AVX512EVEX     | AVX512F_256    | df           | 0b11       | rrr        | EVV 0xDF V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                  | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
 | VPANDNQ              | LOGICAL        | AVX512EVEX     | AVX512F_256    | df           | mm         | rrr        | EVV 0xDF V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPANDNQ              | LOGICAL        | AVX512EVEX     | AVX512F_512    | df           | 0b11       | rrr        | EVV 0xDF V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                  | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
 | VPANDNQ              | LOGICAL        | AVX512EVEX     | AVX512F_512    | df           | mm         | rrr        | EVV 0xDF V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPANDQ               | KNC            | KNCE           |                | db           | mm         | rrr        | KVV 0xDB V0F REXW=1 V66  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT64()                  | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VPANDQ               | KNC            | KNCE           |                | db           | 0b11       | rrr        | KVV 0xDB V0F REXW=1 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE64()                       | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=REGSWIZ
 | VPANDQ               | KNC            | KNCE           |                | db           | 0b11       | rrr        | KVV 0xDB V0F REXW=1 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1                                       | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VPANDQ               | LOGICAL        | AVX512EVEX     | AVX512F_128    | db           | 0b11       | rrr        | EVV 0xDB V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                  | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
 | VPANDQ               | LOGICAL        | AVX512EVEX     | AVX512F_128    | db           | mm         | rrr        | EVV 0xDB V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPANDQ               | LOGICAL        | AVX512EVEX     | AVX512F_256    | db           | 0b11       | rrr        | EVV 0xDB V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                  | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
 | VPANDQ               | LOGICAL        | AVX512EVEX     | AVX512F_256    | db           | mm         | rrr        | EVV 0xDB V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPANDQ               | LOGICAL        | AVX512EVEX     | AVX512F_512    | db           | 0b11       | rrr        | EVV 0xDB V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                  | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
 | VPANDQ               | LOGICAL        | AVX512EVEX     | AVX512F_512    | db           | mm         | rrr        | EVV 0xDB V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPAVGB               | AVX            | AVX            |                | e0           | mm         | rrr        | VV1 0xE0  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 MEM0:r:dq:u8
 | VPAVGB               | AVX            | AVX            |                | e0           | 0b11       | rrr        | VV1 0xE0  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:u8
 | VPAVGB               | AVX2           | AVX2           |                | e0           | mm         | rrr        | VV1 0xE0  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u8 REG1=YMM_N():r:qq:u8 MEM0:r:qq:u8
 | VPAVGB               | AVX2           | AVX2           |                | e0           | 0b11       | rrr        | VV1 0xE0  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:u8 REG1=YMM_N():r:qq:u8 REG2=YMM_B():r:qq:u8
 | VPAVGB               | AVX512         | AVX512EVEX     | AVX512BW_128   | e0           | 0b11       | rrr        | EVV 0xE0 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                      | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
 | VPAVGB               | AVX512         | AVX512EVEX     | AVX512BW_128   | e0           | mm         | rrr        | EVV 0xE0 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
 | VPAVGB               | AVX512         | AVX512EVEX     | AVX512BW_256   | e0           | 0b11       | rrr        | EVV 0xE0 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                      | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
 | VPAVGB               | AVX512         | AVX512EVEX     | AVX512BW_256   | e0           | mm         | rrr        | EVV 0xE0 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
 | VPAVGB               | AVX512         | AVX512EVEX     | AVX512BW_512   | e0           | 0b11       | rrr        | EVV 0xE0 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                      | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
 | VPAVGB               | AVX512         | AVX512EVEX     | AVX512BW_512   | e0           | mm         | rrr        | EVV 0xE0 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
 | VPAVGW               | AVX            | AVX            |                | e3           | mm         | rrr        | VV1 0xE3  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 MEM0:r:dq:u16
 | VPAVGW               | AVX            | AVX            |                | e3           | 0b11       | rrr        | VV1 0xE3  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 REG2=XMM_B():r:dq:u16
 | VPAVGW               | AVX2           | AVX2           |                | e3           | mm         | rrr        | VV1 0xE3  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u16 REG1=YMM_N():r:qq:u16 MEM0:r:qq:u16
 | VPAVGW               | AVX2           | AVX2           |                | e3           | 0b11       | rrr        | VV1 0xE3  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:u16 REG1=YMM_N():r:qq:u16 REG2=YMM_B():r:qq:u16
 | VPAVGW               | AVX512         | AVX512EVEX     | AVX512BW_128   | e3           | 0b11       | rrr        | EVV 0xE3 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                      | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
 | VPAVGW               | AVX512         | AVX512EVEX     | AVX512BW_128   | e3           | mm         | rrr        | EVV 0xE3 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
 | VPAVGW               | AVX512         | AVX512EVEX     | AVX512BW_256   | e3           | 0b11       | rrr        | EVV 0xE3 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                      | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
 | VPAVGW               | AVX512         | AVX512EVEX     | AVX512BW_256   | e3           | mm         | rrr        | EVV 0xE3 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
 | VPAVGW               | AVX512         | AVX512EVEX     | AVX512BW_512   | e3           | 0b11       | rrr        | EVV 0xE3 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                      | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
 | VPAVGW               | AVX512         | AVX512EVEX     | AVX512BW_512   | e3           | mm         | rrr        | EVV 0xE3 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
 | VPBLENDD             | AVX2           | AVX2           |                | 02           | mm         | rrr        | VV1 0x02  VL128 V66 V0F3A W0  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                        | REG0=XMM_R():w:dq:u32  REG1=XMM_N():r:dq:u32  MEM0:r:dq:u32         IMM0:r:b
 | VPBLENDD             | AVX2           | AVX2           |                | 02           | 0b11       | rrr        | VV1 0x02  VL128 V66 V0F3A W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                | REG0=XMM_R():w:dq:u32  REG1=XMM_N():r:dq:u32  REG2=XMM_B():r:dq:u32 IMM0:r:b
 | VPBLENDD             | AVX2           | AVX2           |                | 02           | mm         | rrr        | VV1 0x02  VL256 V66 V0F3A W0  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                        | REG0=YMM_R():w:qq:u32  REG1=YMM_N():r:qq:u32  MEM0:r:qq:u32         IMM0:r:b
 | VPBLENDD             | AVX2           | AVX2           |                | 02           | 0b11       | rrr        | VV1 0x02  VL256 V66 V0F3A W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                | REG0=YMM_R():w:qq:u32  REG1=YMM_N():r:qq:u32  REG2=YMM_B():r:qq:u32 IMM0:r:b
 | VPBLENDMB            | BLEND          | AVX512EVEX     | AVX512BW_128   | 66           | 0b11       | rrr        | EVV 0x66 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
 | VPBLENDMB            | BLEND          | AVX512EVEX     | AVX512BW_128   | 66           | mm         | rrr        | EVV 0x66 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0    ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
 | VPBLENDMB            | BLEND          | AVX512EVEX     | AVX512BW_256   | 66           | 0b11       | rrr        | EVV 0x66 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
 | VPBLENDMB            | BLEND          | AVX512EVEX     | AVX512BW_256   | 66           | mm         | rrr        | EVV 0x66 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0    ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
 | VPBLENDMB            | BLEND          | AVX512EVEX     | AVX512BW_512   | 66           | 0b11       | rrr        | EVV 0x66 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
 | VPBLENDMB            | BLEND          | AVX512EVEX     | AVX512BW_512   | 66           | mm         | rrr        | EVV 0x66 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0    ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
 | VPBLENDMD            | BLEND          | AVX512EVEX     | AVX512F_128    | 64           | 0b11       | rrr        | EVV 0x64 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
 | VPBLENDMD            | BLEND          | AVX512EVEX     | AVX512F_128    | 64           | mm         | rrr        | EVV 0x64 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPBLENDMD            | BLEND          | AVX512EVEX     | AVX512F_256    | 64           | 0b11       | rrr        | EVV 0x64 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
 | VPBLENDMD            | BLEND          | AVX512EVEX     | AVX512F_256    | 64           | mm         | rrr        | EVV 0x64 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPBLENDMD            | BLEND          | AVX512EVEX     | AVX512F_512    | 64           | 0b11       | rrr        | EVV 0x64 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
 | VPBLENDMD            | BLEND          | AVX512EVEX     | AVX512F_512    | 64           | mm         | rrr        | EVV 0x64 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPBLENDMD            | BLEND          | KNCE           |                | 64           | mm         | rrr        | KVV 0x64 V0F38 REXW=0 V66  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT32()                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VPBLENDMD            | BLEND          | KNCE           |                | 64           | 0b11       | rrr        | KVV 0x64 V0F38 REXW=0 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VPBLENDMD            | BLEND          | KNCE           |                | 64           | 0b11       | rrr        | KVV 0x64 V0F38 REXW=0 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1                                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VPBLENDMQ            | BLEND          | AVX512EVEX     | AVX512F_128    | 64           | 0b11       | rrr        | EVV 0x64 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
 | VPBLENDMQ            | BLEND          | AVX512EVEX     | AVX512F_128    | 64           | mm         | rrr        | EVV 0x64 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPBLENDMQ            | BLEND          | AVX512EVEX     | AVX512F_256    | 64           | 0b11       | rrr        | EVV 0x64 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
 | VPBLENDMQ            | BLEND          | AVX512EVEX     | AVX512F_256    | 64           | mm         | rrr        | EVV 0x64 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPBLENDMQ            | BLEND          | AVX512EVEX     | AVX512F_512    | 64           | 0b11       | rrr        | EVV 0x64 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
 | VPBLENDMQ            | BLEND          | AVX512EVEX     | AVX512F_512    | 64           | mm         | rrr        | EVV 0x64 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPBLENDMQ            | BLEND          | KNCE           |                | 64           | mm         | rrr        | KVV 0x64 V0F38 REXW=1 V66  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT64()                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VPBLENDMQ            | BLEND          | KNCE           |                | 64           | 0b11       | rrr        | KVV 0x64 V0F38 REXW=1 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE64()                     | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=REGSWIZ
 | VPBLENDMQ            | BLEND          | KNCE           |                | 64           | 0b11       | rrr        | KVV 0x64 V0F38 REXW=1 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1                                     | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VPBLENDMW            | BLEND          | AVX512EVEX     | AVX512BW_128   | 66           | 0b11       | rrr        | EVV 0x66 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
 | VPBLENDMW            | BLEND          | AVX512EVEX     | AVX512BW_128   | 66           | mm         | rrr        | EVV 0x66 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
 | VPBLENDMW            | BLEND          | AVX512EVEX     | AVX512BW_256   | 66           | 0b11       | rrr        | EVV 0x66 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
 | VPBLENDMW            | BLEND          | AVX512EVEX     | AVX512BW_256   | 66           | mm         | rrr        | EVV 0x66 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
 | VPBLENDMW            | BLEND          | AVX512EVEX     | AVX512BW_512   | 66           | 0b11       | rrr        | EVV 0x66 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
 | VPBLENDMW            | BLEND          | AVX512EVEX     | AVX512BW_512   | 66           | mm         | rrr        | EVV 0x66 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
 | VPBLENDVB            | AVX            | AVX            |                | 4c           | mm         | rrr        | VV1 0x4C   VL128 V66 V0F3A norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()           | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 MEM0:r:dq:i8 REG2=XMM_SE():r:dq:i8
 | VPBLENDVB            | AVX            | AVX            |                | 4c           | 0b11       | rrr        | VV1 0x4C   VL128 V66 V0F3A norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                  | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 REG2=XMM_B():r:dq:i8 REG3=XMM_SE():r:dq:i8
 | VPBLENDVB            | AVX2           | AVX2           |                | 4c           | mm         | rrr        | VV1 0x4C   VL256 V66 V0F3A norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()           | REG0=YMM_R():w:qq:u8 REG1=YMM_N():r:qq:u8 MEM0:r:qq:u8 REG2=YMM_SE():r:qq:u8
 | VPBLENDVB            | AVX2           | AVX2           |                | 4c           | 0b11       | rrr        | VV1 0x4C   VL256 V66 V0F3A norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                  | REG0=YMM_R():w:qq:u8 REG1=YMM_N():r:qq:u8 REG2=YMM_B():r:qq:u8 REG3=YMM_SE():r:qq:u8
 | VPBLENDW             | AVX            | AVX            |                | 0e           | mm         | rrr        | VV1 0x0E  VL128 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                            | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 MEM0:r:dq:u16 IMM0:r:b
 | VPBLENDW             | AVX            | AVX            |                | 0e           | 0b11       | rrr        | VV1 0x0E  VL128 V66 V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                   | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 REG2=XMM_B():r:dq:u16 IMM0:r:b
 | VPBLENDW             | AVX2           | AVX2           |                | 0e           | mm         | rrr        | VV1 0x0E  VL256 V66 V0F3A  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                           | REG0=YMM_R():w:qq:u16 REG1=YMM_N():r:qq:u16 MEM0:r:qq:u16 IMM0:r:b
 | VPBLENDW             | AVX2           | AVX2           |                | 0e           | 0b11       | rrr        | VV1 0x0E  VL256 V66 V0F3A  MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                  | REG0=YMM_R():w:qq:u16 REG1=YMM_N():r:qq:u16 REG2=YMM_B():r:qq:u16 IMM0:r:b
 | VPBROADCASTB         | BROADCAST      | AVX2           |                | 78           | mm         | rrr        | VV1 0x78 VL128 V66 V0F38 W0 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                            | REG0=XMM_R():w:dq:u8 MEM0:r:b:u8 EMX_BROADCAST_1TO16_8
 | VPBROADCASTB         | BROADCAST      | AVX2           |                | 78           | 0b11       | rrr        | VV1 0x78 VL128 V66 V0F38 W0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                   | REG0=XMM_R():w:dq:u8  REG1=XMM_B():r:b:u8 EMX_BROADCAST_1TO16_8
 | VPBROADCASTB         | BROADCAST      | AVX2           |                | 78           | mm         | rrr        | VV1 0x78 VL256 V66 V0F38 W0 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                            | REG0=YMM_R():w:qq:u8 MEM0:r:b:u8 EMX_BROADCAST_1TO32_8
 | VPBROADCASTB         | BROADCAST      | AVX2           |                | 78           | 0b11       | rrr        | VV1 0x78 VL256 V66 V0F38 W0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                   | REG0=YMM_R():w:qq:u8  REG1=XMM_B():r:b:u8 EMX_BROADCAST_1TO32_8
 | VPBROADCASTB         | BROADCAST      | AVX512EVEX     | AVX512BW_128   | 78           | 0b11       | rrr        | EVV 0x78 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u8 EMX_BROADCAST_1TO16_8
 | VPBROADCASTB         | BROADCAST      | AVX512EVEX     | AVX512BW_128   | 78           | mm         | rrr        | EVV 0x78 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_8_BITS() NELEM_TUPLE1_BYTE() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:b:u8 EMX_BROADCAST_1TO16_8
 | VPBROADCASTB         | BROADCAST      | AVX512EVEX     | AVX512BW_128   | 7a           | 0b11       | rrr        | EVV 0x7A V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=GPR32_B():r:d:u8 EMX_BROADCAST_1TO16_8
 | VPBROADCASTB         | BROADCAST      | AVX512EVEX     | AVX512BW_256   | 78           | 0b11       | rrr        | EVV 0x78 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u8 EMX_BROADCAST_1TO32_8
 | VPBROADCASTB         | BROADCAST      | AVX512EVEX     | AVX512BW_256   | 78           | mm         | rrr        | EVV 0x78 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_8_BITS() NELEM_TUPLE1_BYTE() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:b:u8 EMX_BROADCAST_1TO32_8
 | VPBROADCASTB         | BROADCAST      | AVX512EVEX     | AVX512BW_256   | 7a           | 0b11       | rrr        | EVV 0x7A V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=GPR32_B():r:d:u8 EMX_BROADCAST_1TO32_8
 | VPBROADCASTB         | BROADCAST      | AVX512EVEX     | AVX512BW_512   | 78           | 0b11       | rrr        | EVV 0x78 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u8 EMX_BROADCAST_1TO64_8
 | VPBROADCASTB         | BROADCAST      | AVX512EVEX     | AVX512BW_512   | 78           | mm         | rrr        | EVV 0x78 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_8_BITS() NELEM_TUPLE1_BYTE() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:b:u8 EMX_BROADCAST_1TO64_8
 | VPBROADCASTB         | BROADCAST      | AVX512EVEX     | AVX512BW_512   | 7a           | 0b11       | rrr        | EVV 0x7A V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=GPR32_B():r:d:u8 EMX_BROADCAST_1TO64_8
 | VPBROADCASTD         | BROADCAST      | AVX2           |                | 58           | mm         | rrr        | VV1 0x58 VL128 V66 V0F38 W0 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                            | REG0=XMM_R():w:dq:u32 MEM0:r:d:u32 EMX_BROADCAST_1TO4_32
 | VPBROADCASTD         | BROADCAST      | AVX2           |                | 58           | 0b11       | rrr        | VV1 0x58 VL128 V66 V0F38 W0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                   | REG0=XMM_R():w:dq:u32  REG1=XMM_B():r:d:u32  EMX_BROADCAST_1TO4_32
 | VPBROADCASTD         | BROADCAST      | AVX2           |                | 58           | mm         | rrr        | VV1 0x58 VL256 V66 V0F38 W0 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                            | REG0=YMM_R():w:qq:u32 MEM0:r:d:u32  EMX_BROADCAST_1TO8_32
 | VPBROADCASTD         | BROADCAST      | AVX2           |                | 58           | 0b11       | rrr        | VV1 0x58 VL256 V66 V0F38 W0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                   | REG0=YMM_R():w:qq:u32  REG1=XMM_B():r:d:u32 EMX_BROADCAST_1TO8_32
 | VPBROADCASTD         | BROADCAST      | AVX512EVEX     | AVX512F_128    | 58           | mm         | rrr        | EVV 0x58 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_TUPLE1() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:d:u32 EMX_BROADCAST_1TO4_32
 | VPBROADCASTD         | BROADCAST      | AVX512EVEX     | AVX512F_128    | 58           | 0b11       | rrr        | EVV 0x58 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32 EMX_BROADCAST_1TO4_32
 | VPBROADCASTD         | BROADCAST      | AVX512EVEX     | AVX512F_128    | 7c           | 0b11       | rrr        | EVV 0x7C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  not64  NOEVSR                     | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=GPR32_B():r:d:u32 EMX_BROADCAST_1TO4_32
 | VPBROADCASTD         | BROADCAST      | AVX512EVEX     | AVX512F_128    | 7c           | 0b11       | rrr        | EVV 0x7C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  mode64 W0  NOEVSR                 | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=GPR32_B():r:d:u32 EMX_BROADCAST_1TO4_32
 | VPBROADCASTD         | BROADCAST      | AVX512EVEX     | AVX512F_256    | 58           | mm         | rrr        | EVV 0x58 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_TUPLE1() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:d:u32 EMX_BROADCAST_1TO8_32
 | VPBROADCASTD         | BROADCAST      | AVX512EVEX     | AVX512F_256    | 58           | 0b11       | rrr        | EVV 0x58 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32 EMX_BROADCAST_1TO8_32
 | VPBROADCASTD         | BROADCAST      | AVX512EVEX     | AVX512F_256    | 7c           | 0b11       | rrr        | EVV 0x7C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  not64  NOEVSR                     | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=GPR32_B():r:d:u32 EMX_BROADCAST_1TO8_32
 | VPBROADCASTD         | BROADCAST      | AVX512EVEX     | AVX512F_256    | 7c           | 0b11       | rrr        | EVV 0x7C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  mode64 W0  NOEVSR                 | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=GPR32_B():r:d:u32 EMX_BROADCAST_1TO8_32
 | VPBROADCASTD         | BROADCAST      | AVX512EVEX     | AVX512F_512    | 58           | mm         | rrr        | EVV 0x58 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_TUPLE1() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:d:u32 EMX_BROADCAST_1TO16_32
 | VPBROADCASTD         | BROADCAST      | AVX512EVEX     | AVX512F_512    | 58           | 0b11       | rrr        | EVV 0x58 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32 EMX_BROADCAST_1TO16_32
 | VPBROADCASTD         | BROADCAST      | AVX512EVEX     | AVX512F_512    | 7c           | 0b11       | rrr        | EVV 0x7C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  not64  NOEVSR                     | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=GPR32_B():r:d:u32 EMX_BROADCAST_1TO16_32
 | VPBROADCASTD         | BROADCAST      | AVX512EVEX     | AVX512F_512    | 7c           | 0b11       | rrr        | EVV 0x7C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  mode64 W0  NOEVSR                 | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=GPR32_B():r:d:u32 EMX_BROADCAST_1TO16_32
 | VPBROADCASTD         | BROADCAST      | KNCE           |                | 58           | mm         | rrr        | KVV 0x58 V0F38 V66 REXW=0  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM()  UPCONVERT_INT32_LOAD()   | REG0=ZMM_R3():rw:zd:i32 REG1=MASK1():r:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT  NELEM=1:SUPP EMX_BROADCAST_1TO16_32
 | VPBROADCASTMB2Q      | BROADCAST      | AVX512EVEX     | AVX512CD_128   | 2a           | 0b11       | rrr        | EVV 0x2A VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR  ZEROING=0 MASK=0      | REG0=XMM_R3():w:dq:u64 REG1=MASK_B():r:mskw:u64 EMX_BROADCAST_1TO2_8
 | VPBROADCASTMB2Q      | BROADCAST      | AVX512EVEX     | AVX512CD_256   | 2a           | 0b11       | rrr        | EVV 0x2A VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR  ZEROING=0 MASK=0      | REG0=YMM_R3():w:qq:u64 REG1=MASK_B():r:mskw:u64 EMX_BROADCAST_1TO4_8
 | VPBROADCASTMB2Q      | BROADCAST      | AVX512EVEX     | AVX512CD_512   | 2a           | 0b11       | rrr        | EVV 0x2A VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR  ZEROING=0 MASK=0      | REG0=ZMM_R3():w:zu64 REG1=MASK_B():r:mskw:u64 EMX_BROADCAST_1TO8_8
 | VPBROADCASTMW2D      | BROADCAST      | AVX512EVEX     | AVX512CD_128   | 3a           | 0b11       | rrr        | EVV 0x3A VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR  ZEROING=0 MASK=0      | REG0=XMM_R3():w:dq:u32 REG1=MASK_B():r:mskw:u32 EMX_BROADCAST_1TO4_16
 | VPBROADCASTMW2D      | BROADCAST      | AVX512EVEX     | AVX512CD_256   | 3a           | 0b11       | rrr        | EVV 0x3A VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR  ZEROING=0 MASK=0      | REG0=YMM_R3():w:qq:u32 REG1=MASK_B():r:mskw:u32 EMX_BROADCAST_1TO8_16
 | VPBROADCASTMW2D      | BROADCAST      | AVX512EVEX     | AVX512CD_512   | 3a           | 0b11       | rrr        | EVV 0x3A VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR  ZEROING=0 MASK=0      | REG0=ZMM_R3():w:zu32 REG1=MASK_B():r:mskw:u32 EMX_BROADCAST_1TO16_16
 | VPBROADCASTQ         | BROADCAST      | AVX2           |                | 59           | mm         | rrr        | VV1 0x59 VL128 V66 V0F38 W0 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                            | REG0=XMM_R():w:dq:u64 MEM0:r:q:u64 EMX_BROADCAST_1TO2_64
 | VPBROADCASTQ         | BROADCAST      | AVX2           |                | 59           | 0b11       | rrr        | VV1 0x59 VL128 V66 V0F38 W0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                   | REG0=XMM_R():w:dq:u64  REG1=XMM_B():r:q:u64  EMX_BROADCAST_1TO2_64
 | VPBROADCASTQ         | BROADCAST      | AVX2           |                | 59           | mm         | rrr        | VV1 0x59 VL256 V66 V0F38 W0 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                            | REG0=YMM_R():w:qq:u64 MEM0:r:q:u64 EMX_BROADCAST_1TO4_64
 | VPBROADCASTQ         | BROADCAST      | AVX2           |                | 59           | 0b11       | rrr        | VV1 0x59 VL256 V66 V0F38 W0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                   | REG0=YMM_R():w:qq:u64  REG1=XMM_B():r:q:u64  EMX_BROADCAST_1TO4_64
 | VPBROADCASTQ         | BROADCAST      | AVX512EVEX     | AVX512F_128    | 59           | mm         | rrr        | EVV 0x59 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_TUPLE1() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:u64 EMX_BROADCAST_1TO2_64
 | VPBROADCASTQ         | BROADCAST      | AVX512EVEX     | AVX512F_128    | 59           | 0b11       | rrr        | EVV 0x59 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                        | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u64 EMX_BROADCAST_1TO2_64
 | VPBROADCASTQ         | BROADCAST      | AVX512EVEX     | AVX512F_128    | 7c           | 0b11       | rrr        | EVV 0x7C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  mode64  NOEVSR                | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=GPR64_B():r:q:u64 EMX_BROADCAST_1TO2_64
 | VPBROADCASTQ         | BROADCAST      | AVX512EVEX     | AVX512F_256    | 59           | mm         | rrr        | EVV 0x59 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_TUPLE1() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:u64 EMX_BROADCAST_1TO4_64
 | VPBROADCASTQ         | BROADCAST      | AVX512EVEX     | AVX512F_256    | 59           | 0b11       | rrr        | EVV 0x59 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                        | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u64 EMX_BROADCAST_1TO4_64
 | VPBROADCASTQ         | BROADCAST      | AVX512EVEX     | AVX512F_256    | 7c           | 0b11       | rrr        | EVV 0x7C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  mode64  NOEVSR                | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=GPR64_B():r:q:u64 EMX_BROADCAST_1TO4_64
 | VPBROADCASTQ         | BROADCAST      | AVX512EVEX     | AVX512F_512    | 59           | mm         | rrr        | EVV 0x59 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_TUPLE1() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:u64 EMX_BROADCAST_1TO8_64
 | VPBROADCASTQ         | BROADCAST      | AVX512EVEX     | AVX512F_512    | 59           | 0b11       | rrr        | EVV 0x59 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                        | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u64 EMX_BROADCAST_1TO8_64
 | VPBROADCASTQ         | BROADCAST      | AVX512EVEX     | AVX512F_512    | 7c           | 0b11       | rrr        | EVV 0x7C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  mode64  W1  NOEVSR                | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=GPR64_B():r:q:u64 EMX_BROADCAST_1TO8_64
 | VPBROADCASTQ         | BROADCAST      | KNCE           |                | 59           | mm         | rrr        | KVV 0x59 V0F38 V66 REXW=1  REXW=1  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM()  UPCONVERT_FLT64_LOAD() | REG0=ZMM_R3():rw:zq:i64 REG1=MASK1():r:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT  NELEM=1:SUPP EMX_BROADCAST_1TO8_64
 | VPBROADCASTW         | BROADCAST      | AVX2           |                | 79           | mm         | rrr        | VV1 0x79 VL128 V66 V0F38 W0 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                            | REG0=XMM_R():w:dq:u16 MEM0:r:w:u16 EMX_BROADCAST_1TO8_16
 | VPBROADCASTW         | BROADCAST      | AVX2           |                | 79           | 0b11       | rrr        | VV1 0x79 VL128 V66 V0F38 W0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                   | REG0=XMM_R():w:dq:u16  REG1=XMM_B():r:w:u16  EMX_BROADCAST_1TO8_16
 | VPBROADCASTW         | BROADCAST      | AVX2           |                | 79           | mm         | rrr        | VV1 0x79 VL256 V66 V0F38 W0 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                            | REG0=YMM_R():w:qq:u16 MEM0:r:w:u16 EMX_BROADCAST_1TO16_16
 | VPBROADCASTW         | BROADCAST      | AVX2           |                | 79           | 0b11       | rrr        | VV1 0x79 VL256 V66 V0F38 W0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                   | REG0=YMM_R():w:qq:u16  REG1=XMM_B():r:w:u16 EMX_BROADCAST_1TO16_16
 | VPBROADCASTW         | BROADCAST      | AVX512EVEX     | AVX512BW_128   | 79           | 0b11       | rrr        | EVV 0x79 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u16 EMX_BROADCAST_1TO8_16
 | VPBROADCASTW         | BROADCAST      | AVX512EVEX     | AVX512BW_128   | 79           | mm         | rrr        | EVV 0x79 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_16_BITS() NELEM_TUPLE1_WORD() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:wrd:u16 EMX_BROADCAST_1TO8_16
 | VPBROADCASTW         | BROADCAST      | AVX512EVEX     | AVX512BW_128   | 7b           | 0b11       | rrr        | EVV 0x7B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=GPR32_B():r:d:u16 EMX_BROADCAST_1TO8_16
 | VPBROADCASTW         | BROADCAST      | AVX512EVEX     | AVX512BW_256   | 79           | 0b11       | rrr        | EVV 0x79 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u16 EMX_BROADCAST_1TO16_16
 | VPBROADCASTW         | BROADCAST      | AVX512EVEX     | AVX512BW_256   | 79           | mm         | rrr        | EVV 0x79 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_16_BITS() NELEM_TUPLE1_WORD() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:wrd:u16 EMX_BROADCAST_1TO16_16
 | VPBROADCASTW         | BROADCAST      | AVX512EVEX     | AVX512BW_256   | 7b           | 0b11       | rrr        | EVV 0x7B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=GPR32_B():r:d:u16 EMX_BROADCAST_1TO16_16
 | VPBROADCASTW         | BROADCAST      | AVX512EVEX     | AVX512BW_512   | 79           | 0b11       | rrr        | EVV 0x79 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u16 EMX_BROADCAST_1TO32_16
 | VPBROADCASTW         | BROADCAST      | AVX512EVEX     | AVX512BW_512   | 79           | mm         | rrr        | EVV 0x79 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_16_BITS() NELEM_TUPLE1_WORD() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:wrd:u16 EMX_BROADCAST_1TO32_16
 | VPBROADCASTW         | BROADCAST      | AVX512EVEX     | AVX512BW_512   | 7b           | 0b11       | rrr        | EVV 0x7B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=GPR32_B():r:d:u16 EMX_BROADCAST_1TO32_16
 | VPCLMULQDQ           | AVX            | AVX            |                | 44           | 0b11       | rrr        | VV1 0x44  V66 V0F3A  MOD[0b11]  MOD=3  REG[rrr] RM[nnn] VL128 UIMM8()                                | REG0=XMM_R():w:dq:u128  REG1=XMM_N():r:dq:u64 REG2=XMM_B():r:dq:u64 IMM0:r:b
 | VPCLMULQDQ           | AVX            | AVX            |                | 44           | mm         | rrr        | VV1 0x44  V66 V0F3A  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() VL128 UIMM8()                          | REG0=XMM_R():w:dq:u128  REG1=XMM_N():r:dq:u64 MEM0:r:dq:u64 IMM0:r:b
 | VPCLMULQDQ           | VPCLMULQDQ     | AVX512EVEX     | AVX512_VPCLMULQDQ_128 | 44           | 0b11       | rrr        | EVV 0x44 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128      ZEROING=0 MASK=0 UIMM8()      | REG0=XMM_R3():w:dq:u128 REG1=XMM_N3():r:dq:u64 REG2=XMM_B3():r:dq:u64 IMM0:r:b
 | VPCLMULQDQ           | VPCLMULQDQ     | AVX512EVEX     | AVX512_VPCLMULQDQ_128 | 44           | mm         | rrr        | EVV 0x44 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ZEROING=0 MASK=0 UIMM8()  ESIZE_64_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u128 REG1=XMM_N3():r:dq:u64 MEM0:r:dq:u64 IMM0:r:b
 | VPCLMULQDQ           | VPCLMULQDQ     | AVX512EVEX     | AVX512_VPCLMULQDQ_256 | 44           | 0b11       | rrr        | EVV 0x44 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256      ZEROING=0 MASK=0 UIMM8()      | REG0=YMM_R3():w:qq:u128 REG1=YMM_N3():r:qq:u64 REG2=YMM_B3():r:qq:u64 IMM0:r:b
 | VPCLMULQDQ           | VPCLMULQDQ     | AVX512EVEX     | AVX512_VPCLMULQDQ_256 | 44           | mm         | rrr        | EVV 0x44 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ZEROING=0 MASK=0 UIMM8()  ESIZE_64_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u128 REG1=YMM_N3():r:qq:u64 MEM0:r:qq:u64 IMM0:r:b
 | VPCLMULQDQ           | VPCLMULQDQ     | AVX512EVEX     | AVX512_VPCLMULQDQ_512 | 44           | 0b11       | rrr        | EVV 0x44 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512      ZEROING=0 MASK=0 UIMM8()      | REG0=ZMM_R3():w:zu128 REG1=ZMM_N3():r:zu64 REG2=ZMM_B3():r:zu64 IMM0:r:b
 | VPCLMULQDQ           | VPCLMULQDQ     | AVX512EVEX     | AVX512_VPCLMULQDQ_512 | 44           | mm         | rrr        | EVV 0x44 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ZEROING=0 MASK=0 UIMM8()  ESIZE_64_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu128 REG1=ZMM_N3():r:zu64 MEM0:r:zd:u64 IMM0:r:b
 | VPCLMULQDQ           | VPCLMULQDQ     | VPCLMULQDQ     | VPCLMULQDQ     | 44           | 0b11       | rrr        | VV1 0x44 V66 V0F3A MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256     UIMM8()                              | REG0=YMM_R():w:qq:u128 REG1=YMM_N():r:qq:u64 REG2=YMM_B():r:qq:u64 IMM0:r:b
 | VPCLMULQDQ           | VPCLMULQDQ     | VPCLMULQDQ     | VPCLMULQDQ     | 44           | mm         | rrr        | VV1 0x44 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256     UIMM8()                       | REG0=YMM_R():w:qq:u128 REG1=YMM_N():r:qq:u64 MEM0:r:qq:u64 IMM0:r:b
 | VPCMOV               | XOP            | XOP            | XOP            | a2           | mm         | rrr        | XOPV 0xA2 VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                      | REG0=XMM_R():w:dq:i1 REG1=XMM_N():r:dq:i1 MEM0:r:dq:i1 REG2=XMM_SE():r:dq:i1
 | VPCMOV               | XOP            | XOP            | XOP            | a2           | 0b11       | rrr        | XOPV 0xA2 VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                             | REG0=XMM_R():w:dq:i1 REG1=XMM_N():r:dq:i1 REG2=XMM_B():r:dq:i1 REG3=XMM_SE():r:dq:i1
 | VPCMOV               | XOP            | XOP            | XOP            | a2           | mm         | rrr        | XOPV 0xA2 VNP W1 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                      | REG0=XMM_R():w:dq:i1 REG1=XMM_N():r:dq:i1 REG2=XMM_SE():r:dq:i1 MEM0:r:dq:i1
 | VPCMOV               | XOP            | XOP            | XOP            | a2           | 0b11       | rrr        | XOPV 0xA2 VNP W1 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                             | REG0=XMM_R():w:dq:i1 REG1=XMM_N():r:dq:i1 REG2=XMM_SE():r:dq:i1 REG3=XMM_B():r:dq:i1
 | VPCMOV               | XOP            | XOP            | XOP            | a2           | mm         | rrr        | XOPV 0xA2 VNP W0 VL256  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                      | REG0=YMM_R():w:qq:i1 REG1=YMM_N():r:qq:i1 MEM0:r:qq:i1 REG2=YMM_SE():r:qq:i1
 | VPCMOV               | XOP            | XOP            | XOP            | a2           | 0b11       | rrr        | XOPV 0xA2 VNP W0 VL256  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                             | REG0=YMM_R():w:qq:i1 REG1=YMM_N():r:qq:i1 REG2=YMM_B():r:qq:i1 REG3=YMM_SE():r:qq:i1
 | VPCMOV               | XOP            | XOP            | XOP            | a2           | mm         | rrr        | XOPV 0xA2 VNP W1 VL256  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                      | REG0=YMM_R():w:qq:i1 REG1=YMM_N():r:qq:i1 REG2=YMM_SE():r:qq:i1 MEM0:r:qq:i1
 | VPCMOV               | XOP            | XOP            | XOP            | a2           | 0b11       | rrr        | XOPV 0xA2 VNP W1 VL256  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                             | REG0=YMM_R():w:qq:i1 REG1=YMM_N():r:qq:i1 REG2=YMM_SE():r:qq:i1 REG3=YMM_B():r:qq:i1
 | VPCMPB               | AVX512         | AVX512EVEX     | AVX512BW_128   | 3f           | 0b11       | rrr        | EVV 0x3F V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0    ZEROING=0 UIMM8()           | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:i8 REG3=XMM_B3():r:dq:i8 IMM0:r:b
 | VPCMPB               | AVX512         | AVX512EVEX     | AVX512BW_128   | 3f           | mm         | rrr        | EVV 0x3F V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0    ZEROING=0 UIMM8()  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:i8 MEM0:r:dq:i8 IMM0:r:b
 | VPCMPB               | AVX512         | AVX512EVEX     | AVX512BW_256   | 3f           | 0b11       | rrr        | EVV 0x3F V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0    ZEROING=0 UIMM8()           | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:i8 REG3=YMM_B3():r:qq:i8 IMM0:r:b
 | VPCMPB               | AVX512         | AVX512EVEX     | AVX512BW_256   | 3f           | mm         | rrr        | EVV 0x3F V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0    ZEROING=0 UIMM8()  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:i8 MEM0:r:qq:i8 IMM0:r:b
 | VPCMPB               | AVX512         | AVX512EVEX     | AVX512BW_512   | 3f           | 0b11       | rrr        | EVV 0x3F V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0    ZEROING=0 UIMM8()           | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zi8 REG3=ZMM_B3():r:zi8 IMM0:r:b
 | VPCMPB               | AVX512         | AVX512EVEX     | AVX512BW_512   | 3f           | mm         | rrr        | EVV 0x3F V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0    ZEROING=0 UIMM8()  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zi8 MEM0:r:zd:i8 IMM0:r:b
 | VPCMPD               | AVX512         | AVX512EVEX     | AVX512F_128    | 1f           | 0b11       | rrr        | EVV 0x1F V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0    ZEROING=0 UIMM8()           | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:i32 REG3=XMM_B3():r:dq:i32 IMM0:r:b
 | VPCMPD               | AVX512         | AVX512EVEX     | AVX512F_128    | 1f           | mm         | rrr        | EVV 0x1F V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:i32 MEM0:r:vv:i32:TXT=BCASTSTR IMM0:r:b
 | VPCMPD               | AVX512         | AVX512EVEX     | AVX512F_256    | 1f           | 0b11       | rrr        | EVV 0x1F V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0    ZEROING=0 UIMM8()           | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:i32 REG3=YMM_B3():r:qq:i32 IMM0:r:b
 | VPCMPD               | AVX512         | AVX512EVEX     | AVX512F_256    | 1f           | mm         | rrr        | EVV 0x1F V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:i32 MEM0:r:vv:i32:TXT=BCASTSTR IMM0:r:b
 | VPCMPD               | AVX512         | AVX512EVEX     | AVX512F_512    | 1f           | 0b11       | rrr        | EVV 0x1F V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0    ZEROING=0 UIMM8()           | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zi32 REG3=ZMM_B3():r:zi32 IMM0:r:b
 | VPCMPD               | AVX512         | AVX512EVEX     | AVX512F_512    | 1f           | mm         | rrr        | EVV 0x1F V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zi32 MEM0:r:vv:i32:TXT=BCASTSTR IMM0:r:b
 | VPCMPD               | KNC            | KNCE           |                | 1f           | mm         | rrr        | KVV 0x1F V66 V0F3A  W0 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() UPCONVERT_INT32()            | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zd MEM0:r:zv:TXT=CONVERT:TXT=NT IMM0:r:b
 | VPCMPD               | KNC            | KNCE           |                | 1f           | 0b11       | rrr        | KVV 0x1F V66 V0F3A  W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8() NR=0 REG_SWIZZLE64()                 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zd REG3=ZMM_B3():r:zd:TXT=REGSWIZ IMM0:r:b
 | VPCMPD               | KNC            | KNCE           |                | 1f           | 0b11       | rrr        | KVV 0x1F V66 V0F3A  W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8() NR=1  SWIZ=0                         | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zd REG3=ZMM_B3():r:zd IMM0:r:b
 | VPCMPEQB             | AVX            | AVX            |                | 74           | mm         | rrr        | VV1 0x74  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 MEM0:r:dq:u8
 | VPCMPEQB             | AVX            | AVX            |                | 74           | 0b11       | rrr        | VV1 0x74  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:u8
 | VPCMPEQB             | AVX2           | AVX2           |                | 74           | mm         | rrr        | VV1 0x74  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u8 REG1=YMM_N():r:qq:u8 MEM0:r:qq:u8
 | VPCMPEQB             | AVX2           | AVX2           |                | 74           | 0b11       | rrr        | VV1 0x74  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:u8 REG1=YMM_N():r:qq:u8 REG2=YMM_B():r:qq:u8
 | VPCMPEQB             | AVX512         | AVX512EVEX     | AVX512BW_128   | 74           | 0b11       | rrr        | EVV 0x74 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128      ZEROING=0                       | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
 | VPCMPEQB             | AVX512         | AVX512EVEX     | AVX512BW_128   | 74           | mm         | rrr        | EVV 0x74 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ZEROING=0  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
 | VPCMPEQB             | AVX512         | AVX512EVEX     | AVX512BW_256   | 74           | 0b11       | rrr        | EVV 0x74 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256      ZEROING=0                       | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
 | VPCMPEQB             | AVX512         | AVX512EVEX     | AVX512BW_256   | 74           | mm         | rrr        | EVV 0x74 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ZEROING=0  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
 | VPCMPEQB             | AVX512         | AVX512EVEX     | AVX512BW_512   | 74           | 0b11       | rrr        | EVV 0x74 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512      ZEROING=0                       | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
 | VPCMPEQB             | AVX512         | AVX512EVEX     | AVX512BW_512   | 74           | mm         | rrr        | EVV 0x74 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ZEROING=0  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
 | VPCMPEQD             | AVX            | AVX            |                | 76           | mm         | rrr        | VV1 0x76  V66 V0F VL128  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 MEM0:r:dq:u32
 | VPCMPEQD             | AVX            | AVX            |                | 76           | 0b11       | rrr        | VV1 0x76  V66 V0F VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 REG2=XMM_B():r:dq:u32
 | VPCMPEQD             | AVX2           | AVX2           |                | 76           | mm         | rrr        | VV1 0x76  V66 V0F VL256  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u32 REG1=YMM_N():r:qq:u32 MEM0:r:qq:u32
 | VPCMPEQD             | AVX2           | AVX2           |                | 76           | 0b11       | rrr        | VV1 0x76  V66 V0F VL256 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:u32 REG1=YMM_N():r:qq:u32 REG2=YMM_B():r:qq:u32
 | VPCMPEQD             | AVX512         | AVX512EVEX     | AVX512F_128    | 76           | 0b11       | rrr        | EVV 0x76 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0    ZEROING=0                     | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
 | VPCMPEQD             | AVX512         | AVX512EVEX     | AVX512F_128    | 76           | mm         | rrr        | EVV 0x76 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ZEROING=0  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPCMPEQD             | AVX512         | AVX512EVEX     | AVX512F_256    | 76           | 0b11       | rrr        | EVV 0x76 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0    ZEROING=0                     | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
 | VPCMPEQD             | AVX512         | AVX512EVEX     | AVX512F_256    | 76           | mm         | rrr        | EVV 0x76 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ZEROING=0  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPCMPEQD             | AVX512         | AVX512EVEX     | AVX512F_512    | 76           | 0b11       | rrr        | EVV 0x76 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0    ZEROING=0                     | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
 | VPCMPEQD             | AVX512         | AVX512EVEX     | AVX512F_512    | 76           | mm         | rrr        | EVV 0x76 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ZEROING=0  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPCMPEQD             | KNC            | KNCE           |                | 76           | mm         | rrr        | KVV 0x76 V66 V0F  W0 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM()  UPCONVERT_INT32()                     | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zd MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VPCMPEQD             | KNC            | KNCE           |                | 76           | 0b11       | rrr        | KVV 0x76 V66 V0F  W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE64()                           | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zd REG3=ZMM_B3():r:zd:TXT=REGSWIZ
 | VPCMPEQD             | KNC            | KNCE           |                | 76           | 0b11       | rrr        | KVV 0x76 V66 V0F  W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1  SWIZ=0                                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zd REG3=ZMM_B3():r:zd
 | VPCMPEQQ             | AVX            | AVX            |                | 29           | mm         | rrr        | VV1 0x29  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 MEM0:r:dq:u64
 | VPCMPEQQ             | AVX            | AVX            |                | 29           | 0b11       | rrr        | VV1 0x29  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 REG2=XMM_B():r:dq:u64
 | VPCMPEQQ             | AVX2           | AVX2           |                | 29           | mm         | rrr        | VV1 0x29  VL256 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=YMM_R():w:qq:u64 REG1=YMM_N():r:qq:u64 MEM0:r:qq:u64
 | VPCMPEQQ             | AVX2           | AVX2           |                | 29           | 0b11       | rrr        | VV1 0x29  VL256 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=YMM_R():w:qq:u64 REG1=YMM_N():r:qq:u64 REG2=YMM_B():r:qq:u64
 | VPCMPEQQ             | AVX512         | AVX512EVEX     | AVX512F_128    | 29           | 0b11       | rrr        | EVV 0x29 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1    ZEROING=0                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
 | VPCMPEQQ             | AVX512         | AVX512EVEX     | AVX512F_128    | 29           | mm         | rrr        | EVV 0x29 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ZEROING=0  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPCMPEQQ             | AVX512         | AVX512EVEX     | AVX512F_256    | 29           | 0b11       | rrr        | EVV 0x29 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1    ZEROING=0                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
 | VPCMPEQQ             | AVX512         | AVX512EVEX     | AVX512F_256    | 29           | mm         | rrr        | EVV 0x29 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ZEROING=0  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPCMPEQQ             | AVX512         | AVX512EVEX     | AVX512F_512    | 29           | 0b11       | rrr        | EVV 0x29 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1    ZEROING=0                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
 | VPCMPEQQ             | AVX512         | AVX512EVEX     | AVX512F_512    | 29           | mm         | rrr        | EVV 0x29 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ZEROING=0  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPCMPEQW             | AVX            | AVX            |                | 75           | mm         | rrr        | VV1 0x75  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 MEM0:r:dq:u16
 | VPCMPEQW             | AVX            | AVX            |                | 75           | 0b11       | rrr        | VV1 0x75  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 REG2=XMM_B():r:dq:u16
 | VPCMPEQW             | AVX2           | AVX2           |                | 75           | mm         | rrr        | VV1 0x75  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u16 REG1=YMM_N():r:qq:u16 MEM0:r:qq:u16
 | VPCMPEQW             | AVX2           | AVX2           |                | 75           | 0b11       | rrr        | VV1 0x75  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:u16 REG1=YMM_N():r:qq:u16 REG2=YMM_B():r:qq:u16
 | VPCMPEQW             | AVX512         | AVX512EVEX     | AVX512BW_128   | 75           | 0b11       | rrr        | EVV 0x75 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128      ZEROING=0                       | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
 | VPCMPEQW             | AVX512         | AVX512EVEX     | AVX512BW_128   | 75           | mm         | rrr        | EVV 0x75 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ZEROING=0  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
 | VPCMPEQW             | AVX512         | AVX512EVEX     | AVX512BW_256   | 75           | 0b11       | rrr        | EVV 0x75 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256      ZEROING=0                       | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
 | VPCMPEQW             | AVX512         | AVX512EVEX     | AVX512BW_256   | 75           | mm         | rrr        | EVV 0x75 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ZEROING=0  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
 | VPCMPEQW             | AVX512         | AVX512EVEX     | AVX512BW_512   | 75           | 0b11       | rrr        | EVV 0x75 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512      ZEROING=0                       | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
 | VPCMPEQW             | AVX512         | AVX512EVEX     | AVX512BW_512   | 75           | mm         | rrr        | EVV 0x75 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ZEROING=0  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
 | VPCMPESTRI           | STTNI          | AVX            |                | 61           | mm         | rrr        | VV1 0x61  VL128 V66 V0F3A NOVSR not64 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_EAX:r:SUPP REG2=XED_REG_EDX:r:SUPP REG3=XED_REG_ECX:w:SUPP
 | VPCMPESTRI           | STTNI          | AVX            |                | 61           | 0b11       | rrr        | VV1 0x61  VL128 V66 V0F3A NOVSR not64 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                       | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_EAX:r:SUPP REG3=XED_REG_EDX:r:SUPP REG4=XED_REG_ECX:w:SUPP
 | VPCMPESTRI           | STTNI          | AVX            |                | 61           | mm         | rrr        | VV1 0x61  VL128 V66 V0F3A NOVSR mode64 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()            | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_EAX:r:SUPP REG2=XED_REG_EDX:r:SUPP REG3=XED_REG_ECX:w:SUPP
 | VPCMPESTRI           | STTNI          | AVX            |                | 61           | 0b11       | rrr        | VV1 0x61  VL128 V66 V0F3A NOVSR mode64 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                   | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_EAX:r:SUPP REG3=XED_REG_EDX:r:SUPP REG4=XED_REG_ECX:w:SUPP
 | VPCMPESTRI64         | STTNI          | AVX            |                | 61           | mm         | rrr        | VV1 0x61  VL128 V66 V0F3A NOVSR mode64 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()            | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_RAX:r:SUPP REG2=XED_REG_RDX:r:SUPP REG3=XED_REG_RCX:w:SUPP
 | VPCMPESTRI64         | STTNI          | AVX            |                | 61           | 0b11       | rrr        | VV1 0x61  VL128 V66 V0F3A NOVSR mode64 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                   | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_RAX:r:SUPP REG3=XED_REG_RDX:r:SUPP REG4=XED_REG_RCX:w:SUPP
 | VPCMPESTRM           | STTNI          | AVX            |                | 60           | mm         | rrr        | VV1 0x60  VL128 V66 V0F3A NOVSR not64 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_EAX:r:SUPP REG2=XED_REG_EDX:r:SUPP REG3=XED_REG_XMM0:w:dq:SUPP
 | VPCMPESTRM           | STTNI          | AVX            |                | 60           | 0b11       | rrr        | VV1 0x60  VL128 V66 V0F3A NOVSR not64 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                       | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_EAX:r:SUPP REG3=XED_REG_EDX:r:SUPP REG4=XED_REG_XMM0:w:dq:SUPP
 | VPCMPESTRM           | STTNI          | AVX            |                | 60           | mm         | rrr        | VV1 0x60  VL128 V66 V0F3A NOVSR mode64 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()            | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_EAX:r:SUPP REG2=XED_REG_EDX:r:SUPP REG3=XED_REG_XMM0:w:dq:SUPP
 | VPCMPESTRM           | STTNI          | AVX            |                | 60           | 0b11       | rrr        | VV1 0x60  VL128 V66 V0F3A NOVSR mode64 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                   | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_EAX:r:SUPP REG3=XED_REG_EDX:r:SUPP REG4=XED_REG_XMM0:w:dq:SUPP
 | VPCMPESTRM64         | STTNI          | AVX            |                | 60           | mm         | rrr        | VV1 0x60  VL128 V66 V0F3A NOVSR mode64 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()            | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_RAX:r:SUPP REG2=XED_REG_RDX:r:SUPP REG3=XED_REG_XMM0:w:dq:SUPP
 | VPCMPESTRM64         | STTNI          | AVX            |                | 60           | 0b11       | rrr        | VV1 0x60  VL128 V66 V0F3A NOVSR mode64 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                   | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_RAX:r:SUPP REG3=XED_REG_RDX:r:SUPP REG4=XED_REG_XMM0:w:dq:SUPP
 | VPCMPGTB             | AVX            | AVX            |                | 64           | mm         | rrr        | VV1 0x64  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 MEM0:r:dq:i8
 | VPCMPGTB             | AVX            | AVX            |                | 64           | 0b11       | rrr        | VV1 0x64  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 REG2=XMM_B():r:dq:i8
 | VPCMPGTB             | AVX2           | AVX2           |                | 64           | mm         | rrr        | VV1 0x64  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:i8 REG1=YMM_N():r:qq:i8 MEM0:r:qq:i8
 | VPCMPGTB             | AVX2           | AVX2           |                | 64           | 0b11       | rrr        | VV1 0x64  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:i8 REG1=YMM_N():r:qq:i8 REG2=YMM_B():r:qq:i8
 | VPCMPGTB             | AVX512         | AVX512EVEX     | AVX512BW_128   | 64           | 0b11       | rrr        | EVV 0x64 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128      ZEROING=0                       | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
 | VPCMPGTB             | AVX512         | AVX512EVEX     | AVX512BW_128   | 64           | mm         | rrr        | EVV 0x64 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ZEROING=0  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
 | VPCMPGTB             | AVX512         | AVX512EVEX     | AVX512BW_256   | 64           | 0b11       | rrr        | EVV 0x64 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256      ZEROING=0                       | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
 | VPCMPGTB             | AVX512         | AVX512EVEX     | AVX512BW_256   | 64           | mm         | rrr        | EVV 0x64 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ZEROING=0  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
 | VPCMPGTB             | AVX512         | AVX512EVEX     | AVX512BW_512   | 64           | 0b11       | rrr        | EVV 0x64 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512      ZEROING=0                       | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
 | VPCMPGTB             | AVX512         | AVX512EVEX     | AVX512BW_512   | 64           | mm         | rrr        | EVV 0x64 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ZEROING=0  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
 | VPCMPGTD             | AVX            | AVX            |                | 66           | mm         | rrr        | VV1 0x66  V66 V0F VL128  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32
 | VPCMPGTD             | AVX            | AVX            |                | 66           | 0b11       | rrr        | VV1 0x66  V66 V0F VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32
 | VPCMPGTD             | AVX2           | AVX2           |                | 66           | mm         | rrr        | VV1 0x66  V66 V0F VL256  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:i32 REG1=YMM_N():r:qq:i32 MEM0:r:qq:i32
 | VPCMPGTD             | AVX2           | AVX2           |                | 66           | 0b11       | rrr        | VV1 0x66  V66 V0F VL256 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:i32 REG1=YMM_N():r:qq:i32 REG2=YMM_B():r:qq:i32
 | VPCMPGTD             | AVX512         | AVX512EVEX     | AVX512F_128    | 66           | 0b11       | rrr        | EVV 0x66 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0    ZEROING=0                     | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:i32 REG3=XMM_B3():r:dq:i32
 | VPCMPGTD             | AVX512         | AVX512EVEX     | AVX512F_128    | 66           | mm         | rrr        | EVV 0x66 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ZEROING=0  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:i32 MEM0:r:vv:i32:TXT=BCASTSTR
 | VPCMPGTD             | AVX512         | AVX512EVEX     | AVX512F_256    | 66           | 0b11       | rrr        | EVV 0x66 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0    ZEROING=0                     | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:i32 REG3=YMM_B3():r:qq:i32
 | VPCMPGTD             | AVX512         | AVX512EVEX     | AVX512F_256    | 66           | mm         | rrr        | EVV 0x66 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ZEROING=0  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:i32 MEM0:r:vv:i32:TXT=BCASTSTR
 | VPCMPGTD             | AVX512         | AVX512EVEX     | AVX512F_512    | 66           | 0b11       | rrr        | EVV 0x66 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0    ZEROING=0                     | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zi32 REG3=ZMM_B3():r:zi32
 | VPCMPGTD             | AVX512         | AVX512EVEX     | AVX512F_512    | 66           | mm         | rrr        | EVV 0x66 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ZEROING=0  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zi32 MEM0:r:vv:i32:TXT=BCASTSTR
 | VPCMPGTD             | KNC            | KNCE           |                | 66           | mm         | rrr        | KVV 0x66 V66 V0F  W0 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM()  UPCONVERT_INT32()                     | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zd MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VPCMPGTD             | KNC            | KNCE           |                | 66           | 0b11       | rrr        | KVV 0x66 V66 V0F  W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE64()                           | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zd REG3=ZMM_B3():r:zd:TXT=REGSWIZ
 | VPCMPGTD             | KNC            | KNCE           |                | 66           | 0b11       | rrr        | KVV 0x66 V66 V0F  W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1  SWIZ=0                                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zd REG3=ZMM_B3():r:zd
 | VPCMPGTQ             | AVX            | AVX            |                | 37           | mm         | rrr        | VV1 0x37  V66 V0F38 VL128  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i64 MEM0:r:dq:i64
 | VPCMPGTQ             | AVX            | AVX            |                | 37           | 0b11       | rrr        | VV1 0x37  V66 V0F38 VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i64 REG2=XMM_B():r:dq:i64
 | VPCMPGTQ             | AVX2           | AVX2           |                | 37           | mm         | rrr        | VV1 0x37  V66 V0F38 VL256  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=YMM_R():w:qq:i64 REG1=YMM_N():r:qq:i64 MEM0:r:qq:i64
 | VPCMPGTQ             | AVX2           | AVX2           |                | 37           | 0b11       | rrr        | VV1 0x37  V66 V0F38 VL256 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=YMM_R():w:qq:i64 REG1=YMM_N():r:qq:i64 REG2=YMM_B():r:qq:i64
 | VPCMPGTQ             | AVX512         | AVX512EVEX     | AVX512F_128    | 37           | 0b11       | rrr        | EVV 0x37 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1    ZEROING=0                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:i64 REG3=XMM_B3():r:dq:i64
 | VPCMPGTQ             | AVX512         | AVX512EVEX     | AVX512F_128    | 37           | mm         | rrr        | EVV 0x37 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ZEROING=0  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:i64 MEM0:r:vv:i64:TXT=BCASTSTR
 | VPCMPGTQ             | AVX512         | AVX512EVEX     | AVX512F_256    | 37           | 0b11       | rrr        | EVV 0x37 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1    ZEROING=0                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:i64 REG3=YMM_B3():r:qq:i64
 | VPCMPGTQ             | AVX512         | AVX512EVEX     | AVX512F_256    | 37           | mm         | rrr        | EVV 0x37 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ZEROING=0  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:i64 MEM0:r:vv:i64:TXT=BCASTSTR
 | VPCMPGTQ             | AVX512         | AVX512EVEX     | AVX512F_512    | 37           | 0b11       | rrr        | EVV 0x37 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1    ZEROING=0                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zi64 REG3=ZMM_B3():r:zi64
 | VPCMPGTQ             | AVX512         | AVX512EVEX     | AVX512F_512    | 37           | mm         | rrr        | EVV 0x37 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ZEROING=0  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zi64 MEM0:r:vv:i64:TXT=BCASTSTR
 | VPCMPGTW             | AVX            | AVX            |                | 65           | mm         | rrr        | VV1 0x65  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
 | VPCMPGTW             | AVX            | AVX            |                | 65           | 0b11       | rrr        | VV1 0x65  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
 | VPCMPGTW             | AVX2           | AVX2           |                | 65           | mm         | rrr        | VV1 0x65  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:i16 REG1=YMM_N():r:qq:i16 MEM0:r:qq:i16
 | VPCMPGTW             | AVX2           | AVX2           |                | 65           | 0b11       | rrr        | VV1 0x65  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:i16 REG1=YMM_N():r:qq:i16 REG2=YMM_B():r:qq:i16
 | VPCMPGTW             | AVX512         | AVX512EVEX     | AVX512BW_128   | 65           | 0b11       | rrr        | EVV 0x65 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128      ZEROING=0                       | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
 | VPCMPGTW             | AVX512         | AVX512EVEX     | AVX512BW_128   | 65           | mm         | rrr        | EVV 0x65 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ZEROING=0  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
 | VPCMPGTW             | AVX512         | AVX512EVEX     | AVX512BW_256   | 65           | 0b11       | rrr        | EVV 0x65 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256      ZEROING=0                       | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
 | VPCMPGTW             | AVX512         | AVX512EVEX     | AVX512BW_256   | 65           | mm         | rrr        | EVV 0x65 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ZEROING=0  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
 | VPCMPGTW             | AVX512         | AVX512EVEX     | AVX512BW_512   | 65           | 0b11       | rrr        | EVV 0x65 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512      ZEROING=0                       | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
 | VPCMPGTW             | AVX512         | AVX512EVEX     | AVX512BW_512   | 65           | mm         | rrr        | EVV 0x65 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ZEROING=0  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
 | VPCMPISTRI           | STTNI          | AVX            |                | 63           | mm         | rrr        | VV1 0x63  VL128 V66 V0F3A NOVSR  not64  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()              | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_ECX:w:SUPP
 | VPCMPISTRI           | STTNI          | AVX            |                | 63           | 0b11       | rrr        | VV1 0x63  VL128 V66 V0F3A NOVSR  not64  MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                     | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_ECX:w:SUPP
 | VPCMPISTRI           | STTNI          | AVX            |                | 63           | mm         | rrr        | VV1 0x63  VL128 V66 V0F3A NOVSR mode64 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()            | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_ECX:w:SUPP
 | VPCMPISTRI           | STTNI          | AVX            |                | 63           | 0b11       | rrr        | VV1 0x63  VL128 V66 V0F3A NOVSR mode64 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                   | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_ECX:w:SUPP
 | VPCMPISTRI64         | STTNI          | AVX            |                | 63           | mm         | rrr        | VV1 0x63  VL128 V66 V0F3A NOVSR mode64 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()            | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_RCX:w:SUPP
 | VPCMPISTRI64         | STTNI          | AVX            |                | 63           | 0b11       | rrr        | VV1 0x63  VL128 V66 V0F3A NOVSR mode64 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                   | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_RCX:w:SUPP
 | VPCMPISTRM           | STTNI          | AVX            |                | 62           | mm         | rrr        | VV1 0x62  VL128 V66 V0F3A NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                      | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_XMM0:w:dq:SUPP
 | VPCMPISTRM           | STTNI          | AVX            |                | 62           | 0b11       | rrr        | VV1 0x62  VL128 V66 V0F3A NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                             | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_XMM0:w:dq:SUPP
 | VPCMPLTD             | KNC            | KNCE           |                | 74           | mm         | rrr        | KVV 0x74 V66 V0F38  W0 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM()  UPCONVERT_INT32()                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zd MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VPCMPLTD             | KNC            | KNCE           |                | 74           | 0b11       | rrr        | KVV 0x74 V66 V0F38  W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE64()                         | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zd REG3=ZMM_B3():r:zd:TXT=REGSWIZ
 | VPCMPLTD             | KNC            | KNCE           |                | 74           | 0b11       | rrr        | KVV 0x74 V66 V0F38  W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1  SWIZ=0                                 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zd REG3=ZMM_B3():r:zd
 | VPCMPQ               | AVX512         | AVX512EVEX     | AVX512F_128    | 1f           | 0b11       | rrr        | EVV 0x1F V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1    ZEROING=0 UIMM8()           | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:i64 REG3=XMM_B3():r:dq:i64 IMM0:r:b
 | VPCMPQ               | AVX512         | AVX512EVEX     | AVX512F_128    | 1f           | mm         | rrr        | EVV 0x1F V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:i64 MEM0:r:vv:i64:TXT=BCASTSTR IMM0:r:b
 | VPCMPQ               | AVX512         | AVX512EVEX     | AVX512F_256    | 1f           | 0b11       | rrr        | EVV 0x1F V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1    ZEROING=0 UIMM8()           | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:i64 REG3=YMM_B3():r:qq:i64 IMM0:r:b
 | VPCMPQ               | AVX512         | AVX512EVEX     | AVX512F_256    | 1f           | mm         | rrr        | EVV 0x1F V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:i64 MEM0:r:vv:i64:TXT=BCASTSTR IMM0:r:b
 | VPCMPQ               | AVX512         | AVX512EVEX     | AVX512F_512    | 1f           | 0b11       | rrr        | EVV 0x1F V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1    ZEROING=0 UIMM8()           | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zi64 REG3=ZMM_B3():r:zi64 IMM0:r:b
 | VPCMPQ               | AVX512         | AVX512EVEX     | AVX512F_512    | 1f           | mm         | rrr        | EVV 0x1F V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zi64 MEM0:r:vv:i64:TXT=BCASTSTR IMM0:r:b
 | VPCMPUB              | AVX512         | AVX512EVEX     | AVX512BW_128   | 3e           | 0b11       | rrr        | EVV 0x3E V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0    ZEROING=0 UIMM8()           | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8 IMM0:r:b
 | VPCMPUB              | AVX512         | AVX512EVEX     | AVX512BW_128   | 3e           | mm         | rrr        | EVV 0x3E V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0    ZEROING=0 UIMM8()  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8 IMM0:r:b
 | VPCMPUB              | AVX512         | AVX512EVEX     | AVX512BW_256   | 3e           | 0b11       | rrr        | EVV 0x3E V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0    ZEROING=0 UIMM8()           | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8 IMM0:r:b
 | VPCMPUB              | AVX512         | AVX512EVEX     | AVX512BW_256   | 3e           | mm         | rrr        | EVV 0x3E V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0    ZEROING=0 UIMM8()  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8 IMM0:r:b
 | VPCMPUB              | AVX512         | AVX512EVEX     | AVX512BW_512   | 3e           | 0b11       | rrr        | EVV 0x3E V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0    ZEROING=0 UIMM8()           | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8 IMM0:r:b
 | VPCMPUB              | AVX512         | AVX512EVEX     | AVX512BW_512   | 3e           | mm         | rrr        | EVV 0x3E V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0    ZEROING=0 UIMM8()  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8 IMM0:r:b
 | VPCMPUD              | AVX512         | AVX512EVEX     | AVX512F_128    | 1e           | 0b11       | rrr        | EVV 0x1E V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0    ZEROING=0 UIMM8()           | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32 IMM0:r:b
 | VPCMPUD              | AVX512         | AVX512EVEX     | AVX512F_128    | 1e           | mm         | rrr        | EVV 0x1E V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
 | VPCMPUD              | AVX512         | AVX512EVEX     | AVX512F_256    | 1e           | 0b11       | rrr        | EVV 0x1E V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0    ZEROING=0 UIMM8()           | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32 IMM0:r:b
 | VPCMPUD              | AVX512         | AVX512EVEX     | AVX512F_256    | 1e           | mm         | rrr        | EVV 0x1E V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
 | VPCMPUD              | AVX512         | AVX512EVEX     | AVX512F_512    | 1e           | 0b11       | rrr        | EVV 0x1E V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0    ZEROING=0 UIMM8()           | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32 IMM0:r:b
 | VPCMPUD              | AVX512         | AVX512EVEX     | AVX512F_512    | 1e           | mm         | rrr        | EVV 0x1E V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
 | VPCMPUD              | KNC            | KNCE           |                | 1e           | mm         | rrr        | KVV 0x1E V66 V0F3A  W0 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() UPCONVERT_INT32()            | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zud MEM0:r:zv:TXT=CONVERT:TXT=NT IMM0:r:b
 | VPCMPUD              | KNC            | KNCE           |                | 1e           | 0b11       | rrr        | KVV 0x1E V66 V0F3A  W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8() NR=0 REG_SWIZZLE64()                 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zud REG3=ZMM_B3():r:zud:TXT=REGSWIZ IMM0:r:b
 | VPCMPUD              | KNC            | KNCE           |                | 1e           | 0b11       | rrr        | KVV 0x1E V66 V0F3A  W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8() NR=1  SWIZ=0                         | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zud REG3=ZMM_B3():r:zud IMM0:r:b
 | VPCMPUQ              | AVX512         | AVX512EVEX     | AVX512F_128    | 1e           | 0b11       | rrr        | EVV 0x1E V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1    ZEROING=0 UIMM8()           | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64 IMM0:r:b
 | VPCMPUQ              | AVX512         | AVX512EVEX     | AVX512F_128    | 1e           | mm         | rrr        | EVV 0x1E V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VPCMPUQ              | AVX512         | AVX512EVEX     | AVX512F_256    | 1e           | 0b11       | rrr        | EVV 0x1E V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1    ZEROING=0 UIMM8()           | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64 IMM0:r:b
 | VPCMPUQ              | AVX512         | AVX512EVEX     | AVX512F_256    | 1e           | mm         | rrr        | EVV 0x1E V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VPCMPUQ              | AVX512         | AVX512EVEX     | AVX512F_512    | 1e           | 0b11       | rrr        | EVV 0x1E V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1    ZEROING=0 UIMM8()           | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64 IMM0:r:b
 | VPCMPUQ              | AVX512         | AVX512EVEX     | AVX512F_512    | 1e           | mm         | rrr        | EVV 0x1E V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VPCMPUW              | AVX512         | AVX512EVEX     | AVX512BW_128   | 3e           | 0b11       | rrr        | EVV 0x3E V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1    ZEROING=0 UIMM8()           | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16 IMM0:r:b
 | VPCMPUW              | AVX512         | AVX512EVEX     | AVX512BW_128   | 3e           | mm         | rrr        | EVV 0x3E V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1    ZEROING=0 UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16 IMM0:r:b
 | VPCMPUW              | AVX512         | AVX512EVEX     | AVX512BW_256   | 3e           | 0b11       | rrr        | EVV 0x3E V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1    ZEROING=0 UIMM8()           | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16 IMM0:r:b
 | VPCMPUW              | AVX512         | AVX512EVEX     | AVX512BW_256   | 3e           | mm         | rrr        | EVV 0x3E V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1    ZEROING=0 UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16 IMM0:r:b
 | VPCMPUW              | AVX512         | AVX512EVEX     | AVX512BW_512   | 3e           | 0b11       | rrr        | EVV 0x3E V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1    ZEROING=0 UIMM8()           | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16 IMM0:r:b
 | VPCMPUW              | AVX512         | AVX512EVEX     | AVX512BW_512   | 3e           | mm         | rrr        | EVV 0x3E V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1    ZEROING=0 UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16 IMM0:r:b
 | VPCMPW               | AVX512         | AVX512EVEX     | AVX512BW_128   | 3f           | 0b11       | rrr        | EVV 0x3F V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1    ZEROING=0 UIMM8()           | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:i16 REG3=XMM_B3():r:dq:i16 IMM0:r:b
 | VPCMPW               | AVX512         | AVX512EVEX     | AVX512BW_128   | 3f           | mm         | rrr        | EVV 0x3F V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1    ZEROING=0 UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:i16 MEM0:r:dq:i16 IMM0:r:b
 | VPCMPW               | AVX512         | AVX512EVEX     | AVX512BW_256   | 3f           | 0b11       | rrr        | EVV 0x3F V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1    ZEROING=0 UIMM8()           | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:i16 REG3=YMM_B3():r:qq:i16 IMM0:r:b
 | VPCMPW               | AVX512         | AVX512EVEX     | AVX512BW_256   | 3f           | mm         | rrr        | EVV 0x3F V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1    ZEROING=0 UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:i16 MEM0:r:qq:i16 IMM0:r:b
 | VPCMPW               | AVX512         | AVX512EVEX     | AVX512BW_512   | 3f           | 0b11       | rrr        | EVV 0x3F V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1    ZEROING=0 UIMM8()           | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zi16 REG3=ZMM_B3():r:zi16 IMM0:r:b
 | VPCMPW               | AVX512         | AVX512EVEX     | AVX512BW_512   | 3f           | mm         | rrr        | EVV 0x3F V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1    ZEROING=0 UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zi16 MEM0:r:zd:i16 IMM0:r:b
 | VPCOMB               | XOP            | XOP            | XOP            | cc           | mm         | rrr        | XOPV 0xCC VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                        | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 MEM0:r:dq:i8 IMM0:r:b:u8
 | VPCOMB               | XOP            | XOP            | XOP            | cc           | 0b11       | rrr        | XOPV 0xCC VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                               | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 REG2=XMM_B():r:dq:i8 IMM0:r:b:u8
 | VPCOMD               | XOP            | XOP            | XOP            | ce           | mm         | rrr        | XOPV 0xCE VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                        | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32 IMM0:r:b:u8
 | VPCOMD               | XOP            | XOP            | XOP            | ce           | 0b11       | rrr        | XOPV 0xCE VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                               | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32 IMM0:r:b:u8
 | VPCOMPRESSB          | COMPRESS       | AVX512EVEX     | AVX512_VBMI2_128 | 63           | mm         | rrr        | EVV 0x63 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_GSCAT() | MEM0:w:dq:u8 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u8
 | VPCOMPRESSB          | COMPRESS       | AVX512EVEX     | AVX512_VBMI2_128 | 63           | 0b11       | rrr        | EVV 0x63 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_B3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u8
 | VPCOMPRESSB          | COMPRESS       | AVX512EVEX     | AVX512_VBMI2_256 | 63           | mm         | rrr        | EVV 0x63 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_GSCAT() | MEM0:w:qq:u8 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u8
 | VPCOMPRESSB          | COMPRESS       | AVX512EVEX     | AVX512_VBMI2_256 | 63           | 0b11       | rrr        | EVV 0x63 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=YMM_B3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u8
 | VPCOMPRESSB          | COMPRESS       | AVX512EVEX     | AVX512_VBMI2_512 | 63           | mm         | rrr        | EVV 0x63 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_GSCAT() | MEM0:w:zd:u8 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu8
 | VPCOMPRESSB          | COMPRESS       | AVX512EVEX     | AVX512_VBMI2_512 | 63           | 0b11       | rrr        | EVV 0x63 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=ZMM_B3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu8
 | VPCOMPRESSD          | COMPRESS       | AVX512EVEX     | AVX512F_128    | 8b           | mm         | rrr        | EVV 0x8B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:w:dq:u32 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u32
 | VPCOMPRESSD          | COMPRESS       | AVX512EVEX     | AVX512F_128    | 8b           | 0b11       | rrr        | EVV 0x8B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_B3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u32
 | VPCOMPRESSD          | COMPRESS       | AVX512EVEX     | AVX512F_256    | 8b           | mm         | rrr        | EVV 0x8B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:w:qq:u32 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u32
 | VPCOMPRESSD          | COMPRESS       | AVX512EVEX     | AVX512F_256    | 8b           | 0b11       | rrr        | EVV 0x8B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=YMM_B3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u32
 | VPCOMPRESSD          | COMPRESS       | AVX512EVEX     | AVX512F_512    | 8b           | mm         | rrr        | EVV 0x8B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:w:zd:u32 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu32
 | VPCOMPRESSD          | COMPRESS       | AVX512EVEX     | AVX512F_512    | 8b           | 0b11       | rrr        | EVV 0x8B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=ZMM_B3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu32
 | VPCOMPRESSQ          | COMPRESS       | AVX512EVEX     | AVX512F_128    | 8b           | mm         | rrr        | EVV 0x8B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:w:dq:u64 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u64
 | VPCOMPRESSQ          | COMPRESS       | AVX512EVEX     | AVX512F_128    | 8b           | 0b11       | rrr        | EVV 0x8B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                        | REG0=XMM_B3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u64
 | VPCOMPRESSQ          | COMPRESS       | AVX512EVEX     | AVX512F_256    | 8b           | mm         | rrr        | EVV 0x8B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:w:qq:u64 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u64
 | VPCOMPRESSQ          | COMPRESS       | AVX512EVEX     | AVX512F_256    | 8b           | 0b11       | rrr        | EVV 0x8B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                        | REG0=YMM_B3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u64
 | VPCOMPRESSQ          | COMPRESS       | AVX512EVEX     | AVX512F_512    | 8b           | mm         | rrr        | EVV 0x8B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:w:zd:u64 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu64
 | VPCOMPRESSQ          | COMPRESS       | AVX512EVEX     | AVX512F_512    | 8b           | 0b11       | rrr        | EVV 0x8B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                        | REG0=ZMM_B3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu64
 | VPCOMPRESSW          | COMPRESS       | AVX512EVEX     | AVX512_VBMI2_128 | 63           | mm         | rrr        | EVV 0x63 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_GSCAT() | MEM0:w:dq:u16 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u16
 | VPCOMPRESSW          | COMPRESS       | AVX512EVEX     | AVX512_VBMI2_128 | 63           | 0b11       | rrr        | EVV 0x63 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                        | REG0=XMM_B3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u16
 | VPCOMPRESSW          | COMPRESS       | AVX512EVEX     | AVX512_VBMI2_256 | 63           | mm         | rrr        | EVV 0x63 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_GSCAT() | MEM0:w:qq:u16 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u16
 | VPCOMPRESSW          | COMPRESS       | AVX512EVEX     | AVX512_VBMI2_256 | 63           | 0b11       | rrr        | EVV 0x63 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                        | REG0=YMM_B3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u16
 | VPCOMPRESSW          | COMPRESS       | AVX512EVEX     | AVX512_VBMI2_512 | 63           | mm         | rrr        | EVV 0x63 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_GSCAT() | MEM0:w:zd:u16 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu16
 | VPCOMPRESSW          | COMPRESS       | AVX512EVEX     | AVX512_VBMI2_512 | 63           | 0b11       | rrr        | EVV 0x63 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                        | REG0=ZMM_B3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu16
 | VPCOMQ               | XOP            | XOP            | XOP            | cf           | mm         | rrr        | XOPV 0xCF VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                        | REG0=XMM_R():w:dq:i64 REG1=XMM_N():r:dq:i64 MEM0:r:dq:i64 IMM0:r:b:u8
 | VPCOMQ               | XOP            | XOP            | XOP            | cf           | 0b11       | rrr        | XOPV 0xCF VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                               | REG0=XMM_R():w:dq:i64 REG1=XMM_N():r:dq:i64 REG2=XMM_B():r:dq:i64 IMM0:r:b:u8
 | VPCOMUB              | XOP            | XOP            | XOP            | ec           | mm         | rrr        | XOPV 0xEC VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                        | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 MEM0:r:dq:u8 IMM0:r:b:u8
 | VPCOMUB              | XOP            | XOP            | XOP            | ec           | 0b11       | rrr        | XOPV 0xEC VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                               | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:u8 IMM0:r:b:u8
 | VPCOMUD              | XOP            | XOP            | XOP            | ee           | mm         | rrr        | XOPV 0xEE VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                        | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 MEM0:r:dq:u32 IMM0:r:b:u8
 | VPCOMUD              | XOP            | XOP            | XOP            | ee           | 0b11       | rrr        | XOPV 0xEE VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                               | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 REG2=XMM_B():r:dq:u32 IMM0:r:b:u8
 | VPCOMUQ              | XOP            | XOP            | XOP            | ef           | mm         | rrr        | XOPV 0xEF VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                        | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 MEM0:r:dq:u64 IMM0:r:b:u8
 | VPCOMUQ              | XOP            | XOP            | XOP            | ef           | 0b11       | rrr        | XOPV 0xEF VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                               | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 REG2=XMM_B():r:dq:u64 IMM0:r:b:u8
 | VPCOMUW              | XOP            | XOP            | XOP            | ed           | mm         | rrr        | XOPV 0xED VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                        | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 MEM0:r:dq:u16 IMM0:r:b:u8
 | VPCOMUW              | XOP            | XOP            | XOP            | ed           | 0b11       | rrr        | XOPV 0xED VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                               | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 REG2=XMM_B():r:dq:u16 IMM0:r:b:u8
 | VPCOMW               | XOP            | XOP            | XOP            | cd           | mm         | rrr        | XOPV 0xCD VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                        | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16 IMM0:r:b:u8
 | VPCOMW               | XOP            | XOP            | XOP            | cd           | 0b11       | rrr        | XOPV 0xCD VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                               | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16 IMM0:r:b:u8
 | VPCONFLICTD          | CONFLICT       | AVX512EVEX     | AVX512CD_128   | c4           | 0b11       | rrr        | EVV 0xC4 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32
 | VPCONFLICTD          | CONFLICT       | AVX512EVEX     | AVX512CD_128   | c4           | mm         | rrr        | EVV 0xC4 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR
 | VPCONFLICTD          | CONFLICT       | AVX512EVEX     | AVX512CD_256   | c4           | 0b11       | rrr        | EVV 0xC4 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u32
 | VPCONFLICTD          | CONFLICT       | AVX512EVEX     | AVX512CD_256   | c4           | mm         | rrr        | EVV 0xC4 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR
 | VPCONFLICTD          | CONFLICT       | AVX512EVEX     | AVX512CD_512   | c4           | 0b11       | rrr        | EVV 0xC4 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu32
 | VPCONFLICTD          | CONFLICT       | AVX512EVEX     | AVX512CD_512   | c4           | mm         | rrr        | EVV 0xC4 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR
 | VPCONFLICTQ          | CONFLICT       | AVX512EVEX     | AVX512CD_128   | c4           | 0b11       | rrr        | EVV 0xC4 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                        | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u64
 | VPCONFLICTQ          | CONFLICT       | AVX512EVEX     | AVX512CD_128   | c4           | mm         | rrr        | EVV 0xC4 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR
 | VPCONFLICTQ          | CONFLICT       | AVX512EVEX     | AVX512CD_256   | c4           | 0b11       | rrr        | EVV 0xC4 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                        | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u64
 | VPCONFLICTQ          | CONFLICT       | AVX512EVEX     | AVX512CD_256   | c4           | mm         | rrr        | EVV 0xC4 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR
 | VPCONFLICTQ          | CONFLICT       | AVX512EVEX     | AVX512CD_512   | c4           | 0b11       | rrr        | EVV 0xC4 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                        | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu64
 | VPCONFLICTQ          | CONFLICT       | AVX512EVEX     | AVX512CD_512   | c4           | mm         | rrr        | EVV 0xC4 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR
 | VPDPBUSD             | AVX512         | AVX512EVEX     | AVX512_VNNI_128 | 50           | 0b11       | rrr        | EVV 0x50 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():rw:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u32
 | VPDPBUSD             | AVX512         | AVX512EVEX     | AVX512_VNNI_128 | 50           | mm         | rrr        | EVV 0x50 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPDPBUSD             | AVX512         | AVX512EVEX     | AVX512_VNNI_256 | 50           | 0b11       | rrr        | EVV 0x50 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():rw:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u32
 | VPDPBUSD             | AVX512         | AVX512EVEX     | AVX512_VNNI_256 | 50           | mm         | rrr        | EVV 0x50 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPDPBUSD             | AVX512         | AVX512EVEX     | AVX512_VNNI_512 | 50           | 0b11       | rrr        | EVV 0x50 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():rw:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu32
 | VPDPBUSD             | AVX512         | AVX512EVEX     | AVX512_VNNI_512 | 50           | mm         | rrr        | EVV 0x50 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPDPBUSDS            | AVX512         | AVX512EVEX     | AVX512_VNNI_128 | 51           | 0b11       | rrr        | EVV 0x51 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():rw:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u32
 | VPDPBUSDS            | AVX512         | AVX512EVEX     | AVX512_VNNI_128 | 51           | mm         | rrr        | EVV 0x51 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPDPBUSDS            | AVX512         | AVX512EVEX     | AVX512_VNNI_256 | 51           | 0b11       | rrr        | EVV 0x51 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():rw:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u32
 | VPDPBUSDS            | AVX512         | AVX512EVEX     | AVX512_VNNI_256 | 51           | mm         | rrr        | EVV 0x51 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPDPBUSDS            | AVX512         | AVX512EVEX     | AVX512_VNNI_512 | 51           | 0b11       | rrr        | EVV 0x51 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():rw:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu32
 | VPDPBUSDS            | AVX512         | AVX512EVEX     | AVX512_VNNI_512 | 51           | mm         | rrr        | EVV 0x51 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPDPWSSD             | AVX512         | AVX512EVEX     | AVX512_VNNI_128 | 52           | 0b11       | rrr        | EVV 0x52 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():rw:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 REG3=XMM_B3():r:dq:u32
 | VPDPWSSD             | AVX512         | AVX512EVEX     | AVX512_VNNI_128 | 52           | mm         | rrr        | EVV 0x52 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPDPWSSD             | AVX512         | AVX512EVEX     | AVX512_VNNI_256 | 52           | 0b11       | rrr        | EVV 0x52 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():rw:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 REG3=YMM_B3():r:qq:u32
 | VPDPWSSD             | AVX512         | AVX512EVEX     | AVX512_VNNI_256 | 52           | mm         | rrr        | EVV 0x52 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPDPWSSD             | AVX512         | AVX512EVEX     | AVX512_VNNI_512 | 52           | 0b11       | rrr        | EVV 0x52 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():rw:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 REG3=ZMM_B3():r:zu32
 | VPDPWSSD             | AVX512         | AVX512EVEX     | AVX512_VNNI_512 | 52           | mm         | rrr        | EVV 0x52 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPDPWSSDS            | AVX512         | AVX512EVEX     | AVX512_VNNI_128 | 53           | 0b11       | rrr        | EVV 0x53 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():rw:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 REG3=XMM_B3():r:dq:u32
 | VPDPWSSDS            | AVX512         | AVX512EVEX     | AVX512_VNNI_128 | 53           | mm         | rrr        | EVV 0x53 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPDPWSSDS            | AVX512         | AVX512EVEX     | AVX512_VNNI_256 | 53           | 0b11       | rrr        | EVV 0x53 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():rw:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 REG3=YMM_B3():r:qq:u32
 | VPDPWSSDS            | AVX512         | AVX512EVEX     | AVX512_VNNI_256 | 53           | mm         | rrr        | EVV 0x53 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPDPWSSDS            | AVX512         | AVX512EVEX     | AVX512_VNNI_512 | 53           | 0b11       | rrr        | EVV 0x53 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():rw:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 REG3=ZMM_B3():r:zu32
 | VPDPWSSDS            | AVX512         | AVX512EVEX     | AVX512_VNNI_512 | 53           | mm         | rrr        | EVV 0x53 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPERM2F128           | AVX            | AVX            |                | 06           | mm         | rrr        | VV1 0x06 VL256 V66 V0F3A norexw_prefix  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()              | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64 IMM0:r:b
 | VPERM2F128           | AVX            | AVX            |                | 06           | 0b11       | rrr        | VV1 0x06 VL256 V66 V0F3A norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                      | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64 IMM0:r:b
 | VPERM2I128           | AVX2           | AVX2           |                | 46           | mm         | rrr        | VV1 0x46  VL256 V66 V0F3A W0  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                        | REG0=YMM_R():w:qq:u128  REG1=YMM_N():r:qq:u128  MEM0:r:qq:u128         IMM0:r:b
 | VPERM2I128           | AVX2           | AVX2           |                | 46           | 0b11       | rrr        | VV1 0x46  VL256 V66 V0F3A W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                | REG0=YMM_R():w:qq:u128  REG1=YMM_N():r:qq:u128  REG2=YMM_B():r:qq:u128 IMM0:r:b
 | VPERMB               | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_128 | 8d           | 0b11       | rrr        | EVV 0x8D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
 | VPERMB               | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_128 | 8d           | mm         | rrr        | EVV 0x8D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0    ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
 | VPERMB               | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_256 | 8d           | 0b11       | rrr        | EVV 0x8D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
 | VPERMB               | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_256 | 8d           | mm         | rrr        | EVV 0x8D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0    ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
 | VPERMB               | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_512 | 8d           | 0b11       | rrr        | EVV 0x8D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
 | VPERMB               | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_512 | 8d           | mm         | rrr        | EVV 0x8D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0    ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
 | VPERMD               | AVX2           | AVX2           |                | 36           | mm         | rrr        | VV1 0x36  VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                 | REG0=YMM_R():w:qq:u32  REG1=YMM_N():r:qq:u32  MEM0:r:qq:u32
 | VPERMD               | AVX2           | AVX2           |                | 36           | 0b11       | rrr        | VV1 0x36  VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=YMM_R():w:qq:u32  REG1=YMM_N():r:qq:u32  REG2=YMM_B():r:qq:u32
 | VPERMD               | AVX512         | AVX512EVEX     | AVX512F_256    | 36           | 0b11       | rrr        | EVV 0x36 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
 | VPERMD               | AVX512         | AVX512EVEX     | AVX512F_256    | 36           | mm         | rrr        | EVV 0x36 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPERMD               | AVX512         | AVX512EVEX     | AVX512F_512    | 36           | 0b11       | rrr        | EVV 0x36 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
 | VPERMD               | AVX512         | AVX512EVEX     | AVX512F_512    | 36           | mm         | rrr        | EVV 0x36 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPERMD               | KNC            | KNCE           |                | 36           | mm         | rrr        | KVV 0x36 V0F38 V66  W0 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() NOSWIZD()                            | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zd:TXT=NT
 | VPERMD               | KNC            | KNCE           |                | 36           | 0b11       | rrr        | KVV 0x36 V0F38 V66  W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0  SWIZ=0                                 | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VPERMF32X4           | KNC            | KNCE           |                | 07           | mm         | rrr        | KVV 0x07 V0F3A V66  REXW=0  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() NOSWIZD()        | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  MEM0:r:zd:TXT=NT IMM0:r:b
 | VPERMF32X4           | KNC            | KNCE           |                | 07           | 0b11       | rrr        | KVV 0x07 V0F3A V66  REXW=0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] UIMM8() NR=1 SWIZ=0             | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw REG2=ZMM_B3():r:zd  IMM0:r:b
 | VPERMF32X4           | KNC            | KNCE           |                | 07           | 0b11       | rrr        | KVV 0x07 V0F3A V66  REXW=0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] UIMM8() NR=0 SWIZ=0             | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw REG2=ZMM_B3():r:zd IMM0:r:b
 | VPERMI2B             | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_128 | 75           | 0b11       | rrr        | EVV 0x75 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():rw:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
 | VPERMI2B             | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_128 | 75           | mm         | rrr        | EVV 0x75 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0    ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():rw:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
 | VPERMI2B             | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_256 | 75           | 0b11       | rrr        | EVV 0x75 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():rw:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
 | VPERMI2B             | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_256 | 75           | mm         | rrr        | EVV 0x75 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0    ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():rw:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
 | VPERMI2B             | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_512 | 75           | 0b11       | rrr        | EVV 0x75 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():rw:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
 | VPERMI2B             | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_512 | 75           | mm         | rrr        | EVV 0x75 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0    ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():rw:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
 | VPERMI2D             | AVX512         | AVX512EVEX     | AVX512F_128    | 76           | 0b11       | rrr        | EVV 0x76 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():rw:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
 | VPERMI2D             | AVX512         | AVX512EVEX     | AVX512F_128    | 76           | mm         | rrr        | EVV 0x76 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPERMI2D             | AVX512         | AVX512EVEX     | AVX512F_256    | 76           | 0b11       | rrr        | EVV 0x76 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():rw:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
 | VPERMI2D             | AVX512         | AVX512EVEX     | AVX512F_256    | 76           | mm         | rrr        | EVV 0x76 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPERMI2D             | AVX512         | AVX512EVEX     | AVX512F_512    | 76           | 0b11       | rrr        | EVV 0x76 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():rw:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
 | VPERMI2D             | AVX512         | AVX512EVEX     | AVX512F_512    | 76           | mm         | rrr        | EVV 0x76 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPERMI2PD            | AVX512         | AVX512EVEX     | AVX512F_128    | 77           | 0b11       | rrr        | EVV 0x77 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VPERMI2PD            | AVX512         | AVX512EVEX     | AVX512F_128    | 77           | mm         | rrr        | EVV 0x77 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VPERMI2PD            | AVX512         | AVX512EVEX     | AVX512F_256    | 77           | 0b11       | rrr        | EVV 0x77 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
 | VPERMI2PD            | AVX512         | AVX512EVEX     | AVX512F_256    | 77           | mm         | rrr        | EVV 0x77 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VPERMI2PD            | AVX512         | AVX512EVEX     | AVX512F_512    | 77           | 0b11       | rrr        | EVV 0x77 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VPERMI2PD            | AVX512         | AVX512EVEX     | AVX512F_512    | 77           | mm         | rrr        | EVV 0x77 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VPERMI2PS            | AVX512         | AVX512EVEX     | AVX512F_128    | 77           | 0b11       | rrr        | EVV 0x77 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VPERMI2PS            | AVX512         | AVX512EVEX     | AVX512F_128    | 77           | mm         | rrr        | EVV 0x77 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VPERMI2PS            | AVX512         | AVX512EVEX     | AVX512F_256    | 77           | 0b11       | rrr        | EVV 0x77 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
 | VPERMI2PS            | AVX512         | AVX512EVEX     | AVX512F_256    | 77           | mm         | rrr        | EVV 0x77 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VPERMI2PS            | AVX512         | AVX512EVEX     | AVX512F_512    | 77           | 0b11       | rrr        | EVV 0x77 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VPERMI2PS            | AVX512         | AVX512EVEX     | AVX512F_512    | 77           | mm         | rrr        | EVV 0x77 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VPERMI2Q             | AVX512         | AVX512EVEX     | AVX512F_128    | 76           | 0b11       | rrr        | EVV 0x76 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():rw:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
 | VPERMI2Q             | AVX512         | AVX512EVEX     | AVX512F_128    | 76           | mm         | rrr        | EVV 0x76 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPERMI2Q             | AVX512         | AVX512EVEX     | AVX512F_256    | 76           | 0b11       | rrr        | EVV 0x76 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():rw:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
 | VPERMI2Q             | AVX512         | AVX512EVEX     | AVX512F_256    | 76           | mm         | rrr        | EVV 0x76 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPERMI2Q             | AVX512         | AVX512EVEX     | AVX512F_512    | 76           | 0b11       | rrr        | EVV 0x76 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():rw:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
 | VPERMI2Q             | AVX512         | AVX512EVEX     | AVX512F_512    | 76           | mm         | rrr        | EVV 0x76 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPERMI2W             | AVX512         | AVX512EVEX     | AVX512BW_128   | 75           | 0b11       | rrr        | EVV 0x75 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():rw:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
 | VPERMI2W             | AVX512         | AVX512EVEX     | AVX512BW_128   | 75           | mm         | rrr        | EVV 0x75 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():rw:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
 | VPERMI2W             | AVX512         | AVX512EVEX     | AVX512BW_256   | 75           | 0b11       | rrr        | EVV 0x75 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():rw:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
 | VPERMI2W             | AVX512         | AVX512EVEX     | AVX512BW_256   | 75           | mm         | rrr        | EVV 0x75 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():rw:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
 | VPERMI2W             | AVX512         | AVX512EVEX     | AVX512BW_512   | 75           | 0b11       | rrr        | EVV 0x75 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():rw:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
 | VPERMI2W             | AVX512         | AVX512EVEX     | AVX512BW_512   | 75           | mm         | rrr        | EVV 0x75 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():rw:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
 | VPERMIL2PD           | XOP            | XOP            | XOP            | 49           | mm         | rrr        | VV1 0x49 VL128 V66 V0F3A W0  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64 REG2=XMM_SE():r:dq:f64 IMM0:r:b
 | VPERMIL2PD           | XOP            | XOP            | XOP            | 49           | 0b11       | rrr        | VV1 0x49 VL128 V66 V0F3A W0  MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64 REG3=XMM_SE():r:dq:f64 IMM0:r:b
 | VPERMIL2PD           | XOP            | XOP            | XOP            | 49           | mm         | rrr        | VV1 0x49 VL256 V66 V0F3A W0   MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                      | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64 REG2=YMM_SE():r:qq:f64 IMM0:r:b
 | VPERMIL2PD           | XOP            | XOP            | XOP            | 49           | 0b11       | rrr        | VV1 0x49 VL256 V66 V0F3A W0   MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                             | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64 REG3=YMM_SE():r:qq:f64 IMM0:r:b
 | VPERMIL2PD           | XOP            | XOP            | XOP            | 49           | mm         | rrr        | VV1 0x49 VL128 V66 V0F3A W1  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_SE():r:dq:f64 MEM0:r:dq:f64  IMM0:r:b
 | VPERMIL2PD           | XOP            | XOP            | XOP            | 49           | 0b11       | rrr        | VV1 0x49 VL128 V66 V0F3A W1  MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_SE():r:dq:f64 REG3=XMM_B():r:dq:f64  IMM0:r:b
 | VPERMIL2PD           | XOP            | XOP            | XOP            | 49           | mm         | rrr        | VV1 0x49 VL256 V66 V0F3A W1   MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                      | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64  REG2=YMM_SE():r:qq:f64  MEM0:r:qq:f64 IMM0:r:b
 | VPERMIL2PD           | XOP            | XOP            | XOP            | 49           | 0b11       | rrr        | VV1 0x49 VL256 V66 V0F3A W1   MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                             | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_SE():r:qq:f64 REG3=YMM_B():r:qq:f64  IMM0:r:b
 | VPERMIL2PS           | XOP            | XOP            | XOP            | 48           | mm         | rrr        | VV1 0x48 VL128 V66 V0F3A W0  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32 REG2=XMM_SE():r:dq:f32 IMM0:r:b
 | VPERMIL2PS           | XOP            | XOP            | XOP            | 48           | 0b11       | rrr        | VV1 0x48 VL128 V66 V0F3A W0  MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32 REG3=XMM_SE():r:dq:f32 IMM0:r:b
 | VPERMIL2PS           | XOP            | XOP            | XOP            | 48           | mm         | rrr        | VV1 0x48 VL256 V66 V0F3A W0   MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                      | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32 REG2=YMM_SE():r:qq:f32 IMM0:r:b
 | VPERMIL2PS           | XOP            | XOP            | XOP            | 48           | 0b11       | rrr        | VV1 0x48 VL256 V66 V0F3A W0   MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                             | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32 REG3=YMM_SE():r:qq:f32 IMM0:r:b
 | VPERMIL2PS           | XOP            | XOP            | XOP            | 48           | mm         | rrr        | VV1 0x48 VL128 V66 V0F3A W1  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                       | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_SE():r:dq:f32 MEM0:r:dq:f32  IMM0:r:b
 | VPERMIL2PS           | XOP            | XOP            | XOP            | 48           | 0b11       | rrr        | VV1 0x48 VL128 V66 V0F3A W1  MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                              | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_SE():r:dq:f32 REG3=XMM_B():r:dq:f32  IMM0:r:b
 | VPERMIL2PS           | XOP            | XOP            | XOP            | 48           | mm         | rrr        | VV1 0x48 VL256 V66 V0F3A W1   MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                      | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32  REG2=YMM_SE():r:qq:f32  MEM0:r:qq:f32 IMM0:r:b
 | VPERMIL2PS           | XOP            | XOP            | XOP            | 48           | 0b11       | rrr        | VV1 0x48 VL256 V66 V0F3A W1   MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                             | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_SE():r:qq:f32 REG3=YMM_B():r:qq:f32  IMM0:r:b
 | VPERMILPD            | AVX            | AVX            |                | 0d           | mm         | rrr        | VV1 0x0D VL128 V66 V0F38 norexw_prefix  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                      | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:u64
 | VPERMILPD            | AVX            | AVX            |                | 0d           | 0b11       | rrr        | VV1 0x0D  VL128 V66 V0F38 norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                             | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:u64
 | VPERMILPD            | AVX            | AVX            |                | 0d           | mm         | rrr        | VV1 0x0D  VL256 V66 V0F38 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                      | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:u64
 | VPERMILPD            | AVX            | AVX            |                | 0d           | 0b11       | rrr        | VV1 0x0D  VL256 V66 V0F38 norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                             | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:u64
 | VPERMILPD            | AVX            | AVX            |                | 05           | mm         | rrr        | VV1 0x05  VL128 V66 V0F3A norexw_prefix NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()        | REG0=XMM_R():w:dq:f64 MEM0:r:dq:f64 IMM0:r:b
 | VPERMILPD            | AVX            | AVX            |                | 05           | 0b11       | rrr        | VV1 0x05  VL128 V66 V0F3A norexw_prefix NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()               | REG0=XMM_R():w:dq:f64 REG1=XMM_B():r:dq:f64 IMM0:r:b
 | VPERMILPD            | AVX            | AVX            |                | 05           | mm         | rrr        | VV1 0x05  VL256 V66 V0F3A norexw_prefix NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()        | REG0=YMM_R():w:qq:f64 MEM0:r:qq:f64 IMM0:r:b
 | VPERMILPD            | AVX            | AVX            |                | 05           | 0b11       | rrr        | VV1 0x05  VL256 V66 V0F3A norexw_prefix NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()               | REG0=YMM_R():w:qq:f64 REG1=YMM_B():r:qq:f64 IMM0:r:b
 | VPERMILPD            | AVX512         | AVX512EVEX     | AVX512F_128    | 05           | 0b11       | rrr        | EVV 0x05 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR UIMM8()                | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64 IMM0:r:b
 | VPERMILPD            | AVX512         | AVX512EVEX     | AVX512F_128    | 05           | mm         | rrr        | EVV 0x05 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
 | VPERMILPD            | AVX512         | AVX512EVEX     | AVX512F_128    | 0d           | 0b11       | rrr        | EVV 0x0D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VPERMILPD            | AVX512         | AVX512EVEX     | AVX512F_128    | 0d           | mm         | rrr        | EVV 0x0D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VPERMILPD            | AVX512         | AVX512EVEX     | AVX512F_256    | 05           | 0b11       | rrr        | EVV 0x05 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR UIMM8()                | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64 IMM0:r:b
 | VPERMILPD            | AVX512         | AVX512EVEX     | AVX512F_256    | 05           | mm         | rrr        | EVV 0x05 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
 | VPERMILPD            | AVX512         | AVX512EVEX     | AVX512F_256    | 0d           | 0b11       | rrr        | EVV 0x0D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
 | VPERMILPD            | AVX512         | AVX512EVEX     | AVX512F_256    | 0d           | mm         | rrr        | EVV 0x0D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VPERMILPD            | AVX512         | AVX512EVEX     | AVX512F_512    | 05           | 0b11       | rrr        | EVV 0x05 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR UIMM8()                | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64 IMM0:r:b
 | VPERMILPD            | AVX512         | AVX512EVEX     | AVX512F_512    | 05           | mm         | rrr        | EVV 0x05 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
 | VPERMILPD            | AVX512         | AVX512EVEX     | AVX512F_512    | 0d           | 0b11       | rrr        | EVV 0x0D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VPERMILPD            | AVX512         | AVX512EVEX     | AVX512F_512    | 0d           | mm         | rrr        | EVV 0x0D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VPERMILPS            | AVX            | AVX            |                | 0c           | mm         | rrr        | VV1 0x0C VL128 V66 V0F38 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                       | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:u32
 | VPERMILPS            | AVX            | AVX            |                | 0c           | 0b11       | rrr        | VV1 0x0C  VL128 V66 V0F38 norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                             | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:u32
 | VPERMILPS            | AVX            | AVX            |                | 0c           | mm         | rrr        | VV1 0x0C  VL256 V66 V0F38  norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                     | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:u32
 | VPERMILPS            | AVX            | AVX            |                | 0c           | 0b11       | rrr        | VV1 0x0C  VL256 V66 V0F38  norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                            | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:u32
 | VPERMILPS            | AVX            | AVX            |                | 04           | mm         | rrr        | VV1 0x04 VL128 V66 V0F3A norexw_prefix NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()         | REG0=XMM_R():w:dq:f32 MEM0:r:dq:f32 IMM0:r:b
 | VPERMILPS            | AVX            | AVX            |                | 04           | 0b11       | rrr        | VV1 0x04 VL128 V66 V0F3A norexw_prefix NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                | REG0=XMM_R():w:dq:f32 REG1=XMM_B():r:dq:f32 IMM0:r:b
 | VPERMILPS            | AVX            | AVX            |                | 04           | mm         | rrr        | VV1 0x04 VL256 V66 V0F3A norexw_prefix NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()         | REG0=YMM_R():w:qq:f32 MEM0:r:qq:f32 IMM0:r:b
 | VPERMILPS            | AVX            | AVX            |                | 04           | 0b11       | rrr        | VV1 0x04 VL256 V66 V0F3A norexw_prefix NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                | REG0=YMM_R():w:qq:f32 REG1=YMM_B():r:qq:f32 IMM0:r:b
 | VPERMILPS            | AVX512         | AVX512EVEX     | AVX512F_128    | 04           | 0b11       | rrr        | EVV 0x04 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR UIMM8()                | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32 IMM0:r:b
 | VPERMILPS            | AVX512         | AVX512EVEX     | AVX512F_128    | 04           | mm         | rrr        | EVV 0x04 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
 | VPERMILPS            | AVX512         | AVX512EVEX     | AVX512F_128    | 0c           | 0b11       | rrr        | EVV 0x0C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VPERMILPS            | AVX512         | AVX512EVEX     | AVX512F_128    | 0c           | mm         | rrr        | EVV 0x0C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VPERMILPS            | AVX512         | AVX512EVEX     | AVX512F_256    | 04           | 0b11       | rrr        | EVV 0x04 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR UIMM8()                | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32 IMM0:r:b
 | VPERMILPS            | AVX512         | AVX512EVEX     | AVX512F_256    | 04           | mm         | rrr        | EVV 0x04 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
 | VPERMILPS            | AVX512         | AVX512EVEX     | AVX512F_256    | 0c           | 0b11       | rrr        | EVV 0x0C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
 | VPERMILPS            | AVX512         | AVX512EVEX     | AVX512F_256    | 0c           | mm         | rrr        | EVV 0x0C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VPERMILPS            | AVX512         | AVX512EVEX     | AVX512F_512    | 04           | 0b11       | rrr        | EVV 0x04 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR UIMM8()                | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32 IMM0:r:b
 | VPERMILPS            | AVX512         | AVX512EVEX     | AVX512F_512    | 04           | mm         | rrr        | EVV 0x04 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
 | VPERMILPS            | AVX512         | AVX512EVEX     | AVX512F_512    | 0c           | 0b11       | rrr        | EVV 0x0C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VPERMILPS            | AVX512         | AVX512EVEX     | AVX512F_512    | 0c           | mm         | rrr        | EVV 0x0C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VPERMPD              | AVX2           | AVX2           |                | 01           | mm         | rrr        | VV1 0x01 VL256 V0F3A V66 W1 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                    | REG0=YMM_R():w:qq:f64 MEM0:r:qq:f64  IMM0:r:b
 | VPERMPD              | AVX2           | AVX2           |                | 01           | 0b11       | rrr        | VV1 0x01 VL256 V0F3A V66 W1 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                           | REG0=YMM_R():w:qq:f64 REG1=YMM_B():r:qq:f64 IMM0:r:b
 | VPERMPD              | AVX512         | AVX512EVEX     | AVX512F_256    | 01           | 0b11       | rrr        | EVV 0x01 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR UIMM8()                | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64 IMM0:r:b
 | VPERMPD              | AVX512         | AVX512EVEX     | AVX512F_256    | 01           | mm         | rrr        | EVV 0x01 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
 | VPERMPD              | AVX512         | AVX512EVEX     | AVX512F_256    | 16           | 0b11       | rrr        | EVV 0x16 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
 | VPERMPD              | AVX512         | AVX512EVEX     | AVX512F_256    | 16           | mm         | rrr        | EVV 0x16 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VPERMPD              | AVX512         | AVX512EVEX     | AVX512F_512    | 01           | 0b11       | rrr        | EVV 0x01 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR UIMM8()                | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64 IMM0:r:b
 | VPERMPD              | AVX512         | AVX512EVEX     | AVX512F_512    | 01           | mm         | rrr        | EVV 0x01 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
 | VPERMPD              | AVX512         | AVX512EVEX     | AVX512F_512    | 16           | 0b11       | rrr        | EVV 0x16 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VPERMPD              | AVX512         | AVX512EVEX     | AVX512F_512    | 16           | mm         | rrr        | EVV 0x16 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VPERMPS              | AVX2           | AVX2           |                | 16           | mm         | rrr        | VV1 0x16  VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                 | REG0=YMM_R():w:qq:f32  REG1=YMM_N():r:qq:f32  MEM0:r:qq:f32
 | VPERMPS              | AVX2           | AVX2           |                | 16           | 0b11       | rrr        | VV1 0x16  VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=YMM_R():w:qq:f32  REG1=YMM_N():r:qq:f32  REG2=YMM_B():r:qq:f32
 | VPERMPS              | AVX512         | AVX512EVEX     | AVX512F_256    | 16           | 0b11       | rrr        | EVV 0x16 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
 | VPERMPS              | AVX512         | AVX512EVEX     | AVX512F_256    | 16           | mm         | rrr        | EVV 0x16 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VPERMPS              | AVX512         | AVX512EVEX     | AVX512F_512    | 16           | 0b11       | rrr        | EVV 0x16 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VPERMPS              | AVX512         | AVX512EVEX     | AVX512F_512    | 16           | mm         | rrr        | EVV 0x16 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VPERMQ               | AVX2           | AVX2           |                | 00           | mm         | rrr        | VV1 0x00 VL256 V0F3A V66 W1 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                    | REG0=YMM_R():w:qq:u64 MEM0:r:qq:u64  IMM0:r:b
 | VPERMQ               | AVX2           | AVX2           |                | 00           | 0b11       | rrr        | VV1 0x00 VL256 V0F3A V66 W1 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                           | REG0=YMM_R():w:qq:u64 REG1=YMM_B():r:qq:u64 IMM0:r:b
 | VPERMQ               | AVX512         | AVX512EVEX     | AVX512F_256    | 00           | 0b11       | rrr        | EVV 0x00 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR UIMM8()                | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u64 IMM0:r:b
 | VPERMQ               | AVX512         | AVX512EVEX     | AVX512F_256    | 00           | mm         | rrr        | EVV 0x00 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VPERMQ               | AVX512         | AVX512EVEX     | AVX512F_256    | 36           | 0b11       | rrr        | EVV 0x36 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
 | VPERMQ               | AVX512         | AVX512EVEX     | AVX512F_256    | 36           | mm         | rrr        | EVV 0x36 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPERMQ               | AVX512         | AVX512EVEX     | AVX512F_512    | 00           | 0b11       | rrr        | EVV 0x00 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR UIMM8()                | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu64 IMM0:r:b
 | VPERMQ               | AVX512         | AVX512EVEX     | AVX512F_512    | 00           | mm         | rrr        | EVV 0x00 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VPERMQ               | AVX512         | AVX512EVEX     | AVX512F_512    | 36           | 0b11       | rrr        | EVV 0x36 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
 | VPERMQ               | AVX512         | AVX512EVEX     | AVX512F_512    | 36           | mm         | rrr        | EVV 0x36 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPERMT2B             | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_128 | 7d           | 0b11       | rrr        | EVV 0x7D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():rw:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
 | VPERMT2B             | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_128 | 7d           | mm         | rrr        | EVV 0x7D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0    ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():rw:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
 | VPERMT2B             | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_256 | 7d           | 0b11       | rrr        | EVV 0x7D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():rw:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
 | VPERMT2B             | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_256 | 7d           | mm         | rrr        | EVV 0x7D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0    ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():rw:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
 | VPERMT2B             | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_512 | 7d           | 0b11       | rrr        | EVV 0x7D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():rw:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
 | VPERMT2B             | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_512 | 7d           | mm         | rrr        | EVV 0x7D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0    ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():rw:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
 | VPERMT2D             | AVX512         | AVX512EVEX     | AVX512F_128    | 7e           | 0b11       | rrr        | EVV 0x7E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():rw:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
 | VPERMT2D             | AVX512         | AVX512EVEX     | AVX512F_128    | 7e           | mm         | rrr        | EVV 0x7E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPERMT2D             | AVX512         | AVX512EVEX     | AVX512F_256    | 7e           | 0b11       | rrr        | EVV 0x7E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():rw:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
 | VPERMT2D             | AVX512         | AVX512EVEX     | AVX512F_256    | 7e           | mm         | rrr        | EVV 0x7E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPERMT2D             | AVX512         | AVX512EVEX     | AVX512F_512    | 7e           | 0b11       | rrr        | EVV 0x7E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():rw:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
 | VPERMT2D             | AVX512         | AVX512EVEX     | AVX512F_512    | 7e           | mm         | rrr        | EVV 0x7E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPERMT2PD            | AVX512         | AVX512EVEX     | AVX512F_128    | 7f           | 0b11       | rrr        | EVV 0x7F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VPERMT2PD            | AVX512         | AVX512EVEX     | AVX512F_128    | 7f           | mm         | rrr        | EVV 0x7F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VPERMT2PD            | AVX512         | AVX512EVEX     | AVX512F_256    | 7f           | 0b11       | rrr        | EVV 0x7F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
 | VPERMT2PD            | AVX512         | AVX512EVEX     | AVX512F_256    | 7f           | mm         | rrr        | EVV 0x7F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VPERMT2PD            | AVX512         | AVX512EVEX     | AVX512F_512    | 7f           | 0b11       | rrr        | EVV 0x7F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VPERMT2PD            | AVX512         | AVX512EVEX     | AVX512F_512    | 7f           | mm         | rrr        | EVV 0x7F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VPERMT2PS            | AVX512         | AVX512EVEX     | AVX512F_128    | 7f           | 0b11       | rrr        | EVV 0x7F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VPERMT2PS            | AVX512         | AVX512EVEX     | AVX512F_128    | 7f           | mm         | rrr        | EVV 0x7F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VPERMT2PS            | AVX512         | AVX512EVEX     | AVX512F_256    | 7f           | 0b11       | rrr        | EVV 0x7F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
 | VPERMT2PS            | AVX512         | AVX512EVEX     | AVX512F_256    | 7f           | mm         | rrr        | EVV 0x7F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VPERMT2PS            | AVX512         | AVX512EVEX     | AVX512F_512    | 7f           | 0b11       | rrr        | EVV 0x7F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VPERMT2PS            | AVX512         | AVX512EVEX     | AVX512F_512    | 7f           | mm         | rrr        | EVV 0x7F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VPERMT2Q             | AVX512         | AVX512EVEX     | AVX512F_128    | 7e           | 0b11       | rrr        | EVV 0x7E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():rw:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
 | VPERMT2Q             | AVX512         | AVX512EVEX     | AVX512F_128    | 7e           | mm         | rrr        | EVV 0x7E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPERMT2Q             | AVX512         | AVX512EVEX     | AVX512F_256    | 7e           | 0b11       | rrr        | EVV 0x7E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():rw:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
 | VPERMT2Q             | AVX512         | AVX512EVEX     | AVX512F_256    | 7e           | mm         | rrr        | EVV 0x7E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPERMT2Q             | AVX512         | AVX512EVEX     | AVX512F_512    | 7e           | 0b11       | rrr        | EVV 0x7E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():rw:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
 | VPERMT2Q             | AVX512         | AVX512EVEX     | AVX512F_512    | 7e           | mm         | rrr        | EVV 0x7E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPERMT2W             | AVX512         | AVX512EVEX     | AVX512BW_128   | 7d           | 0b11       | rrr        | EVV 0x7D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():rw:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
 | VPERMT2W             | AVX512         | AVX512EVEX     | AVX512BW_128   | 7d           | mm         | rrr        | EVV 0x7D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():rw:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
 | VPERMT2W             | AVX512         | AVX512EVEX     | AVX512BW_256   | 7d           | 0b11       | rrr        | EVV 0x7D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():rw:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
 | VPERMT2W             | AVX512         | AVX512EVEX     | AVX512BW_256   | 7d           | mm         | rrr        | EVV 0x7D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():rw:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
 | VPERMT2W             | AVX512         | AVX512EVEX     | AVX512BW_512   | 7d           | 0b11       | rrr        | EVV 0x7D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():rw:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
 | VPERMT2W             | AVX512         | AVX512EVEX     | AVX512BW_512   | 7d           | mm         | rrr        | EVV 0x7D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():rw:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
 | VPERMW               | AVX512         | AVX512EVEX     | AVX512BW_128   | 8d           | 0b11       | rrr        | EVV 0x8D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
 | VPERMW               | AVX512         | AVX512EVEX     | AVX512BW_128   | 8d           | mm         | rrr        | EVV 0x8D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
 | VPERMW               | AVX512         | AVX512EVEX     | AVX512BW_256   | 8d           | 0b11       | rrr        | EVV 0x8D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
 | VPERMW               | AVX512         | AVX512EVEX     | AVX512BW_256   | 8d           | mm         | rrr        | EVV 0x8D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
 | VPERMW               | AVX512         | AVX512EVEX     | AVX512BW_512   | 8d           | 0b11       | rrr        | EVV 0x8D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
 | VPERMW               | AVX512         | AVX512EVEX     | AVX512BW_512   | 8d           | mm         | rrr        | EVV 0x8D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
 | VPEXPANDB            | EXPAND         | AVX512EVEX     | AVX512_VBMI2_128 | 62           | mm         | rrr        | EVV 0x62 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_8_BITS() NELEM_GSCAT() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u8
 | VPEXPANDB            | EXPAND         | AVX512EVEX     | AVX512_VBMI2_128 | 62           | 0b11       | rrr        | EVV 0x62 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u8
 | VPEXPANDB            | EXPAND         | AVX512EVEX     | AVX512_VBMI2_256 | 62           | mm         | rrr        | EVV 0x62 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_8_BITS() NELEM_GSCAT() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u8
 | VPEXPANDB            | EXPAND         | AVX512EVEX     | AVX512_VBMI2_256 | 62           | 0b11       | rrr        | EVV 0x62 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u8
 | VPEXPANDB            | EXPAND         | AVX512EVEX     | AVX512_VBMI2_512 | 62           | mm         | rrr        | EVV 0x62 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_8_BITS() NELEM_GSCAT() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:u8
 | VPEXPANDB            | EXPAND         | AVX512EVEX     | AVX512_VBMI2_512 | 62           | 0b11       | rrr        | EVV 0x62 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu8
 | VPEXPANDD            | EXPAND         | AVX512EVEX     | AVX512F_128    | 89           | mm         | rrr        | EVV 0x89 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_GSCAT() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u32
 | VPEXPANDD            | EXPAND         | AVX512EVEX     | AVX512F_128    | 89           | 0b11       | rrr        | EVV 0x89 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32
 | VPEXPANDD            | EXPAND         | AVX512EVEX     | AVX512F_256    | 89           | mm         | rrr        | EVV 0x89 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_GSCAT() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u32
 | VPEXPANDD            | EXPAND         | AVX512EVEX     | AVX512F_256    | 89           | 0b11       | rrr        | EVV 0x89 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u32
 | VPEXPANDD            | EXPAND         | AVX512EVEX     | AVX512F_512    | 89           | mm         | rrr        | EVV 0x89 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_GSCAT() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:u32
 | VPEXPANDD            | EXPAND         | AVX512EVEX     | AVX512F_512    | 89           | 0b11       | rrr        | EVV 0x89 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu32
 | VPEXPANDQ            | EXPAND         | AVX512EVEX     | AVX512F_128    | 89           | mm         | rrr        | EVV 0x89 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_GSCAT() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u64
 | VPEXPANDQ            | EXPAND         | AVX512EVEX     | AVX512F_128    | 89           | 0b11       | rrr        | EVV 0x89 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                        | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u64
 | VPEXPANDQ            | EXPAND         | AVX512EVEX     | AVX512F_256    | 89           | mm         | rrr        | EVV 0x89 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_GSCAT() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u64
 | VPEXPANDQ            | EXPAND         | AVX512EVEX     | AVX512F_256    | 89           | 0b11       | rrr        | EVV 0x89 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                        | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u64
 | VPEXPANDQ            | EXPAND         | AVX512EVEX     | AVX512F_512    | 89           | mm         | rrr        | EVV 0x89 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_GSCAT() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:u64
 | VPEXPANDQ            | EXPAND         | AVX512EVEX     | AVX512F_512    | 89           | 0b11       | rrr        | EVV 0x89 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                        | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu64
 | VPEXPANDW            | EXPAND         | AVX512EVEX     | AVX512_VBMI2_128 | 62           | mm         | rrr        | EVV 0x62 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ESIZE_16_BITS() NELEM_GSCAT() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u16
 | VPEXPANDW            | EXPAND         | AVX512EVEX     | AVX512_VBMI2_128 | 62           | 0b11       | rrr        | EVV 0x62 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                        | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u16
 | VPEXPANDW            | EXPAND         | AVX512EVEX     | AVX512_VBMI2_256 | 62           | mm         | rrr        | EVV 0x62 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ESIZE_16_BITS() NELEM_GSCAT() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u16
 | VPEXPANDW            | EXPAND         | AVX512EVEX     | AVX512_VBMI2_256 | 62           | 0b11       | rrr        | EVV 0x62 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                        | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u16
 | VPEXPANDW            | EXPAND         | AVX512EVEX     | AVX512_VBMI2_512 | 62           | mm         | rrr        | EVV 0x62 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ESIZE_16_BITS() NELEM_GSCAT() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:u16
 | VPEXPANDW            | EXPAND         | AVX512EVEX     | AVX512_VBMI2_512 | 62           | 0b11       | rrr        | EVV 0x62 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                        | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu16
 | VPEXTRB              | AVX            | AVX            |                | 14           | mm         | rrr        | VV1 0x14  VL128 V66 V0F3A NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                      | MEM0:w:b           REG0=XMM_R():r:dq:u8 IMM0:r:b
 | VPEXTRB              | AVX            | AVX            |                | 14           | 0b11       | rrr        | VV1 0x14  VL128 V66 V0F3A NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                             | REG0=GPR32_B():w:d REG1=XMM_R():r:dq:u8 IMM0:r:b
 | VPEXTRB              | AVX512         | AVX512EVEX     | AVX512BW_128N  | 14           | 0b11       | rrr        | EVV 0x14 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR  ZEROING=0 MASK=0 UIMM8() | REG0=GPR32_B():w:d:u8 REG1=XMM_R3():r:dq:u8 IMM0:r:b
 | VPEXTRB              | AVX512         | AVX512EVEX     | AVX512BW_128N  | 14           | mm         | rrr        | EVV 0x14 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128    NOEVSR  ZEROING=0 MASK=0 UIMM8()  ESIZE_8_BITS() NELEM_GPR_WRITER_STORE_BYTE() | MEM0:w:b:u8 REG0=XMM_R3():r:dq:u8 IMM0:r:b
 | VPEXTRD              | AVX            | AVX            |                | 16           | mm         | rrr        | VV1 0x16 VL128 V66 V0F3A mode64 norexw_prefix NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()  | MEM0:w:d REG0=XMM_R():r:dq:u32 IMM0:r:b
 | VPEXTRD              | AVX            | AVX            |                | 16           | 0b11       | rrr        | VV1 0x16 VL128 V66 V0F3A mode64 norexw_prefix NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()         | REG0=GPR32_B():w:d REG1=XMM_R():r:dq:u32 IMM0:r:b
 | VPEXTRD              | AVX            | AVX            |                | 16           | mm         | rrr        | VV1 0x16 VL128 V66 V0F3A not64  NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                | MEM0:w:d REG0=XMM_R():r:dq:u32 IMM0:r:b
 | VPEXTRD              | AVX            | AVX            |                | 16           | 0b11       | rrr        | VV1 0x16 VL128 V66 V0F3A not64  NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                       | REG0=GPR32_B():w:d REG1=XMM_R():r:dq:u32 IMM0:r:b
 | VPEXTRD              | AVX512         | AVX512EVEX     | AVX512DQ_128N  | 16           | 0b11       | rrr        | EVV 0x16 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  not64  NOEVSR  ZEROING=0 MASK=0 UIMM8() | REG0=GPR32_B():w:d:u32 REG1=XMM_R3():r:dq:u32 IMM0:r:b
 | VPEXTRD              | AVX512         | AVX512EVEX     | AVX512DQ_128N  | 16           | 0b11       | rrr        | EVV 0x16 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  mode64 W0  NOEVSR  ZEROING=0 MASK=0 UIMM8() | REG0=GPR32_B():w:d:u32 REG1=XMM_R3():r:dq:u32 IMM0:r:b
 | VPEXTRD              | AVX512         | AVX512EVEX     | AVX512DQ_128N  | 16           | mm         | rrr        | EVV 0x16 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  not64  NOEVSR  ZEROING=0 MASK=0 UIMM8()  ESIZE_32_BITS() NELEM_GPR_WRITER_STORE() | MEM0:w:d:u32 REG0=XMM_R3():r:dq:u32 IMM0:r:b
 | VPEXTRD              | AVX512         | AVX512EVEX     | AVX512DQ_128N  | 16           | mm         | rrr        | EVV 0x16 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  mode64 W0  NOEVSR  ZEROING=0 MASK=0 UIMM8()  ESIZE_32_BITS() NELEM_GPR_WRITER_STORE() | MEM0:w:d:u32 REG0=XMM_R3():r:dq:u32 IMM0:r:b
 | VPEXTRQ              | AVX            | AVX            |                | 16           | mm         | rrr        | VV1 0x16  VL128 V66 V0F3A mode64 rexw_prefix  NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()  | MEM0:w:q              REG0=XMM_R():r:dq:u64 IMM0:r:b
 | VPEXTRQ              | AVX            | AVX            |                | 16           | 0b11       | rrr        | VV1 0x16  VL128 V66 V0F3A mode64 rexw_prefix NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()          | REG0=GPR64_B():w:q    REG1=XMM_R():r:dq:u64 IMM0:r:b
 | VPEXTRQ              | AVX512         | AVX512EVEX     | AVX512DQ_128N  | 16           | 0b11       | rrr        | EVV 0x16 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  mode64  NOEVSR  ZEROING=0 MASK=0 UIMM8() | REG0=GPR64_B():w:q:u64 REG1=XMM_R3():r:dq:u64 IMM0:r:b
 | VPEXTRQ              | AVX512         | AVX512EVEX     | AVX512DQ_128N  | 16           | mm         | rrr        | EVV 0x16 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  mode64  NOEVSR  ZEROING=0 MASK=0 UIMM8()  ESIZE_64_BITS() NELEM_GPR_WRITER_STORE() | MEM0:w:q:u64 REG0=XMM_R3():r:dq:u64 IMM0:r:b
 | VPEXTRW              | AVX            | AVX            |                | 15           | mm         | rrr        | VV1 0x15  VL128 V66 V0F3A NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                      | MEM0:w:w           REG0=XMM_R():r:dq:u16 IMM0:r:b
 | VPEXTRW              | AVX            | AVX            |                | 15           | 0b11       | rrr        | VV1 0x15  VL128 V66 V0F3A NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                             | REG0=GPR32_B():w:d REG1=XMM_R():r:dq:u16 IMM0:r:b
 | VPEXTRW              | AVX            | AVX            |                | c5           | 0b11       | rrr        | VV1 0xC5  VL128 V66 V0F  NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                              | REG0=GPR32_R():w:d    REG1=XMM_B():r:dq:u16 IMM0:r:b
 | VPEXTRW              | AVX512         | AVX512EVEX     | AVX512BW_128N  | 15           | 0b11       | rrr        | EVV 0x15 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR  ZEROING=0 MASK=0 UIMM8() | REG0=GPR32_B():w:d:u16 REG1=XMM_R3():r:dq:u16 IMM0:r:b
 | VPEXTRW              | AVX512         | AVX512EVEX     | AVX512BW_128N  | 15           | mm         | rrr        | EVV 0x15 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128    NOEVSR  ZEROING=0 MASK=0 UIMM8()  ESIZE_16_BITS() NELEM_GPR_WRITER_STORE_WORD() | MEM0:w:wrd:u16 REG0=XMM_R3():r:dq:u16 IMM0:r:b
 | VPEXTRW_C5           | AVX512         | AVX512EVEX     | AVX512BW_128N  | c5           | 0b11       | rrr        | EVV 0xC5 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR  ZEROING=0 MASK=0 UIMM8() not64 | REG0=GPR32_R():w:d:u16 REG1=XMM_B3():r:dq:u16 IMM0:r:b
 | VPEXTRW_C5           | AVX512         | AVX512EVEX     | AVX512BW_128N  | c5           | 0b11       | rrr        | EVV 0xC5 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR  ZEROING=0 MASK=0 UIMM8() mode64 EVEXRR_ONE | REG0=GPR32_R():w:d:u16 REG1=XMM_B3():r:dq:u16 IMM0:r:b
 | VPGATHERDD           | AVX2GATHER     | AVX2GATHER     |                | 90           | mm         | rrr        | VV1 0x90   VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] RM=4 VMODRM_YMM() eanot16              | REG0=YMM_R():crw:qq:u32   MEM0:r:d:u32 REG1=YMM_N():rw:qq:i32
 | VPGATHERDD           | AVX2GATHER     | AVX2GATHER     |                | 90           | mm         | rrr        | VV1 0x90   VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] RM=4 VMODRM_XMM() eanot16              | REG0=XMM_R():crw:dq:u32   MEM0:r:d:u32 REG1=XMM_N():rw:dq:i32
 | VPGATHERDD           | GATHER         | AVX512EVEX     | AVX512F_128    | 90           | mm         | rrr        | EVV 0x90 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL128  W0 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | REG0=XMM_R3():w:dq:u32 REG1=MASKNOT0():rw:mskw MEM0:r:d:u32
 | VPGATHERDD           | GATHER         | AVX512EVEX     | AVX512F_256    | 90           | mm         | rrr        | EVV 0x90 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL256  W0 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | REG0=YMM_R3():w:qq:u32 REG1=MASKNOT0():rw:mskw MEM0:r:d:u32
 | VPGATHERDD           | GATHER         | AVX512EVEX     | AVX512F_512    | 90           | mm         | rrr        | EVV 0x90 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL512  W0 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | REG0=ZMM_R3():w:zu32 REG1=MASKNOT0():rw:mskw MEM0:r:d:u32
 | VPGATHERDD           | KNC            | KNCE           | KNCE           | 90           | mm         | rrr        | KVV 0x90 V66 V0F38  W0 NO_SPARSE_EVSR MOD[mm] MOD!=3 REG[rrr] RM[0b100] KNC_VMODRM() UPCONVERT_INT32_LOAD() | REG0=ZMM_R3():rw:zd REG1=MASK1():rw:mskw  MEM0:r:zv:TXT=NT:TXT=CONVERT NELEM=1:SUPP
 | VPGATHERDQ           | AVX2GATHER     | AVX2GATHER     |                | 90           | mm         | rrr        | VV1 0x90   VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] RM=4 VMODRM_XMM() eanot16              | REG0=YMM_R():crw:qq:u64   MEM0:r:q:u64 REG1=YMM_N():rw:qq:i64
 | VPGATHERDQ           | AVX2GATHER     | AVX2GATHER     |                | 90           | mm         | rrr        | VV1 0x90   VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] RM=4 VMODRM_XMM() eanot16              | REG0=XMM_R():crw:dq:u64   MEM0:r:q:u64 REG1=XMM_N():rw:dq:i64
 | VPGATHERDQ           | GATHER         | AVX512EVEX     | AVX512F_128    | 90           | mm         | rrr        | EVV 0x90 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL128  W1 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | REG0=XMM_R3():w:dq:u64 REG1=MASKNOT0():rw:mskw MEM0:r:q:u64
 | VPGATHERDQ           | GATHER         | AVX512EVEX     | AVX512F_256    | 90           | mm         | rrr        | EVV 0x90 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL256  W1 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | REG0=YMM_R3():w:qq:u64 REG1=MASKNOT0():rw:mskw MEM0:r:q:u64
 | VPGATHERDQ           | GATHER         | AVX512EVEX     | AVX512F_512    | 90           | mm         | rrr        | EVV 0x90 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL512  W1 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | REG0=ZMM_R3():w:zu64 REG1=MASKNOT0():rw:mskw MEM0:r:q:u64
 | VPGATHERDQ           | KNC            | KNCE           | KNCE           | 90           | mm         | rrr        | KVV 0x90 V66 V0F38  W1 NO_SPARSE_EVSR MOD[mm] MOD!=3 REG[rrr] RM[0b100] KNC_VMODRM() UPCONVERT_INT64_LOAD() | REG0=ZMM_R3():rw:zq REG1=MASK1():rw:mskw  MEM0:r:zv:TXT=NT:TXT=CONVERT NELEM=1:SUPP
 | VPGATHERQD           | AVX2GATHER     | AVX2GATHER     |                | 91           | mm         | rrr        | VV1 0x91   VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] RM=4 VMODRM_YMM() eanot16              | REG0=XMM_R():crw:dq:u32   MEM0:r:d:u32 REG1=XMM_N():rw:dq:i32
 | VPGATHERQD           | AVX2GATHER     | AVX2GATHER     |                | 91           | mm         | rrr        | VV1 0x91   VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] RM=4 VMODRM_XMM() eanot16              | REG0=XMM_R():crw:q:u32   MEM0:r:d:u32 REG1=XMM_N():rw:q:i32
 | VPGATHERQD           | GATHER         | AVX512EVEX     | AVX512F_128    | 91           | mm         | rrr        | EVV 0x91 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL128  W0 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | REG0=XMM_R3():w:dq:u32 REG1=MASKNOT0():rw:mskw MEM0:r:d:u32
 | VPGATHERQD           | GATHER         | AVX512EVEX     | AVX512F_256    | 91           | mm         | rrr        | EVV 0x91 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL256  W0 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | REG0=XMM_R3():w:dq:u32 REG1=MASKNOT0():rw:mskw MEM0:r:d:u32
 | VPGATHERQD           | GATHER         | AVX512EVEX     | AVX512F_512    | 91           | mm         | rrr        | EVV 0x91 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL512  W0 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | REG0=YMM_R3():w:qq:u32 REG1=MASKNOT0():rw:mskw MEM0:r:d:u32
 | VPGATHERQQ           | AVX2GATHER     | AVX2GATHER     |                | 91           | mm         | rrr        | VV1 0x91   VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] RM=4 VMODRM_YMM() eanot16              | REG0=YMM_R():crw:qq:u64   MEM0:r:q:u64 REG1=YMM_N():rw:qq:i64
 | VPGATHERQQ           | AVX2GATHER     | AVX2GATHER     |                | 91           | mm         | rrr        | VV1 0x91   VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] RM=4 VMODRM_XMM() eanot16              | REG0=XMM_R():crw:dq:u64   MEM0:r:q:u64 REG1=XMM_N():rw:dq:i64
 | VPGATHERQQ           | GATHER         | AVX512EVEX     | AVX512F_128    | 91           | mm         | rrr        | EVV 0x91 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL128  W1 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | REG0=XMM_R3():w:dq:u64 REG1=MASKNOT0():rw:mskw MEM0:r:q:u64
 | VPGATHERQQ           | GATHER         | AVX512EVEX     | AVX512F_256    | 91           | mm         | rrr        | EVV 0x91 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL256  W1 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | REG0=YMM_R3():w:qq:u64 REG1=MASKNOT0():rw:mskw MEM0:r:q:u64
 | VPGATHERQQ           | GATHER         | AVX512EVEX     | AVX512F_512    | 91           | mm         | rrr        | EVV 0x91 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL512  W1 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | REG0=ZMM_R3():w:zu64 REG1=MASKNOT0():rw:mskw MEM0:r:q:u64
 | VPHADDBD             | XOP            | XOP            | XOP            | c2           | mm         | rrr        | XOPV 0xC2 VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                           | REG0=XMM_R():w:dq:i32 MEM0:r:dq:i8
 | VPHADDBD             | XOP            | XOP            | XOP            | c2           | 0b11       | rrr        | XOPV 0xC2 VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                  | REG0=XMM_R():w:dq:i32 REG1=XMM_B():r:dq:i8
 | VPHADDBQ             | XOP            | XOP            | XOP            | c3           | mm         | rrr        | XOPV 0xC3 VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                           | REG0=XMM_R():w:dq:i64 MEM0:r:dq:i8
 | VPHADDBQ             | XOP            | XOP            | XOP            | c3           | 0b11       | rrr        | XOPV 0xC3 VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                  | REG0=XMM_R():w:dq:i64 REG1=XMM_B():r:dq:i8
 | VPHADDBW             | XOP            | XOP            | XOP            | c1           | mm         | rrr        | XOPV 0xC1 VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                           | REG0=XMM_R():w:dq:i8 MEM0:r:dq:i8
 | VPHADDBW             | XOP            | XOP            | XOP            | c1           | 0b11       | rrr        | XOPV 0xC1 VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                  | REG0=XMM_R():w:dq:i8 REG1=XMM_B():r:dq:i8
 | VPHADDD              | AVX            | AVX            |                | 02           | mm         | rrr        | VV1 0x02  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32
 | VPHADDD              | AVX            | AVX            |                | 02           | 0b11       | rrr        | VV1 0x02  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32
 | VPHADDD              | AVX2           | AVX2           |                | 02           | mm         | rrr        | VV1 0x02  VL256 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=YMM_R():w:qq:i32 REG1=YMM_N():r:qq:i32 MEM0:r:qq:i32
 | VPHADDD              | AVX2           | AVX2           |                | 02           | 0b11       | rrr        | VV1 0x02  VL256 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=YMM_R():w:qq:i32 REG1=YMM_N():r:qq:i32 REG2=YMM_B():r:qq:i32
 | VPHADDDQ             | XOP            | XOP            | XOP            | cb           | mm         | rrr        | XOPV 0xCB VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                           | REG0=XMM_R():w:dq:i64 MEM0:r:dq:i32
 | VPHADDDQ             | XOP            | XOP            | XOP            | cb           | 0b11       | rrr        | XOPV 0xCB VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                  | REG0=XMM_R():w:dq:i64 REG1=XMM_B():r:dq:i32
 | VPHADDSW             | AVX            | AVX            |                | 03           | mm         | rrr        | VV1 0x03  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
 | VPHADDSW             | AVX            | AVX            |                | 03           | 0b11       | rrr        | VV1 0x03  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
 | VPHADDSW             | AVX2           | AVX2           |                | 03           | mm         | rrr        | VV1 0x03  VL256 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=YMM_R():w:qq:i16 REG1=YMM_N():r:qq:i16 MEM0:r:qq:i16
 | VPHADDSW             | AVX2           | AVX2           |                | 03           | 0b11       | rrr        | VV1 0x03  VL256 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=YMM_R():w:qq:i16 REG1=YMM_N():r:qq:i16 REG2=YMM_B():r:qq:i16
 | VPHADDUBD            | XOP            | XOP            | XOP            | d2           | mm         | rrr        | XOPV 0xD2 VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                           | REG0=XMM_R():w:dq:u32 MEM0:r:dq:u8
 | VPHADDUBD            | XOP            | XOP            | XOP            | d2           | 0b11       | rrr        | XOPV 0xD2 VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                  | REG0=XMM_R():w:dq:u32 REG1=XMM_B():r:dq:u8
 | VPHADDUBQ            | XOP            | XOP            | XOP            | d3           | mm         | rrr        | XOPV 0xD3 VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                           | REG0=XMM_R():w:dq:u64 MEM0:r:dq:u8
 | VPHADDUBQ            | XOP            | XOP            | XOP            | d3           | 0b11       | rrr        | XOPV 0xD3 VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                  | REG0=XMM_R():w:dq:u64 REG1=XMM_B():r:dq:u8
 | VPHADDUBW            | XOP            | XOP            | XOP            | d1           | mm         | rrr        | XOPV 0xD1 VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                           | REG0=XMM_R():w:dq:u16 MEM0:r:dq:u8
 | VPHADDUBW            | XOP            | XOP            | XOP            | d1           | 0b11       | rrr        | XOPV 0xD1 VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                  | REG0=XMM_R():w:dq:u16 REG1=XMM_B():r:dq:u8
 | VPHADDUDQ            | XOP            | XOP            | XOP            | db           | mm         | rrr        | XOPV 0xDB VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                           | REG0=XMM_R():w:dq:u64 MEM0:r:dq:u32
 | VPHADDUDQ            | XOP            | XOP            | XOP            | db           | 0b11       | rrr        | XOPV 0xDB VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                  | REG0=XMM_R():w:dq:u64 REG1=XMM_B():r:dq:u32
 | VPHADDUWD            | XOP            | XOP            | XOP            | d6           | mm         | rrr        | XOPV 0xD6 VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                           | REG0=XMM_R():w:dq:u32 MEM0:r:dq:u16
 | VPHADDUWD            | XOP            | XOP            | XOP            | d6           | 0b11       | rrr        | XOPV 0xD6 VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                  | REG0=XMM_R():w:dq:u32 REG1=XMM_B():r:dq:u16
 | VPHADDUWQ            | XOP            | XOP            | XOP            | d7           | mm         | rrr        | XOPV 0xD7 VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                           | REG0=XMM_R():w:dq:u64 MEM0:r:dq:u16
 | VPHADDUWQ            | XOP            | XOP            | XOP            | d7           | 0b11       | rrr        | XOPV 0xD7 VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                  | REG0=XMM_R():w:dq:u64 REG1=XMM_B():r:dq:u16
 | VPHADDW              | AVX            | AVX            |                | 01           | mm         | rrr        | VV1 0x01  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
 | VPHADDW              | AVX            | AVX            |                | 01           | 0b11       | rrr        | VV1 0x01  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
 | VPHADDW              | AVX2           | AVX2           |                | 01           | mm         | rrr        | VV1 0x01  VL256 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=YMM_R():w:qq:i16 REG1=YMM_N():r:qq:i16 MEM0:r:qq:i16
 | VPHADDW              | AVX2           | AVX2           |                | 01           | 0b11       | rrr        | VV1 0x01  VL256 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=YMM_R():w:qq:i16 REG1=YMM_N():r:qq:i16 REG2=YMM_B():r:qq:i16
 | VPHADDWD             | XOP            | XOP            | XOP            | c6           | mm         | rrr        | XOPV 0xC6 VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                           | REG0=XMM_R():w:dq:i32 MEM0:r:dq:i16
 | VPHADDWD             | XOP            | XOP            | XOP            | c6           | 0b11       | rrr        | XOPV 0xC6 VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                  | REG0=XMM_R():w:dq:i32 REG1=XMM_B():r:dq:i16
 | VPHADDWQ             | XOP            | XOP            | XOP            | c7           | mm         | rrr        | XOPV 0xC7 VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                           | REG0=XMM_R():w:dq:i64 MEM0:r:dq:i16
 | VPHADDWQ             | XOP            | XOP            | XOP            | c7           | 0b11       | rrr        | XOPV 0xC7 VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                  | REG0=XMM_R():w:dq:i64 REG1=XMM_B():r:dq:i16
 | VPHMINPOSUW          | AVX            | AVX            |                | 41           | mm         | rrr        | VV1 0x41   V66 V0F38 VL128 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                             | REG0=XMM_R():w:dq:u16 MEM0:r:dq:u16
 | VPHMINPOSUW          | AVX            | AVX            |                | 41           | 0b11       | rrr        | VV1 0x41  V66 V0F38 VL128 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=XMM_R():w:dq:u16  REG1=XMM_B():r:dq:u16
 | VPHSUBBW             | XOP            | XOP            | XOP            | e1           | mm         | rrr        | XOPV 0xE1 VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                           | REG0=XMM_R():w:dq:i16 MEM0:r:dq:i8
 | VPHSUBBW             | XOP            | XOP            | XOP            | e1           | 0b11       | rrr        | XOPV 0xE1 VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                  | REG0=XMM_R():w:dq:i16 REG1=XMM_B():r:dq:i8
 | VPHSUBD              | AVX            | AVX            |                | 06           | mm         | rrr        | VV1 0x06  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32
 | VPHSUBD              | AVX            | AVX            |                | 06           | 0b11       | rrr        | VV1 0x06  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32
 | VPHSUBD              | AVX2           | AVX2           |                | 06           | mm         | rrr        | VV1 0x06  VL256 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=YMM_R():w:qq:i32 REG1=YMM_N():r:qq:i32 MEM0:r:qq:i32
 | VPHSUBD              | AVX2           | AVX2           |                | 06           | 0b11       | rrr        | VV1 0x06  VL256 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=YMM_R():w:qq:i32 REG1=YMM_N():r:qq:i32 REG2=YMM_B():r:qq:i32
 | VPHSUBDQ             | XOP            | XOP            | XOP            | e3           | mm         | rrr        | XOPV 0xE3 VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                           | REG0=XMM_R():w:dq:i64 MEM0:r:dq:i32
 | VPHSUBDQ             | XOP            | XOP            | XOP            | e3           | 0b11       | rrr        | XOPV 0xE3 VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                  | REG0=XMM_R():w:dq:i64 REG1=XMM_B():r:dq:i32
 | VPHSUBSW             | AVX            | AVX            |                | 07           | mm         | rrr        | VV1 0x07  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
 | VPHSUBSW             | AVX            | AVX            |                | 07           | 0b11       | rrr        | VV1 0x07  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
 | VPHSUBSW             | AVX2           | AVX2           |                | 07           | mm         | rrr        | VV1 0x07  VL256 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=YMM_R():w:qq:i16 REG1=YMM_N():r:qq:i16 MEM0:r:qq:i16
 | VPHSUBSW             | AVX2           | AVX2           |                | 07           | 0b11       | rrr        | VV1 0x07  VL256 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=YMM_R():w:qq:i16 REG1=YMM_N():r:qq:i16 REG2=YMM_B():r:qq:i16
 | VPHSUBW              | AVX            | AVX            |                | 05           | mm         | rrr        | VV1 0x05  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
 | VPHSUBW              | AVX            | AVX            |                | 05           | 0b11       | rrr        | VV1 0x05  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
 | VPHSUBW              | AVX2           | AVX2           |                | 05           | mm         | rrr        | VV1 0x05  VL256 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=YMM_R():w:qq:i16 REG1=YMM_N():r:qq:i16 MEM0:r:qq:i16
 | VPHSUBW              | AVX2           | AVX2           |                | 05           | 0b11       | rrr        | VV1 0x05  VL256 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=YMM_R():w:qq:i16 REG1=YMM_N():r:qq:i16 REG2=YMM_B():r:qq:i16
 | VPHSUBWD             | XOP            | XOP            | XOP            | e2           | mm         | rrr        | XOPV 0xE2 VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                           | REG0=XMM_R():w:dq:i32 MEM0:r:dq:i16
 | VPHSUBWD             | XOP            | XOP            | XOP            | e2           | 0b11       | rrr        | XOPV 0xE2 VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                  | REG0=XMM_R():w:dq:i32 REG1=XMM_B():r:dq:i16
 | VPINSRB              | AVX            | AVX            |                | 20           | mm         | rrr        | VV1 0x20  VL128 V66 V0F3A  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                           | REG0=XMM_R():w:dq:u8     REG1=XMM_N():r:dq:u8  MEM0:r:b:u8            IMM0:r:b
 | VPINSRB              | AVX            | AVX            |                | 20           | 0b11       | rrr        | VV1 0x20  VL128 V66 V0F3A  MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                  | REG0=XMM_R():w:dq:u8     REG1=XMM_N():r:dq:u8  REG2=GPR32_B():r:d:u8  IMM0:r:b
 | VPINSRB              | AVX512         | AVX512EVEX     | AVX512BW_128N  | 20           | 0b11       | rrr        | EVV 0x20 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128      ZEROING=0 MASK=0 UIMM8()      | REG0=XMM_R3():w:dq:u8 REG1=XMM_N3():r:dq:u8 REG2=GPR32_B():r:d:u8 IMM0:r:b
 | VPINSRB              | AVX512         | AVX512EVEX     | AVX512BW_128N  | 20           | mm         | rrr        | EVV 0x20 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ZEROING=0 MASK=0 UIMM8()  ESIZE_8_BITS() NELEM_GPR_READER_BYTE() | REG0=XMM_R3():w:dq:u8 REG1=XMM_N3():r:dq:u8 MEM0:r:b:u8 IMM0:r:b
 | VPINSRD              | AVX            | AVX            |                | 22           | mm         | rrr        | VV1 0x22  VL128 V66 V0F3A mode64 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()       | REG0=XMM_R():w:dq:u32     REG1=XMM_N():r:dq:u32  MEM0:r:d:u32            IMM0:r:b
 | VPINSRD              | AVX            | AVX            |                | 22           | 0b11       | rrr        | VV1 0x22  VL128 V66 V0F3A mode64 norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()              | REG0=XMM_R():w:dq:u32     REG1=XMM_N():r:dq:u32  REG2=GPR32_B():r:d:u32  IMM0:r:b
 | VPINSRD              | AVX            | AVX            |                | 22           | mm         | rrr        | VV1 0x22  VL128 V66 V0F3A not64 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                      | REG0=XMM_R():w:dq:u32     REG1=XMM_N():r:dq:u32  MEM0:r:d:u32            IMM0:r:b
 | VPINSRD              | AVX            | AVX            |                | 22           | 0b11       | rrr        | VV1 0x22  VL128 V66 V0F3A not64 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                             | REG0=XMM_R():w:dq:u32     REG1=XMM_N():r:dq:u32  REG2=GPR32_B():r:d:u32  IMM0:r:b
 | VPINSRD              | AVX512         | AVX512EVEX     | AVX512DQ_128N  | 22           | 0b11       | rrr        | EVV 0x22 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  not64    ZEROING=0 MASK=0 UIMM8() | REG0=XMM_R3():w:dq:u32 REG1=XMM_N3():r:dq:u32 REG2=GPR32_B():r:d:u32 IMM0:r:b
 | VPINSRD              | AVX512         | AVX512EVEX     | AVX512DQ_128N  | 22           | 0b11       | rrr        | EVV 0x22 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  mode64 W0    ZEROING=0 MASK=0 UIMM8() | REG0=XMM_R3():w:dq:u32 REG1=XMM_N3():r:dq:u32 REG2=GPR32_B():r:d:u32 IMM0:r:b
 | VPINSRD              | AVX512         | AVX512EVEX     | AVX512DQ_128N  | 22           | mm         | rrr        | EVV 0x22 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  not64    ZEROING=0 MASK=0 UIMM8()  ESIZE_32_BITS() NELEM_GPR_READER() | REG0=XMM_R3():w:dq:u32 REG1=XMM_N3():r:dq:u32 MEM0:r:d:u32 IMM0:r:b
 | VPINSRD              | AVX512         | AVX512EVEX     | AVX512DQ_128N  | 22           | mm         | rrr        | EVV 0x22 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  mode64 W0    ZEROING=0 MASK=0 UIMM8()  ESIZE_32_BITS() NELEM_GPR_READER() | REG0=XMM_R3():w:dq:u32 REG1=XMM_N3():r:dq:u32 MEM0:r:d:u32 IMM0:r:b
 | VPINSRQ              | AVX            | AVX            |                | 22           | mm         | rrr        | VV1 0x22  VL128 V66 V0F3A mode64 rexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()         | REG0=XMM_R():w:dq:u64     REG1=XMM_N():r:dq:u64  MEM0:r:q:u64            IMM0:r:b
 | VPINSRQ              | AVX            | AVX            |                | 22           | 0b11       | rrr        | VV1 0x22  VL128 V66 V0F3A mode64 rexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                | REG0=XMM_R():w:dq:u64     REG1=XMM_N():r:dq:u64  REG2=GPR64_B():r:q:u64  IMM0:r:b
 | VPINSRQ              | AVX512         | AVX512EVEX     | AVX512DQ_128N  | 22           | 0b11       | rrr        | EVV 0x22 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  mode64    ZEROING=0 MASK=0 UIMM8() | REG0=XMM_R3():w:dq:u64 REG1=XMM_N3():r:dq:u64 REG2=GPR64_B():r:q:u64 IMM0:r:b
 | VPINSRQ              | AVX512         | AVX512EVEX     | AVX512DQ_128N  | 22           | mm         | rrr        | EVV 0x22 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  mode64    ZEROING=0 MASK=0 UIMM8()  ESIZE_64_BITS() NELEM_GPR_READER() | REG0=XMM_R3():w:dq:u64 REG1=XMM_N3():r:dq:u64 MEM0:r:q:u64 IMM0:r:b
 | VPINSRW              | AVX            | AVX            |                | c4           | mm         | rrr        | VV1 0xC4  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                             | REG0=XMM_R():w:dq:u16     REG1=XMM_N():r:dq:u16  MEM0:r:w:u16           IMM0:r:b
 | VPINSRW              | AVX            | AVX            |                | c4           | 0b11       | rrr        | VV1 0xC4  VL128 V66 V0F  MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                    | REG0=XMM_R():w:dq:u16     REG1=XMM_N():r:dq:u16  REG2=GPR32_B():r:d:u16  IMM0:r:b
 | VPINSRW              | AVX512         | AVX512EVEX     | AVX512BW_128N  | c4           | 0b11       | rrr        | EVV 0xC4 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128      ZEROING=0 MASK=0 UIMM8()        | REG0=XMM_R3():w:dq:u16 REG1=XMM_N3():r:dq:u16 REG2=GPR32_B():r:d:u16 IMM0:r:b
 | VPINSRW              | AVX512         | AVX512EVEX     | AVX512BW_128N  | c4           | mm         | rrr        | EVV 0xC4 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ZEROING=0 MASK=0 UIMM8()  ESIZE_16_BITS() NELEM_GPR_READER_WORD() | REG0=XMM_R3():w:dq:u16 REG1=XMM_N3():r:dq:u16 MEM0:r:wrd:u16 IMM0:r:b
 | VPLZCNTD             | CONFLICT       | AVX512EVEX     | AVX512CD_128   | 44           | 0b11       | rrr        | EVV 0x44 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32
 | VPLZCNTD             | CONFLICT       | AVX512EVEX     | AVX512CD_128   | 44           | mm         | rrr        | EVV 0x44 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR
 | VPLZCNTD             | CONFLICT       | AVX512EVEX     | AVX512CD_256   | 44           | 0b11       | rrr        | EVV 0x44 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u32
 | VPLZCNTD             | CONFLICT       | AVX512EVEX     | AVX512CD_256   | 44           | mm         | rrr        | EVV 0x44 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR
 | VPLZCNTD             | CONFLICT       | AVX512EVEX     | AVX512CD_512   | 44           | 0b11       | rrr        | EVV 0x44 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu32
 | VPLZCNTD             | CONFLICT       | AVX512EVEX     | AVX512CD_512   | 44           | mm         | rrr        | EVV 0x44 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR
 | VPLZCNTQ             | CONFLICT       | AVX512EVEX     | AVX512CD_128   | 44           | 0b11       | rrr        | EVV 0x44 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                        | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u64
 | VPLZCNTQ             | CONFLICT       | AVX512EVEX     | AVX512CD_128   | 44           | mm         | rrr        | EVV 0x44 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR
 | VPLZCNTQ             | CONFLICT       | AVX512EVEX     | AVX512CD_256   | 44           | 0b11       | rrr        | EVV 0x44 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                        | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u64
 | VPLZCNTQ             | CONFLICT       | AVX512EVEX     | AVX512CD_256   | 44           | mm         | rrr        | EVV 0x44 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR
 | VPLZCNTQ             | CONFLICT       | AVX512EVEX     | AVX512CD_512   | 44           | 0b11       | rrr        | EVV 0x44 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                        | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu64
 | VPLZCNTQ             | CONFLICT       | AVX512EVEX     | AVX512CD_512   | 44           | mm         | rrr        | EVV 0x44 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR
 | VPMACSDD             | XOP            | XOP            | XOP            | 9e           | mm         | rrr        | XOPV 0x9E VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                      | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32 REG2=XMM_SE():r:dq:i32
 | VPMACSDD             | XOP            | XOP            | XOP            | 9e           | 0b11       | rrr        | XOPV 0x9E VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                             | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32 REG3=XMM_SE():r:dq:i32
 | VPMACSDQH            | XOP            | XOP            | XOP            | 9f           | mm         | rrr        | XOPV 0x9F VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                      | REG0=XMM_R():w:dq:i64 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32 REG2=XMM_SE():r:dq:i64
 | VPMACSDQH            | XOP            | XOP            | XOP            | 9f           | 0b11       | rrr        | XOPV 0x9F VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                             | REG0=XMM_R():w:dq:i64 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32 REG3=XMM_SE():r:dq:i64
 | VPMACSDQL            | XOP            | XOP            | XOP            | 97           | mm         | rrr        | XOPV 0x97 VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                      | REG0=XMM_R():w:dq:i64 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32 REG2=XMM_SE():r:dq:i64
 | VPMACSDQL            | XOP            | XOP            | XOP            | 97           | 0b11       | rrr        | XOPV 0x97 VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                             | REG0=XMM_R():w:dq:i64 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32 REG3=XMM_SE():r:dq:i64
 | VPMACSSDD            | XOP            | XOP            | XOP            | 8e           | mm         | rrr        | XOPV 0x8E VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                      | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32 REG2=XMM_SE():r:dq:i32
 | VPMACSSDD            | XOP            | XOP            | XOP            | 8e           | 0b11       | rrr        | XOPV 0x8E VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                             | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32 REG3=XMM_SE():r:dq:i32
 | VPMACSSDQH           | XOP            | XOP            | XOP            | 8f           | mm         | rrr        | XOPV 0x8F VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                      | REG0=XMM_R():w:dq:i64 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32 REG2=XMM_SE():r:dq:i64
 | VPMACSSDQH           | XOP            | XOP            | XOP            | 8f           | 0b11       | rrr        | XOPV 0x8F VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                             | REG0=XMM_R():w:dq:i64 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32 REG3=XMM_SE():r:dq:i64
 | VPMACSSDQL           | XOP            | XOP            | XOP            | 87           | mm         | rrr        | XOPV 0x87 VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                      | REG0=XMM_R():w:dq:i64 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32 REG2=XMM_SE():r:dq:i64
 | VPMACSSDQL           | XOP            | XOP            | XOP            | 87           | 0b11       | rrr        | XOPV 0x87 VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                             | REG0=XMM_R():w:dq:i64 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32 REG3=XMM_SE():r:dq:i64
 | VPMACSSWD            | XOP            | XOP            | XOP            | 86           | mm         | rrr        | XOPV 0x86 VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                      | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16 REG2=XMM_SE():r:dq:i32
 | VPMACSSWD            | XOP            | XOP            | XOP            | 86           | 0b11       | rrr        | XOPV 0x86 VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                             | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16 REG3=XMM_SE():r:dq:i32
 | VPMACSSWW            | XOP            | XOP            | XOP            | 85           | mm         | rrr        | XOPV 0x85 VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                      | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16 REG2=XMM_SE():r:dq:i16
 | VPMACSSWW            | XOP            | XOP            | XOP            | 85           | 0b11       | rrr        | XOPV 0x85 VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                             | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16 REG3=XMM_SE():r:dq:i16
 | VPMACSWD             | XOP            | XOP            | XOP            | 96           | mm         | rrr        | XOPV 0x96 VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                      | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16 REG2=XMM_SE():r:dq:i32
 | VPMACSWD             | XOP            | XOP            | XOP            | 96           | 0b11       | rrr        | XOPV 0x96 VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                             | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16 REG3=XMM_SE():r:dq:i32
 | VPMACSWW             | XOP            | XOP            | XOP            | 95           | mm         | rrr        | XOPV 0x95 VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                      | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16 REG2=XMM_SE():r:dq:i16
 | VPMACSWW             | XOP            | XOP            | XOP            | 95           | 0b11       | rrr        | XOPV 0x95 VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                             | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16 REG3=XMM_SE():r:dq:i16
 | VPMADCSSWD           | XOP            | XOP            | XOP            | a6           | mm         | rrr        | XOPV 0xA6 VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                      | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16 REG2=XMM_SE():r:dq:i32
 | VPMADCSSWD           | XOP            | XOP            | XOP            | a6           | 0b11       | rrr        | XOPV 0xA6 VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                             | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16 REG3=XMM_SE():r:dq:i32
 | VPMADCSWD            | XOP            | XOP            | XOP            | b6           | mm         | rrr        | XOPV 0xB6 VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                      | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16 REG2=XMM_SE():r:dq:i32
 | VPMADCSWD            | XOP            | XOP            | XOP            | b6           | 0b11       | rrr        | XOPV 0xB6 VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                             | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16 REG3=XMM_SE():r:dq:i32
 | VPMADD231D           | KNC            | KNCE           |                | b5           | mm         | rrr        | KVV 0xB5 V0F38 REXW=0 V66  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT32()                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VPMADD231D           | KNC            | KNCE           |                | b5           | 0b11       | rrr        | KVV 0xB5 V0F38 REXW=0 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VPMADD231D           | KNC            | KNCE           |                | b5           | 0b11       | rrr        | KVV 0xB5 V0F38 REXW=0 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1                                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VPMADD233D           | KNC            | KNCE           |                | b4           | mm         | rrr        | KVV 0xB4 V0F38 REXW=0 V66  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT32_LIMITED()        | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VPMADD233D           | KNC            | KNCE           |                | b4           | 0b11       | rrr        | KVV 0xB4 V0F38 REXW=0 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VPMADD233D           | KNC            | KNCE           |                | b4           | 0b11       | rrr        | KVV 0xB4 V0F38 REXW=0 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1                                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VPMADD52HUQ          | IFMA           | AVX512EVEX     | AVX512_IFMA_128 | b5           | 0b11       | rrr        | EVV 0xB5 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():rw:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
 | VPMADD52HUQ          | IFMA           | AVX512EVEX     | AVX512_IFMA_128 | b5           | mm         | rrr        | EVV 0xB5 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPMADD52HUQ          | IFMA           | AVX512EVEX     | AVX512_IFMA_256 | b5           | 0b11       | rrr        | EVV 0xB5 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():rw:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
 | VPMADD52HUQ          | IFMA           | AVX512EVEX     | AVX512_IFMA_256 | b5           | mm         | rrr        | EVV 0xB5 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPMADD52HUQ          | IFMA           | AVX512EVEX     | AVX512_IFMA_512 | b5           | 0b11       | rrr        | EVV 0xB5 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():rw:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
 | VPMADD52HUQ          | IFMA           | AVX512EVEX     | AVX512_IFMA_512 | b5           | mm         | rrr        | EVV 0xB5 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPMADD52LUQ          | IFMA           | AVX512EVEX     | AVX512_IFMA_128 | b4           | 0b11       | rrr        | EVV 0xB4 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():rw:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
 | VPMADD52LUQ          | IFMA           | AVX512EVEX     | AVX512_IFMA_128 | b4           | mm         | rrr        | EVV 0xB4 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPMADD52LUQ          | IFMA           | AVX512EVEX     | AVX512_IFMA_256 | b4           | 0b11       | rrr        | EVV 0xB4 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():rw:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
 | VPMADD52LUQ          | IFMA           | AVX512EVEX     | AVX512_IFMA_256 | b4           | mm         | rrr        | EVV 0xB4 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPMADD52LUQ          | IFMA           | AVX512EVEX     | AVX512_IFMA_512 | b4           | 0b11       | rrr        | EVV 0xB4 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():rw:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
 | VPMADD52LUQ          | IFMA           | AVX512EVEX     | AVX512_IFMA_512 | b4           | mm         | rrr        | EVV 0xB4 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPMADDUBSW           | AVX            | AVX            |                | 04           | mm         | rrr        | VV1 0x04  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:u8 MEM0:r:dq:i8
 | VPMADDUBSW           | AVX            | AVX            |                | 04           | 0b11       | rrr        | VV1 0x04  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:i8
 | VPMADDUBSW           | AVX2           | AVX2           |                | 04           | mm         | rrr        | VV1 0x04  VL256 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=YMM_R():w:qq:i16 REG1=YMM_N():r:qq:u8 MEM0:r:qq:i8
 | VPMADDUBSW           | AVX2           | AVX2           |                | 04           | 0b11       | rrr        | VV1 0x04  VL256 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=YMM_R():w:qq:i16 REG1=YMM_N():r:qq:u8 REG2=YMM_B():r:qq:i8
 | VPMADDUBSW           | AVX512         | AVX512EVEX     | AVX512BW_128   | 04           | 0b11       | rrr        | EVV 0x04 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                    | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 REG3=XMM_B3():r:dq:i16
 | VPMADDUBSW           | AVX512         | AVX512EVEX     | AVX512BW_128   | 04           | mm         | rrr        | EVV 0x04 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 MEM0:r:dq:i16
 | VPMADDUBSW           | AVX512         | AVX512EVEX     | AVX512BW_256   | 04           | 0b11       | rrr        | EVV 0x04 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                    | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 REG3=YMM_B3():r:qq:i16
 | VPMADDUBSW           | AVX512         | AVX512EVEX     | AVX512BW_256   | 04           | mm         | rrr        | EVV 0x04 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 MEM0:r:qq:i16
 | VPMADDUBSW           | AVX512         | AVX512EVEX     | AVX512BW_512   | 04           | 0b11       | rrr        | EVV 0x04 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                    | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 REG3=ZMM_B3():r:zi16
 | VPMADDUBSW           | AVX512         | AVX512EVEX     | AVX512BW_512   | 04           | mm         | rrr        | EVV 0x04 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 MEM0:r:zd:i16
 | VPMADDWD             | AVX            | AVX            |                | f5           | mm         | rrr        | VV1 0xF5  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
 | VPMADDWD             | AVX            | AVX            |                | f5           | 0b11       | rrr        | VV1 0xF5  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
 | VPMADDWD             | AVX2           | AVX2           |                | f5           | mm         | rrr        | VV1 0xF5  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:i32 REG1=YMM_N():r:qq:i16 MEM0:r:qq:i16
 | VPMADDWD             | AVX2           | AVX2           |                | f5           | 0b11       | rrr        | VV1 0xF5  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:i32 REG1=YMM_N():r:qq:i16 REG2=YMM_B():r:qq:i16
 | VPMADDWD             | AVX512         | AVX512EVEX     | AVX512BW_128   | f5           | 0b11       | rrr        | EVV 0xF5 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                      | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 REG3=XMM_B3():r:dq:i16
 | VPMADDWD             | AVX512         | AVX512EVEX     | AVX512BW_128   | f5           | mm         | rrr        | EVV 0xF5 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 MEM0:r:dq:i16
 | VPMADDWD             | AVX512         | AVX512EVEX     | AVX512BW_256   | f5           | 0b11       | rrr        | EVV 0xF5 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                      | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 REG3=YMM_B3():r:qq:i16
 | VPMADDWD             | AVX512         | AVX512EVEX     | AVX512BW_256   | f5           | mm         | rrr        | EVV 0xF5 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 MEM0:r:qq:i16
 | VPMADDWD             | AVX512         | AVX512EVEX     | AVX512BW_512   | f5           | 0b11       | rrr        | EVV 0xF5 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                      | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 REG3=ZMM_B3():r:zi16
 | VPMADDWD             | AVX512         | AVX512EVEX     | AVX512BW_512   | f5           | mm         | rrr        | EVV 0xF5 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 MEM0:r:zd:i16
 | VPMASKMOVD           | AVX2           | AVX2           |                | 8c           | mm         | rrr        | VV1 0x8C  VL128 V66 V0F38 W0  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:u32  REG1=XMM_N():r:dq:u32  MEM0:r:dq:u32
 | VPMASKMOVD           | AVX2           | AVX2           |                | 8c           | mm         | rrr        | VV1 0x8C  VL256 V66 V0F38 W0  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=YMM_R():w:qq:u32  REG1=YMM_N():r:qq:u32  MEM0:r:qq:u32
 | VPMASKMOVD           | AVX2           | AVX2           |                | 8e           | mm         | rrr        | VV1 0x8E  VL128 V66 V0F38 W0  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:dq:u32  REG0=XMM_N():r:dq:u32  REG1=XMM_R():r:dq:u32
 | VPMASKMOVD           | AVX2           | AVX2           |                | 8e           | mm         | rrr        | VV1 0x8E  VL256 V66 V0F38 W0  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:qq:u32  REG0=YMM_N():r:qq:u32  REG1=YMM_R():r:qq:u32
 | VPMASKMOVQ           | AVX2           | AVX2           |                | 8c           | mm         | rrr        | VV1 0x8C  VL128 V66 V0F38 W1  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:u64  REG1=XMM_N():r:dq:u64  MEM0:r:dq:u64
 | VPMASKMOVQ           | AVX2           | AVX2           |                | 8c           | mm         | rrr        | VV1 0x8C  VL256 V66 V0F38 W1  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=YMM_R():w:qq:u64  REG1=YMM_N():r:qq:u64  MEM0:r:qq:u64
 | VPMASKMOVQ           | AVX2           | AVX2           |                | 8e           | mm         | rrr        | VV1 0x8E  VL128 V66 V0F38 W1  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:dq:u64  REG0=XMM_N():r:dq:u64  REG1=XMM_R():r:dq:u64
 | VPMASKMOVQ           | AVX2           | AVX2           |                | 8e           | mm         | rrr        | VV1 0x8E  VL256 V66 V0F38 W1  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:qq:u64  REG0=YMM_N():r:qq:u64  REG1=YMM_R():r:qq:u64
 | VPMAXSB              | AVX            | AVX            |                | 3c           | mm         | rrr        | VV1 0x3C  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 MEM0:r:dq:i8
 | VPMAXSB              | AVX            | AVX            |                | 3c           | 0b11       | rrr        | VV1 0x3C  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 REG2=XMM_B():r:dq:i8
 | VPMAXSB              | AVX2           | AVX2           |                | 3c           | mm         | rrr        | VV1 0x3C  VL256 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=YMM_R():w:qq:i8 REG1=YMM_N():r:qq:i8 MEM0:r:qq:i8
 | VPMAXSB              | AVX2           | AVX2           |                | 3c           | 0b11       | rrr        | VV1 0x3C  VL256 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=YMM_R():w:qq:i8 REG1=YMM_N():r:qq:i8 REG2=YMM_B():r:qq:i8
 | VPMAXSB              | AVX512         | AVX512EVEX     | AVX512BW_128   | 3c           | 0b11       | rrr        | EVV 0x3C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                    | REG0=XMM_R3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i8 REG3=XMM_B3():r:dq:i8
 | VPMAXSB              | AVX512         | AVX512EVEX     | AVX512BW_128   | 3c           | mm         | rrr        | EVV 0x3C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i8 MEM0:r:dq:i8
 | VPMAXSB              | AVX512         | AVX512EVEX     | AVX512BW_256   | 3c           | 0b11       | rrr        | EVV 0x3C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                    | REG0=YMM_R3():w:qq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i8 REG3=YMM_B3():r:qq:i8
 | VPMAXSB              | AVX512         | AVX512EVEX     | AVX512BW_256   | 3c           | mm         | rrr        | EVV 0x3C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i8 MEM0:r:qq:i8
 | VPMAXSB              | AVX512         | AVX512EVEX     | AVX512BW_512   | 3c           | 0b11       | rrr        | EVV 0x3C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                    | REG0=ZMM_R3():w:zi8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi8 REG3=ZMM_B3():r:zi8
 | VPMAXSB              | AVX512         | AVX512EVEX     | AVX512BW_512   | 3c           | mm         | rrr        | EVV 0x3C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zi8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi8 MEM0:r:zd:i8
 | VPMAXSD              | AVX            | AVX            |                | 3d           | mm         | rrr        | VV1 0x3D  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32
 | VPMAXSD              | AVX            | AVX            |                | 3d           | 0b11       | rrr        | VV1 0x3D  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32
 | VPMAXSD              | AVX2           | AVX2           |                | 3d           | mm         | rrr        | VV1 0x3D  VL256 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=YMM_R():w:qq:i32 REG1=YMM_N():r:qq:i32 MEM0:r:qq:i32
 | VPMAXSD              | AVX2           | AVX2           |                | 3d           | 0b11       | rrr        | VV1 0x3D  VL256 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=YMM_R():w:qq:i32 REG1=YMM_N():r:qq:i32 REG2=YMM_B():r:qq:i32
 | VPMAXSD              | AVX512         | AVX512EVEX     | AVX512F_128    | 3d           | 0b11       | rrr        | EVV 0x3D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i32 REG3=XMM_B3():r:dq:i32
 | VPMAXSD              | AVX512         | AVX512EVEX     | AVX512F_128    | 3d           | mm         | rrr        | EVV 0x3D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i32 MEM0:r:vv:i32:TXT=BCASTSTR
 | VPMAXSD              | AVX512         | AVX512EVEX     | AVX512F_256    | 3d           | 0b11       | rrr        | EVV 0x3D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i32 REG3=YMM_B3():r:qq:i32
 | VPMAXSD              | AVX512         | AVX512EVEX     | AVX512F_256    | 3d           | mm         | rrr        | EVV 0x3D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i32 MEM0:r:vv:i32:TXT=BCASTSTR
 | VPMAXSD              | AVX512         | AVX512EVEX     | AVX512F_512    | 3d           | 0b11       | rrr        | EVV 0x3D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi32 REG3=ZMM_B3():r:zi32
 | VPMAXSD              | AVX512         | AVX512EVEX     | AVX512F_512    | 3d           | mm         | rrr        | EVV 0x3D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi32 MEM0:r:vv:i32:TXT=BCASTSTR
 | VPMAXSD              | KNC            | KNCE           |                | 3d           | mm         | rrr        | KVV 0x3D V0F38 REXW=0 V66  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT32()                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VPMAXSD              | KNC            | KNCE           |                | 3d           | 0b11       | rrr        | KVV 0x3D V0F38 REXW=0 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VPMAXSD              | KNC            | KNCE           |                | 3d           | 0b11       | rrr        | KVV 0x3D V0F38 REXW=0 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1                                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VPMAXSQ              | AVX512         | AVX512EVEX     | AVX512F_128    | 3d           | 0b11       | rrr        | EVV 0x3D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i64 REG3=XMM_B3():r:dq:i64
 | VPMAXSQ              | AVX512         | AVX512EVEX     | AVX512F_128    | 3d           | mm         | rrr        | EVV 0x3D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i64 MEM0:r:vv:i64:TXT=BCASTSTR
 | VPMAXSQ              | AVX512         | AVX512EVEX     | AVX512F_256    | 3d           | 0b11       | rrr        | EVV 0x3D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i64 REG3=YMM_B3():r:qq:i64
 | VPMAXSQ              | AVX512         | AVX512EVEX     | AVX512F_256    | 3d           | mm         | rrr        | EVV 0x3D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i64 MEM0:r:vv:i64:TXT=BCASTSTR
 | VPMAXSQ              | AVX512         | AVX512EVEX     | AVX512F_512    | 3d           | 0b11       | rrr        | EVV 0x3D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi64 REG3=ZMM_B3():r:zi64
 | VPMAXSQ              | AVX512         | AVX512EVEX     | AVX512F_512    | 3d           | mm         | rrr        | EVV 0x3D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi64 MEM0:r:vv:i64:TXT=BCASTSTR
 | VPMAXSW              | AVX            | AVX            |                | ee           | mm         | rrr        | VV1 0xEE  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
 | VPMAXSW              | AVX            | AVX            |                | ee           | 0b11       | rrr        | VV1 0xEE  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
 | VPMAXSW              | AVX2           | AVX2           |                | ee           | mm         | rrr        | VV1 0xEE  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:i16 REG1=YMM_N():r:qq:i16 MEM0:r:qq:i16
 | VPMAXSW              | AVX2           | AVX2           |                | ee           | 0b11       | rrr        | VV1 0xEE  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:i16 REG1=YMM_N():r:qq:i16 REG2=YMM_B():r:qq:i16
 | VPMAXSW              | AVX512         | AVX512EVEX     | AVX512BW_128   | ee           | 0b11       | rrr        | EVV 0xEE V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                      | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 REG3=XMM_B3():r:dq:i16
 | VPMAXSW              | AVX512         | AVX512EVEX     | AVX512BW_128   | ee           | mm         | rrr        | EVV 0xEE V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 MEM0:r:dq:i16
 | VPMAXSW              | AVX512         | AVX512EVEX     | AVX512BW_256   | ee           | 0b11       | rrr        | EVV 0xEE V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                      | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 REG3=YMM_B3():r:qq:i16
 | VPMAXSW              | AVX512         | AVX512EVEX     | AVX512BW_256   | ee           | mm         | rrr        | EVV 0xEE V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 MEM0:r:qq:i16
 | VPMAXSW              | AVX512         | AVX512EVEX     | AVX512BW_512   | ee           | 0b11       | rrr        | EVV 0xEE V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                      | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 REG3=ZMM_B3():r:zi16
 | VPMAXSW              | AVX512         | AVX512EVEX     | AVX512BW_512   | ee           | mm         | rrr        | EVV 0xEE V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 MEM0:r:zd:i16
 | VPMAXUB              | AVX            | AVX            |                | de           | mm         | rrr        | VV1 0xDE  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 MEM0:r:dq:u8
 | VPMAXUB              | AVX            | AVX            |                | de           | 0b11       | rrr        | VV1 0xDE  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:u8
 | VPMAXUB              | AVX2           | AVX2           |                | de           | mm         | rrr        | VV1 0xDE  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u8 REG1=YMM_N():r:qq:u8 MEM0:r:qq:u8
 | VPMAXUB              | AVX2           | AVX2           |                | de           | 0b11       | rrr        | VV1 0xDE  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:u8 REG1=YMM_N():r:qq:u8 REG2=YMM_B():r:qq:u8
 | VPMAXUB              | AVX512         | AVX512EVEX     | AVX512BW_128   | de           | 0b11       | rrr        | EVV 0xDE V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                      | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
 | VPMAXUB              | AVX512         | AVX512EVEX     | AVX512BW_128   | de           | mm         | rrr        | EVV 0xDE V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
 | VPMAXUB              | AVX512         | AVX512EVEX     | AVX512BW_256   | de           | 0b11       | rrr        | EVV 0xDE V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                      | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
 | VPMAXUB              | AVX512         | AVX512EVEX     | AVX512BW_256   | de           | mm         | rrr        | EVV 0xDE V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
 | VPMAXUB              | AVX512         | AVX512EVEX     | AVX512BW_512   | de           | 0b11       | rrr        | EVV 0xDE V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                      | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
 | VPMAXUB              | AVX512         | AVX512EVEX     | AVX512BW_512   | de           | mm         | rrr        | EVV 0xDE V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
 | VPMAXUD              | AVX            | AVX            |                | 3f           | mm         | rrr        | VV1 0x3F  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 MEM0:r:dq:u32
 | VPMAXUD              | AVX            | AVX            |                | 3f           | 0b11       | rrr        | VV1 0x3F  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 REG2=XMM_B():r:dq:u32
 | VPMAXUD              | AVX2           | AVX2           |                | 3f           | mm         | rrr        | VV1 0x3F  VL256 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=YMM_R():w:qq:u32 REG1=YMM_N():r:qq:u32 MEM0:r:qq:u32
 | VPMAXUD              | AVX2           | AVX2           |                | 3f           | 0b11       | rrr        | VV1 0x3F  VL256 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=YMM_R():w:qq:u32 REG1=YMM_N():r:qq:u32 REG2=YMM_B():r:qq:u32
 | VPMAXUD              | AVX512         | AVX512EVEX     | AVX512F_128    | 3f           | 0b11       | rrr        | EVV 0x3F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
 | VPMAXUD              | AVX512         | AVX512EVEX     | AVX512F_128    | 3f           | mm         | rrr        | EVV 0x3F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPMAXUD              | AVX512         | AVX512EVEX     | AVX512F_256    | 3f           | 0b11       | rrr        | EVV 0x3F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
 | VPMAXUD              | AVX512         | AVX512EVEX     | AVX512F_256    | 3f           | mm         | rrr        | EVV 0x3F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPMAXUD              | AVX512         | AVX512EVEX     | AVX512F_512    | 3f           | 0b11       | rrr        | EVV 0x3F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
 | VPMAXUD              | AVX512         | AVX512EVEX     | AVX512F_512    | 3f           | mm         | rrr        | EVV 0x3F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPMAXUD              | KNC            | KNCE           |                | 3f           | mm         | rrr        | KVV 0x3F V0F38 REXW=0 V66  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT32()                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VPMAXUD              | KNC            | KNCE           |                | 3f           | 0b11       | rrr        | KVV 0x3F V0F38 REXW=0 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VPMAXUD              | KNC            | KNCE           |                | 3f           | 0b11       | rrr        | KVV 0x3F V0F38 REXW=0 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1                                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VPMAXUQ              | AVX512         | AVX512EVEX     | AVX512F_128    | 3f           | 0b11       | rrr        | EVV 0x3F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
 | VPMAXUQ              | AVX512         | AVX512EVEX     | AVX512F_128    | 3f           | mm         | rrr        | EVV 0x3F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPMAXUQ              | AVX512         | AVX512EVEX     | AVX512F_256    | 3f           | 0b11       | rrr        | EVV 0x3F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
 | VPMAXUQ              | AVX512         | AVX512EVEX     | AVX512F_256    | 3f           | mm         | rrr        | EVV 0x3F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPMAXUQ              | AVX512         | AVX512EVEX     | AVX512F_512    | 3f           | 0b11       | rrr        | EVV 0x3F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
 | VPMAXUQ              | AVX512         | AVX512EVEX     | AVX512F_512    | 3f           | mm         | rrr        | EVV 0x3F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPMAXUW              | AVX            | AVX            |                | 3e           | mm         | rrr        | VV1 0x3E  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 MEM0:r:dq:u16
 | VPMAXUW              | AVX            | AVX            |                | 3e           | 0b11       | rrr        | VV1 0x3E  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 REG2=XMM_B():r:dq:u16
 | VPMAXUW              | AVX2           | AVX2           |                | 3e           | mm         | rrr        | VV1 0x3E  VL256 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=YMM_R():w:qq:u16 REG1=YMM_N():r:qq:u16 MEM0:r:qq:u16
 | VPMAXUW              | AVX2           | AVX2           |                | 3e           | 0b11       | rrr        | VV1 0x3E  VL256 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=YMM_R():w:qq:u16 REG1=YMM_N():r:qq:u16 REG2=YMM_B():r:qq:u16
 | VPMAXUW              | AVX512         | AVX512EVEX     | AVX512BW_128   | 3e           | 0b11       | rrr        | EVV 0x3E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                    | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
 | VPMAXUW              | AVX512         | AVX512EVEX     | AVX512BW_128   | 3e           | mm         | rrr        | EVV 0x3E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
 | VPMAXUW              | AVX512         | AVX512EVEX     | AVX512BW_256   | 3e           | 0b11       | rrr        | EVV 0x3E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                    | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
 | VPMAXUW              | AVX512         | AVX512EVEX     | AVX512BW_256   | 3e           | mm         | rrr        | EVV 0x3E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
 | VPMAXUW              | AVX512         | AVX512EVEX     | AVX512BW_512   | 3e           | 0b11       | rrr        | EVV 0x3E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                    | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
 | VPMAXUW              | AVX512         | AVX512EVEX     | AVX512BW_512   | 3e           | mm         | rrr        | EVV 0x3E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
 | VPMINSB              | AVX            | AVX            |                | 38           | mm         | rrr        | VV1 0x38  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 MEM0:r:dq:i8
 | VPMINSB              | AVX            | AVX            |                | 38           | 0b11       | rrr        | VV1 0x38  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 REG2=XMM_B():r:dq:i8
 | VPMINSB              | AVX2           | AVX2           |                | 38           | mm         | rrr        | VV1 0x38  VL256 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=YMM_R():w:qq:i8 REG1=YMM_N():r:qq:i8 MEM0:r:qq:i8
 | VPMINSB              | AVX2           | AVX2           |                | 38           | 0b11       | rrr        | VV1 0x38  VL256 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=YMM_R():w:qq:i8 REG1=YMM_N():r:qq:i8 REG2=YMM_B():r:qq:i8
 | VPMINSB              | AVX512         | AVX512EVEX     | AVX512BW_128   | 38           | 0b11       | rrr        | EVV 0x38 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                    | REG0=XMM_R3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i8 REG3=XMM_B3():r:dq:i8
 | VPMINSB              | AVX512         | AVX512EVEX     | AVX512BW_128   | 38           | mm         | rrr        | EVV 0x38 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i8 MEM0:r:dq:i8
 | VPMINSB              | AVX512         | AVX512EVEX     | AVX512BW_256   | 38           | 0b11       | rrr        | EVV 0x38 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                    | REG0=YMM_R3():w:qq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i8 REG3=YMM_B3():r:qq:i8
 | VPMINSB              | AVX512         | AVX512EVEX     | AVX512BW_256   | 38           | mm         | rrr        | EVV 0x38 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i8 MEM0:r:qq:i8
 | VPMINSB              | AVX512         | AVX512EVEX     | AVX512BW_512   | 38           | 0b11       | rrr        | EVV 0x38 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                    | REG0=ZMM_R3():w:zi8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi8 REG3=ZMM_B3():r:zi8
 | VPMINSB              | AVX512         | AVX512EVEX     | AVX512BW_512   | 38           | mm         | rrr        | EVV 0x38 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zi8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi8 MEM0:r:zd:i8
 | VPMINSD              | AVX            | AVX            |                | 39           | mm         | rrr        | VV1 0x39  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32
 | VPMINSD              | AVX            | AVX            |                | 39           | 0b11       | rrr        | VV1 0x39  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32
 | VPMINSD              | AVX2           | AVX2           |                | 39           | mm         | rrr        | VV1 0x39  VL256 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=YMM_R():w:qq:i32 REG1=YMM_N():r:qq:i32 MEM0:r:qq:i32
 | VPMINSD              | AVX2           | AVX2           |                | 39           | 0b11       | rrr        | VV1 0x39  VL256 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=YMM_R():w:qq:i32 REG1=YMM_N():r:qq:i32 REG2=YMM_B():r:qq:i32
 | VPMINSD              | AVX512         | AVX512EVEX     | AVX512F_128    | 39           | 0b11       | rrr        | EVV 0x39 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i32 REG3=XMM_B3():r:dq:i32
 | VPMINSD              | AVX512         | AVX512EVEX     | AVX512F_128    | 39           | mm         | rrr        | EVV 0x39 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i32 MEM0:r:vv:i32:TXT=BCASTSTR
 | VPMINSD              | AVX512         | AVX512EVEX     | AVX512F_256    | 39           | 0b11       | rrr        | EVV 0x39 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i32 REG3=YMM_B3():r:qq:i32
 | VPMINSD              | AVX512         | AVX512EVEX     | AVX512F_256    | 39           | mm         | rrr        | EVV 0x39 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i32 MEM0:r:vv:i32:TXT=BCASTSTR
 | VPMINSD              | AVX512         | AVX512EVEX     | AVX512F_512    | 39           | 0b11       | rrr        | EVV 0x39 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi32 REG3=ZMM_B3():r:zi32
 | VPMINSD              | AVX512         | AVX512EVEX     | AVX512F_512    | 39           | mm         | rrr        | EVV 0x39 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi32 MEM0:r:vv:i32:TXT=BCASTSTR
 | VPMINSD              | KNC            | KNCE           |                | 39           | mm         | rrr        | KVV 0x39 V0F38 REXW=0 V66  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT32()                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VPMINSD              | KNC            | KNCE           |                | 39           | 0b11       | rrr        | KVV 0x39 V0F38 REXW=0 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VPMINSD              | KNC            | KNCE           |                | 39           | 0b11       | rrr        | KVV 0x39 V0F38 REXW=0 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1                                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VPMINSQ              | AVX512         | AVX512EVEX     | AVX512F_128    | 39           | 0b11       | rrr        | EVV 0x39 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i64 REG3=XMM_B3():r:dq:i64
 | VPMINSQ              | AVX512         | AVX512EVEX     | AVX512F_128    | 39           | mm         | rrr        | EVV 0x39 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i64 MEM0:r:vv:i64:TXT=BCASTSTR
 | VPMINSQ              | AVX512         | AVX512EVEX     | AVX512F_256    | 39           | 0b11       | rrr        | EVV 0x39 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i64 REG3=YMM_B3():r:qq:i64
 | VPMINSQ              | AVX512         | AVX512EVEX     | AVX512F_256    | 39           | mm         | rrr        | EVV 0x39 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i64 MEM0:r:vv:i64:TXT=BCASTSTR
 | VPMINSQ              | AVX512         | AVX512EVEX     | AVX512F_512    | 39           | 0b11       | rrr        | EVV 0x39 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi64 REG3=ZMM_B3():r:zi64
 | VPMINSQ              | AVX512         | AVX512EVEX     | AVX512F_512    | 39           | mm         | rrr        | EVV 0x39 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi64 MEM0:r:vv:i64:TXT=BCASTSTR
 | VPMINSW              | AVX            | AVX            |                | ea           | mm         | rrr        | VV1 0xEA  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
 | VPMINSW              | AVX            | AVX            |                | ea           | 0b11       | rrr        | VV1 0xEA  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
 | VPMINSW              | AVX2           | AVX2           |                | ea           | mm         | rrr        | VV1 0xEA  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:i16 REG1=YMM_N():r:qq:i16 MEM0:r:qq:i16
 | VPMINSW              | AVX2           | AVX2           |                | ea           | 0b11       | rrr        | VV1 0xEA  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:i16 REG1=YMM_N():r:qq:i16 REG2=YMM_B():r:qq:i16
 | VPMINSW              | AVX512         | AVX512EVEX     | AVX512BW_128   | ea           | 0b11       | rrr        | EVV 0xEA V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                      | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 REG3=XMM_B3():r:dq:i16
 | VPMINSW              | AVX512         | AVX512EVEX     | AVX512BW_128   | ea           | mm         | rrr        | EVV 0xEA V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 MEM0:r:dq:i16
 | VPMINSW              | AVX512         | AVX512EVEX     | AVX512BW_256   | ea           | 0b11       | rrr        | EVV 0xEA V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                      | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 REG3=YMM_B3():r:qq:i16
 | VPMINSW              | AVX512         | AVX512EVEX     | AVX512BW_256   | ea           | mm         | rrr        | EVV 0xEA V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 MEM0:r:qq:i16
 | VPMINSW              | AVX512         | AVX512EVEX     | AVX512BW_512   | ea           | 0b11       | rrr        | EVV 0xEA V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                      | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 REG3=ZMM_B3():r:zi16
 | VPMINSW              | AVX512         | AVX512EVEX     | AVX512BW_512   | ea           | mm         | rrr        | EVV 0xEA V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 MEM0:r:zd:i16
 | VPMINUB              | AVX            | AVX            |                | da           | mm         | rrr        | VV1 0xDA  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 MEM0:r:dq:u8
 | VPMINUB              | AVX            | AVX            |                | da           | 0b11       | rrr        | VV1 0xDA  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:u8
 | VPMINUB              | AVX2           | AVX2           |                | da           | mm         | rrr        | VV1 0xDA  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u8 REG1=YMM_N():r:qq:u8 MEM0:r:qq:u8
 | VPMINUB              | AVX2           | AVX2           |                | da           | 0b11       | rrr        | VV1 0xDA  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:u8 REG1=YMM_N():r:qq:u8 REG2=YMM_B():r:qq:u8
 | VPMINUB              | AVX512         | AVX512EVEX     | AVX512BW_128   | da           | 0b11       | rrr        | EVV 0xDA V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                      | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
 | VPMINUB              | AVX512         | AVX512EVEX     | AVX512BW_128   | da           | mm         | rrr        | EVV 0xDA V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
 | VPMINUB              | AVX512         | AVX512EVEX     | AVX512BW_256   | da           | 0b11       | rrr        | EVV 0xDA V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                      | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
 | VPMINUB              | AVX512         | AVX512EVEX     | AVX512BW_256   | da           | mm         | rrr        | EVV 0xDA V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
 | VPMINUB              | AVX512         | AVX512EVEX     | AVX512BW_512   | da           | 0b11       | rrr        | EVV 0xDA V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                      | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
 | VPMINUB              | AVX512         | AVX512EVEX     | AVX512BW_512   | da           | mm         | rrr        | EVV 0xDA V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
 | VPMINUD              | AVX            | AVX            |                | 3b           | mm         | rrr        | VV1 0x3B  V66 V0F38 VL128  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 MEM0:r:dq:u32
 | VPMINUD              | AVX            | AVX            |                | 3b           | 0b11       | rrr        | VV1 0x3B  V66 V0F38 VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 REG2=XMM_B():r:dq:u32
 | VPMINUD              | AVX2           | AVX2           |                | 3b           | mm         | rrr        | VV1 0x3B  V66 V0F38 VL256  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=YMM_R():w:qq:u32 REG1=YMM_N():r:qq:u32 MEM0:r:qq:u32
 | VPMINUD              | AVX2           | AVX2           |                | 3b           | 0b11       | rrr        | VV1 0x3B  V66 V0F38 VL256 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=YMM_R():w:qq:u32 REG1=YMM_N():r:qq:u32 REG2=YMM_B():r:qq:u32
 | VPMINUD              | AVX512         | AVX512EVEX     | AVX512F_128    | 3b           | 0b11       | rrr        | EVV 0x3B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
 | VPMINUD              | AVX512         | AVX512EVEX     | AVX512F_128    | 3b           | mm         | rrr        | EVV 0x3B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPMINUD              | AVX512         | AVX512EVEX     | AVX512F_256    | 3b           | 0b11       | rrr        | EVV 0x3B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
 | VPMINUD              | AVX512         | AVX512EVEX     | AVX512F_256    | 3b           | mm         | rrr        | EVV 0x3B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPMINUD              | AVX512         | AVX512EVEX     | AVX512F_512    | 3b           | 0b11       | rrr        | EVV 0x3B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
 | VPMINUD              | AVX512         | AVX512EVEX     | AVX512F_512    | 3b           | mm         | rrr        | EVV 0x3B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPMINUD              | KNC            | KNCE           |                | 3b           | mm         | rrr        | KVV 0x3B V0F38 REXW=0 V66  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT32()                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VPMINUD              | KNC            | KNCE           |                | 3b           | 0b11       | rrr        | KVV 0x3B V0F38 REXW=0 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VPMINUD              | KNC            | KNCE           |                | 3b           | 0b11       | rrr        | KVV 0x3B V0F38 REXW=0 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1                                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VPMINUQ              | AVX512         | AVX512EVEX     | AVX512F_128    | 3b           | 0b11       | rrr        | EVV 0x3B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
 | VPMINUQ              | AVX512         | AVX512EVEX     | AVX512F_128    | 3b           | mm         | rrr        | EVV 0x3B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPMINUQ              | AVX512         | AVX512EVEX     | AVX512F_256    | 3b           | 0b11       | rrr        | EVV 0x3B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
 | VPMINUQ              | AVX512         | AVX512EVEX     | AVX512F_256    | 3b           | mm         | rrr        | EVV 0x3B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPMINUQ              | AVX512         | AVX512EVEX     | AVX512F_512    | 3b           | 0b11       | rrr        | EVV 0x3B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
 | VPMINUQ              | AVX512         | AVX512EVEX     | AVX512F_512    | 3b           | mm         | rrr        | EVV 0x3B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPMINUW              | AVX            | AVX            |                | 3a           | mm         | rrr        | VV1 0x3A  V66 V0F38 VL128  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 MEM0:r:dq:u16
 | VPMINUW              | AVX            | AVX            |                | 3a           | 0b11       | rrr        | VV1 0x3A  V66 V0F38 VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 REG2=XMM_B():r:dq:u16
 | VPMINUW              | AVX2           | AVX2           |                | 3a           | mm         | rrr        | VV1 0x3A  V66 V0F38 VL256  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=YMM_R():w:qq:u16 REG1=YMM_N():r:qq:u16 MEM0:r:qq:u16
 | VPMINUW              | AVX2           | AVX2           |                | 3a           | 0b11       | rrr        | VV1 0x3A  V66 V0F38 VL256 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=YMM_R():w:qq:u16 REG1=YMM_N():r:qq:u16 REG2=YMM_B():r:qq:u16
 | VPMINUW              | AVX512         | AVX512EVEX     | AVX512BW_128   | 3a           | 0b11       | rrr        | EVV 0x3A V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                    | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
 | VPMINUW              | AVX512         | AVX512EVEX     | AVX512BW_128   | 3a           | mm         | rrr        | EVV 0x3A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
 | VPMINUW              | AVX512         | AVX512EVEX     | AVX512BW_256   | 3a           | 0b11       | rrr        | EVV 0x3A V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                    | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
 | VPMINUW              | AVX512         | AVX512EVEX     | AVX512BW_256   | 3a           | mm         | rrr        | EVV 0x3A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
 | VPMINUW              | AVX512         | AVX512EVEX     | AVX512BW_512   | 3a           | 0b11       | rrr        | EVV 0x3A V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                    | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
 | VPMINUW              | AVX512         | AVX512EVEX     | AVX512BW_512   | 3a           | mm         | rrr        | EVV 0x3A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
 | VPMOVB2M             | DATAXFER       | AVX512EVEX     | AVX512BW_128   | 29           | 0b11       | rrr        | EVV 0x29 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR  ZEROING=0 MASK=0      | REG0=MASK_R():w:mskw REG1=XMM_B3():r:dq:u8
 | VPMOVB2M             | DATAXFER       | AVX512EVEX     | AVX512BW_256   | 29           | 0b11       | rrr        | EVV 0x29 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR  ZEROING=0 MASK=0      | REG0=MASK_R():w:mskw REG1=YMM_B3():r:qq:u8
 | VPMOVB2M             | DATAXFER       | AVX512EVEX     | AVX512BW_512   | 29           | 0b11       | rrr        | EVV 0x29 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR  ZEROING=0 MASK=0      | REG0=MASK_R():w:mskw REG1=ZMM_B3():r:zu8
 | VPMOVD2M             | DATAXFER       | AVX512EVEX     | AVX512DQ_128   | 39           | 0b11       | rrr        | EVV 0x39 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR  ZEROING=0 MASK=0      | REG0=MASK_R():w:mskw REG1=XMM_B3():r:dq:u32
 | VPMOVD2M             | DATAXFER       | AVX512EVEX     | AVX512DQ_256   | 39           | 0b11       | rrr        | EVV 0x39 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR  ZEROING=0 MASK=0      | REG0=MASK_R():w:mskw REG1=YMM_B3():r:qq:u32
 | VPMOVD2M             | DATAXFER       | AVX512EVEX     | AVX512DQ_512   | 39           | 0b11       | rrr        | EVV 0x39 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR  ZEROING=0 MASK=0      | REG0=MASK_R():w:mskw REG1=ZMM_B3():r:zu32
 | VPMOVDB              | DATAXFER       | AVX512EVEX     | AVX512F_128    | 31           | 0b11       | rrr        | EVV 0x31 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_B3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u32
 | VPMOVDB              | DATAXFER       | AVX512EVEX     | AVX512F_128    | 31           | mm         | rrr        | EVV 0x31 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_QUARTERMEM() | MEM0:w:d:u8 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u32
 | VPMOVDB              | DATAXFER       | AVX512EVEX     | AVX512F_256    | 31           | 0b11       | rrr        | EVV 0x31 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=XMM_B3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u32
 | VPMOVDB              | DATAXFER       | AVX512EVEX     | AVX512F_256    | 31           | mm         | rrr        | EVV 0x31 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_QUARTERMEM() | MEM0:w:q:u8 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u32
 | VPMOVDB              | DATAXFER       | AVX512EVEX     | AVX512F_512    | 31           | 0b11       | rrr        | EVV 0x31 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=XMM_B3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu32
 | VPMOVDB              | DATAXFER       | AVX512EVEX     | AVX512F_512    | 31           | mm         | rrr        | EVV 0x31 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_QUARTERMEM() | MEM0:w:dq:u8 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu32
 | VPMOVDW              | DATAXFER       | AVX512EVEX     | AVX512F_128    | 33           | 0b11       | rrr        | EVV 0x33 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_B3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u32
 | VPMOVDW              | DATAXFER       | AVX512EVEX     | AVX512F_128    | 33           | mm         | rrr        | EVV 0x33 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_HALFMEM() | MEM0:w:q:u16 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u32
 | VPMOVDW              | DATAXFER       | AVX512EVEX     | AVX512F_256    | 33           | 0b11       | rrr        | EVV 0x33 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=XMM_B3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u32
 | VPMOVDW              | DATAXFER       | AVX512EVEX     | AVX512F_256    | 33           | mm         | rrr        | EVV 0x33 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_HALFMEM() | MEM0:w:dq:u16 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u32
 | VPMOVDW              | DATAXFER       | AVX512EVEX     | AVX512F_512    | 33           | 0b11       | rrr        | EVV 0x33 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=YMM_B3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu32
 | VPMOVDW              | DATAXFER       | AVX512EVEX     | AVX512F_512    | 33           | mm         | rrr        | EVV 0x33 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_HALFMEM() | MEM0:w:qq:u16 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu32
 | VPMOVM2B             | DATAXFER       | AVX512EVEX     | AVX512BW_128   | 28           | 0b11       | rrr        | EVV 0x28 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR  ZEROING=0 MASK=0      | REG0=XMM_R3():w:dq:u8 REG1=MASK_B():r:mskw
 | VPMOVM2B             | DATAXFER       | AVX512EVEX     | AVX512BW_256   | 28           | 0b11       | rrr        | EVV 0x28 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR  ZEROING=0 MASK=0      | REG0=YMM_R3():w:qq:u8 REG1=MASK_B():r:mskw
 | VPMOVM2B             | DATAXFER       | AVX512EVEX     | AVX512BW_512   | 28           | 0b11       | rrr        | EVV 0x28 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR  ZEROING=0 MASK=0      | REG0=ZMM_R3():w:zu8 REG1=MASK_B():r:mskw
 | VPMOVM2D             | DATAXFER       | AVX512EVEX     | AVX512DQ_128   | 38           | 0b11       | rrr        | EVV 0x38 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR  ZEROING=0 MASK=0      | REG0=XMM_R3():w:dq:u32 REG1=MASK_B():r:mskw
 | VPMOVM2D             | DATAXFER       | AVX512EVEX     | AVX512DQ_256   | 38           | 0b11       | rrr        | EVV 0x38 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR  ZEROING=0 MASK=0      | REG0=YMM_R3():w:qq:u32 REG1=MASK_B():r:mskw
 | VPMOVM2D             | DATAXFER       | AVX512EVEX     | AVX512DQ_512   | 38           | 0b11       | rrr        | EVV 0x38 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR  ZEROING=0 MASK=0      | REG0=ZMM_R3():w:zu32 REG1=MASK_B():r:mskw
 | VPMOVM2Q             | DATAXFER       | AVX512EVEX     | AVX512DQ_128   | 38           | 0b11       | rrr        | EVV 0x38 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR  ZEROING=0 MASK=0      | REG0=XMM_R3():w:dq:u64 REG1=MASK_B():r:mskw
 | VPMOVM2Q             | DATAXFER       | AVX512EVEX     | AVX512DQ_256   | 38           | 0b11       | rrr        | EVV 0x38 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR  ZEROING=0 MASK=0      | REG0=YMM_R3():w:qq:u64 REG1=MASK_B():r:mskw
 | VPMOVM2Q             | DATAXFER       | AVX512EVEX     | AVX512DQ_512   | 38           | 0b11       | rrr        | EVV 0x38 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR  ZEROING=0 MASK=0      | REG0=ZMM_R3():w:zu64 REG1=MASK_B():r:mskw
 | VPMOVM2W             | DATAXFER       | AVX512EVEX     | AVX512BW_128   | 28           | 0b11       | rrr        | EVV 0x28 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR  ZEROING=0 MASK=0      | REG0=XMM_R3():w:dq:u16 REG1=MASK_B():r:mskw
 | VPMOVM2W             | DATAXFER       | AVX512EVEX     | AVX512BW_256   | 28           | 0b11       | rrr        | EVV 0x28 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR  ZEROING=0 MASK=0      | REG0=YMM_R3():w:qq:u16 REG1=MASK_B():r:mskw
 | VPMOVM2W             | DATAXFER       | AVX512EVEX     | AVX512BW_512   | 28           | 0b11       | rrr        | EVV 0x28 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR  ZEROING=0 MASK=0      | REG0=ZMM_R3():w:zu16 REG1=MASK_B():r:mskw
 | VPMOVMSKB            | AVX            | AVX            |                | d7           | 0b11       | rrr        | VV1 0xD7  VL128 V66 V0F  NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=GPR32_R():w:d:u32   REG1=XMM_B():r:dq:i8
 | VPMOVMSKB            | AVX2           | AVX2           |                | d7           | 0b11       | rrr        | VV1 0xD7  VL256 V66 V0F  NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=GPR32_R():w:d:u32   REG1=YMM_B():r:qq:i8
 | VPMOVQ2M             | DATAXFER       | AVX512EVEX     | AVX512DQ_128   | 39           | 0b11       | rrr        | EVV 0x39 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR  ZEROING=0 MASK=0      | REG0=MASK_R():w:mskw REG1=XMM_B3():r:dq:u64
 | VPMOVQ2M             | DATAXFER       | AVX512EVEX     | AVX512DQ_256   | 39           | 0b11       | rrr        | EVV 0x39 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR  ZEROING=0 MASK=0      | REG0=MASK_R():w:mskw REG1=YMM_B3():r:qq:u64
 | VPMOVQ2M             | DATAXFER       | AVX512EVEX     | AVX512DQ_512   | 39           | 0b11       | rrr        | EVV 0x39 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR  ZEROING=0 MASK=0      | REG0=MASK_R():w:mskw REG1=ZMM_B3():r:zu64
 | VPMOVQB              | DATAXFER       | AVX512EVEX     | AVX512F_128    | 32           | 0b11       | rrr        | EVV 0x32 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_B3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u64
 | VPMOVQB              | DATAXFER       | AVX512EVEX     | AVX512F_128    | 32           | mm         | rrr        | EVV 0x32 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_EIGHTHMEM() | MEM0:w:wrd:u8 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u64
 | VPMOVQB              | DATAXFER       | AVX512EVEX     | AVX512F_256    | 32           | 0b11       | rrr        | EVV 0x32 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=XMM_B3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u64
 | VPMOVQB              | DATAXFER       | AVX512EVEX     | AVX512F_256    | 32           | mm         | rrr        | EVV 0x32 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_EIGHTHMEM() | MEM0:w:d:u8 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u64
 | VPMOVQB              | DATAXFER       | AVX512EVEX     | AVX512F_512    | 32           | 0b11       | rrr        | EVV 0x32 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=XMM_B3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu64
 | VPMOVQB              | DATAXFER       | AVX512EVEX     | AVX512F_512    | 32           | mm         | rrr        | EVV 0x32 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_EIGHTHMEM() | MEM0:w:q:u8 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu64
 | VPMOVQD              | DATAXFER       | AVX512EVEX     | AVX512F_128    | 35           | 0b11       | rrr        | EVV 0x35 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_B3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u64
 | VPMOVQD              | DATAXFER       | AVX512EVEX     | AVX512F_128    | 35           | mm         | rrr        | EVV 0x35 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_HALFMEM() | MEM0:w:q:u32 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u64
 | VPMOVQD              | DATAXFER       | AVX512EVEX     | AVX512F_256    | 35           | 0b11       | rrr        | EVV 0x35 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=XMM_B3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u64
 | VPMOVQD              | DATAXFER       | AVX512EVEX     | AVX512F_256    | 35           | mm         | rrr        | EVV 0x35 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_HALFMEM() | MEM0:w:dq:u32 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u64
 | VPMOVQD              | DATAXFER       | AVX512EVEX     | AVX512F_512    | 35           | 0b11       | rrr        | EVV 0x35 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=YMM_B3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu64
 | VPMOVQD              | DATAXFER       | AVX512EVEX     | AVX512F_512    | 35           | mm         | rrr        | EVV 0x35 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_HALFMEM() | MEM0:w:qq:u32 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu64
 | VPMOVQW              | DATAXFER       | AVX512EVEX     | AVX512F_128    | 34           | 0b11       | rrr        | EVV 0x34 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_B3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u64
 | VPMOVQW              | DATAXFER       | AVX512EVEX     | AVX512F_128    | 34           | mm         | rrr        | EVV 0x34 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_QUARTERMEM() | MEM0:w:d:u16 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u64
 | VPMOVQW              | DATAXFER       | AVX512EVEX     | AVX512F_256    | 34           | 0b11       | rrr        | EVV 0x34 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=XMM_B3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u64
 | VPMOVQW              | DATAXFER       | AVX512EVEX     | AVX512F_256    | 34           | mm         | rrr        | EVV 0x34 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_QUARTERMEM() | MEM0:w:q:u16 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u64
 | VPMOVQW              | DATAXFER       | AVX512EVEX     | AVX512F_512    | 34           | 0b11       | rrr        | EVV 0x34 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=XMM_B3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu64
 | VPMOVQW              | DATAXFER       | AVX512EVEX     | AVX512F_512    | 34           | mm         | rrr        | EVV 0x34 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_QUARTERMEM() | MEM0:w:dq:u16 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu64
 | VPMOVSDB             | DATAXFER       | AVX512EVEX     | AVX512F_128    | 21           | 0b11       | rrr        | EVV 0x21 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_B3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:i32
 | VPMOVSDB             | DATAXFER       | AVX512EVEX     | AVX512F_128    | 21           | mm         | rrr        | EVV 0x21 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_QUARTERMEM() | MEM0:w:d:i8 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:i32
 | VPMOVSDB             | DATAXFER       | AVX512EVEX     | AVX512F_256    | 21           | 0b11       | rrr        | EVV 0x21 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=XMM_B3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:i32
 | VPMOVSDB             | DATAXFER       | AVX512EVEX     | AVX512F_256    | 21           | mm         | rrr        | EVV 0x21 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_QUARTERMEM() | MEM0:w:q:i8 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:i32
 | VPMOVSDB             | DATAXFER       | AVX512EVEX     | AVX512F_512    | 21           | 0b11       | rrr        | EVV 0x21 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=XMM_B3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zi32
 | VPMOVSDB             | DATAXFER       | AVX512EVEX     | AVX512F_512    | 21           | mm         | rrr        | EVV 0x21 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_QUARTERMEM() | MEM0:w:dq:i8 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zi32
 | VPMOVSDW             | DATAXFER       | AVX512EVEX     | AVX512F_128    | 23           | 0b11       | rrr        | EVV 0x23 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_B3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:i32
 | VPMOVSDW             | DATAXFER       | AVX512EVEX     | AVX512F_128    | 23           | mm         | rrr        | EVV 0x23 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_HALFMEM() | MEM0:w:q:i16 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:i32
 | VPMOVSDW             | DATAXFER       | AVX512EVEX     | AVX512F_256    | 23           | 0b11       | rrr        | EVV 0x23 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=XMM_B3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:i32
 | VPMOVSDW             | DATAXFER       | AVX512EVEX     | AVX512F_256    | 23           | mm         | rrr        | EVV 0x23 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_HALFMEM() | MEM0:w:dq:i16 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:i32
 | VPMOVSDW             | DATAXFER       | AVX512EVEX     | AVX512F_512    | 23           | 0b11       | rrr        | EVV 0x23 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=YMM_B3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zi32
 | VPMOVSDW             | DATAXFER       | AVX512EVEX     | AVX512F_512    | 23           | mm         | rrr        | EVV 0x23 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_HALFMEM() | MEM0:w:qq:i16 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zi32
 | VPMOVSQB             | DATAXFER       | AVX512EVEX     | AVX512F_128    | 22           | 0b11       | rrr        | EVV 0x22 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_B3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:i64
 | VPMOVSQB             | DATAXFER       | AVX512EVEX     | AVX512F_128    | 22           | mm         | rrr        | EVV 0x22 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_EIGHTHMEM() | MEM0:w:wrd:i8 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:i64
 | VPMOVSQB             | DATAXFER       | AVX512EVEX     | AVX512F_256    | 22           | 0b11       | rrr        | EVV 0x22 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=XMM_B3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:i64
 | VPMOVSQB             | DATAXFER       | AVX512EVEX     | AVX512F_256    | 22           | mm         | rrr        | EVV 0x22 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_EIGHTHMEM() | MEM0:w:d:i8 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:i64
 | VPMOVSQB             | DATAXFER       | AVX512EVEX     | AVX512F_512    | 22           | 0b11       | rrr        | EVV 0x22 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=XMM_B3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zi64
 | VPMOVSQB             | DATAXFER       | AVX512EVEX     | AVX512F_512    | 22           | mm         | rrr        | EVV 0x22 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_EIGHTHMEM() | MEM0:w:q:i8 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zi64
 | VPMOVSQD             | DATAXFER       | AVX512EVEX     | AVX512F_128    | 25           | 0b11       | rrr        | EVV 0x25 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_B3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:i64
 | VPMOVSQD             | DATAXFER       | AVX512EVEX     | AVX512F_128    | 25           | mm         | rrr        | EVV 0x25 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_HALFMEM() | MEM0:w:q:i32 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:i64
 | VPMOVSQD             | DATAXFER       | AVX512EVEX     | AVX512F_256    | 25           | 0b11       | rrr        | EVV 0x25 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=XMM_B3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:i64
 | VPMOVSQD             | DATAXFER       | AVX512EVEX     | AVX512F_256    | 25           | mm         | rrr        | EVV 0x25 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_HALFMEM() | MEM0:w:dq:i32 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:i64
 | VPMOVSQD             | DATAXFER       | AVX512EVEX     | AVX512F_512    | 25           | 0b11       | rrr        | EVV 0x25 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=YMM_B3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zi64
 | VPMOVSQD             | DATAXFER       | AVX512EVEX     | AVX512F_512    | 25           | mm         | rrr        | EVV 0x25 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_HALFMEM() | MEM0:w:qq:i32 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zi64
 | VPMOVSQW             | DATAXFER       | AVX512EVEX     | AVX512F_128    | 24           | 0b11       | rrr        | EVV 0x24 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_B3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:i64
 | VPMOVSQW             | DATAXFER       | AVX512EVEX     | AVX512F_128    | 24           | mm         | rrr        | EVV 0x24 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_QUARTERMEM() | MEM0:w:d:i16 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:i64
 | VPMOVSQW             | DATAXFER       | AVX512EVEX     | AVX512F_256    | 24           | 0b11       | rrr        | EVV 0x24 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=XMM_B3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:i64
 | VPMOVSQW             | DATAXFER       | AVX512EVEX     | AVX512F_256    | 24           | mm         | rrr        | EVV 0x24 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_QUARTERMEM() | MEM0:w:q:i16 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:i64
 | VPMOVSQW             | DATAXFER       | AVX512EVEX     | AVX512F_512    | 24           | 0b11       | rrr        | EVV 0x24 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=XMM_B3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zi64
 | VPMOVSQW             | DATAXFER       | AVX512EVEX     | AVX512F_512    | 24           | mm         | rrr        | EVV 0x24 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_QUARTERMEM() | MEM0:w:dq:i16 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zi64
 | VPMOVSWB             | DATAXFER       | AVX512EVEX     | AVX512BW_128   | 20           | 0b11       | rrr        | EVV 0x20 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_B3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:i16
 | VPMOVSWB             | DATAXFER       | AVX512EVEX     | AVX512BW_128   | 20           | mm         | rrr        | EVV 0x20 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_HALFMEM() | MEM0:w:q:i8 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:i16
 | VPMOVSWB             | DATAXFER       | AVX512EVEX     | AVX512BW_256   | 20           | 0b11       | rrr        | EVV 0x20 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=XMM_B3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:i16
 | VPMOVSWB             | DATAXFER       | AVX512EVEX     | AVX512BW_256   | 20           | mm         | rrr        | EVV 0x20 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_HALFMEM() | MEM0:w:dq:i8 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:i16
 | VPMOVSWB             | DATAXFER       | AVX512EVEX     | AVX512BW_512   | 20           | 0b11       | rrr        | EVV 0x20 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=YMM_B3():w:qq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zi16
 | VPMOVSWB             | DATAXFER       | AVX512EVEX     | AVX512BW_512   | 20           | mm         | rrr        | EVV 0x20 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_HALFMEM() | MEM0:w:qq:i8 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zi16
 | VPMOVSXBD            | AVX            | AVX            |                | 21           | 0b11       | rrr        | VV1 0x21  VL128 V66 V0F38 NOVSR  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                    | REG0=XMM_R():w:dq:i32   REG1=XMM_B():r:d:i8
 | VPMOVSXBD            | AVX            | AVX            |                | 21           | mm         | rrr        | VV1 0x21  VL128 V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                              | REG0=XMM_R():w:dq:i32   MEM0:r:d:i8
 | VPMOVSXBD            | AVX2           | AVX2           |                | 21           | 0b11       | rrr        | VV1 0x21   VL256  V66 V0F38 NOVSR  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                  | REG0=YMM_R():w:qq:i32   REG1=XMM_B():r:q:i8
 | VPMOVSXBD            | AVX2           | AVX2           |                | 21           | mm         | rrr        | VV1 0x21   VL256  V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                            | REG0=YMM_R():w:qq:i32   MEM0:r:q:i8
 | VPMOVSXBD            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 21           | 0b11       | rrr        | EVV 0x21 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR                          | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i8
 | VPMOVSXBD            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 21           | mm         | rrr        | EVV 0x21 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128    NOEVSR  ESIZE_8_BITS() NELEM_QUARTERMEM() | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:d:i8
 | VPMOVSXBD            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 21           | 0b11       | rrr        | EVV 0x21 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256    NOEVSR                          | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i8
 | VPMOVSXBD            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 21           | mm         | rrr        | EVV 0x21 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256    NOEVSR  ESIZE_8_BITS() NELEM_QUARTERMEM() | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:i8
 | VPMOVSXBD            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 21           | 0b11       | rrr        | EVV 0x21 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512    NOEVSR                          | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i8
 | VPMOVSXBD            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 21           | mm         | rrr        | EVV 0x21 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512    NOEVSR  ESIZE_8_BITS() NELEM_QUARTERMEM() | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:i8
 | VPMOVSXBQ            | AVX            | AVX            |                | 22           | 0b11       | rrr        | VV1 0x22  VL128 V66 V0F38 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=XMM_R():w:dq:i64   REG1=XMM_B():r:w:i8
 | VPMOVSXBQ            | AVX            | AVX            |                | 22           | mm         | rrr        | VV1 0x22  VL128 V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                              | REG0=XMM_R():w:dq:i64   MEM0:r:w:i8
 | VPMOVSXBQ            | AVX2           | AVX2           |                | 22           | 0b11       | rrr        | VV1 0x22   VL256  V66 V0F38 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                   | REG0=YMM_R():w:qq:i64   REG1=XMM_B():r:d:i8
 | VPMOVSXBQ            | AVX2           | AVX2           |                | 22           | mm         | rrr        | VV1 0x22   VL256  V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                            | REG0=YMM_R():w:qq:i64   MEM0:r:d:i8
 | VPMOVSXBQ            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 22           | 0b11       | rrr        | EVV 0x22 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR                          | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i8
 | VPMOVSXBQ            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 22           | mm         | rrr        | EVV 0x22 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128    NOEVSR  ESIZE_8_BITS() NELEM_EIGHTHMEM() | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:wrd:i8
 | VPMOVSXBQ            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 22           | 0b11       | rrr        | EVV 0x22 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256    NOEVSR                          | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i8
 | VPMOVSXBQ            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 22           | mm         | rrr        | EVV 0x22 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256    NOEVSR  ESIZE_8_BITS() NELEM_EIGHTHMEM() | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:d:i8
 | VPMOVSXBQ            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 22           | 0b11       | rrr        | EVV 0x22 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512    NOEVSR                          | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i8
 | VPMOVSXBQ            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 22           | mm         | rrr        | EVV 0x22 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512    NOEVSR  ESIZE_8_BITS() NELEM_EIGHTHMEM() | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:i8
 | VPMOVSXBW            | AVX            | AVX            |                | 20           | 0b11       | rrr        | VV1 0x20  VL128 V66 V0F38 NOVSR  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                    | REG0=XMM_R():w:dq:i16   REG1=XMM_B():r:q:i8
 | VPMOVSXBW            | AVX            | AVX            |                | 20           | mm         | rrr        | VV1 0x20  VL128 V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                              | REG0=XMM_R():w:dq:i16  MEM0:r:q:i8
 | VPMOVSXBW            | AVX2           | AVX2           |                | 20           | 0b11       | rrr        | VV1 0x20   VL256  V66 V0F38 NOVSR  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                  | REG0=YMM_R():w:qq:i16   REG1=XMM_B():r:dq:i8
 | VPMOVSXBW            | AVX2           | AVX2           |                | 20           | mm         | rrr        | VV1 0x20   VL256  V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                            | REG0=YMM_R():w:qq:i16   MEM0:r:dq:i8
 | VPMOVSXBW            | DATAXFER       | AVX512EVEX     | AVX512BW_128   | 20           | 0b11       | rrr        | EVV 0x20 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR                          | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i8
 | VPMOVSXBW            | DATAXFER       | AVX512EVEX     | AVX512BW_128   | 20           | mm         | rrr        | EVV 0x20 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128    NOEVSR  ESIZE_8_BITS() NELEM_HALFMEM() | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:i8
 | VPMOVSXBW            | DATAXFER       | AVX512EVEX     | AVX512BW_256   | 20           | 0b11       | rrr        | EVV 0x20 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256    NOEVSR                          | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i8
 | VPMOVSXBW            | DATAXFER       | AVX512EVEX     | AVX512BW_256   | 20           | mm         | rrr        | EVV 0x20 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256    NOEVSR  ESIZE_8_BITS() NELEM_HALFMEM() | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:i8
 | VPMOVSXBW            | DATAXFER       | AVX512EVEX     | AVX512BW_512   | 20           | 0b11       | rrr        | EVV 0x20 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512    NOEVSR                          | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:i8
 | VPMOVSXBW            | DATAXFER       | AVX512EVEX     | AVX512BW_512   | 20           | mm         | rrr        | EVV 0x20 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512    NOEVSR  ESIZE_8_BITS() NELEM_HALFMEM() | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:i8
 | VPMOVSXDQ            | AVX            | AVX            |                | 25           | 0b11       | rrr        | VV1 0x25  VL128 V66 V0F38 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=XMM_R():w:dq:i64   REG1=XMM_B():r:q:i32
 | VPMOVSXDQ            | AVX            | AVX            |                | 25           | mm         | rrr        | VV1 0x25  VL128 V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                              | REG0=XMM_R():w:dq:i64   MEM0:r:q:i32
 | VPMOVSXDQ            | AVX2           | AVX2           |                | 25           | 0b11       | rrr        | VV1 0x25   VL256  V66 V0F38 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                   | REG0=YMM_R():w:qq:i64   REG1=XMM_B():r:dq:i32
 | VPMOVSXDQ            | AVX2           | AVX2           |                | 25           | mm         | rrr        | VV1 0x25   VL256  V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                            | REG0=YMM_R():w:qq:i64   MEM0:r:dq:i32
 | VPMOVSXDQ            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 25           | 0b11       | rrr        | EVV 0x25 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i32
 | VPMOVSXDQ            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 25           | mm         | rrr        | EVV 0x25 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALFMEM() | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:i32
 | VPMOVSXDQ            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 25           | 0b11       | rrr        | EVV 0x25 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i32
 | VPMOVSXDQ            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 25           | mm         | rrr        | EVV 0x25 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALFMEM() | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:i32
 | VPMOVSXDQ            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 25           | 0b11       | rrr        | EVV 0x25 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:i32
 | VPMOVSXDQ            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 25           | mm         | rrr        | EVV 0x25 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALFMEM() | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:i32
 | VPMOVSXWD            | AVX            | AVX            |                | 23           | 0b11       | rrr        | VV1 0x23  VL128 V66 V0F38 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=XMM_R():w:dq:i32   REG1=XMM_B():r:q:i16
 | VPMOVSXWD            | AVX            | AVX            |                | 23           | mm         | rrr        | VV1 0x23  VL128 V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                              | REG0=XMM_R():w:dq:i32   MEM0:r:q:i16
 | VPMOVSXWD            | AVX2           | AVX2           |                | 23           | 0b11       | rrr        | VV1 0x23   VL256  V66 V0F38 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                   | REG0=YMM_R():w:qq:i32   REG1=XMM_B():r:dq:i16
 | VPMOVSXWD            | AVX2           | AVX2           |                | 23           | mm         | rrr        | VV1 0x23   VL256  V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                            | REG0=YMM_R():w:qq:i32   MEM0:r:dq:i16
 | VPMOVSXWD            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 23           | 0b11       | rrr        | EVV 0x23 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR                          | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i16
 | VPMOVSXWD            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 23           | mm         | rrr        | EVV 0x23 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128    NOEVSR  ESIZE_16_BITS() NELEM_HALFMEM() | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:i16
 | VPMOVSXWD            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 23           | 0b11       | rrr        | EVV 0x23 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256    NOEVSR                          | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i16
 | VPMOVSXWD            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 23           | mm         | rrr        | EVV 0x23 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256    NOEVSR  ESIZE_16_BITS() NELEM_HALFMEM() | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:i16
 | VPMOVSXWD            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 23           | 0b11       | rrr        | EVV 0x23 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512    NOEVSR                          | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:i16
 | VPMOVSXWD            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 23           | mm         | rrr        | EVV 0x23 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512    NOEVSR  ESIZE_16_BITS() NELEM_HALFMEM() | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:i16
 | VPMOVSXWQ            | AVX            | AVX            |                | 24           | 0b11       | rrr        | VV1 0x24  VL128 V66 V0F38 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=XMM_R():w:dq:i64   REG1=XMM_B():r:d:i16
 | VPMOVSXWQ            | AVX            | AVX            |                | 24           | mm         | rrr        | VV1 0x24  VL128 V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                              | REG0=XMM_R():w:dq:i64   MEM0:r:d:i16
 | VPMOVSXWQ            | AVX2           | AVX2           |                | 24           | 0b11       | rrr        | VV1 0x24   VL256  V66 V0F38 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                   | REG0=YMM_R():w:qq:i64   REG1=XMM_B():r:q:i16
 | VPMOVSXWQ            | AVX2           | AVX2           |                | 24           | mm         | rrr        | VV1 0x24   VL256  V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                            | REG0=YMM_R():w:qq:i64   MEM0:r:q:i16
 | VPMOVSXWQ            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 24           | 0b11       | rrr        | EVV 0x24 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR                          | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i16
 | VPMOVSXWQ            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 24           | mm         | rrr        | EVV 0x24 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128    NOEVSR  ESIZE_16_BITS() NELEM_QUARTERMEM() | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:d:i16
 | VPMOVSXWQ            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 24           | 0b11       | rrr        | EVV 0x24 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256    NOEVSR                          | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i16
 | VPMOVSXWQ            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 24           | mm         | rrr        | EVV 0x24 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256    NOEVSR  ESIZE_16_BITS() NELEM_QUARTERMEM() | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:i16
 | VPMOVSXWQ            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 24           | 0b11       | rrr        | EVV 0x24 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512    NOEVSR                          | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i16
 | VPMOVSXWQ            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 24           | mm         | rrr        | EVV 0x24 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512    NOEVSR  ESIZE_16_BITS() NELEM_QUARTERMEM() | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:i16
 | VPMOVUSDB            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 11           | 0b11       | rrr        | EVV 0x11 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_B3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u32
 | VPMOVUSDB            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 11           | mm         | rrr        | EVV 0x11 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_QUARTERMEM() | MEM0:w:d:u8 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u32
 | VPMOVUSDB            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 11           | 0b11       | rrr        | EVV 0x11 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=XMM_B3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u32
 | VPMOVUSDB            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 11           | mm         | rrr        | EVV 0x11 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_QUARTERMEM() | MEM0:w:q:u8 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u32
 | VPMOVUSDB            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 11           | 0b11       | rrr        | EVV 0x11 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=XMM_B3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu32
 | VPMOVUSDB            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 11           | mm         | rrr        | EVV 0x11 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_QUARTERMEM() | MEM0:w:dq:u8 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu32
 | VPMOVUSDW            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 13           | 0b11       | rrr        | EVV 0x13 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_B3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u32
 | VPMOVUSDW            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 13           | mm         | rrr        | EVV 0x13 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_HALFMEM() | MEM0:w:q:u16 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u32
 | VPMOVUSDW            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 13           | 0b11       | rrr        | EVV 0x13 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=XMM_B3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u32
 | VPMOVUSDW            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 13           | mm         | rrr        | EVV 0x13 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_HALFMEM() | MEM0:w:dq:u16 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u32
 | VPMOVUSDW            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 13           | 0b11       | rrr        | EVV 0x13 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=YMM_B3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu32
 | VPMOVUSDW            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 13           | mm         | rrr        | EVV 0x13 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_HALFMEM() | MEM0:w:qq:u16 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu32
 | VPMOVUSQB            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 12           | 0b11       | rrr        | EVV 0x12 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_B3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u64
 | VPMOVUSQB            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 12           | mm         | rrr        | EVV 0x12 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_EIGHTHMEM() | MEM0:w:wrd:u8 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u64
 | VPMOVUSQB            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 12           | 0b11       | rrr        | EVV 0x12 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=XMM_B3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u64
 | VPMOVUSQB            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 12           | mm         | rrr        | EVV 0x12 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_EIGHTHMEM() | MEM0:w:d:u8 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u64
 | VPMOVUSQB            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 12           | 0b11       | rrr        | EVV 0x12 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=XMM_B3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu64
 | VPMOVUSQB            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 12           | mm         | rrr        | EVV 0x12 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_EIGHTHMEM() | MEM0:w:q:u8 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu64
 | VPMOVUSQD            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 15           | 0b11       | rrr        | EVV 0x15 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_B3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u64
 | VPMOVUSQD            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 15           | mm         | rrr        | EVV 0x15 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_HALFMEM() | MEM0:w:q:u32 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u64
 | VPMOVUSQD            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 15           | 0b11       | rrr        | EVV 0x15 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=XMM_B3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u64
 | VPMOVUSQD            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 15           | mm         | rrr        | EVV 0x15 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_HALFMEM() | MEM0:w:dq:u32 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u64
 | VPMOVUSQD            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 15           | 0b11       | rrr        | EVV 0x15 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=YMM_B3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu64
 | VPMOVUSQD            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 15           | mm         | rrr        | EVV 0x15 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_HALFMEM() | MEM0:w:qq:u32 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu64
 | VPMOVUSQW            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 14           | 0b11       | rrr        | EVV 0x14 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_B3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u64
 | VPMOVUSQW            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 14           | mm         | rrr        | EVV 0x14 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_QUARTERMEM() | MEM0:w:d:u16 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u64
 | VPMOVUSQW            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 14           | 0b11       | rrr        | EVV 0x14 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=XMM_B3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u64
 | VPMOVUSQW            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 14           | mm         | rrr        | EVV 0x14 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_QUARTERMEM() | MEM0:w:q:u16 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u64
 | VPMOVUSQW            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 14           | 0b11       | rrr        | EVV 0x14 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=XMM_B3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu64
 | VPMOVUSQW            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 14           | mm         | rrr        | EVV 0x14 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_QUARTERMEM() | MEM0:w:dq:u16 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu64
 | VPMOVUSWB            | DATAXFER       | AVX512EVEX     | AVX512BW_128   | 10           | 0b11       | rrr        | EVV 0x10 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_B3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u16
 | VPMOVUSWB            | DATAXFER       | AVX512EVEX     | AVX512BW_128   | 10           | mm         | rrr        | EVV 0x10 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_HALFMEM() | MEM0:w:q:u8 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u16
 | VPMOVUSWB            | DATAXFER       | AVX512EVEX     | AVX512BW_256   | 10           | 0b11       | rrr        | EVV 0x10 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=XMM_B3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u16
 | VPMOVUSWB            | DATAXFER       | AVX512EVEX     | AVX512BW_256   | 10           | mm         | rrr        | EVV 0x10 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_HALFMEM() | MEM0:w:dq:u8 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u16
 | VPMOVUSWB            | DATAXFER       | AVX512EVEX     | AVX512BW_512   | 10           | 0b11       | rrr        | EVV 0x10 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=YMM_B3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu16
 | VPMOVUSWB            | DATAXFER       | AVX512EVEX     | AVX512BW_512   | 10           | mm         | rrr        | EVV 0x10 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_HALFMEM() | MEM0:w:qq:u8 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu16
 | VPMOVW2M             | DATAXFER       | AVX512EVEX     | AVX512BW_128   | 29           | 0b11       | rrr        | EVV 0x29 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR  ZEROING=0 MASK=0      | REG0=MASK_R():w:mskw REG1=XMM_B3():r:dq:u16
 | VPMOVW2M             | DATAXFER       | AVX512EVEX     | AVX512BW_256   | 29           | 0b11       | rrr        | EVV 0x29 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR  ZEROING=0 MASK=0      | REG0=MASK_R():w:mskw REG1=YMM_B3():r:qq:u16
 | VPMOVW2M             | DATAXFER       | AVX512EVEX     | AVX512BW_512   | 29           | 0b11       | rrr        | EVV 0x29 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR  ZEROING=0 MASK=0      | REG0=MASK_R():w:mskw REG1=ZMM_B3():r:zu16
 | VPMOVWB              | DATAXFER       | AVX512EVEX     | AVX512BW_128   | 30           | 0b11       | rrr        | EVV 0x30 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_B3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u16
 | VPMOVWB              | DATAXFER       | AVX512EVEX     | AVX512BW_128   | 30           | mm         | rrr        | EVV 0x30 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_HALFMEM() | MEM0:w:q:u8 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u16
 | VPMOVWB              | DATAXFER       | AVX512EVEX     | AVX512BW_256   | 30           | 0b11       | rrr        | EVV 0x30 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=XMM_B3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u16
 | VPMOVWB              | DATAXFER       | AVX512EVEX     | AVX512BW_256   | 30           | mm         | rrr        | EVV 0x30 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_HALFMEM() | MEM0:w:dq:u8 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u16
 | VPMOVWB              | DATAXFER       | AVX512EVEX     | AVX512BW_512   | 30           | 0b11       | rrr        | EVV 0x30 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=YMM_B3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu16
 | VPMOVWB              | DATAXFER       | AVX512EVEX     | AVX512BW_512   | 30           | mm         | rrr        | EVV 0x30 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_HALFMEM() | MEM0:w:qq:u8 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu16
 | VPMOVZXBD            | AVX            | AVX            |                | 31           | 0b11       | rrr        | VV1 0x31  VL128 V66 V0F38 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=XMM_R():w:dq:u32   REG1=XMM_B():r:d:u8
 | VPMOVZXBD            | AVX            | AVX            |                | 31           | mm         | rrr        | VV1 0x31  VL128 V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                              | REG0=XMM_R():w:dq:u32   MEM0:r:d:u8
 | VPMOVZXBD            | AVX2           | AVX2           |                | 31           | 0b11       | rrr        | VV1 0x31   VL256  V66 V0F38 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                   | REG0=YMM_R():w:qq:u32   REG1=XMM_B():r:q:u8
 | VPMOVZXBD            | AVX2           | AVX2           |                | 31           | mm         | rrr        | VV1 0x31   VL256  V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                            | REG0=YMM_R():w:qq:u32   MEM0:r:q:u8
 | VPMOVZXBD            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 31           | 0b11       | rrr        | EVV 0x31 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR                          | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i8
 | VPMOVZXBD            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 31           | mm         | rrr        | EVV 0x31 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128    NOEVSR  ESIZE_8_BITS() NELEM_QUARTERMEM() | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:d:i8
 | VPMOVZXBD            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 31           | 0b11       | rrr        | EVV 0x31 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256    NOEVSR                          | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i8
 | VPMOVZXBD            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 31           | mm         | rrr        | EVV 0x31 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256    NOEVSR  ESIZE_8_BITS() NELEM_QUARTERMEM() | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:i8
 | VPMOVZXBD            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 31           | 0b11       | rrr        | EVV 0x31 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512    NOEVSR                          | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i8
 | VPMOVZXBD            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 31           | mm         | rrr        | EVV 0x31 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512    NOEVSR  ESIZE_8_BITS() NELEM_QUARTERMEM() | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:i8
 | VPMOVZXBQ            | AVX            | AVX            |                | 32           | 0b11       | rrr        | VV1 0x32  V66 V0F38 VL128 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=XMM_R():w:dq:u64   REG1=XMM_B():r:w:u8
 | VPMOVZXBQ            | AVX            | AVX            |                | 32           | mm         | rrr        | VV1 0x32  V66 V0F38 VL128 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                              | REG0=XMM_R():w:dq:u64   MEM0:r:w:u8
 | VPMOVZXBQ            | AVX2           | AVX2           |                | 32           | 0b11       | rrr        | VV1 0x32   V66  V0F38 VL256 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                   | REG0=YMM_R():w:qq:u64  REG1=XMM_B():r:d:u8
 | VPMOVZXBQ            | AVX2           | AVX2           |                | 32           | mm         | rrr        | VV1 0x32   V66  V0F38 VL256 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                            | REG0=YMM_R():w:qq:u64   MEM0:r:d:u8
 | VPMOVZXBQ            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 32           | 0b11       | rrr        | EVV 0x32 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR                          | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i8
 | VPMOVZXBQ            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 32           | mm         | rrr        | EVV 0x32 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128    NOEVSR  ESIZE_8_BITS() NELEM_EIGHTHMEM() | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:wrd:i8
 | VPMOVZXBQ            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 32           | 0b11       | rrr        | EVV 0x32 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256    NOEVSR                          | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i8
 | VPMOVZXBQ            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 32           | mm         | rrr        | EVV 0x32 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256    NOEVSR  ESIZE_8_BITS() NELEM_EIGHTHMEM() | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:d:i8
 | VPMOVZXBQ            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 32           | 0b11       | rrr        | EVV 0x32 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512    NOEVSR                          | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i8
 | VPMOVZXBQ            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 32           | mm         | rrr        | EVV 0x32 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512    NOEVSR  ESIZE_8_BITS() NELEM_EIGHTHMEM() | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:i8
 | VPMOVZXBW            | AVX            | AVX            |                | 30           | 0b11       | rrr        | VV1 0x30  VL128 V66 V0F38 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=XMM_R():w:dq:u16   REG1=XMM_B():r:q:u8
 | VPMOVZXBW            | AVX            | AVX            |                | 30           | mm         | rrr        | VV1 0x30  VL128 V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                              | REG0=XMM_R():w:dq:u16   MEM0:r:q:u8
 | VPMOVZXBW            | AVX2           | AVX2           |                | 30           | 0b11       | rrr        | VV1 0x30   VL256  V66 V0F38 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                   | REG0=YMM_R():w:qq:u16   REG1=XMM_B():r:dq:u8
 | VPMOVZXBW            | AVX2           | AVX2           |                | 30           | mm         | rrr        | VV1 0x30   VL256  V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                            | REG0=YMM_R():w:qq:u16   MEM0:r:dq:u8
 | VPMOVZXBW            | DATAXFER       | AVX512EVEX     | AVX512BW_128   | 30           | 0b11       | rrr        | EVV 0x30 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR                          | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i8
 | VPMOVZXBW            | DATAXFER       | AVX512EVEX     | AVX512BW_128   | 30           | mm         | rrr        | EVV 0x30 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128    NOEVSR  ESIZE_8_BITS() NELEM_HALFMEM() | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:i8
 | VPMOVZXBW            | DATAXFER       | AVX512EVEX     | AVX512BW_256   | 30           | 0b11       | rrr        | EVV 0x30 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256    NOEVSR                          | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i8
 | VPMOVZXBW            | DATAXFER       | AVX512EVEX     | AVX512BW_256   | 30           | mm         | rrr        | EVV 0x30 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256    NOEVSR  ESIZE_8_BITS() NELEM_HALFMEM() | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:i8
 | VPMOVZXBW            | DATAXFER       | AVX512EVEX     | AVX512BW_512   | 30           | 0b11       | rrr        | EVV 0x30 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512    NOEVSR                          | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:i8
 | VPMOVZXBW            | DATAXFER       | AVX512EVEX     | AVX512BW_512   | 30           | mm         | rrr        | EVV 0x30 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512    NOEVSR  ESIZE_8_BITS() NELEM_HALFMEM() | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:i8
 | VPMOVZXDQ            | AVX            | AVX            |                | 35           | 0b11       | rrr        | VV1 0x35  VL128 V66 V0F38 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=XMM_R():w:dq:u64   REG1=XMM_B():r:q:u32
 | VPMOVZXDQ            | AVX            | AVX            |                | 35           | mm         | rrr        | VV1 0x35  VL128 V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                              | REG0=XMM_R():w:dq:u64   MEM0:r:q:u32
 | VPMOVZXDQ            | AVX2           | AVX2           |                | 35           | 0b11       | rrr        | VV1 0x35   VL256  V66 V0F38 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                   | REG0=YMM_R():w:qq:u64   REG1=XMM_B():r:dq:u32
 | VPMOVZXDQ            | AVX2           | AVX2           |                | 35           | mm         | rrr        | VV1 0x35   VL256  V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                            | REG0=YMM_R():w:qq:u64   MEM0:r:dq:u32
 | VPMOVZXDQ            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 35           | 0b11       | rrr        | EVV 0x35 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i32
 | VPMOVZXDQ            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 35           | mm         | rrr        | EVV 0x35 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALFMEM() | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:i32
 | VPMOVZXDQ            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 35           | 0b11       | rrr        | EVV 0x35 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i32
 | VPMOVZXDQ            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 35           | mm         | rrr        | EVV 0x35 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALFMEM() | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:i32
 | VPMOVZXDQ            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 35           | 0b11       | rrr        | EVV 0x35 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:i32
 | VPMOVZXDQ            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 35           | mm         | rrr        | EVV 0x35 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALFMEM() | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:i32
 | VPMOVZXWD            | AVX            | AVX            |                | 33           | 0b11       | rrr        | VV1 0x33  V66 V0F38 VL128 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=XMM_R():w:dq:u32   REG1=XMM_B():r:q:u16
 | VPMOVZXWD            | AVX            | AVX            |                | 33           | mm         | rrr        | VV1 0x33  V66 V0F38 VL128 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                              | REG0=XMM_R():w:dq:u32   MEM0:r:q:u16
 | VPMOVZXWD            | AVX2           | AVX2           |                | 33           | 0b11       | rrr        | VV1 0x33   V66  V0F38 VL256 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                   | REG0=YMM_R():w:qq:u32   REG1=XMM_B():r:dq:u16
 | VPMOVZXWD            | AVX2           | AVX2           |                | 33           | mm         | rrr        | VV1 0x33   V66  V0F38 VL256 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                            | REG0=YMM_R():w:qq:u32   MEM0:r:dq:u16
 | VPMOVZXWD            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 33           | 0b11       | rrr        | EVV 0x33 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR                          | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i16
 | VPMOVZXWD            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 33           | mm         | rrr        | EVV 0x33 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128    NOEVSR  ESIZE_16_BITS() NELEM_HALFMEM() | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:i16
 | VPMOVZXWD            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 33           | 0b11       | rrr        | EVV 0x33 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256    NOEVSR                          | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i16
 | VPMOVZXWD            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 33           | mm         | rrr        | EVV 0x33 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256    NOEVSR  ESIZE_16_BITS() NELEM_HALFMEM() | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:i16
 | VPMOVZXWD            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 33           | 0b11       | rrr        | EVV 0x33 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512    NOEVSR                          | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:i16
 | VPMOVZXWD            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 33           | mm         | rrr        | EVV 0x33 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512    NOEVSR  ESIZE_16_BITS() NELEM_HALFMEM() | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:i16
 | VPMOVZXWQ            | AVX            | AVX            |                | 34           | 0b11       | rrr        | VV1 0x34  VL128 V66 V0F38 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=XMM_R():w:dq:u64   REG1=XMM_B():r:d:u16
 | VPMOVZXWQ            | AVX            | AVX            |                | 34           | mm         | rrr        | VV1 0x34  VL128 V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                              | REG0=XMM_R():w:dq:u64   MEM0:r:d:u16
 | VPMOVZXWQ            | AVX2           | AVX2           |                | 34           | 0b11       | rrr        | VV1 0x34   VL256  V66 V0F38 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                   | REG0=YMM_R():w:qq:u64   REG1=XMM_B():r:q:u16
 | VPMOVZXWQ            | AVX2           | AVX2           |                | 34           | mm         | rrr        | VV1 0x34   VL256  V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                            | REG0=YMM_R():w:qq:u64   MEM0:r:q:u16
 | VPMOVZXWQ            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 34           | 0b11       | rrr        | EVV 0x34 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR                          | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i16
 | VPMOVZXWQ            | DATAXFER       | AVX512EVEX     | AVX512F_128    | 34           | mm         | rrr        | EVV 0x34 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128    NOEVSR  ESIZE_16_BITS() NELEM_QUARTERMEM() | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:d:i16
 | VPMOVZXWQ            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 34           | 0b11       | rrr        | EVV 0x34 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256    NOEVSR                          | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i16
 | VPMOVZXWQ            | DATAXFER       | AVX512EVEX     | AVX512F_256    | 34           | mm         | rrr        | EVV 0x34 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256    NOEVSR  ESIZE_16_BITS() NELEM_QUARTERMEM() | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:i16
 | VPMOVZXWQ            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 34           | 0b11       | rrr        | EVV 0x34 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512    NOEVSR                          | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i16
 | VPMOVZXWQ            | DATAXFER       | AVX512EVEX     | AVX512F_512    | 34           | mm         | rrr        | EVV 0x34 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512    NOEVSR  ESIZE_16_BITS() NELEM_QUARTERMEM() | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:i16
 | VPMULDQ              | AVX            | AVX            |                | 28           | mm         | rrr        | VV1 0x28  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=XMM_R():w:dq:i64 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32
 | VPMULDQ              | AVX            | AVX            |                | 28           | 0b11       | rrr        | VV1 0x28  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=XMM_R():w:dq:i64 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32
 | VPMULDQ              | AVX2           | AVX2           |                | 28           | mm         | rrr        | VV1 0x28  VL256 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=YMM_R():w:qq:i64 REG1=YMM_N():r:qq:i32 MEM0:r:qq:i32
 | VPMULDQ              | AVX2           | AVX2           |                | 28           | 0b11       | rrr        | VV1 0x28  VL256 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=YMM_R():w:qq:i64 REG1=YMM_N():r:qq:i32 REG2=YMM_B():r:qq:i32
 | VPMULDQ              | AVX512         | AVX512EVEX     | AVX512F_128    | 28           | 0b11       | rrr        | EVV 0x28 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i64 REG3=XMM_B3():r:dq:i64
 | VPMULDQ              | AVX512         | AVX512EVEX     | AVX512F_128    | 28           | mm         | rrr        | EVV 0x28 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i64 MEM0:r:vv:i64:TXT=BCASTSTR
 | VPMULDQ              | AVX512         | AVX512EVEX     | AVX512F_256    | 28           | 0b11       | rrr        | EVV 0x28 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i64 REG3=YMM_B3():r:qq:i64
 | VPMULDQ              | AVX512         | AVX512EVEX     | AVX512F_256    | 28           | mm         | rrr        | EVV 0x28 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i64 MEM0:r:vv:i64:TXT=BCASTSTR
 | VPMULDQ              | AVX512         | AVX512EVEX     | AVX512F_512    | 28           | 0b11       | rrr        | EVV 0x28 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi64 REG3=ZMM_B3():r:zi64
 | VPMULDQ              | AVX512         | AVX512EVEX     | AVX512F_512    | 28           | mm         | rrr        | EVV 0x28 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi64 MEM0:r:vv:i64:TXT=BCASTSTR
 | VPMULHD              | KNC            | KNCE           |                | 87           | mm         | rrr        | KVV 0x87 V0F38 REXW=0 V66  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT32()                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VPMULHD              | KNC            | KNCE           |                | 87           | 0b11       | rrr        | KVV 0x87 V0F38 REXW=0 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VPMULHD              | KNC            | KNCE           |                | 87           | 0b11       | rrr        | KVV 0x87 V0F38 REXW=0 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1                                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VPMULHRSW            | AVX            | AVX            |                | 0b           | mm         | rrr        | VV1 0x0B  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
 | VPMULHRSW            | AVX            | AVX            |                | 0b           | 0b11       | rrr        | VV1 0x0B  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
 | VPMULHRSW            | AVX2           | AVX2           |                | 0b           | mm         | rrr        | VV1 0x0B  VL256 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=YMM_R():w:qq:i16 REG1=YMM_N():r:qq:i16 MEM0:r:qq:i16
 | VPMULHRSW            | AVX2           | AVX2           |                | 0b           | 0b11       | rrr        | VV1 0x0B  VL256 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=YMM_R():w:qq:i16 REG1=YMM_N():r:qq:i16 REG2=YMM_B():r:qq:i16
 | VPMULHRSW            | AVX512         | AVX512EVEX     | AVX512BW_128   | 0b           | 0b11       | rrr        | EVV 0x0B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                    | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 REG3=XMM_B3():r:dq:i16
 | VPMULHRSW            | AVX512         | AVX512EVEX     | AVX512BW_128   | 0b           | mm         | rrr        | EVV 0x0B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 MEM0:r:dq:i16
 | VPMULHRSW            | AVX512         | AVX512EVEX     | AVX512BW_256   | 0b           | 0b11       | rrr        | EVV 0x0B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                    | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 REG3=YMM_B3():r:qq:i16
 | VPMULHRSW            | AVX512         | AVX512EVEX     | AVX512BW_256   | 0b           | mm         | rrr        | EVV 0x0B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 MEM0:r:qq:i16
 | VPMULHRSW            | AVX512         | AVX512EVEX     | AVX512BW_512   | 0b           | 0b11       | rrr        | EVV 0x0B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                    | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 REG3=ZMM_B3():r:zi16
 | VPMULHRSW            | AVX512         | AVX512EVEX     | AVX512BW_512   | 0b           | mm         | rrr        | EVV 0x0B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 MEM0:r:zd:i16
 | VPMULHUD             | KNC            | KNCE           |                | 86           | mm         | rrr        | KVV 0x86 V0F38 REXW=0 V66  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT32()                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VPMULHUD             | KNC            | KNCE           |                | 86           | 0b11       | rrr        | KVV 0x86 V0F38 REXW=0 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VPMULHUD             | KNC            | KNCE           |                | 86           | 0b11       | rrr        | KVV 0x86 V0F38 REXW=0 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1                                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VPMULHUW             | AVX            | AVX            |                | e4           | mm         | rrr        | VV1 0xE4  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 MEM0:r:dq:u16
 | VPMULHUW             | AVX            | AVX            |                | e4           | 0b11       | rrr        | VV1 0xE4  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 REG2=XMM_B():r:dq:u16
 | VPMULHUW             | AVX2           | AVX2           |                | e4           | mm         | rrr        | VV1 0xE4  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u16 REG1=YMM_N():r:qq:u16 MEM0:r:qq:u16
 | VPMULHUW             | AVX2           | AVX2           |                | e4           | 0b11       | rrr        | VV1 0xE4  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:u16 REG1=YMM_N():r:qq:u16 REG2=YMM_B():r:qq:u16
 | VPMULHUW             | AVX512         | AVX512EVEX     | AVX512BW_128   | e4           | 0b11       | rrr        | EVV 0xE4 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                      | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
 | VPMULHUW             | AVX512         | AVX512EVEX     | AVX512BW_128   | e4           | mm         | rrr        | EVV 0xE4 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
 | VPMULHUW             | AVX512         | AVX512EVEX     | AVX512BW_256   | e4           | 0b11       | rrr        | EVV 0xE4 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                      | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
 | VPMULHUW             | AVX512         | AVX512EVEX     | AVX512BW_256   | e4           | mm         | rrr        | EVV 0xE4 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
 | VPMULHUW             | AVX512         | AVX512EVEX     | AVX512BW_512   | e4           | 0b11       | rrr        | EVV 0xE4 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                      | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
 | VPMULHUW             | AVX512         | AVX512EVEX     | AVX512BW_512   | e4           | mm         | rrr        | EVV 0xE4 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
 | VPMULHW              | AVX            | AVX            |                | e5           | mm         | rrr        | VV1 0xE5  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
 | VPMULHW              | AVX            | AVX            |                | e5           | 0b11       | rrr        | VV1 0xE5  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
 | VPMULHW              | AVX2           | AVX2           |                | e5           | mm         | rrr        | VV1 0xE5  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:i16 REG1=YMM_N():r:qq:i16 MEM0:r:qq:i16
 | VPMULHW              | AVX2           | AVX2           |                | e5           | 0b11       | rrr        | VV1 0xE5  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:i16 REG1=YMM_N():r:qq:i16 REG2=YMM_B():r:qq:i16
 | VPMULHW              | AVX512         | AVX512EVEX     | AVX512BW_128   | e5           | 0b11       | rrr        | EVV 0xE5 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                      | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
 | VPMULHW              | AVX512         | AVX512EVEX     | AVX512BW_128   | e5           | mm         | rrr        | EVV 0xE5 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
 | VPMULHW              | AVX512         | AVX512EVEX     | AVX512BW_256   | e5           | 0b11       | rrr        | EVV 0xE5 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                      | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
 | VPMULHW              | AVX512         | AVX512EVEX     | AVX512BW_256   | e5           | mm         | rrr        | EVV 0xE5 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
 | VPMULHW              | AVX512         | AVX512EVEX     | AVX512BW_512   | e5           | 0b11       | rrr        | EVV 0xE5 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                      | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
 | VPMULHW              | AVX512         | AVX512EVEX     | AVX512BW_512   | e5           | mm         | rrr        | EVV 0xE5 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
 | VPMULLD              | AVX            | AVX            |                | 40           | mm         | rrr        | VV1 0x40  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32
 | VPMULLD              | AVX            | AVX            |                | 40           | 0b11       | rrr        | VV1 0x40  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32
 | VPMULLD              | AVX2           | AVX2           |                | 40           | mm         | rrr        | VV1 0x40  VL256 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=YMM_R():w:qq:i32 REG1=YMM_N():r:qq:i32 MEM0:r:qq:i32
 | VPMULLD              | AVX2           | AVX2           |                | 40           | 0b11       | rrr        | VV1 0x40  VL256 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=YMM_R():w:qq:i32 REG1=YMM_N():r:qq:i32 REG2=YMM_B():r:qq:i32
 | VPMULLD              | AVX512         | AVX512EVEX     | AVX512F_128    | 40           | 0b11       | rrr        | EVV 0x40 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
 | VPMULLD              | AVX512         | AVX512EVEX     | AVX512F_128    | 40           | mm         | rrr        | EVV 0x40 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPMULLD              | AVX512         | AVX512EVEX     | AVX512F_256    | 40           | 0b11       | rrr        | EVV 0x40 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
 | VPMULLD              | AVX512         | AVX512EVEX     | AVX512F_256    | 40           | mm         | rrr        | EVV 0x40 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPMULLD              | AVX512         | AVX512EVEX     | AVX512F_512    | 40           | 0b11       | rrr        | EVV 0x40 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
 | VPMULLD              | AVX512         | AVX512EVEX     | AVX512F_512    | 40           | mm         | rrr        | EVV 0x40 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPMULLD              | KNC            | KNCE           |                | 40           | mm         | rrr        | KVV 0x40 V0F38 REXW=0 V66  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT32()                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VPMULLD              | KNC            | KNCE           |                | 40           | 0b11       | rrr        | KVV 0x40 V0F38 REXW=0 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VPMULLD              | KNC            | KNCE           |                | 40           | 0b11       | rrr        | KVV 0x40 V0F38 REXW=0 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1                                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VPMULLQ              | AVX512         | AVX512EVEX     | AVX512DQ_128   | 40           | 0b11       | rrr        | EVV 0x40 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
 | VPMULLQ              | AVX512         | AVX512EVEX     | AVX512DQ_128   | 40           | mm         | rrr        | EVV 0x40 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPMULLQ              | AVX512         | AVX512EVEX     | AVX512DQ_256   | 40           | 0b11       | rrr        | EVV 0x40 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
 | VPMULLQ              | AVX512         | AVX512EVEX     | AVX512DQ_256   | 40           | mm         | rrr        | EVV 0x40 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPMULLQ              | AVX512         | AVX512EVEX     | AVX512DQ_512   | 40           | 0b11       | rrr        | EVV 0x40 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
 | VPMULLQ              | AVX512         | AVX512EVEX     | AVX512DQ_512   | 40           | mm         | rrr        | EVV 0x40 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPMULLW              | AVX            | AVX            |                | d5           | mm         | rrr        | VV1 0xD5  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
 | VPMULLW              | AVX            | AVX            |                | d5           | 0b11       | rrr        | VV1 0xD5  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
 | VPMULLW              | AVX2           | AVX2           |                | d5           | mm         | rrr        | VV1 0xD5  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:i16 REG1=YMM_N():r:qq:i16 MEM0:r:qq:i16
 | VPMULLW              | AVX2           | AVX2           |                | d5           | 0b11       | rrr        | VV1 0xD5  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:i16 REG1=YMM_N():r:qq:i16 REG2=YMM_B():r:qq:i16
 | VPMULLW              | AVX512         | AVX512EVEX     | AVX512BW_128   | d5           | 0b11       | rrr        | EVV 0xD5 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                      | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
 | VPMULLW              | AVX512         | AVX512EVEX     | AVX512BW_128   | d5           | mm         | rrr        | EVV 0xD5 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
 | VPMULLW              | AVX512         | AVX512EVEX     | AVX512BW_256   | d5           | 0b11       | rrr        | EVV 0xD5 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                      | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
 | VPMULLW              | AVX512         | AVX512EVEX     | AVX512BW_256   | d5           | mm         | rrr        | EVV 0xD5 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
 | VPMULLW              | AVX512         | AVX512EVEX     | AVX512BW_512   | d5           | 0b11       | rrr        | EVV 0xD5 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                      | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
 | VPMULLW              | AVX512         | AVX512EVEX     | AVX512BW_512   | d5           | mm         | rrr        | EVV 0xD5 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
 | VPMULTISHIFTQB       | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_128 | 83           | 0b11       | rrr        | EVV 0x83 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u64
 | VPMULTISHIFTQB       | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_128 | 83           | mm         | rrr        | EVV 0x83 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPMULTISHIFTQB       | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_256 | 83           | 0b11       | rrr        | EVV 0x83 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u64
 | VPMULTISHIFTQB       | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_256 | 83           | mm         | rrr        | EVV 0x83 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPMULTISHIFTQB       | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_512 | 83           | 0b11       | rrr        | EVV 0x83 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu64
 | VPMULTISHIFTQB       | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_512 | 83           | mm         | rrr        | EVV 0x83 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPMULUDQ             | AVX            | AVX            |                | f4           | mm         | rrr        | VV1 0xF4  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u32 MEM0:r:dq:u32
 | VPMULUDQ             | AVX            | AVX            |                | f4           | 0b11       | rrr        | VV1 0xF4  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u32 REG2=XMM_B():r:dq:u32
 | VPMULUDQ             | AVX2           | AVX2           |                | f4           | mm         | rrr        | VV1 0xF4  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u64 REG1=YMM_N():r:qq:u32 MEM0:r:qq:u32
 | VPMULUDQ             | AVX2           | AVX2           |                | f4           | 0b11       | rrr        | VV1 0xF4  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:u64 REG1=YMM_N():r:qq:u32 REG2=YMM_B():r:qq:u32
 | VPMULUDQ             | AVX512         | AVX512EVEX     | AVX512F_128    | f4           | 0b11       | rrr        | EVV 0xF4 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                  | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
 | VPMULUDQ             | AVX512         | AVX512EVEX     | AVX512F_128    | f4           | mm         | rrr        | EVV 0xF4 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPMULUDQ             | AVX512         | AVX512EVEX     | AVX512F_256    | f4           | 0b11       | rrr        | EVV 0xF4 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                  | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
 | VPMULUDQ             | AVX512         | AVX512EVEX     | AVX512F_256    | f4           | mm         | rrr        | EVV 0xF4 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPMULUDQ             | AVX512         | AVX512EVEX     | AVX512F_512    | f4           | 0b11       | rrr        | EVV 0xF4 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                  | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
 | VPMULUDQ             | AVX512         | AVX512EVEX     | AVX512F_512    | f4           | mm         | rrr        | EVV 0xF4 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPOPCNTB             | AVX512         | AVX512EVEX     | AVX512_BITALG_128 | 54           | 0b11       | rrr        | EVV 0x54 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u8
 | VPOPCNTB             | AVX512         | AVX512EVEX     | AVX512_BITALG_128 | 54           | mm         | rrr        | EVV 0x54 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u8
 | VPOPCNTB             | AVX512         | AVX512EVEX     | AVX512_BITALG_256 | 54           | 0b11       | rrr        | EVV 0x54 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u8
 | VPOPCNTB             | AVX512         | AVX512EVEX     | AVX512_BITALG_256 | 54           | mm         | rrr        | EVV 0x54 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u8
 | VPOPCNTB             | AVX512_BITALG  | AVX512EVEX     | AVX512_BITALG_512 | 54           | 0b11       | rrr        | EVV 0x54 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu8
 | VPOPCNTB             | AVX512_BITALG  | AVX512EVEX     | AVX512_BITALG_512 | 54           | mm         | rrr        | EVV 0x54 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:u8
 | VPOPCNTD             | AVX512         | AVX512EVEX     | AVX512_VPOPCNTDQ_128 | 55           | 0b11       | rrr        | EVV 0x55 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32
 | VPOPCNTD             | AVX512         | AVX512EVEX     | AVX512_VPOPCNTDQ_128 | 55           | mm         | rrr        | EVV 0x55 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR
 | VPOPCNTD             | AVX512         | AVX512EVEX     | AVX512_VPOPCNTDQ_256 | 55           | 0b11       | rrr        | EVV 0x55 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u32
 | VPOPCNTD             | AVX512         | AVX512EVEX     | AVX512_VPOPCNTDQ_256 | 55           | mm         | rrr        | EVV 0x55 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR
 | VPOPCNTD             | AVX512         | AVX512EVEX     | AVX512_VPOPCNTDQ_512 | 55           | 0b11       | rrr        | EVV 0x55 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu32
 | VPOPCNTD             | AVX512         | AVX512EVEX     | AVX512_VPOPCNTDQ_512 | 55           | mm         | rrr        | EVV 0x55 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR
 | VPOPCNTQ             | AVX512         | AVX512EVEX     | AVX512_VPOPCNTDQ_128 | 55           | 0b11       | rrr        | EVV 0x55 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                        | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u64
 | VPOPCNTQ             | AVX512         | AVX512EVEX     | AVX512_VPOPCNTDQ_128 | 55           | mm         | rrr        | EVV 0x55 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR
 | VPOPCNTQ             | AVX512         | AVX512EVEX     | AVX512_VPOPCNTDQ_256 | 55           | 0b11       | rrr        | EVV 0x55 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                        | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u64
 | VPOPCNTQ             | AVX512         | AVX512EVEX     | AVX512_VPOPCNTDQ_256 | 55           | mm         | rrr        | EVV 0x55 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR
 | VPOPCNTQ             | AVX512         | AVX512EVEX     | AVX512_VPOPCNTDQ_512 | 55           | 0b11       | rrr        | EVV 0x55 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                        | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu64
 | VPOPCNTQ             | AVX512         | AVX512EVEX     | AVX512_VPOPCNTDQ_512 | 55           | mm         | rrr        | EVV 0x55 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR
 | VPOPCNTW             | AVX512         | AVX512EVEX     | AVX512_BITALG_128 | 54           | 0b11       | rrr        | EVV 0x54 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                        | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u16
 | VPOPCNTW             | AVX512         | AVX512EVEX     | AVX512_BITALG_128 | 54           | mm         | rrr        | EVV 0x54 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u16
 | VPOPCNTW             | AVX512         | AVX512EVEX     | AVX512_BITALG_256 | 54           | 0b11       | rrr        | EVV 0x54 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                        | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u16
 | VPOPCNTW             | AVX512         | AVX512EVEX     | AVX512_BITALG_256 | 54           | mm         | rrr        | EVV 0x54 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u16
 | VPOPCNTW             | AVX512_BITALG  | AVX512EVEX     | AVX512_BITALG_512 | 54           | 0b11       | rrr        | EVV 0x54 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                        | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu16
 | VPOPCNTW             | AVX512_BITALG  | AVX512EVEX     | AVX512_BITALG_512 | 54           | mm         | rrr        | EVV 0x54 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:u16
 | VPOR                 | LOGICAL        | AVX            |                | eb           | mm         | rrr        | VV1 0xEB  VL128 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq:u128 REG1=XMM_N():r:dq:u128 MEM0:r:dq:u128
 | VPOR                 | LOGICAL        | AVX            |                | eb           | 0b11       | rrr        | VV1 0xEB  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u128 REG1=XMM_N():r:dq:u128 REG2=XMM_B():r:dq:u128
 | VPOR                 | LOGICAL        | AVX2           |                | eb           | mm         | rrr        | VV1 0xEB  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u256 REG1=YMM_N():r:qq:u256 MEM0:r:qq:u256
 | VPOR                 | LOGICAL        | AVX2           |                | eb           | 0b11       | rrr        | VV1 0xEB   VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=YMM_R():w:qq:u256 REG1=YMM_N():r:qq:u256 REG2=YMM_B():r:qq:u256
 | VPORD                | KNC            | KNCE           |                | eb           | mm         | rrr        | KVV 0xEB V0F REXW=0 V66  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT32()                  | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VPORD                | KNC            | KNCE           |                | eb           | 0b11       | rrr        | KVV 0xEB V0F REXW=0 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                       | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VPORD                | KNC            | KNCE           |                | eb           | 0b11       | rrr        | KVV 0xEB V0F REXW=0 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1                                       | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VPORD                | LOGICAL        | AVX512EVEX     | AVX512F_128    | eb           | 0b11       | rrr        | EVV 0xEB V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                  | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
 | VPORD                | LOGICAL        | AVX512EVEX     | AVX512F_128    | eb           | mm         | rrr        | EVV 0xEB V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPORD                | LOGICAL        | AVX512EVEX     | AVX512F_256    | eb           | 0b11       | rrr        | EVV 0xEB V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                  | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
 | VPORD                | LOGICAL        | AVX512EVEX     | AVX512F_256    | eb           | mm         | rrr        | EVV 0xEB V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPORD                | LOGICAL        | AVX512EVEX     | AVX512F_512    | eb           | 0b11       | rrr        | EVV 0xEB V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                  | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
 | VPORD                | LOGICAL        | AVX512EVEX     | AVX512F_512    | eb           | mm         | rrr        | EVV 0xEB V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPORQ                | KNC            | KNCE           |                | eb           | mm         | rrr        | KVV 0xEB V0F REXW=1 V66  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT64()                  | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VPORQ                | KNC            | KNCE           |                | eb           | 0b11       | rrr        | KVV 0xEB V0F REXW=1 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE64()                       | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=REGSWIZ
 | VPORQ                | KNC            | KNCE           |                | eb           | 0b11       | rrr        | KVV 0xEB V0F REXW=1 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1                                       | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VPORQ                | LOGICAL        | AVX512EVEX     | AVX512F_128    | eb           | 0b11       | rrr        | EVV 0xEB V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                  | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
 | VPORQ                | LOGICAL        | AVX512EVEX     | AVX512F_128    | eb           | mm         | rrr        | EVV 0xEB V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPORQ                | LOGICAL        | AVX512EVEX     | AVX512F_256    | eb           | 0b11       | rrr        | EVV 0xEB V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                  | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
 | VPORQ                | LOGICAL        | AVX512EVEX     | AVX512F_256    | eb           | mm         | rrr        | EVV 0xEB V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPORQ                | LOGICAL        | AVX512EVEX     | AVX512F_512    | eb           | 0b11       | rrr        | EVV 0xEB V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                  | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
 | VPORQ                | LOGICAL        | AVX512EVEX     | AVX512F_512    | eb           | mm         | rrr        | EVV 0xEB V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPPERM               | XOP            | XOP            | XOP            | a3           | mm         | rrr        | XOPV 0xA3 VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                      | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16 REG2=XMM_SE():r:dq:i16
 | VPPERM               | XOP            | XOP            | XOP            | a3           | 0b11       | rrr        | XOPV 0xA3 VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                             | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16 REG3=XMM_SE():r:dq:i16
 | VPPERM               | XOP            | XOP            | XOP            | a3           | mm         | rrr        | XOPV 0xA3 VNP W1 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()                      | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_SE():r:dq:i16 MEM0:r:dq:i16
 | VPPERM               | XOP            | XOP            | XOP            | a3           | 0b11       | rrr        | XOPV 0xA3 VNP W1 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                             | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_SE():r:dq:i16 REG3=XMM_B():r:dq:i16
 | VPREFETCH0           | PREFETCH       | KNC            | KNCV           | 18           | mm         | 0b001      | VV1 0x18  VL128 VNP V0F  NOVSR MOD[mm] MOD!=3 REG[0b001] RM[nnn]  MODRM()                            | MEM0:r:mprefetch
 | VPREFETCH0_EVEX      | PREFETCH       | KNCE           | KNCE           | 18           | mm         | 0b001      | KVV 0x18  VL512 VNP V0F  NOEVSR MOD[mm] MOD!=3 REG[0b001] RM[nnn]  MODRM() NOSWIZD()                 | MEM0:r:mprefetch
 | VPREFETCH1           | PREFETCH       | KNC            | KNCV           | 18           | mm         | 0b010      | VV1 0x18  VL128 VNP V0F  NOVSR MOD[mm] MOD!=3 REG[0b010] RM[nnn]  MODRM()                            | MEM0:r:mprefetch
 | VPREFETCH1_EVEX      | PREFETCH       | KNCE           | KNCE           | 18           | mm         | 0b010      | KVV 0x18  VL512 VNP V0F  NOEVSR MOD[mm] MOD!=3 REG[0b010] RM[nnn]  MODRM() NOSWIZD()                 | MEM0:r:mprefetch
 | VPREFETCH2           | PREFETCH       | KNC            | KNCV           | 18           | mm         | 0b011      | VV1 0x18  VL128 VNP V0F  NOVSR MOD[mm] MOD!=3 REG[0b011] RM[nnn]  MODRM()                            | MEM0:r:mprefetch
 | VPREFETCH2_EVEX      | PREFETCH       | KNCE           | KNCE           | 18           | mm         | 0b011      | KVV 0x18  VL512 VNP V0F  NOEVSR MOD[mm] MOD!=3 REG[0b011] RM[nnn]  MODRM() NOSWIZD()                 | MEM0:r:mprefetch
 | VPREFETCHE0          | PREFETCH       | KNC            | KNCV           | 18           | mm         | 0b101      | VV1 0x18  VL128 VNP V0F  NOVSR MOD[mm] MOD!=3 REG[0b101] RM[nnn]  MODRM()                            | MEM0:r:mprefetch
 | VPREFETCHE0_EVEX     | PREFETCH       | KNCE           | KNCE           | 18           | mm         | 0b101      | KVV 0x18  VL512 VNP V0F  NOEVSR MOD[mm] MOD!=3 REG[0b101] RM[nnn]  MODRM() NOSWIZD()                 | MEM0:r:mprefetch
 | VPREFETCHE1          | PREFETCH       | KNC            | KNCV           | 18           | mm         | 0b110      | VV1 0x18  VL128 VNP V0F  NOVSR MOD[mm] MOD!=3 REG[0b110] RM[nnn]  MODRM()                            | MEM0:r:mprefetch
 | VPREFETCHE1_EVEX     | PREFETCH       | KNCE           | KNCE           | 18           | mm         | 0b110      | KVV 0x18  VL512 VNP V0F  NOEVSR MOD[mm] MOD!=3 REG[0b110] RM[nnn]  MODRM() NOSWIZD()                 | MEM0:r:mprefetch
 | VPREFETCHE2          | PREFETCH       | KNC            | KNCV           | 18           | mm         | 0b111      | VV1 0x18  VL128 VNP V0F  NOVSR MOD[mm] MOD!=3 REG[0b111] RM[nnn]  MODRM()                            | MEM0:r:mprefetch
 | VPREFETCHE2_EVEX     | PREFETCH       | KNCE           | KNCE           | 18           | mm         | 0b111      | KVV 0x18  VL512 VNP V0F  NOEVSR MOD[mm] MOD!=3 REG[0b111] RM[nnn]  MODRM() NOSWIZD()                 | MEM0:r:mprefetch
 | VPREFETCHENTA        | PREFETCH       | KNC            | KNCV           | 18           | mm         | 0b100      | VV1 0x18  VL128 VNP V0F  NOVSR MOD[mm] MOD!=3 REG[0b100] RM[nnn]  MODRM()                            | MEM0:r:mprefetch
 | VPREFETCHENTA_EVEX   | PREFETCH       | KNCE           | KNCE           | 18           | mm         | 0b100      | KVV 0x18  VL512 VNP V0F  NOEVSR MOD[mm] MOD!=3 REG[0b100] RM[nnn]  MODRM() NOSWIZD()                 | MEM0:r:mprefetch
 | VPREFETCHNTA         | PREFETCH       | KNC            | KNCV           | 18           | mm         | 0b000      | VV1 0x18  VL128 VNP V0F  NOVSR MOD[mm] MOD!=3 REG[0b000] RM[nnn]  MODRM()                            | MEM0:r:mprefetch
 | VPREFETCHNTA_EVEX    | PREFETCH       | KNCE           | KNCE           | 18           | mm         | 0b000      | KVV 0x18  VL512 VNP V0F  NOEVSR MOD[mm] MOD!=3 REG[0b000] RM[nnn]  MODRM() NOSWIZD()                 | MEM0:r:mprefetch
 | VPROLD               | AVX512         | AVX512EVEX     | AVX512F_128    | 72           | 0b11       | 0b001      | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b001] RM[nnn]  VL128  W0   UIMM8()                      | REG0=XMM_N3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32 IMM0:r:b
 | VPROLD               | AVX512         | AVX512EVEX     | AVX512F_128    | 72           | mm         | 0b001      | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b001] RM[nnn]  MODRM()  VL128  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_N3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
 | VPROLD               | AVX512         | AVX512EVEX     | AVX512F_256    | 72           | 0b11       | 0b001      | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b001] RM[nnn]  VL256  W0   UIMM8()                      | REG0=YMM_N3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u32 IMM0:r:b
 | VPROLD               | AVX512         | AVX512EVEX     | AVX512F_256    | 72           | mm         | 0b001      | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b001] RM[nnn]  MODRM()  VL256  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_N3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
 | VPROLD               | AVX512         | AVX512EVEX     | AVX512F_512    | 72           | 0b11       | 0b001      | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b001] RM[nnn]  VL512  W0   UIMM8()                      | REG0=ZMM_N3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu32 IMM0:r:b
 | VPROLD               | AVX512         | AVX512EVEX     | AVX512F_512    | 72           | mm         | 0b001      | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b001] RM[nnn]  MODRM()  VL512  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_N3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
 | VPROLQ               | AVX512         | AVX512EVEX     | AVX512F_128    | 72           | 0b11       | 0b001      | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b001] RM[nnn]  VL128  W1   UIMM8()                      | REG0=XMM_N3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u64 IMM0:r:b
 | VPROLQ               | AVX512         | AVX512EVEX     | AVX512F_128    | 72           | mm         | 0b001      | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b001] RM[nnn]  MODRM()  VL128  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_N3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VPROLQ               | AVX512         | AVX512EVEX     | AVX512F_256    | 72           | 0b11       | 0b001      | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b001] RM[nnn]  VL256  W1   UIMM8()                      | REG0=YMM_N3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u64 IMM0:r:b
 | VPROLQ               | AVX512         | AVX512EVEX     | AVX512F_256    | 72           | mm         | 0b001      | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b001] RM[nnn]  MODRM()  VL256  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_N3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VPROLQ               | AVX512         | AVX512EVEX     | AVX512F_512    | 72           | 0b11       | 0b001      | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b001] RM[nnn]  VL512  W1   UIMM8()                      | REG0=ZMM_N3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu64 IMM0:r:b
 | VPROLQ               | AVX512         | AVX512EVEX     | AVX512F_512    | 72           | mm         | 0b001      | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b001] RM[nnn]  MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_N3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VPROLVD              | AVX512         | AVX512EVEX     | AVX512F_128    | 15           | 0b11       | rrr        | EVV 0x15 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
 | VPROLVD              | AVX512         | AVX512EVEX     | AVX512F_128    | 15           | mm         | rrr        | EVV 0x15 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPROLVD              | AVX512         | AVX512EVEX     | AVX512F_256    | 15           | 0b11       | rrr        | EVV 0x15 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
 | VPROLVD              | AVX512         | AVX512EVEX     | AVX512F_256    | 15           | mm         | rrr        | EVV 0x15 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPROLVD              | AVX512         | AVX512EVEX     | AVX512F_512    | 15           | 0b11       | rrr        | EVV 0x15 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
 | VPROLVD              | AVX512         | AVX512EVEX     | AVX512F_512    | 15           | mm         | rrr        | EVV 0x15 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPROLVQ              | AVX512         | AVX512EVEX     | AVX512F_128    | 15           | 0b11       | rrr        | EVV 0x15 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
 | VPROLVQ              | AVX512         | AVX512EVEX     | AVX512F_128    | 15           | mm         | rrr        | EVV 0x15 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPROLVQ              | AVX512         | AVX512EVEX     | AVX512F_256    | 15           | 0b11       | rrr        | EVV 0x15 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
 | VPROLVQ              | AVX512         | AVX512EVEX     | AVX512F_256    | 15           | mm         | rrr        | EVV 0x15 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPROLVQ              | AVX512         | AVX512EVEX     | AVX512F_512    | 15           | 0b11       | rrr        | EVV 0x15 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
 | VPROLVQ              | AVX512         | AVX512EVEX     | AVX512F_512    | 15           | mm         | rrr        | EVV 0x15 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPRORD               | AVX512         | AVX512EVEX     | AVX512F_128    | 72           | 0b11       | 0b000      | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b000] RM[nnn]  VL128  W0   UIMM8()                      | REG0=XMM_N3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32 IMM0:r:b
 | VPRORD               | AVX512         | AVX512EVEX     | AVX512F_128    | 72           | mm         | 0b000      | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b000] RM[nnn]  MODRM()  VL128  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_N3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
 | VPRORD               | AVX512         | AVX512EVEX     | AVX512F_256    | 72           | 0b11       | 0b000      | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b000] RM[nnn]  VL256  W0   UIMM8()                      | REG0=YMM_N3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u32 IMM0:r:b
 | VPRORD               | AVX512         | AVX512EVEX     | AVX512F_256    | 72           | mm         | 0b000      | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b000] RM[nnn]  MODRM()  VL256  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_N3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
 | VPRORD               | AVX512         | AVX512EVEX     | AVX512F_512    | 72           | 0b11       | 0b000      | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b000] RM[nnn]  VL512  W0   UIMM8()                      | REG0=ZMM_N3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu32 IMM0:r:b
 | VPRORD               | AVX512         | AVX512EVEX     | AVX512F_512    | 72           | mm         | 0b000      | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b000] RM[nnn]  MODRM()  VL512  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_N3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
 | VPRORQ               | AVX512         | AVX512EVEX     | AVX512F_128    | 72           | 0b11       | 0b000      | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b000] RM[nnn]  VL128  W1   UIMM8()                      | REG0=XMM_N3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u64 IMM0:r:b
 | VPRORQ               | AVX512         | AVX512EVEX     | AVX512F_128    | 72           | mm         | 0b000      | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b000] RM[nnn]  MODRM()  VL128  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_N3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VPRORQ               | AVX512         | AVX512EVEX     | AVX512F_256    | 72           | 0b11       | 0b000      | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b000] RM[nnn]  VL256  W1   UIMM8()                      | REG0=YMM_N3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u64 IMM0:r:b
 | VPRORQ               | AVX512         | AVX512EVEX     | AVX512F_256    | 72           | mm         | 0b000      | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b000] RM[nnn]  MODRM()  VL256  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_N3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VPRORQ               | AVX512         | AVX512EVEX     | AVX512F_512    | 72           | 0b11       | 0b000      | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b000] RM[nnn]  VL512  W1   UIMM8()                      | REG0=ZMM_N3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu64 IMM0:r:b
 | VPRORQ               | AVX512         | AVX512EVEX     | AVX512F_512    | 72           | mm         | 0b000      | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b000] RM[nnn]  MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_N3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VPRORVD              | AVX512         | AVX512EVEX     | AVX512F_128    | 14           | 0b11       | rrr        | EVV 0x14 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
 | VPRORVD              | AVX512         | AVX512EVEX     | AVX512F_128    | 14           | mm         | rrr        | EVV 0x14 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPRORVD              | AVX512         | AVX512EVEX     | AVX512F_256    | 14           | 0b11       | rrr        | EVV 0x14 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
 | VPRORVD              | AVX512         | AVX512EVEX     | AVX512F_256    | 14           | mm         | rrr        | EVV 0x14 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPRORVD              | AVX512         | AVX512EVEX     | AVX512F_512    | 14           | 0b11       | rrr        | EVV 0x14 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
 | VPRORVD              | AVX512         | AVX512EVEX     | AVX512F_512    | 14           | mm         | rrr        | EVV 0x14 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPRORVQ              | AVX512         | AVX512EVEX     | AVX512F_128    | 14           | 0b11       | rrr        | EVV 0x14 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
 | VPRORVQ              | AVX512         | AVX512EVEX     | AVX512F_128    | 14           | mm         | rrr        | EVV 0x14 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPRORVQ              | AVX512         | AVX512EVEX     | AVX512F_256    | 14           | 0b11       | rrr        | EVV 0x14 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
 | VPRORVQ              | AVX512         | AVX512EVEX     | AVX512F_256    | 14           | mm         | rrr        | EVV 0x14 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPRORVQ              | AVX512         | AVX512EVEX     | AVX512F_512    | 14           | 0b11       | rrr        | EVV 0x14 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
 | VPRORVQ              | AVX512         | AVX512EVEX     | AVX512F_512    | 14           | mm         | rrr        | EVV 0x14 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPROTB               | XOP            | XOP            | XOP            | c0           | mm         | rrr        | XOPV 0xC0 VNP W0 VL128 NOVSR XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                   | REG0=XMM_R():w:dq:u8 MEM0:r:dq:u8 IMM0:r:b:u8
 | VPROTB               | XOP            | XOP            | XOP            | c0           | 0b11       | rrr        | XOPV 0xC0 VNP W0 VL128 NOVSR XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                          | REG0=XMM_R():w:dq:u8 REG1=XMM_B():r:dq:u8 IMM0:r:b:u8
 | VPROTB               | XOP            | XOP            | XOP            | 90           | mm         | rrr        | XOPV 0x90 VNP W0 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:u8 MEM0:r:dq:u8 REG1=XMM_N():r:dq:u8
 | VPROTB               | XOP            | XOP            | XOP            | 90           | 0b11       | rrr        | XOPV 0x90 VNP W0 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:u8 REG1=XMM_B():r:dq:u8 REG2=XMM_N():r:dq:u8
 | VPROTB               | XOP            | XOP            | XOP            | 90           | mm         | rrr        | XOPV 0x90 VNP W1 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 MEM0:r:dq:u8
 | VPROTB               | XOP            | XOP            | XOP            | 90           | 0b11       | rrr        | XOPV 0x90 VNP W1 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:u8
 | VPROTD               | XOP            | XOP            | XOP            | c2           | mm         | rrr        | XOPV 0xC2 VNP W0 VL128 NOVSR XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                   | REG0=XMM_R():w:dq:u32 MEM0:r:dq:u32 IMM0:r:b:u8
 | VPROTD               | XOP            | XOP            | XOP            | c2           | 0b11       | rrr        | XOPV 0xC2 VNP W0 VL128 NOVSR XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                          | REG0=XMM_R():w:dq:u32 REG1=XMM_B():r:dq:u32 IMM0:r:b:u8
 | VPROTD               | XOP            | XOP            | XOP            | 92           | mm         | rrr        | XOPV 0x92 VNP W0 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:u32 MEM0:r:dq:u32 REG1=XMM_N():r:dq:u32
 | VPROTD               | XOP            | XOP            | XOP            | 92           | 0b11       | rrr        | XOPV 0x92 VNP W0 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:u32 REG1=XMM_B():r:dq:u32 REG2=XMM_N():r:dq:u32
 | VPROTD               | XOP            | XOP            | XOP            | 92           | mm         | rrr        | XOPV 0x92 VNP W1 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 MEM0:r:dq:u32
 | VPROTD               | XOP            | XOP            | XOP            | 92           | 0b11       | rrr        | XOPV 0x92 VNP W1 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 REG2=XMM_B():r:dq:u32
 | VPROTQ               | XOP            | XOP            | XOP            | c3           | mm         | rrr        | XOPV 0xC3 VNP W0 VL128 NOVSR XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                   | REG0=XMM_R():w:dq:u64 MEM0:r:dq:u64 IMM0:r:b:u8
 | VPROTQ               | XOP            | XOP            | XOP            | c3           | 0b11       | rrr        | XOPV 0xC3 VNP W0 VL128 NOVSR XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                          | REG0=XMM_R():w:dq:u64 REG1=XMM_B():r:dq:u64 IMM0:r:b:u8
 | VPROTQ               | XOP            | XOP            | XOP            | 93           | mm         | rrr        | XOPV 0x93 VNP W0 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:u64 MEM0:r:dq:u64 REG1=XMM_N():r:dq:u64
 | VPROTQ               | XOP            | XOP            | XOP            | 93           | 0b11       | rrr        | XOPV 0x93 VNP W0 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:u64 REG1=XMM_B():r:dq:u64 REG2=XMM_N():r:dq:u64
 | VPROTQ               | XOP            | XOP            | XOP            | 93           | mm         | rrr        | XOPV 0x93 VNP W1 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 MEM0:r:dq:u64
 | VPROTQ               | XOP            | XOP            | XOP            | 93           | 0b11       | rrr        | XOPV 0x93 VNP W1 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 REG2=XMM_B():r:dq:u64
 | VPROTW               | XOP            | XOP            | XOP            | c1           | mm         | rrr        | XOPV 0xC1 VNP W0 VL128 NOVSR XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                   | REG0=XMM_R():w:dq:u16 MEM0:r:dq:u16 IMM0:r:b:u8
 | VPROTW               | XOP            | XOP            | XOP            | c1           | 0b11       | rrr        | XOPV 0xC1 VNP W0 VL128 NOVSR XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                          | REG0=XMM_R():w:dq:u16 REG1=XMM_B():r:dq:u16 IMM0:r:b:u8
 | VPROTW               | XOP            | XOP            | XOP            | 91           | mm         | rrr        | XOPV 0x91 VNP W0 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:u16 MEM0:r:dq:u16 REG1=XMM_N():r:dq:u16
 | VPROTW               | XOP            | XOP            | XOP            | 91           | 0b11       | rrr        | XOPV 0x91 VNP W0 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:u16 REG1=XMM_B():r:dq:u16 REG2=XMM_N():r:dq:u16
 | VPROTW               | XOP            | XOP            | XOP            | 91           | mm         | rrr        | XOPV 0x91 VNP W1 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 MEM0:r:dq:u16
 | VPROTW               | XOP            | XOP            | XOP            | 91           | 0b11       | rrr        | XOPV 0x91 VNP W1 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 REG2=XMM_B():r:dq:u16
 | VPSADBW              | AVX            | AVX            |                | f6           | mm         | rrr        | VV1 0xF6  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u8 MEM0:r:dq:u8
 | VPSADBW              | AVX            | AVX            |                | f6           | 0b11       | rrr        | VV1 0xF6  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:u8
 | VPSADBW              | AVX2           | AVX2           |                | f6           | mm         | rrr        | VV1 0xF6  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u16 REG1=YMM_N():r:qq:u8 MEM0:r:qq:u8
 | VPSADBW              | AVX2           | AVX2           |                | f6           | 0b11       | rrr        | VV1 0xF6  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:u16 REG1=YMM_N():r:qq:u8 REG2=YMM_B():r:qq:u8
 | VPSADBW              | AVX512         | AVX512EVEX     | AVX512BW_128   | f6           | 0b11       | rrr        | EVV 0xF6 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128      ZEROING=0 MASK=0                | REG0=XMM_R3():w:dq:u16 REG1=XMM_N3():r:dq:u8 REG2=XMM_B3():r:dq:u8
 | VPSADBW              | AVX512         | AVX512EVEX     | AVX512BW_128   | f6           | mm         | rrr        | EVV 0xF6 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ZEROING=0 MASK=0  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=XMM_N3():r:dq:u8 MEM0:r:dq:u8
 | VPSADBW              | AVX512         | AVX512EVEX     | AVX512BW_256   | f6           | 0b11       | rrr        | EVV 0xF6 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256      ZEROING=0 MASK=0                | REG0=YMM_R3():w:qq:u16 REG1=YMM_N3():r:qq:u8 REG2=YMM_B3():r:qq:u8
 | VPSADBW              | AVX512         | AVX512EVEX     | AVX512BW_256   | f6           | mm         | rrr        | EVV 0xF6 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ZEROING=0 MASK=0  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=YMM_N3():r:qq:u8 MEM0:r:qq:u8
 | VPSADBW              | AVX512         | AVX512EVEX     | AVX512BW_512   | f6           | 0b11       | rrr        | EVV 0xF6 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512      ZEROING=0 MASK=0                | REG0=ZMM_R3():w:zu16 REG1=ZMM_N3():r:zu8 REG2=ZMM_B3():r:zu8
 | VPSADBW              | AVX512         | AVX512EVEX     | AVX512BW_512   | f6           | mm         | rrr        | EVV 0xF6 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ZEROING=0 MASK=0  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=ZMM_N3():r:zu8 MEM0:r:zd:u8
 | VPSBBD               | KNC            | KNCE           |                | 5e           | mm         | rrr        | KVV 0x5E  V0F38 V66  W0  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT32()                  | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  REG2=MASK_N():w:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VPSBBD               | KNC            | KNCE           |                | 5e           | 0b11       | rrr        | KVV 0x5E  V0F38 V66  W0  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                       | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  REG2=MASK_N():w:mskw  REG3=ZMM_B3():r:zd:TXT=REGSWIZ
 | VPSBBD               | KNC            | KNCE           |                | 5e           | 0b11       | rrr        | KVV 0x5E  V0F38 V66  W0  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 SWIZ=0                                | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  REG2=MASK_N():w:mskw  REG3=ZMM_B3():r:zd
 | VPSBBRD              | KNC            | KNCE           |                | 6e           | mm         | rrr        | KVV 0x6E  V0F38 V66  W0  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT32()                  | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  REG2=MASK_N():w:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VPSBBRD              | KNC            | KNCE           |                | 6e           | 0b11       | rrr        | KVV 0x6E  V0F38 V66  W0  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                       | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  REG2=MASK_N():w:mskw  REG3=ZMM_B3():r:zd:TXT=REGSWIZ
 | VPSBBRD              | KNC            | KNCE           |                | 6e           | 0b11       | rrr        | KVV 0x6E  V0F38 V66  W0  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 SWIZ=0                                | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  REG2=MASK_N():w:mskw  REG3=ZMM_B3():r:zd
 | VPSCATTERDD          | KNC            | KNCE           | KNCE           | a0           | mm         | rrr        | KVV 0xA0 V66 V0F38  W0 NO_SPARSE_EVSR MOD[mm] MOD!=3 REG[rrr] RM[0b100] KNC_VMODRM() DNCONVERT_INT32() | MEM0:w:zv:TXT=NT:TXT=CONVERT  REG1=MASK1():rw:mskw   REG0=ZMM_R3():r:zd NELEM=1:SUPP
 | VPSCATTERDD          | SCATTER        | AVX512EVEX     | AVX512F_128    | a0           | mm         | rrr        | EVV 0xA0 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL128  W0 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:w:d:u32 REG0=MASKNOT0():rw:mskw REG1=XMM_R3():r:dq:u32
 | VPSCATTERDD          | SCATTER        | AVX512EVEX     | AVX512F_256    | a0           | mm         | rrr        | EVV 0xA0 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL256  W0 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:w:d:u32 REG0=MASKNOT0():rw:mskw REG1=YMM_R3():r:qq:u32
 | VPSCATTERDD          | SCATTER        | AVX512EVEX     | AVX512F_512    | a0           | mm         | rrr        | EVV 0xA0 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL512  W0 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:w:d:u32 REG0=MASKNOT0():rw:mskw REG1=ZMM_R3():r:zu32
 | VPSCATTERDQ          | KNC            | KNCE           | KNCE           | a0           | mm         | rrr        | KVV 0xA0 V66 V0F38  W1 NO_SPARSE_EVSR MOD[mm] MOD!=3 REG[rrr] RM[0b100] KNC_VMODRM() DNCONVERT_INT64() | MEM0:w:zv:TXT=NT:TXT=CONVERT  REG1=MASK1():rw:mskw   REG0=ZMM_R3():r:zq NELEM=1:SUPP
 | VPSCATTERDQ          | SCATTER        | AVX512EVEX     | AVX512F_128    | a0           | mm         | rrr        | EVV 0xA0 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL128  W1 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:w:q:u64 REG0=MASKNOT0():rw:mskw REG1=XMM_R3():r:dq:u64
 | VPSCATTERDQ          | SCATTER        | AVX512EVEX     | AVX512F_256    | a0           | mm         | rrr        | EVV 0xA0 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL256  W1 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:w:q:u64 REG0=MASKNOT0():rw:mskw REG1=YMM_R3():r:qq:u64
 | VPSCATTERDQ          | SCATTER        | AVX512EVEX     | AVX512F_512    | a0           | mm         | rrr        | EVV 0xA0 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL512  W1 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:w:q:u64 REG0=MASKNOT0():rw:mskw REG1=ZMM_R3():r:zu64
 | VPSCATTERQD          | SCATTER        | AVX512EVEX     | AVX512F_128    | a1           | mm         | rrr        | EVV 0xA1 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL128  W0 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:w:d:u32 REG0=MASKNOT0():rw:mskw REG1=XMM_R3():r:dq:u32
 | VPSCATTERQD          | SCATTER        | AVX512EVEX     | AVX512F_256    | a1           | mm         | rrr        | EVV 0xA1 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL256  W0 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:w:d:u32 REG0=MASKNOT0():rw:mskw REG1=XMM_R3():r:dq:u32
 | VPSCATTERQD          | SCATTER        | AVX512EVEX     | AVX512F_512    | a1           | mm         | rrr        | EVV 0xA1 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL512  W0 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:w:d:u32 REG0=MASKNOT0():rw:mskw REG1=YMM_R3():r:qq:u32
 | VPSCATTERQQ          | SCATTER        | AVX512EVEX     | AVX512F_128    | a1           | mm         | rrr        | EVV 0xA1 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL128  W1 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:w:q:u64 REG0=MASKNOT0():rw:mskw REG1=XMM_R3():r:dq:u64
 | VPSCATTERQQ          | SCATTER        | AVX512EVEX     | AVX512F_256    | a1           | mm         | rrr        | EVV 0xA1 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL256  W1 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:w:q:u64 REG0=MASKNOT0():rw:mskw REG1=YMM_R3():r:qq:u64
 | VPSCATTERQQ          | SCATTER        | AVX512EVEX     | AVX512F_512    | a1           | mm         | rrr        | EVV 0xA1 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL512  W1 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:w:q:u64 REG0=MASKNOT0():rw:mskw REG1=ZMM_R3():r:zu64
 | VPSHAB               | XOP            | XOP            | XOP            | 98           | mm         | rrr        | XOPV 0x98 VNP W0 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:i8 MEM0:r:dq:i8 REG1=XMM_N():r:dq:i8
 | VPSHAB               | XOP            | XOP            | XOP            | 98           | 0b11       | rrr        | XOPV 0x98 VNP W0 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:i8 REG1=XMM_B():r:dq:i8 REG2=XMM_N():r:dq:i8
 | VPSHAB               | XOP            | XOP            | XOP            | 98           | mm         | rrr        | XOPV 0x98 VNP W1 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 MEM0:r:dq:i8
 | VPSHAB               | XOP            | XOP            | XOP            | 98           | 0b11       | rrr        | XOPV 0x98 VNP W1 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 REG2=XMM_B():r:dq:i8
 | VPSHAD               | XOP            | XOP            | XOP            | 9a           | mm         | rrr        | XOPV 0x9A VNP W0 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:i32 MEM0:r:dq:i32 REG1=XMM_N():r:dq:i32
 | VPSHAD               | XOP            | XOP            | XOP            | 9a           | 0b11       | rrr        | XOPV 0x9A VNP W0 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:i32 REG1=XMM_B():r:dq:i32 REG2=XMM_N():r:dq:i32
 | VPSHAD               | XOP            | XOP            | XOP            | 9a           | mm         | rrr        | XOPV 0x9A VNP W1 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32
 | VPSHAD               | XOP            | XOP            | XOP            | 9a           | 0b11       | rrr        | XOPV 0x9A VNP W1 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32
 | VPSHAQ               | XOP            | XOP            | XOP            | 9b           | mm         | rrr        | XOPV 0x9B VNP W0 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:i64 MEM0:r:dq:i64 REG1=XMM_N():r:dq:i64
 | VPSHAQ               | XOP            | XOP            | XOP            | 9b           | 0b11       | rrr        | XOPV 0x9B VNP W0 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:i64 REG1=XMM_B():r:dq:i64 REG2=XMM_N():r:dq:i64
 | VPSHAQ               | XOP            | XOP            | XOP            | 9b           | mm         | rrr        | XOPV 0x9B VNP W1 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:i64 REG1=XMM_N():r:dq:i64 MEM0:r:dq:i64
 | VPSHAQ               | XOP            | XOP            | XOP            | 9b           | 0b11       | rrr        | XOPV 0x9B VNP W1 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:i64 REG1=XMM_N():r:dq:i64 REG2=XMM_B():r:dq:i64
 | VPSHAW               | XOP            | XOP            | XOP            | 99           | mm         | rrr        | XOPV 0x99 VNP W0 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:i16 MEM0:r:dq:i16 REG1=XMM_N():r:dq:i16
 | VPSHAW               | XOP            | XOP            | XOP            | 99           | 0b11       | rrr        | XOPV 0x99 VNP W0 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:i16 REG1=XMM_B():r:dq:i16 REG2=XMM_N():r:dq:i16
 | VPSHAW               | XOP            | XOP            | XOP            | 99           | mm         | rrr        | XOPV 0x99 VNP W1 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
 | VPSHAW               | XOP            | XOP            | XOP            | 99           | 0b11       | rrr        | XOPV 0x99 VNP W1 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
 | VPSHLB               | XOP            | XOP            | XOP            | 94           | mm         | rrr        | XOPV 0x94 VNP W0 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:u8 MEM0:r:dq:u8 REG1=XMM_N():r:dq:u8
 | VPSHLB               | XOP            | XOP            | XOP            | 94           | 0b11       | rrr        | XOPV 0x94 VNP W0 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:u8 REG1=XMM_B():r:dq:u8 REG2=XMM_N():r:dq:u8
 | VPSHLB               | XOP            | XOP            | XOP            | 94           | mm         | rrr        | XOPV 0x94 VNP W1 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 MEM0:r:dq:u8
 | VPSHLB               | XOP            | XOP            | XOP            | 94           | 0b11       | rrr        | XOPV 0x94 VNP W1 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:u8
 | VPSHLD               | XOP            | XOP            | XOP            | 96           | mm         | rrr        | XOPV 0x96 VNP W0 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:u32 MEM0:r:dq:u32 REG1=XMM_N():r:dq:u32
 | VPSHLD               | XOP            | XOP            | XOP            | 96           | 0b11       | rrr        | XOPV 0x96 VNP W0 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:u32 REG1=XMM_B():r:dq:u32 REG2=XMM_N():r:dq:u32
 | VPSHLD               | XOP            | XOP            | XOP            | 96           | mm         | rrr        | XOPV 0x96 VNP W1 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 MEM0:r:dq:u32
 | VPSHLD               | XOP            | XOP            | XOP            | 96           | 0b11       | rrr        | XOPV 0x96 VNP W1 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 REG2=XMM_B():r:dq:u32
 | VPSHLDD              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | 71           | 0b11       | rrr        | EVV 0x71 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0   UIMM8()                      | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32 IMM0:r:b
 | VPSHLDD              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | 71           | mm         | rrr        | EVV 0x71 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
 | VPSHLDD              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | 71           | 0b11       | rrr        | EVV 0x71 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0   UIMM8()                      | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32 IMM0:r:b
 | VPSHLDD              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | 71           | mm         | rrr        | EVV 0x71 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
 | VPSHLDD              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | 71           | 0b11       | rrr        | EVV 0x71 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0   UIMM8()                      | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32 IMM0:r:b
 | VPSHLDD              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | 71           | mm         | rrr        | EVV 0x71 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
 | VPSHLDQ              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | 71           | 0b11       | rrr        | EVV 0x71 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1   UIMM8()                      | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64 IMM0:r:b
 | VPSHLDQ              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | 71           | mm         | rrr        | EVV 0x71 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VPSHLDQ              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | 71           | 0b11       | rrr        | EVV 0x71 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1   UIMM8()                      | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64 IMM0:r:b
 | VPSHLDQ              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | 71           | mm         | rrr        | EVV 0x71 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VPSHLDQ              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | 71           | 0b11       | rrr        | EVV 0x71 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1   UIMM8()                      | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64 IMM0:r:b
 | VPSHLDQ              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | 71           | mm         | rrr        | EVV 0x71 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VPSHLDVD             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | 71           | 0b11       | rrr        | EVV 0x71 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():rw:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
 | VPSHLDVD             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | 71           | mm         | rrr        | EVV 0x71 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPSHLDVD             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | 71           | 0b11       | rrr        | EVV 0x71 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():rw:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
 | VPSHLDVD             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | 71           | mm         | rrr        | EVV 0x71 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPSHLDVD             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | 71           | 0b11       | rrr        | EVV 0x71 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():rw:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
 | VPSHLDVD             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | 71           | mm         | rrr        | EVV 0x71 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPSHLDVQ             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | 71           | 0b11       | rrr        | EVV 0x71 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():rw:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
 | VPSHLDVQ             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | 71           | mm         | rrr        | EVV 0x71 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPSHLDVQ             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | 71           | 0b11       | rrr        | EVV 0x71 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():rw:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
 | VPSHLDVQ             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | 71           | mm         | rrr        | EVV 0x71 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPSHLDVQ             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | 71           | 0b11       | rrr        | EVV 0x71 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():rw:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
 | VPSHLDVQ             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | 71           | mm         | rrr        | EVV 0x71 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPSHLDVW             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | 70           | 0b11       | rrr        | EVV 0x70 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():rw:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
 | VPSHLDVW             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | 70           | mm         | rrr        | EVV 0x70 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():rw:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
 | VPSHLDVW             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | 70           | 0b11       | rrr        | EVV 0x70 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():rw:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
 | VPSHLDVW             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | 70           | mm         | rrr        | EVV 0x70 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():rw:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
 | VPSHLDVW             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | 70           | 0b11       | rrr        | EVV 0x70 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():rw:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
 | VPSHLDVW             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | 70           | mm         | rrr        | EVV 0x70 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():rw:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
 | VPSHLDW              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | 70           | 0b11       | rrr        | EVV 0x70 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1   UIMM8()                      | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16 IMM0:r:b
 | VPSHLDW              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | 70           | mm         | rrr        | EVV 0x70 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1   UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16 IMM0:r:b
 | VPSHLDW              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | 70           | 0b11       | rrr        | EVV 0x70 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1   UIMM8()                      | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16 IMM0:r:b
 | VPSHLDW              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | 70           | mm         | rrr        | EVV 0x70 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1   UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16 IMM0:r:b
 | VPSHLDW              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | 70           | 0b11       | rrr        | EVV 0x70 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1   UIMM8()                      | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16 IMM0:r:b
 | VPSHLDW              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | 70           | mm         | rrr        | EVV 0x70 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1   UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16 IMM0:r:b
 | VPSHLQ               | XOP            | XOP            | XOP            | 97           | mm         | rrr        | XOPV 0x97 VNP W0 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:u64 MEM0:r:dq:u64 REG1=XMM_N():r:dq:u64
 | VPSHLQ               | XOP            | XOP            | XOP            | 97           | 0b11       | rrr        | XOPV 0x97 VNP W0 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:u64 REG1=XMM_B():r:dq:u64 REG2=XMM_N():r:dq:u64
 | VPSHLQ               | XOP            | XOP            | XOP            | 97           | mm         | rrr        | XOPV 0x97 VNP W1 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 MEM0:r:dq:u64
 | VPSHLQ               | XOP            | XOP            | XOP            | 97           | 0b11       | rrr        | XOPV 0x97 VNP W1 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 REG2=XMM_B():r:dq:u64
 | VPSHLW               | XOP            | XOP            | XOP            | 95           | mm         | rrr        | XOPV 0x95 VNP W0 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:u16 MEM0:r:dq:u16 REG1=XMM_N():r:dq:u16
 | VPSHLW               | XOP            | XOP            | XOP            | 95           | 0b11       | rrr        | XOPV 0x95 VNP W0 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:u16 REG1=XMM_B():r:dq:u16 REG2=XMM_N():r:dq:u16
 | VPSHLW               | XOP            | XOP            | XOP            | 95           | mm         | rrr        | XOPV 0x95 VNP W1 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 MEM0:r:dq:u16
 | VPSHLW               | XOP            | XOP            | XOP            | 95           | 0b11       | rrr        | XOPV 0x95 VNP W1 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 REG2=XMM_B():r:dq:u16
 | VPSHRDD              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | 73           | 0b11       | rrr        | EVV 0x73 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0   UIMM8()                      | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32 IMM0:r:b
 | VPSHRDD              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | 73           | mm         | rrr        | EVV 0x73 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
 | VPSHRDD              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | 73           | 0b11       | rrr        | EVV 0x73 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0   UIMM8()                      | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32 IMM0:r:b
 | VPSHRDD              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | 73           | mm         | rrr        | EVV 0x73 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
 | VPSHRDD              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | 73           | 0b11       | rrr        | EVV 0x73 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0   UIMM8()                      | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32 IMM0:r:b
 | VPSHRDD              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | 73           | mm         | rrr        | EVV 0x73 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
 | VPSHRDQ              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | 73           | 0b11       | rrr        | EVV 0x73 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1   UIMM8()                      | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64 IMM0:r:b
 | VPSHRDQ              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | 73           | mm         | rrr        | EVV 0x73 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VPSHRDQ              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | 73           | 0b11       | rrr        | EVV 0x73 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1   UIMM8()                      | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64 IMM0:r:b
 | VPSHRDQ              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | 73           | mm         | rrr        | EVV 0x73 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VPSHRDQ              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | 73           | 0b11       | rrr        | EVV 0x73 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1   UIMM8()                      | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64 IMM0:r:b
 | VPSHRDQ              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | 73           | mm         | rrr        | EVV 0x73 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VPSHRDVD             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | 73           | 0b11       | rrr        | EVV 0x73 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():rw:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
 | VPSHRDVD             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | 73           | mm         | rrr        | EVV 0x73 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPSHRDVD             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | 73           | 0b11       | rrr        | EVV 0x73 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():rw:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
 | VPSHRDVD             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | 73           | mm         | rrr        | EVV 0x73 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPSHRDVD             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | 73           | 0b11       | rrr        | EVV 0x73 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():rw:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
 | VPSHRDVD             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | 73           | mm         | rrr        | EVV 0x73 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPSHRDVQ             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | 73           | 0b11       | rrr        | EVV 0x73 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():rw:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
 | VPSHRDVQ             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | 73           | mm         | rrr        | EVV 0x73 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPSHRDVQ             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | 73           | 0b11       | rrr        | EVV 0x73 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():rw:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
 | VPSHRDVQ             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | 73           | mm         | rrr        | EVV 0x73 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPSHRDVQ             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | 73           | 0b11       | rrr        | EVV 0x73 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():rw:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
 | VPSHRDVQ             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | 73           | mm         | rrr        | EVV 0x73 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPSHRDVW             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | 72           | 0b11       | rrr        | EVV 0x72 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():rw:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
 | VPSHRDVW             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | 72           | mm         | rrr        | EVV 0x72 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():rw:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
 | VPSHRDVW             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | 72           | 0b11       | rrr        | EVV 0x72 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():rw:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
 | VPSHRDVW             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | 72           | mm         | rrr        | EVV 0x72 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():rw:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
 | VPSHRDVW             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | 72           | 0b11       | rrr        | EVV 0x72 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():rw:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
 | VPSHRDVW             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | 72           | mm         | rrr        | EVV 0x72 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():rw:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
 | VPSHRDW              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | 72           | 0b11       | rrr        | EVV 0x72 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1   UIMM8()                      | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16 IMM0:r:b
 | VPSHRDW              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | 72           | mm         | rrr        | EVV 0x72 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1   UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16 IMM0:r:b
 | VPSHRDW              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | 72           | 0b11       | rrr        | EVV 0x72 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1   UIMM8()                      | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16 IMM0:r:b
 | VPSHRDW              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | 72           | mm         | rrr        | EVV 0x72 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1   UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16 IMM0:r:b
 | VPSHRDW              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | 72           | 0b11       | rrr        | EVV 0x72 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1   UIMM8()                      | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16 IMM0:r:b
 | VPSHRDW              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | 72           | mm         | rrr        | EVV 0x72 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1   UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16 IMM0:r:b
 | VPSHUFB              | AVX            | AVX            |                | 00           | mm         | rrr        | VV1 0x00  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 MEM0:r:dq:u8
 | VPSHUFB              | AVX            | AVX            |                | 00           | 0b11       | rrr        | VV1 0x00  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:u8
 | VPSHUFB              | AVX2           | AVX2           |                | 00           | mm         | rrr        | VV1 0x00  VL256 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=YMM_R():w:qq:u8 REG1=YMM_N():r:qq:u8 MEM0:r:qq:u8
 | VPSHUFB              | AVX2           | AVX2           |                | 00           | 0b11       | rrr        | VV1 0x00  VL256 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=YMM_R():w:qq:u8 REG1=YMM_N():r:qq:u8 REG2=YMM_B():r:qq:u8
 | VPSHUFB              | AVX512         | AVX512EVEX     | AVX512BW_128   | 00           | 0b11       | rrr        | EVV 0x00 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                    | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
 | VPSHUFB              | AVX512         | AVX512EVEX     | AVX512BW_128   | 00           | mm         | rrr        | EVV 0x00 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
 | VPSHUFB              | AVX512         | AVX512EVEX     | AVX512BW_256   | 00           | 0b11       | rrr        | EVV 0x00 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                    | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
 | VPSHUFB              | AVX512         | AVX512EVEX     | AVX512BW_256   | 00           | mm         | rrr        | EVV 0x00 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
 | VPSHUFB              | AVX512         | AVX512EVEX     | AVX512BW_512   | 00           | 0b11       | rrr        | EVV 0x00 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                    | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
 | VPSHUFB              | AVX512         | AVX512EVEX     | AVX512BW_512   | 00           | mm         | rrr        | EVV 0x00 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
 | VPSHUFBITQMB         | AVX512         | AVX512EVEX     | AVX512_BITALG_128 | 8f           | 0b11       | rrr        | EVV 0x8F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0    ZEROING=0                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u8
 | VPSHUFBITQMB         | AVX512         | AVX512EVEX     | AVX512_BITALG_128 | 8f           | mm         | rrr        | EVV 0x8F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0    ZEROING=0  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u64 MEM0:r:dq:u8
 | VPSHUFBITQMB         | AVX512         | AVX512EVEX     | AVX512_BITALG_256 | 8f           | 0b11       | rrr        | EVV 0x8F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0    ZEROING=0                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u8
 | VPSHUFBITQMB         | AVX512         | AVX512EVEX     | AVX512_BITALG_256 | 8f           | mm         | rrr        | EVV 0x8F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0    ZEROING=0  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u64 MEM0:r:qq:u8
 | VPSHUFBITQMB         | AVX512_BITALG  | AVX512EVEX     | AVX512_BITALG_512 | 8f           | 0b11       | rrr        | EVV 0x8F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0    ZEROING=0                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu8
 | VPSHUFBITQMB         | AVX512_BITALG  | AVX512EVEX     | AVX512_BITALG_512 | 8f           | mm         | rrr        | EVV 0x8F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0    ZEROING=0  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu64 MEM0:r:zd:u8
 | VPSHUFD              | AVX            | AVX            |                | 70           | mm         | rrr        | VV1 0x70  VL128 V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                        | REG0=XMM_R():w:dq MEM0:r:dq  IMM0:r:b
 | VPSHUFD              | AVX            | AVX            |                | 70           | 0b11       | rrr        | VV1 0x70  VL128 V66 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                               | REG0=XMM_R():w:dq REG1=XMM_B():r:dq IMM0:r:b
 | VPSHUFD              | AVX2           | AVX2           |                | 70           | mm         | rrr        | VV1 0x70   VL256 V66 V0F NOVSR   MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                     | REG0=YMM_R():w:qq:u32 MEM0:r:qq:u32  IMM0:r:b
 | VPSHUFD              | AVX2           | AVX2           |                | 70           | 0b11       | rrr        | VV1 0x70   VL256 V66 V0F NOVSR  MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                             | REG0=YMM_R():w:qq:u32 REG1=YMM_B():r:qq:u32 IMM0:r:b
 | VPSHUFD              | AVX512         | AVX512EVEX     | AVX512F_128    | 70           | 0b11       | rrr        | EVV 0x70 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR UIMM8()                  | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32 IMM0:r:b
 | VPSHUFD              | AVX512         | AVX512EVEX     | AVX512F_128    | 70           | mm         | rrr        | EVV 0x70 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
 | VPSHUFD              | AVX512         | AVX512EVEX     | AVX512F_256    | 70           | 0b11       | rrr        | EVV 0x70 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR UIMM8()                  | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u32 IMM0:r:b
 | VPSHUFD              | AVX512         | AVX512EVEX     | AVX512F_256    | 70           | mm         | rrr        | EVV 0x70 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
 | VPSHUFD              | AVX512         | AVX512EVEX     | AVX512F_512    | 70           | 0b11       | rrr        | EVV 0x70 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR UIMM8()                  | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu32 IMM0:r:b
 | VPSHUFD              | AVX512         | AVX512EVEX     | AVX512F_512    | 70           | mm         | rrr        | EVV 0x70 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
 | VPSHUFD              | KNC            | KNCE           |                | 70           | mm         | rrr        | KVV 0x70 V0F V66  REXW=0  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() NOSWIZD()          | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  MEM0:r:zd:TXT=NT IMM0:r:b
 | VPSHUFD              | KNC            | KNCE           |                | 70           | 0b11       | rrr        | KVV 0x70 V0F V66  REXW=0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] UIMM8() NR=1 SWIZ=0               | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw REG2=ZMM_B3():r:zd  IMM0:r:b
 | VPSHUFD              | KNC            | KNCE           |                | 70           | 0b11       | rrr        | KVV 0x70 V0F V66  REXW=0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] UIMM8() NR=0 SWIZ=0               | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw REG2=ZMM_B3():r:zd IMM0:r:b
 | VPSHUFHW             | AVX            | AVX            |                | 70           | mm         | rrr        | VV1 0x70  VL128 VF3 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                        | REG0=XMM_R():w:dq MEM0:r:dq  IMM0:r:b
 | VPSHUFHW             | AVX            | AVX            |                | 70           | 0b11       | rrr        | VV1 0x70  VL128 VF3 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                               | REG0=XMM_R():w:dq REG1=XMM_B():r:dq IMM0:r:b
 | VPSHUFHW             | AVX2           | AVX2           |                | 70           | mm         | rrr        | VV1 0x70   VL256 VF3 V0F NOVSR   MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                     | REG0=YMM_R():w:qq:u16 MEM0:r:qq:u16  IMM0:r:b
 | VPSHUFHW             | AVX2           | AVX2           |                | 70           | 0b11       | rrr        | VV1 0x70   VL256 VF3 V0F NOVSR  MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                             | REG0=YMM_R():w:qq:u16 REG1=YMM_B():r:qq:u16 IMM0:r:b
 | VPSHUFHW             | AVX512         | AVX512EVEX     | AVX512BW_128   | 70           | 0b11       | rrr        | EVV 0x70 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR UIMM8()                    | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u16 IMM0:r:b
 | VPSHUFHW             | AVX512         | AVX512EVEX     | AVX512BW_128   | 70           | mm         | rrr        | EVV 0x70 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128    NOEVSR UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u16 IMM0:r:b
 | VPSHUFHW             | AVX512         | AVX512EVEX     | AVX512BW_256   | 70           | 0b11       | rrr        | EVV 0x70 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256    NOEVSR UIMM8()                    | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u16 IMM0:r:b
 | VPSHUFHW             | AVX512         | AVX512EVEX     | AVX512BW_256   | 70           | mm         | rrr        | EVV 0x70 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256    NOEVSR UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u16 IMM0:r:b
 | VPSHUFHW             | AVX512         | AVX512EVEX     | AVX512BW_512   | 70           | 0b11       | rrr        | EVV 0x70 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512    NOEVSR UIMM8()                    | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu16 IMM0:r:b
 | VPSHUFHW             | AVX512         | AVX512EVEX     | AVX512BW_512   | 70           | mm         | rrr        | EVV 0x70 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512    NOEVSR UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:u16 IMM0:r:b
 | VPSHUFLW             | AVX            | AVX            |                | 70           | mm         | rrr        | VV1 0x70  VL128 VF2 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                        | REG0=XMM_R():w:dq MEM0:r:dq  IMM0:r:b
 | VPSHUFLW             | AVX            | AVX            |                | 70           | 0b11       | rrr        | VV1 0x70  VL128 VF2 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                               | REG0=XMM_R():w:dq REG1=XMM_B():r:dq IMM0:r:b
 | VPSHUFLW             | AVX2           | AVX2           |                | 70           | mm         | rrr        | VV1 0x70   VL256 VF2 V0F NOVSR   MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                     | REG0=YMM_R():w:qq:u16 MEM0:r:qq:u16  IMM0:r:b
 | VPSHUFLW             | AVX2           | AVX2           |                | 70           | 0b11       | rrr        | VV1 0x70   VL256 VF2 V0F NOVSR  MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                             | REG0=YMM_R():w:qq:u16 REG1=YMM_B():r:qq:u16 IMM0:r:b
 | VPSHUFLW             | AVX512         | AVX512EVEX     | AVX512BW_128   | 70           | 0b11       | rrr        | EVV 0x70 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR UIMM8()                    | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u16 IMM0:r:b
 | VPSHUFLW             | AVX512         | AVX512EVEX     | AVX512BW_128   | 70           | mm         | rrr        | EVV 0x70 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128    NOEVSR UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u16 IMM0:r:b
 | VPSHUFLW             | AVX512         | AVX512EVEX     | AVX512BW_256   | 70           | 0b11       | rrr        | EVV 0x70 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256    NOEVSR UIMM8()                    | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u16 IMM0:r:b
 | VPSHUFLW             | AVX512         | AVX512EVEX     | AVX512BW_256   | 70           | mm         | rrr        | EVV 0x70 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256    NOEVSR UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u16 IMM0:r:b
 | VPSHUFLW             | AVX512         | AVX512EVEX     | AVX512BW_512   | 70           | 0b11       | rrr        | EVV 0x70 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512    NOEVSR UIMM8()                    | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu16 IMM0:r:b
 | VPSHUFLW             | AVX512         | AVX512EVEX     | AVX512BW_512   | 70           | mm         | rrr        | EVV 0x70 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512    NOEVSR UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:u16 IMM0:r:b
 | VPSIGNB              | AVX            | AVX            |                | 08           | mm         | rrr        | VV1 0x08  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 MEM0:r:dq:i8
 | VPSIGNB              | AVX            | AVX            |                | 08           | 0b11       | rrr        | VV1 0x08  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 REG2=XMM_B():r:dq:i8
 | VPSIGNB              | AVX2           | AVX2           |                | 08           | mm         | rrr        | VV1 0x08  VL256 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=YMM_R():w:qq:i8 REG1=YMM_N():r:qq:i8 MEM0:r:qq:i8
 | VPSIGNB              | AVX2           | AVX2           |                | 08           | 0b11       | rrr        | VV1 0x08  VL256 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=YMM_R():w:qq:i8 REG1=YMM_N():r:qq:i8 REG2=YMM_B():r:qq:i8
 | VPSIGND              | AVX            | AVX            |                | 0a           | mm         | rrr        | VV1 0x0A  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32
 | VPSIGND              | AVX            | AVX            |                | 0a           | 0b11       | rrr        | VV1 0x0A  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32
 | VPSIGND              | AVX2           | AVX2           |                | 0a           | mm         | rrr        | VV1 0x0A  VL256 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=YMM_R():w:qq:i32 REG1=YMM_N():r:qq:i32 MEM0:r:qq:i32
 | VPSIGND              | AVX2           | AVX2           |                | 0a           | 0b11       | rrr        | VV1 0x0A  VL256 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=YMM_R():w:qq:i32 REG1=YMM_N():r:qq:i32 REG2=YMM_B():r:qq:i32
 | VPSIGNW              | AVX            | AVX            |                | 09           | mm         | rrr        | VV1 0x09  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
 | VPSIGNW              | AVX            | AVX            |                | 09           | 0b11       | rrr        | VV1 0x09  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
 | VPSIGNW              | AVX2           | AVX2           |                | 09           | mm         | rrr        | VV1 0x09  VL256 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=YMM_R():w:qq:i16 REG1=YMM_N():r:qq:i16 MEM0:r:qq:i16
 | VPSIGNW              | AVX2           | AVX2           |                | 09           | 0b11       | rrr        | VV1 0x09  VL256 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=YMM_R():w:qq:i16 REG1=YMM_N():r:qq:i16 REG2=YMM_B():r:qq:i16
 | VPSLLD               | AVX            | AVX            |                | f2           | mm         | rrr        | VV1 0xF2  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 MEM0:r:dq:u64
 | VPSLLD               | AVX            | AVX            |                | f2           | 0b11       | rrr        | VV1 0xF2  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 REG2=XMM_B():r:dq:u64
 | VPSLLD               | AVX            | AVX            |                | 72           | 0b11       | 0b110      | VV1 0x72  VL128 V66 V0F MOD[0b11] MOD=3 REG[0b110] RM[nnn] UIMM8()                                   | REG0=XMM_N():w:dq:u32 REG1=XMM_B():r:dq:u32 IMM0:r:b  #NDD
 | VPSLLD               | AVX2           | AVX2           |                | f2           | mm         | rrr        | VV1 0xF2  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u32 REG1=YMM_N():r:qq:u32 MEM0:r:dq:u64
 | VPSLLD               | AVX2           | AVX2           |                | f2           | 0b11       | rrr        | VV1 0xF2  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:u32 REG1=YMM_N():r:qq:u32 REG2=XMM_B():r:q:u64
 | VPSLLD               | AVX2           | AVX2           |                | 72           | 0b11       | 0b110      | VV1 0x72   VL256  V66 V0F MOD[0b11] MOD=3 REG[0b110] RM[nnn] UIMM8()                                 | REG0=YMM_N():w:qq:u32 REG1=YMM_B():r:qq:u32 IMM0:r:b  #NDD
 | VPSLLD               | AVX512         | AVX512EVEX     | AVX512F_128    | f2           | 0b11       | rrr        | EVV 0xF2 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                  | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
 | VPSLLD               | AVX512         | AVX512EVEX     | AVX512F_128    | f2           | mm         | rrr        | EVV 0xF2 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_MEM128() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:dq:u32
 | VPSLLD               | AVX512         | AVX512EVEX     | AVX512F_128    | 72           | 0b11       | 0b110      | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b110] RM[nnn]  VL128  W0   UIMM8()                      | REG0=XMM_N3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32 IMM0:r:b
 | VPSLLD               | AVX512         | AVX512EVEX     | AVX512F_128    | 72           | mm         | 0b110      | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b110] RM[nnn]  MODRM()  VL128  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_N3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
 | VPSLLD               | AVX512         | AVX512EVEX     | AVX512F_256    | f2           | 0b11       | rrr        | EVV 0xF2 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                  | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=XMM_B3():r:dq:u32
 | VPSLLD               | AVX512         | AVX512EVEX     | AVX512F_256    | f2           | mm         | rrr        | EVV 0xF2 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_MEM128() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:dq:u32
 | VPSLLD               | AVX512         | AVX512EVEX     | AVX512F_256    | 72           | 0b11       | 0b110      | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b110] RM[nnn]  VL256  W0   UIMM8()                      | REG0=YMM_N3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u32 IMM0:r:b
 | VPSLLD               | AVX512         | AVX512EVEX     | AVX512F_256    | 72           | mm         | 0b110      | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b110] RM[nnn]  MODRM()  VL256  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_N3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
 | VPSLLD               | AVX512         | AVX512EVEX     | AVX512F_512    | f2           | 0b11       | rrr        | EVV 0xF2 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                  | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=XMM_B3():r:dq:u32
 | VPSLLD               | AVX512         | AVX512EVEX     | AVX512F_512    | f2           | mm         | rrr        | EVV 0xF2 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_MEM128() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:dq:u32
 | VPSLLD               | AVX512         | AVX512EVEX     | AVX512F_512    | 72           | 0b11       | 0b110      | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b110] RM[nnn]  VL512  W0   UIMM8()                      | REG0=ZMM_N3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu32 IMM0:r:b
 | VPSLLD               | AVX512         | AVX512EVEX     | AVX512F_512    | 72           | mm         | 0b110      | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b110] RM[nnn]  MODRM()  VL512  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_N3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
 | VPSLLD               | KNC            | KNCE           |                | 72           | 0b11       | 0b110      | KVV 0x72 V0F V66  REXW=0  MOD[0b11]  MOD=3 REG[0b110] RM[nnn] UIMM8() NR=0 REG_SWIZZLE32()           | REG0=ZMM_N3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_B3():r:zf32:TXT=REGSWIZ IMM0:r:b
 | VPSLLD               | KNC            | KNCE           |                | 72           | 0b11       | 0b110      | KVV 0x72 V0F V66  REXW=0  MOD[0b11]  MOD=3 REG[0b110] RM[nnn] UIMM8() NR=1 SWIZ=0                    | REG0=ZMM_N3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_B3():r:zf32 IMM0:r:b
 | VPSLLD               | KNC            | KNCE           |                | 72           | mm         | 0b110      | KVV 0x72 V0F V66  REXW=0  MOD[mm]  MOD!=3 REG[0b110] RM[nnn] MODRM() UIMM8() UPCONVERT_INT32()       | REG0=ZMM_N3():rw:zf32 REG1=MASK1():r:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT IMM0:r:b
 | VPSLLDQ              | AVX            | AVX            |                | 73           | 0b11       | 0b111      | VV1 0x73  VL128 V66 V0F MOD[0b11] MOD=3 REG[0b111] RM[nnn] UIMM8()                                   | REG0=XMM_N():w:dq:u128 REG1=XMM_B():r:dq:u128 IMM0:r:b   # NDD
 | VPSLLDQ              | AVX2           | AVX2           |                | 73           | 0b11       | 0b111      | VV1 0x73  VL256 V66 V0F   MOD[0b11] MOD=3 REG[0b111] RM[nnn] UIMM8()                                 | REG0=YMM_N():w:qq:u128 REG1=YMM_B():r:qq:u128 IMM0:r:b   # NDD
 | VPSLLDQ              | AVX512         | AVX512EVEX     | AVX512BW_128   | 73           | 0b11       | 0b111      | EVV 0x73 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b111] RM[nnn]  VL128      ZEROING=0 MASK=0 UIMM8()      | REG0=XMM_N3():w:dq:u8 REG1=XMM_B3():r:dq:u8 IMM0:r:b
 | VPSLLDQ              | AVX512         | AVX512EVEX     | AVX512BW_128   | 73           | mm         | 0b111      | EVV 0x73 V66 V0F MOD[mm] MOD!=3 REG[0b111] RM[nnn] BCRC=0 MODRM()  VL128      ZEROING=0 MASK=0 UIMM8()  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_N3():w:dq:u8 MEM0:r:dq:u8 IMM0:r:b
 | VPSLLDQ              | AVX512         | AVX512EVEX     | AVX512BW_256   | 73           | 0b11       | 0b111      | EVV 0x73 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b111] RM[nnn]  VL256      ZEROING=0 MASK=0 UIMM8()      | REG0=YMM_N3():w:qq:u8 REG1=YMM_B3():r:qq:u8 IMM0:r:b
 | VPSLLDQ              | AVX512         | AVX512EVEX     | AVX512BW_256   | 73           | mm         | 0b111      | EVV 0x73 V66 V0F MOD[mm] MOD!=3 REG[0b111] RM[nnn] BCRC=0 MODRM()  VL256      ZEROING=0 MASK=0 UIMM8()  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_N3():w:qq:u8 MEM0:r:qq:u8 IMM0:r:b
 | VPSLLDQ              | AVX512         | AVX512EVEX     | AVX512BW_512   | 73           | 0b11       | 0b111      | EVV 0x73 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b111] RM[nnn]  VL512      ZEROING=0 MASK=0 UIMM8()      | REG0=ZMM_N3():w:zu8 REG1=ZMM_B3():r:zu8 IMM0:r:b
 | VPSLLDQ              | AVX512         | AVX512EVEX     | AVX512BW_512   | 73           | mm         | 0b111      | EVV 0x73 V66 V0F MOD[mm] MOD!=3 REG[0b111] RM[nnn] BCRC=0 MODRM()  VL512      ZEROING=0 MASK=0 UIMM8()  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_N3():w:zu8 MEM0:r:zd:u8 IMM0:r:b
 | VPSLLQ               | AVX            | AVX            |                | f3           | mm         | rrr        | VV1 0xF3  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 MEM0:r:dq:u64
 | VPSLLQ               | AVX            | AVX            |                | f3           | 0b11       | rrr        | VV1 0xF3  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 REG2=XMM_B():r:dq:u64
 | VPSLLQ               | AVX            | AVX            |                | 73           | 0b11       | 0b110      | VV1 0x73  VL128 V66 V0F MOD[0b11] MOD=3 REG[0b110] RM[nnn] UIMM8()                                   | REG0=XMM_N():w:dq:u64 REG1=XMM_B():r:dq:u64 IMM0:r:b # NDD
 | VPSLLQ               | AVX2           | AVX2           |                | f3           | mm         | rrr        | VV1 0xF3  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u64 REG1=YMM_N():r:qq:u64 MEM0:r:dq:u64
 | VPSLLQ               | AVX2           | AVX2           |                | f3           | 0b11       | rrr        | VV1 0xF3  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:u64 REG1=YMM_N():r:qq:u64 REG2=XMM_B():r:q:u64
 | VPSLLQ               | AVX2           | AVX2           |                | 73           | 0b11       | 0b110      | VV1 0x73   VL256  V66 V0F MOD[0b11] MOD=3 REG[0b110] RM[nnn] UIMM8()                                 | REG0=YMM_N():w:qq:u64 REG1=YMM_B():r:qq:u64 IMM0:r:b # NDD
 | VPSLLQ               | AVX512         | AVX512EVEX     | AVX512F_128    | f3           | 0b11       | rrr        | EVV 0xF3 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                  | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
 | VPSLLQ               | AVX512         | AVX512EVEX     | AVX512F_128    | f3           | mm         | rrr        | EVV 0xF3 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_MEM128() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:dq:u64
 | VPSLLQ               | AVX512         | AVX512EVEX     | AVX512F_128    | 73           | 0b11       | 0b110      | EVV 0x73 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b110] RM[nnn]  VL128  W1   UIMM8()                      | REG0=XMM_N3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u64 IMM0:r:b
 | VPSLLQ               | AVX512         | AVX512EVEX     | AVX512F_128    | 73           | mm         | 0b110      | EVV 0x73 V66 V0F MOD[mm] MOD!=3 REG[0b110] RM[nnn]  MODRM()  VL128  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_N3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VPSLLQ               | AVX512         | AVX512EVEX     | AVX512F_256    | f3           | 0b11       | rrr        | EVV 0xF3 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                  | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=XMM_B3():r:dq:u64
 | VPSLLQ               | AVX512         | AVX512EVEX     | AVX512F_256    | f3           | mm         | rrr        | EVV 0xF3 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_MEM128() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:dq:u64
 | VPSLLQ               | AVX512         | AVX512EVEX     | AVX512F_256    | 73           | 0b11       | 0b110      | EVV 0x73 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b110] RM[nnn]  VL256  W1   UIMM8()                      | REG0=YMM_N3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u64 IMM0:r:b
 | VPSLLQ               | AVX512         | AVX512EVEX     | AVX512F_256    | 73           | mm         | 0b110      | EVV 0x73 V66 V0F MOD[mm] MOD!=3 REG[0b110] RM[nnn]  MODRM()  VL256  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_N3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VPSLLQ               | AVX512         | AVX512EVEX     | AVX512F_512    | f3           | 0b11       | rrr        | EVV 0xF3 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                  | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=XMM_B3():r:dq:u64
 | VPSLLQ               | AVX512         | AVX512EVEX     | AVX512F_512    | f3           | mm         | rrr        | EVV 0xF3 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_MEM128() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:dq:u64
 | VPSLLQ               | AVX512         | AVX512EVEX     | AVX512F_512    | 73           | 0b11       | 0b110      | EVV 0x73 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b110] RM[nnn]  VL512  W1   UIMM8()                      | REG0=ZMM_N3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu64 IMM0:r:b
 | VPSLLQ               | AVX512         | AVX512EVEX     | AVX512F_512    | 73           | mm         | 0b110      | EVV 0x73 V66 V0F MOD[mm] MOD!=3 REG[0b110] RM[nnn]  MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_N3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VPSLLVD              | AVX2           | AVX2           |                | 47           | mm         | rrr        | VV1 0x47  VL128 V0F38 V66  W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq REG1=XMM_N():r:dq MEM0:r:dq
 | VPSLLVD              | AVX2           | AVX2           |                | 47           | 0b11       | rrr        | VV1 0x47  VL128 V0F38 V66 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=XMM_R():w:dq REG1=XMM_N():r:dq REG2=XMM_B():r:dq
 | VPSLLVD              | AVX2           | AVX2           |                | 47           | mm         | rrr        | VV1 0x47  VL256 V0F38 V66  W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=YMM_R():w:qq REG1=YMM_N():r:qq MEM0:r:qq
 | VPSLLVD              | AVX2           | AVX2           |                | 47           | 0b11       | rrr        | VV1 0x47  VL256 V0F38 V66 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=YMM_R():w:qq REG1=YMM_N():r:qq REG2=YMM_B():r:qq
 | VPSLLVD              | AVX512         | AVX512EVEX     | AVX512F_128    | 47           | 0b11       | rrr        | EVV 0x47 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
 | VPSLLVD              | AVX512         | AVX512EVEX     | AVX512F_128    | 47           | mm         | rrr        | EVV 0x47 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPSLLVD              | AVX512         | AVX512EVEX     | AVX512F_256    | 47           | 0b11       | rrr        | EVV 0x47 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
 | VPSLLVD              | AVX512         | AVX512EVEX     | AVX512F_256    | 47           | mm         | rrr        | EVV 0x47 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPSLLVD              | AVX512         | AVX512EVEX     | AVX512F_512    | 47           | 0b11       | rrr        | EVV 0x47 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
 | VPSLLVD              | AVX512         | AVX512EVEX     | AVX512F_512    | 47           | mm         | rrr        | EVV 0x47 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPSLLVD              | KNC            | KNCE           |                | 47           | mm         | rrr        | KVV 0x47 V0F38 V66  REXW=0  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT32()               | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw REG2=ZMM_N3():r:zd MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VPSLLVD              | KNC            | KNCE           |                | 47           | 0b11       | rrr        | KVV 0x47 V0F38 V66  REXW=0  MOD[0b11]  MOD=3 REG[rrr] RM[nnn] NR=1                                   | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw REG2=ZMM_N3():r:zd  REG3=ZMM_B3():r:zd
 | VPSLLVD              | KNC            | KNCE           |                | 47           | 0b11       | rrr        | KVV 0x47 V0F38 V66  REXW=0  MOD[0b11]  MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                   | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw REG2=ZMM_N3():r:zd  REG3=ZMM_B3():r:zd:TXT=REGSWIZ
 | VPSLLVQ              | AVX2           | AVX2           |                | 47           | mm         | rrr        | VV1 0x47  VL128 V0F38 V66  W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq REG1=XMM_N():r:dq MEM0:r:dq
 | VPSLLVQ              | AVX2           | AVX2           |                | 47           | 0b11       | rrr        | VV1 0x47  VL128 V0F38 V66 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=XMM_R():w:dq REG1=XMM_N():r:dq REG2=XMM_B():r:dq
 | VPSLLVQ              | AVX2           | AVX2           |                | 47           | mm         | rrr        | VV1 0x47  VL256 V0F38 V66  W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=YMM_R():w:qq REG1=YMM_N():r:qq MEM0:r:qq
 | VPSLLVQ              | AVX2           | AVX2           |                | 47           | 0b11       | rrr        | VV1 0x47  VL256 V0F38 V66 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=YMM_R():w:qq REG1=YMM_N():r:qq REG2=YMM_B():r:qq
 | VPSLLVQ              | AVX512         | AVX512EVEX     | AVX512F_128    | 47           | 0b11       | rrr        | EVV 0x47 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
 | VPSLLVQ              | AVX512         | AVX512EVEX     | AVX512F_128    | 47           | mm         | rrr        | EVV 0x47 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPSLLVQ              | AVX512         | AVX512EVEX     | AVX512F_256    | 47           | 0b11       | rrr        | EVV 0x47 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
 | VPSLLVQ              | AVX512         | AVX512EVEX     | AVX512F_256    | 47           | mm         | rrr        | EVV 0x47 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPSLLVQ              | AVX512         | AVX512EVEX     | AVX512F_512    | 47           | 0b11       | rrr        | EVV 0x47 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
 | VPSLLVQ              | AVX512         | AVX512EVEX     | AVX512F_512    | 47           | mm         | rrr        | EVV 0x47 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPSLLVW              | AVX512         | AVX512EVEX     | AVX512BW_128   | 12           | 0b11       | rrr        | EVV 0x12 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
 | VPSLLVW              | AVX512         | AVX512EVEX     | AVX512BW_128   | 12           | mm         | rrr        | EVV 0x12 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
 | VPSLLVW              | AVX512         | AVX512EVEX     | AVX512BW_256   | 12           | 0b11       | rrr        | EVV 0x12 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
 | VPSLLVW              | AVX512         | AVX512EVEX     | AVX512BW_256   | 12           | mm         | rrr        | EVV 0x12 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
 | VPSLLVW              | AVX512         | AVX512EVEX     | AVX512BW_512   | 12           | 0b11       | rrr        | EVV 0x12 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
 | VPSLLVW              | AVX512         | AVX512EVEX     | AVX512BW_512   | 12           | mm         | rrr        | EVV 0x12 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
 | VPSLLW               | AVX            | AVX            |                | f1           | mm         | rrr        | VV1 0xF1  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 MEM0:r:dq:u64
 | VPSLLW               | AVX            | AVX            |                | f1           | 0b11       | rrr        | VV1 0xF1  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 REG2=XMM_B():r:dq:u64
 | VPSLLW               | AVX            | AVX            |                | 71           | 0b11       | 0b110      | VV1 0x71  VL128 V66 V0F MOD[0b11] MOD=3 REG[0b110] RM[nnn] UIMM8()                                   | REG0=XMM_N():w:dq:u16 REG1=XMM_B():r:dq:u16 IMM0:r:b # NDD
 | VPSLLW               | AVX2           | AVX2           |                | f1           | mm         | rrr        | VV1 0xF1  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u16 REG1=YMM_N():r:qq:u16 MEM0:r:dq:u64
 | VPSLLW               | AVX2           | AVX2           |                | f1           | 0b11       | rrr        | VV1 0xF1  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:u16 REG1=YMM_N():r:qq:u16 REG2=XMM_B():r:q:u64
 | VPSLLW               | AVX2           | AVX2           |                | 71           | 0b11       | 0b110      | VV1 0x71   VL256  V66 V0F MOD[0b11] MOD=3 REG[0b110] RM[nnn] UIMM8()                                 | REG0=YMM_N():w:qq:u16 REG1=YMM_B():r:qq:u16 IMM0:r:b # NDD
 | VPSLLW               | AVX512         | AVX512EVEX     | AVX512BW_128   | f1           | 0b11       | rrr        | EVV 0xF1 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                      | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
 | VPSLLW               | AVX512         | AVX512EVEX     | AVX512BW_128   | f1           | mm         | rrr        | EVV 0xF1 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_MEM128() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
 | VPSLLW               | AVX512         | AVX512EVEX     | AVX512BW_128   | 71           | 0b11       | 0b110      | EVV 0x71 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b110] RM[nnn]  VL128     UIMM8()                        | REG0=XMM_N3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u16 IMM0:r:b
 | VPSLLW               | AVX512         | AVX512EVEX     | AVX512BW_128   | 71           | mm         | 0b110      | EVV 0x71 V66 V0F MOD[mm] MOD!=3 REG[0b110] RM[nnn] BCRC=0 MODRM()  VL128     UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_N3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u16 IMM0:r:b
 | VPSLLW               | AVX512         | AVX512EVEX     | AVX512BW_256   | f1           | 0b11       | rrr        | EVV 0xF1 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                      | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=XMM_B3():r:dq:u16
 | VPSLLW               | AVX512         | AVX512EVEX     | AVX512BW_256   | f1           | mm         | rrr        | EVV 0xF1 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_MEM128() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:dq:u16
 | VPSLLW               | AVX512         | AVX512EVEX     | AVX512BW_256   | 71           | 0b11       | 0b110      | EVV 0x71 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b110] RM[nnn]  VL256     UIMM8()                        | REG0=YMM_N3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u16 IMM0:r:b
 | VPSLLW               | AVX512         | AVX512EVEX     | AVX512BW_256   | 71           | mm         | 0b110      | EVV 0x71 V66 V0F MOD[mm] MOD!=3 REG[0b110] RM[nnn] BCRC=0 MODRM()  VL256     UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_N3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u16 IMM0:r:b
 | VPSLLW               | AVX512         | AVX512EVEX     | AVX512BW_512   | f1           | 0b11       | rrr        | EVV 0xF1 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                      | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=XMM_B3():r:dq:u16
 | VPSLLW               | AVX512         | AVX512EVEX     | AVX512BW_512   | f1           | mm         | rrr        | EVV 0xF1 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_MEM128() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:dq:u16
 | VPSLLW               | AVX512         | AVX512EVEX     | AVX512BW_512   | 71           | 0b11       | 0b110      | EVV 0x71 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b110] RM[nnn]  VL512     UIMM8()                        | REG0=ZMM_N3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu16 IMM0:r:b
 | VPSLLW               | AVX512         | AVX512EVEX     | AVX512BW_512   | 71           | mm         | 0b110      | EVV 0x71 V66 V0F MOD[mm] MOD!=3 REG[0b110] RM[nnn] BCRC=0 MODRM()  VL512     UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_N3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:u16 IMM0:r:b
 | VPSRAD               | AVX            | AVX            |                | e2           | mm         | rrr        | VV1 0xE2  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 MEM0:r:dq:u64
 | VPSRAD               | AVX            | AVX            |                | e2           | 0b11       | rrr        | VV1 0xE2  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:u64
 | VPSRAD               | AVX            | AVX            |                | 72           | 0b11       | 0b100      | VV1 0x72  VL128 V66 V0F MOD[0b11] MOD=3 REG[0b100] RM[nnn] UIMM8()                                   | REG0=XMM_N():w:dq:i32 REG1=XMM_B():r:dq:i32 IMM0:r:b # NDD
 | VPSRAD               | AVX2           | AVX2           |                | e2           | mm         | rrr        | VV1 0xE2  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:i32 REG1=YMM_N():r:qq:i32 MEM0:r:dq:u64
 | VPSRAD               | AVX2           | AVX2           |                | e2           | 0b11       | rrr        | VV1 0xE2  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:i32 REG1=YMM_N():r:qq:i32 REG2=XMM_B():r:q:u64
 | VPSRAD               | AVX2           | AVX2           |                | 72           | 0b11       | 0b100      | VV1 0x72   VL256  V66 V0F MOD[0b11] MOD=3 REG[0b100] RM[nnn] UIMM8()                                 | REG0=YMM_N():w:qq:i32 REG1=YMM_B():r:qq:i32 IMM0:r:b # NDD
 | VPSRAD               | AVX512         | AVX512EVEX     | AVX512F_128    | e2           | 0b11       | rrr        | EVV 0xE2 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                  | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
 | VPSRAD               | AVX512         | AVX512EVEX     | AVX512F_128    | e2           | mm         | rrr        | EVV 0xE2 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_MEM128() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:dq:u32
 | VPSRAD               | AVX512         | AVX512EVEX     | AVX512F_128    | 72           | 0b11       | 0b100      | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b100] RM[nnn]  VL128  W0   UIMM8()                      | REG0=XMM_N3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32 IMM0:r:b
 | VPSRAD               | AVX512         | AVX512EVEX     | AVX512F_128    | 72           | mm         | 0b100      | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b100] RM[nnn]  MODRM()  VL128  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_N3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
 | VPSRAD               | AVX512         | AVX512EVEX     | AVX512F_256    | e2           | 0b11       | rrr        | EVV 0xE2 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                  | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=XMM_B3():r:dq:u32
 | VPSRAD               | AVX512         | AVX512EVEX     | AVX512F_256    | e2           | mm         | rrr        | EVV 0xE2 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_MEM128() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:dq:u32
 | VPSRAD               | AVX512         | AVX512EVEX     | AVX512F_256    | 72           | 0b11       | 0b100      | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b100] RM[nnn]  VL256  W0   UIMM8()                      | REG0=YMM_N3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u32 IMM0:r:b
 | VPSRAD               | AVX512         | AVX512EVEX     | AVX512F_256    | 72           | mm         | 0b100      | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b100] RM[nnn]  MODRM()  VL256  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_N3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
 | VPSRAD               | AVX512         | AVX512EVEX     | AVX512F_512    | e2           | 0b11       | rrr        | EVV 0xE2 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                  | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=XMM_B3():r:dq:u32
 | VPSRAD               | AVX512         | AVX512EVEX     | AVX512F_512    | e2           | mm         | rrr        | EVV 0xE2 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_MEM128() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:dq:u32
 | VPSRAD               | AVX512         | AVX512EVEX     | AVX512F_512    | 72           | 0b11       | 0b100      | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b100] RM[nnn]  VL512  W0   UIMM8()                      | REG0=ZMM_N3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu32 IMM0:r:b
 | VPSRAD               | AVX512         | AVX512EVEX     | AVX512F_512    | 72           | mm         | 0b100      | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b100] RM[nnn]  MODRM()  VL512  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_N3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
 | VPSRAD               | KNC            | KNCE           |                | 72           | 0b11       | 0b100      | KVV 0x72 V0F V66  REXW=0  MOD[0b11]  MOD=3 REG[0b100] RM[nnn] UIMM8() NR=0 REG_SWIZZLE32()           | REG0=ZMM_N3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_B3():r:zf32:TXT=REGSWIZ IMM0:r:b
 | VPSRAD               | KNC            | KNCE           |                | 72           | 0b11       | 0b100      | KVV 0x72 V0F V66  REXW=0  MOD[0b11]  MOD=3 REG[0b100] RM[nnn] UIMM8() NR=1 SWIZ=0                    | REG0=ZMM_N3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_B3():r:zf32 IMM0:r:b
 | VPSRAD               | KNC            | KNCE           |                | 72           | mm         | 0b100      | KVV 0x72 V0F V66  REXW=0  MOD[mm]  MOD!=3 REG[0b100] RM[nnn] MODRM() UIMM8() UPCONVERT_INT32()       | REG0=ZMM_N3():rw:zf32 REG1=MASK1():r:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT IMM0:r:b
 | VPSRAQ               | AVX512         | AVX512EVEX     | AVX512F_128    | e2           | 0b11       | rrr        | EVV 0xE2 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                  | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
 | VPSRAQ               | AVX512         | AVX512EVEX     | AVX512F_128    | e2           | mm         | rrr        | EVV 0xE2 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_MEM128() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:dq:u64
 | VPSRAQ               | AVX512         | AVX512EVEX     | AVX512F_128    | 72           | 0b11       | 0b100      | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b100] RM[nnn]  VL128  W1   UIMM8()                      | REG0=XMM_N3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u64 IMM0:r:b
 | VPSRAQ               | AVX512         | AVX512EVEX     | AVX512F_128    | 72           | mm         | 0b100      | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b100] RM[nnn]  MODRM()  VL128  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_N3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VPSRAQ               | AVX512         | AVX512EVEX     | AVX512F_256    | e2           | 0b11       | rrr        | EVV 0xE2 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                  | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=XMM_B3():r:dq:u64
 | VPSRAQ               | AVX512         | AVX512EVEX     | AVX512F_256    | e2           | mm         | rrr        | EVV 0xE2 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_MEM128() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:dq:u64
 | VPSRAQ               | AVX512         | AVX512EVEX     | AVX512F_256    | 72           | 0b11       | 0b100      | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b100] RM[nnn]  VL256  W1   UIMM8()                      | REG0=YMM_N3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u64 IMM0:r:b
 | VPSRAQ               | AVX512         | AVX512EVEX     | AVX512F_256    | 72           | mm         | 0b100      | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b100] RM[nnn]  MODRM()  VL256  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_N3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VPSRAQ               | AVX512         | AVX512EVEX     | AVX512F_512    | e2           | 0b11       | rrr        | EVV 0xE2 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                  | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=XMM_B3():r:dq:u64
 | VPSRAQ               | AVX512         | AVX512EVEX     | AVX512F_512    | e2           | mm         | rrr        | EVV 0xE2 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_MEM128() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:dq:u64
 | VPSRAQ               | AVX512         | AVX512EVEX     | AVX512F_512    | 72           | 0b11       | 0b100      | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b100] RM[nnn]  VL512  W1   UIMM8()                      | REG0=ZMM_N3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu64 IMM0:r:b
 | VPSRAQ               | AVX512         | AVX512EVEX     | AVX512F_512    | 72           | mm         | 0b100      | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b100] RM[nnn]  MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_N3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VPSRAVD              | AVX2           | AVX2           |                | 46           | mm         | rrr        | VV1 0x46  VL128 V0F38 V66  W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq REG1=XMM_N():r:dq MEM0:r:dq
 | VPSRAVD              | AVX2           | AVX2           |                | 46           | 0b11       | rrr        | VV1 0x46  VL128 V0F38 V66 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=XMM_R():w:dq REG1=XMM_N():r:dq REG2=XMM_B():r:dq
 | VPSRAVD              | AVX2           | AVX2           |                | 46           | mm         | rrr        | VV1 0x46  VL256 V0F38 V66  W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=YMM_R():w:qq REG1=YMM_N():r:qq MEM0:r:qq
 | VPSRAVD              | AVX2           | AVX2           |                | 46           | 0b11       | rrr        | VV1 0x46  VL256 V0F38 V66 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=YMM_R():w:qq REG1=YMM_N():r:qq REG2=YMM_B():r:qq
 | VPSRAVD              | AVX512         | AVX512EVEX     | AVX512F_128    | 46           | 0b11       | rrr        | EVV 0x46 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
 | VPSRAVD              | AVX512         | AVX512EVEX     | AVX512F_128    | 46           | mm         | rrr        | EVV 0x46 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPSRAVD              | AVX512         | AVX512EVEX     | AVX512F_256    | 46           | 0b11       | rrr        | EVV 0x46 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
 | VPSRAVD              | AVX512         | AVX512EVEX     | AVX512F_256    | 46           | mm         | rrr        | EVV 0x46 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPSRAVD              | AVX512         | AVX512EVEX     | AVX512F_512    | 46           | 0b11       | rrr        | EVV 0x46 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
 | VPSRAVD              | AVX512         | AVX512EVEX     | AVX512F_512    | 46           | mm         | rrr        | EVV 0x46 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPSRAVD              | KNC            | KNCE           |                | 46           | mm         | rrr        | KVV 0x46 V0F38 V66  REXW=0  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT32()               | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw REG2=ZMM_N3():r:zd MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VPSRAVD              | KNC            | KNCE           |                | 46           | 0b11       | rrr        | KVV 0x46 V0F38 V66  REXW=0  MOD[0b11]  MOD=3 REG[rrr] RM[nnn] NR=1                                   | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw REG2=ZMM_N3():r:zd  REG3=ZMM_B3():r:zd
 | VPSRAVD              | KNC            | KNCE           |                | 46           | 0b11       | rrr        | KVV 0x46 V0F38 V66  REXW=0  MOD[0b11]  MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                   | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw REG2=ZMM_N3():r:zd  REG3=ZMM_B3():r:zd:TXT=REGSWIZ
 | VPSRAVQ              | AVX512         | AVX512EVEX     | AVX512F_128    | 46           | 0b11       | rrr        | EVV 0x46 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
 | VPSRAVQ              | AVX512         | AVX512EVEX     | AVX512F_128    | 46           | mm         | rrr        | EVV 0x46 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPSRAVQ              | AVX512         | AVX512EVEX     | AVX512F_256    | 46           | 0b11       | rrr        | EVV 0x46 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
 | VPSRAVQ              | AVX512         | AVX512EVEX     | AVX512F_256    | 46           | mm         | rrr        | EVV 0x46 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPSRAVQ              | AVX512         | AVX512EVEX     | AVX512F_512    | 46           | 0b11       | rrr        | EVV 0x46 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
 | VPSRAVQ              | AVX512         | AVX512EVEX     | AVX512F_512    | 46           | mm         | rrr        | EVV 0x46 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPSRAVW              | AVX512         | AVX512EVEX     | AVX512BW_128   | 11           | 0b11       | rrr        | EVV 0x11 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
 | VPSRAVW              | AVX512         | AVX512EVEX     | AVX512BW_128   | 11           | mm         | rrr        | EVV 0x11 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
 | VPSRAVW              | AVX512         | AVX512EVEX     | AVX512BW_256   | 11           | 0b11       | rrr        | EVV 0x11 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
 | VPSRAVW              | AVX512         | AVX512EVEX     | AVX512BW_256   | 11           | mm         | rrr        | EVV 0x11 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
 | VPSRAVW              | AVX512         | AVX512EVEX     | AVX512BW_512   | 11           | 0b11       | rrr        | EVV 0x11 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
 | VPSRAVW              | AVX512         | AVX512EVEX     | AVX512BW_512   | 11           | mm         | rrr        | EVV 0x11 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
 | VPSRAW               | AVX            | AVX            |                | e1           | mm         | rrr        | VV1 0xE1  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:u64
 | VPSRAW               | AVX            | AVX            |                | e1           | 0b11       | rrr        | VV1 0xE1  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:u64
 | VPSRAW               | AVX            | AVX            |                | 71           | 0b11       | 0b100      | VV1 0x71  VL128 V66 V0F MOD[0b11] MOD=3 REG[0b100] RM[nnn] UIMM8()                                   | REG0=XMM_N():w:dq:i16 REG1=XMM_B():r:dq:i16 IMM0:r:b # NDD
 | VPSRAW               | AVX2           | AVX2           |                | e1           | mm         | rrr        | VV1 0xE1  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:i16 REG1=YMM_N():r:qq:i16 MEM0:r:dq:u64
 | VPSRAW               | AVX2           | AVX2           |                | e1           | 0b11       | rrr        | VV1 0xE1  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:i16 REG1=YMM_N():r:qq:i16 REG2=XMM_B():r:q:u64
 | VPSRAW               | AVX2           | AVX2           |                | 71           | 0b11       | 0b100      | VV1 0x71   VL256  V66 V0F MOD[0b11] MOD=3 REG[0b100] RM[nnn] UIMM8()                                 | REG0=YMM_N():w:qq:i16 REG1=YMM_B():r:qq:i16 IMM0:r:b # NDD
 | VPSRAW               | AVX512         | AVX512EVEX     | AVX512BW_128   | e1           | 0b11       | rrr        | EVV 0xE1 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                      | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
 | VPSRAW               | AVX512         | AVX512EVEX     | AVX512BW_128   | e1           | mm         | rrr        | EVV 0xE1 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_MEM128() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
 | VPSRAW               | AVX512         | AVX512EVEX     | AVX512BW_128   | 71           | 0b11       | 0b100      | EVV 0x71 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b100] RM[nnn]  VL128     UIMM8()                        | REG0=XMM_N3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u16 IMM0:r:b
 | VPSRAW               | AVX512         | AVX512EVEX     | AVX512BW_128   | 71           | mm         | 0b100      | EVV 0x71 V66 V0F MOD[mm] MOD!=3 REG[0b100] RM[nnn] BCRC=0 MODRM()  VL128     UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_N3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u16 IMM0:r:b
 | VPSRAW               | AVX512         | AVX512EVEX     | AVX512BW_256   | e1           | 0b11       | rrr        | EVV 0xE1 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                      | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=XMM_B3():r:dq:u16
 | VPSRAW               | AVX512         | AVX512EVEX     | AVX512BW_256   | e1           | mm         | rrr        | EVV 0xE1 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_MEM128() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:dq:u16
 | VPSRAW               | AVX512         | AVX512EVEX     | AVX512BW_256   | 71           | 0b11       | 0b100      | EVV 0x71 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b100] RM[nnn]  VL256     UIMM8()                        | REG0=YMM_N3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u16 IMM0:r:b
 | VPSRAW               | AVX512         | AVX512EVEX     | AVX512BW_256   | 71           | mm         | 0b100      | EVV 0x71 V66 V0F MOD[mm] MOD!=3 REG[0b100] RM[nnn] BCRC=0 MODRM()  VL256     UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_N3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u16 IMM0:r:b
 | VPSRAW               | AVX512         | AVX512EVEX     | AVX512BW_512   | e1           | 0b11       | rrr        | EVV 0xE1 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                      | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=XMM_B3():r:dq:u16
 | VPSRAW               | AVX512         | AVX512EVEX     | AVX512BW_512   | e1           | mm         | rrr        | EVV 0xE1 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_MEM128() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:dq:u16
 | VPSRAW               | AVX512         | AVX512EVEX     | AVX512BW_512   | 71           | 0b11       | 0b100      | EVV 0x71 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b100] RM[nnn]  VL512     UIMM8()                        | REG0=ZMM_N3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu16 IMM0:r:b
 | VPSRAW               | AVX512         | AVX512EVEX     | AVX512BW_512   | 71           | mm         | 0b100      | EVV 0x71 V66 V0F MOD[mm] MOD!=3 REG[0b100] RM[nnn] BCRC=0 MODRM()  VL512     UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_N3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:u16 IMM0:r:b
 | VPSRLD               | AVX            | AVX            |                | d2           | mm         | rrr        | VV1 0xD2  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 MEM0:r:dq:u64
 | VPSRLD               | AVX            | AVX            |                | d2           | 0b11       | rrr        | VV1 0xD2  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 REG2=XMM_B():r:dq:u64
 | VPSRLD               | AVX            | AVX            |                | 72           | 0b11       | 0b010      | VV1 0x72  VL128 V66 V0F MOD[0b11] MOD=3 REG[0b010] RM[nnn] UIMM8()                                   | REG0=XMM_N():w:dq:u32 REG1=XMM_B():r:dq:u32 IMM0:r:b # NDD
 | VPSRLD               | AVX2           | AVX2           |                | d2           | mm         | rrr        | VV1 0xD2  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u32 REG1=YMM_N():r:qq:u32 MEM0:r:dq:u64
 | VPSRLD               | AVX2           | AVX2           |                | d2           | 0b11       | rrr        | VV1 0xD2  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:u32 REG1=YMM_N():r:qq:u32 REG2=XMM_B():r:q:u64
 | VPSRLD               | AVX2           | AVX2           |                | 72           | 0b11       | 0b010      | VV1 0x72   VL256  V66 V0F MOD[0b11] MOD=3 REG[0b010] RM[nnn] UIMM8()                                 | REG0=YMM_N():w:qq:u32 REG1=YMM_B():r:qq:u32 IMM0:r:b # NDD
 | VPSRLD               | AVX512         | AVX512EVEX     | AVX512F_128    | d2           | 0b11       | rrr        | EVV 0xD2 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                  | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
 | VPSRLD               | AVX512         | AVX512EVEX     | AVX512F_128    | d2           | mm         | rrr        | EVV 0xD2 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_MEM128() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:dq:u32
 | VPSRLD               | AVX512         | AVX512EVEX     | AVX512F_128    | 72           | 0b11       | 0b010      | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b010] RM[nnn]  VL128  W0   UIMM8()                      | REG0=XMM_N3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32 IMM0:r:b
 | VPSRLD               | AVX512         | AVX512EVEX     | AVX512F_128    | 72           | mm         | 0b010      | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b010] RM[nnn]  MODRM()  VL128  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_N3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
 | VPSRLD               | AVX512         | AVX512EVEX     | AVX512F_256    | d2           | 0b11       | rrr        | EVV 0xD2 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                  | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=XMM_B3():r:dq:u32
 | VPSRLD               | AVX512         | AVX512EVEX     | AVX512F_256    | d2           | mm         | rrr        | EVV 0xD2 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_MEM128() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:dq:u32
 | VPSRLD               | AVX512         | AVX512EVEX     | AVX512F_256    | 72           | 0b11       | 0b010      | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b010] RM[nnn]  VL256  W0   UIMM8()                      | REG0=YMM_N3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u32 IMM0:r:b
 | VPSRLD               | AVX512         | AVX512EVEX     | AVX512F_256    | 72           | mm         | 0b010      | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b010] RM[nnn]  MODRM()  VL256  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_N3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
 | VPSRLD               | AVX512         | AVX512EVEX     | AVX512F_512    | d2           | 0b11       | rrr        | EVV 0xD2 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                  | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=XMM_B3():r:dq:u32
 | VPSRLD               | AVX512         | AVX512EVEX     | AVX512F_512    | d2           | mm         | rrr        | EVV 0xD2 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_MEM128() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:dq:u32
 | VPSRLD               | AVX512         | AVX512EVEX     | AVX512F_512    | 72           | 0b11       | 0b010      | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b010] RM[nnn]  VL512  W0   UIMM8()                      | REG0=ZMM_N3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu32 IMM0:r:b
 | VPSRLD               | AVX512         | AVX512EVEX     | AVX512F_512    | 72           | mm         | 0b010      | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b010] RM[nnn]  MODRM()  VL512  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_N3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
 | VPSRLD               | KNC            | KNCE           |                | 72           | 0b11       | 0b010      | KVV 0x72 V0F V66  REXW=0  MOD[0b11]  MOD=3 REG[0b010] RM[nnn] UIMM8() NR=0 REG_SWIZZLE32()           | REG0=ZMM_N3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_B3():r:zf32:TXT=REGSWIZ IMM0:r:b
 | VPSRLD               | KNC            | KNCE           |                | 72           | 0b11       | 0b010      | KVV 0x72 V0F V66  REXW=0  MOD[0b11]  MOD=3 REG[0b010] RM[nnn] UIMM8() NR=1 SWIZ=0                    | REG0=ZMM_N3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_B3():r:zf32 IMM0:r:b
 | VPSRLD               | KNC            | KNCE           |                | 72           | mm         | 0b010      | KVV 0x72 V0F V66  REXW=0  MOD[mm]  MOD!=3 REG[0b010] RM[nnn] MODRM() UIMM8() UPCONVERT_INT32()       | REG0=ZMM_N3():rw:zf32 REG1=MASK1():r:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT IMM0:r:b
 | VPSRLDQ              | AVX            | AVX            |                | 73           | 0b11       | 0b011      | VV1 0x73  VL128 V66 V0F MOD[0b11] MOD=3 REG[0b011] RM[nnn] UIMM8()                                   | REG0=XMM_N():w:dq:u128 REG1=XMM_B():r:dq:u128 IMM0:r:b   # NDD
 | VPSRLDQ              | AVX2           | AVX2           |                | 73           | 0b11       | 0b011      | VV1 0x73  VL256 V66 V0F   MOD[0b11] MOD=3 REG[0b011] RM[nnn] UIMM8()                                 | REG0=YMM_N():w:qq:u128 REG1=YMM_B():r:qq:u128 IMM0:r:b   # NDD
 | VPSRLDQ              | AVX512         | AVX512EVEX     | AVX512BW_128   | 73           | 0b11       | 0b011      | EVV 0x73 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b011] RM[nnn]  VL128      ZEROING=0 MASK=0 UIMM8()      | REG0=XMM_N3():w:dq:u8 REG1=XMM_B3():r:dq:u8 IMM0:r:b
 | VPSRLDQ              | AVX512         | AVX512EVEX     | AVX512BW_128   | 73           | mm         | 0b011      | EVV 0x73 V66 V0F MOD[mm] MOD!=3 REG[0b011] RM[nnn] BCRC=0 MODRM()  VL128      ZEROING=0 MASK=0 UIMM8()  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_N3():w:dq:u8 MEM0:r:dq:u8 IMM0:r:b
 | VPSRLDQ              | AVX512         | AVX512EVEX     | AVX512BW_256   | 73           | 0b11       | 0b011      | EVV 0x73 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b011] RM[nnn]  VL256      ZEROING=0 MASK=0 UIMM8()      | REG0=YMM_N3():w:qq:u8 REG1=YMM_B3():r:qq:u8 IMM0:r:b
 | VPSRLDQ              | AVX512         | AVX512EVEX     | AVX512BW_256   | 73           | mm         | 0b011      | EVV 0x73 V66 V0F MOD[mm] MOD!=3 REG[0b011] RM[nnn] BCRC=0 MODRM()  VL256      ZEROING=0 MASK=0 UIMM8()  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_N3():w:qq:u8 MEM0:r:qq:u8 IMM0:r:b
 | VPSRLDQ              | AVX512         | AVX512EVEX     | AVX512BW_512   | 73           | 0b11       | 0b011      | EVV 0x73 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b011] RM[nnn]  VL512      ZEROING=0 MASK=0 UIMM8()      | REG0=ZMM_N3():w:zu8 REG1=ZMM_B3():r:zu8 IMM0:r:b
 | VPSRLDQ              | AVX512         | AVX512EVEX     | AVX512BW_512   | 73           | mm         | 0b011      | EVV 0x73 V66 V0F MOD[mm] MOD!=3 REG[0b011] RM[nnn] BCRC=0 MODRM()  VL512      ZEROING=0 MASK=0 UIMM8()  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_N3():w:zu8 MEM0:r:zd:u8 IMM0:r:b
 | VPSRLQ               | AVX            | AVX            |                | d3           | mm         | rrr        | VV1 0xD3  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 MEM0:r:dq:u64
 | VPSRLQ               | AVX            | AVX            |                | d3           | 0b11       | rrr        | VV1 0xD3  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 REG2=XMM_B():r:dq:u64
 | VPSRLQ               | AVX            | AVX            |                | 73           | 0b11       | 0b010      | VV1 0x73  VL128 V66 V0F MOD[0b11] MOD=3 REG[0b010] RM[nnn] UIMM8()                                   | REG0=XMM_N():w:dq:u64 REG1=XMM_B():r:dq:u64 IMM0:r:b  # NDD
 | VPSRLQ               | AVX2           | AVX2           |                | d3           | mm         | rrr        | VV1 0xD3  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u64 REG1=YMM_N():r:qq:u64 MEM0:r:dq:u64
 | VPSRLQ               | AVX2           | AVX2           |                | d3           | 0b11       | rrr        | VV1 0xD3  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:u64 REG1=YMM_N():r:qq:u64 REG2=XMM_B():r:q:u64
 | VPSRLQ               | AVX2           | AVX2           |                | 73           | 0b11       | 0b010      | VV1 0x73   VL256  V66 V0F MOD[0b11] MOD=3 REG[0b010] RM[nnn] UIMM8()                                 | REG0=YMM_N():w:qq:u64 REG1=YMM_B():r:qq:u64 IMM0:r:b  # NDD
 | VPSRLQ               | AVX512         | AVX512EVEX     | AVX512F_128    | d3           | 0b11       | rrr        | EVV 0xD3 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                  | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
 | VPSRLQ               | AVX512         | AVX512EVEX     | AVX512F_128    | d3           | mm         | rrr        | EVV 0xD3 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_MEM128() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:dq:u64
 | VPSRLQ               | AVX512         | AVX512EVEX     | AVX512F_128    | 73           | 0b11       | 0b010      | EVV 0x73 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b010] RM[nnn]  VL128  W1   UIMM8()                      | REG0=XMM_N3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u64 IMM0:r:b
 | VPSRLQ               | AVX512         | AVX512EVEX     | AVX512F_128    | 73           | mm         | 0b010      | EVV 0x73 V66 V0F MOD[mm] MOD!=3 REG[0b010] RM[nnn]  MODRM()  VL128  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_N3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VPSRLQ               | AVX512         | AVX512EVEX     | AVX512F_256    | d3           | 0b11       | rrr        | EVV 0xD3 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                  | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=XMM_B3():r:dq:u64
 | VPSRLQ               | AVX512         | AVX512EVEX     | AVX512F_256    | d3           | mm         | rrr        | EVV 0xD3 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_MEM128() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:dq:u64
 | VPSRLQ               | AVX512         | AVX512EVEX     | AVX512F_256    | 73           | 0b11       | 0b010      | EVV 0x73 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b010] RM[nnn]  VL256  W1   UIMM8()                      | REG0=YMM_N3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u64 IMM0:r:b
 | VPSRLQ               | AVX512         | AVX512EVEX     | AVX512F_256    | 73           | mm         | 0b010      | EVV 0x73 V66 V0F MOD[mm] MOD!=3 REG[0b010] RM[nnn]  MODRM()  VL256  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_N3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VPSRLQ               | AVX512         | AVX512EVEX     | AVX512F_512    | d3           | 0b11       | rrr        | EVV 0xD3 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                  | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=XMM_B3():r:dq:u64
 | VPSRLQ               | AVX512         | AVX512EVEX     | AVX512F_512    | d3           | mm         | rrr        | EVV 0xD3 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_MEM128() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:dq:u64
 | VPSRLQ               | AVX512         | AVX512EVEX     | AVX512F_512    | 73           | 0b11       | 0b010      | EVV 0x73 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b010] RM[nnn]  VL512  W1   UIMM8()                      | REG0=ZMM_N3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu64 IMM0:r:b
 | VPSRLQ               | AVX512         | AVX512EVEX     | AVX512F_512    | 73           | mm         | 0b010      | EVV 0x73 V66 V0F MOD[mm] MOD!=3 REG[0b010] RM[nnn]  MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_N3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VPSRLVD              | AVX2           | AVX2           |                | 45           | mm         | rrr        | VV1 0x45  VL128 V0F38 V66  W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq REG1=XMM_N():r:dq MEM0:r:dq
 | VPSRLVD              | AVX2           | AVX2           |                | 45           | 0b11       | rrr        | VV1 0x45  VL128 V0F38 V66 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=XMM_R():w:dq REG1=XMM_N():r:dq REG2=XMM_B():r:dq
 | VPSRLVD              | AVX2           | AVX2           |                | 45           | mm         | rrr        | VV1 0x45  VL256 V0F38 V66  W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=YMM_R():w:qq REG1=YMM_N():r:qq MEM0:r:qq
 | VPSRLVD              | AVX2           | AVX2           |                | 45           | 0b11       | rrr        | VV1 0x45  VL256 V0F38 V66 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=YMM_R():w:qq REG1=YMM_N():r:qq REG2=YMM_B():r:qq
 | VPSRLVD              | AVX512         | AVX512EVEX     | AVX512F_128    | 45           | 0b11       | rrr        | EVV 0x45 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
 | VPSRLVD              | AVX512         | AVX512EVEX     | AVX512F_128    | 45           | mm         | rrr        | EVV 0x45 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPSRLVD              | AVX512         | AVX512EVEX     | AVX512F_256    | 45           | 0b11       | rrr        | EVV 0x45 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
 | VPSRLVD              | AVX512         | AVX512EVEX     | AVX512F_256    | 45           | mm         | rrr        | EVV 0x45 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPSRLVD              | AVX512         | AVX512EVEX     | AVX512F_512    | 45           | 0b11       | rrr        | EVV 0x45 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
 | VPSRLVD              | AVX512         | AVX512EVEX     | AVX512F_512    | 45           | mm         | rrr        | EVV 0x45 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPSRLVD              | KNC            | KNCE           |                | 45           | mm         | rrr        | KVV 0x45 V0F38 V66  REXW=0  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT32()               | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw REG2=ZMM_N3():r:zd MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VPSRLVD              | KNC            | KNCE           |                | 45           | 0b11       | rrr        | KVV 0x45 V0F38 V66  REXW=0  MOD[0b11]  MOD=3 REG[rrr] RM[nnn] NR=1                                   | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw REG2=ZMM_N3():r:zd  REG3=ZMM_B3():r:zd
 | VPSRLVD              | KNC            | KNCE           |                | 45           | 0b11       | rrr        | KVV 0x45 V0F38 V66  REXW=0  MOD[0b11]  MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                   | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw REG2=ZMM_N3():r:zd  REG3=ZMM_B3():r:zd:TXT=REGSWIZ
 | VPSRLVQ              | AVX2           | AVX2           |                | 45           | mm         | rrr        | VV1 0x45  VL128 V0F38 V66  W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq REG1=XMM_N():r:dq MEM0:r:dq
 | VPSRLVQ              | AVX2           | AVX2           |                | 45           | 0b11       | rrr        | VV1 0x45  VL128 V0F38 V66 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=XMM_R():w:dq REG1=XMM_N():r:dq REG2=XMM_B():r:dq
 | VPSRLVQ              | AVX2           | AVX2           |                | 45           | mm         | rrr        | VV1 0x45  VL256 V0F38 V66  W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=YMM_R():w:qq REG1=YMM_N():r:qq MEM0:r:qq
 | VPSRLVQ              | AVX2           | AVX2           |                | 45           | 0b11       | rrr        | VV1 0x45  VL256 V0F38 V66 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=YMM_R():w:qq REG1=YMM_N():r:qq REG2=YMM_B():r:qq
 | VPSRLVQ              | AVX512         | AVX512EVEX     | AVX512F_128    | 45           | 0b11       | rrr        | EVV 0x45 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
 | VPSRLVQ              | AVX512         | AVX512EVEX     | AVX512F_128    | 45           | mm         | rrr        | EVV 0x45 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPSRLVQ              | AVX512         | AVX512EVEX     | AVX512F_256    | 45           | 0b11       | rrr        | EVV 0x45 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
 | VPSRLVQ              | AVX512         | AVX512EVEX     | AVX512F_256    | 45           | mm         | rrr        | EVV 0x45 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPSRLVQ              | AVX512         | AVX512EVEX     | AVX512F_512    | 45           | 0b11       | rrr        | EVV 0x45 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
 | VPSRLVQ              | AVX512         | AVX512EVEX     | AVX512F_512    | 45           | mm         | rrr        | EVV 0x45 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPSRLVW              | AVX512         | AVX512EVEX     | AVX512BW_128   | 10           | 0b11       | rrr        | EVV 0x10 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
 | VPSRLVW              | AVX512         | AVX512EVEX     | AVX512BW_128   | 10           | mm         | rrr        | EVV 0x10 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
 | VPSRLVW              | AVX512         | AVX512EVEX     | AVX512BW_256   | 10           | 0b11       | rrr        | EVV 0x10 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
 | VPSRLVW              | AVX512         | AVX512EVEX     | AVX512BW_256   | 10           | mm         | rrr        | EVV 0x10 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
 | VPSRLVW              | AVX512         | AVX512EVEX     | AVX512BW_512   | 10           | 0b11       | rrr        | EVV 0x10 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
 | VPSRLVW              | AVX512         | AVX512EVEX     | AVX512BW_512   | 10           | mm         | rrr        | EVV 0x10 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
 | VPSRLW               | AVX            | AVX            |                | d1           | mm         | rrr        | VV1 0xD1  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 MEM0:r:dq:u64
 | VPSRLW               | AVX            | AVX            |                | d1           | 0b11       | rrr        | VV1 0xD1  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 REG2=XMM_B():r:dq:u64
 | VPSRLW               | AVX            | AVX            |                | 71           | 0b11       | 0b010      | VV1 0x71  VL128 V66 V0F MOD[0b11] MOD=3 REG[0b010] RM[nnn] UIMM8()                                   | REG0=XMM_N():w:dq:u16 REG1=XMM_B():r:dq:u16 IMM0:r:b # NDD
 | VPSRLW               | AVX2           | AVX2           |                | d1           | mm         | rrr        | VV1 0xD1  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u16 REG1=YMM_N():r:qq:u16 MEM0:r:dq:u64
 | VPSRLW               | AVX2           | AVX2           |                | d1           | 0b11       | rrr        | VV1 0xD1  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:u16 REG1=YMM_N():r:qq:u16 REG2=XMM_B():r:q:u64
 | VPSRLW               | AVX2           | AVX2           |                | 71           | 0b11       | 0b010      | VV1 0x71   VL256  V66 V0F MOD[0b11] MOD=3 REG[0b010] RM[nnn] UIMM8()                                 | REG0=YMM_N():w:qq:u16 REG1=YMM_B():r:qq:u16 IMM0:r:b # NDD
 | VPSRLW               | AVX512         | AVX512EVEX     | AVX512BW_128   | d1           | 0b11       | rrr        | EVV 0xD1 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                      | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
 | VPSRLW               | AVX512         | AVX512EVEX     | AVX512BW_128   | d1           | mm         | rrr        | EVV 0xD1 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_MEM128() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
 | VPSRLW               | AVX512         | AVX512EVEX     | AVX512BW_128   | 71           | 0b11       | 0b010      | EVV 0x71 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b010] RM[nnn]  VL128     UIMM8()                        | REG0=XMM_N3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u16 IMM0:r:b
 | VPSRLW               | AVX512         | AVX512EVEX     | AVX512BW_128   | 71           | mm         | 0b010      | EVV 0x71 V66 V0F MOD[mm] MOD!=3 REG[0b010] RM[nnn] BCRC=0 MODRM()  VL128     UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_N3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u16 IMM0:r:b
 | VPSRLW               | AVX512         | AVX512EVEX     | AVX512BW_256   | d1           | 0b11       | rrr        | EVV 0xD1 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                      | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=XMM_B3():r:dq:u16
 | VPSRLW               | AVX512         | AVX512EVEX     | AVX512BW_256   | d1           | mm         | rrr        | EVV 0xD1 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_MEM128() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:dq:u16
 | VPSRLW               | AVX512         | AVX512EVEX     | AVX512BW_256   | 71           | 0b11       | 0b010      | EVV 0x71 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b010] RM[nnn]  VL256     UIMM8()                        | REG0=YMM_N3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u16 IMM0:r:b
 | VPSRLW               | AVX512         | AVX512EVEX     | AVX512BW_256   | 71           | mm         | 0b010      | EVV 0x71 V66 V0F MOD[mm] MOD!=3 REG[0b010] RM[nnn] BCRC=0 MODRM()  VL256     UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_N3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u16 IMM0:r:b
 | VPSRLW               | AVX512         | AVX512EVEX     | AVX512BW_512   | d1           | 0b11       | rrr        | EVV 0xD1 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                      | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=XMM_B3():r:dq:u16
 | VPSRLW               | AVX512         | AVX512EVEX     | AVX512BW_512   | d1           | mm         | rrr        | EVV 0xD1 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_MEM128() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:dq:u16
 | VPSRLW               | AVX512         | AVX512EVEX     | AVX512BW_512   | 71           | 0b11       | 0b010      | EVV 0x71 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b010] RM[nnn]  VL512     UIMM8()                        | REG0=ZMM_N3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu16 IMM0:r:b
 | VPSRLW               | AVX512         | AVX512EVEX     | AVX512BW_512   | 71           | mm         | 0b010      | EVV 0x71 V66 V0F MOD[mm] MOD!=3 REG[0b010] RM[nnn] BCRC=0 MODRM()  VL512     UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_N3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:u16 IMM0:r:b
 | VPSUBB               | AVX            | AVX            |                | f8           | mm         | rrr        | VV1 0xF8  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 MEM0:r:dq:i8
 | VPSUBB               | AVX            | AVX            |                | f8           | 0b11       | rrr        | VV1 0xF8  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 REG2=XMM_B():r:dq:i8
 | VPSUBB               | AVX2           | AVX2           |                | f8           | mm         | rrr        | VV1 0xF8  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:i8 REG1=YMM_N():r:qq:i8 MEM0:r:qq:i8
 | VPSUBB               | AVX2           | AVX2           |                | f8           | 0b11       | rrr        | VV1 0xF8  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:i8 REG1=YMM_N():r:qq:i8 REG2=YMM_B():r:qq:i8
 | VPSUBB               | AVX512         | AVX512EVEX     | AVX512BW_128   | f8           | 0b11       | rrr        | EVV 0xF8 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                      | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
 | VPSUBB               | AVX512         | AVX512EVEX     | AVX512BW_128   | f8           | mm         | rrr        | EVV 0xF8 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
 | VPSUBB               | AVX512         | AVX512EVEX     | AVX512BW_256   | f8           | 0b11       | rrr        | EVV 0xF8 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                      | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
 | VPSUBB               | AVX512         | AVX512EVEX     | AVX512BW_256   | f8           | mm         | rrr        | EVV 0xF8 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
 | VPSUBB               | AVX512         | AVX512EVEX     | AVX512BW_512   | f8           | 0b11       | rrr        | EVV 0xF8 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                      | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
 | VPSUBB               | AVX512         | AVX512EVEX     | AVX512BW_512   | f8           | mm         | rrr        | EVV 0xF8 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
 | VPSUBD               | AVX            | AVX            |                | fa           | mm         | rrr        | VV1 0xFA  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32
 | VPSUBD               | AVX            | AVX            |                | fa           | 0b11       | rrr        | VV1 0xFA  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32
 | VPSUBD               | AVX2           | AVX2           |                | fa           | mm         | rrr        | VV1 0xFA  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:i32 REG1=YMM_N():r:qq:i32 MEM0:r:qq:i32
 | VPSUBD               | AVX2           | AVX2           |                | fa           | 0b11       | rrr        | VV1 0xFA  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:i32 REG1=YMM_N():r:qq:i32 REG2=YMM_B():r:qq:i32
 | VPSUBD               | AVX512         | AVX512EVEX     | AVX512F_128    | fa           | 0b11       | rrr        | EVV 0xFA V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                  | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
 | VPSUBD               | AVX512         | AVX512EVEX     | AVX512F_128    | fa           | mm         | rrr        | EVV 0xFA V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPSUBD               | AVX512         | AVX512EVEX     | AVX512F_256    | fa           | 0b11       | rrr        | EVV 0xFA V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                  | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
 | VPSUBD               | AVX512         | AVX512EVEX     | AVX512F_256    | fa           | mm         | rrr        | EVV 0xFA V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPSUBD               | AVX512         | AVX512EVEX     | AVX512F_512    | fa           | 0b11       | rrr        | EVV 0xFA V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                  | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
 | VPSUBD               | AVX512         | AVX512EVEX     | AVX512F_512    | fa           | mm         | rrr        | EVV 0xFA V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPSUBD               | KNC            | KNCE           |                | fa           | mm         | rrr        | KVV 0xFA V0F V66 REXW=0  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT32()                  | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VPSUBD               | KNC            | KNCE           |                | fa           | 0b11       | rrr        | KVV 0xFA V0F V66 REXW=0  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                       | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VPSUBD               | KNC            | KNCE           |                | fa           | 0b11       | rrr        | KVV 0xFA V0F V66 REXW=0  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1                                       | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VPSUBQ               | AVX            | AVX            |                | fb           | mm         | rrr        | VV1 0xFB  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:i64 REG1=XMM_N():r:dq:i64 MEM0:r:dq:i64
 | VPSUBQ               | AVX            | AVX            |                | fb           | 0b11       | rrr        | VV1 0xFB  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:i64 REG1=XMM_N():r:dq:i64 REG2=XMM_B():r:dq:i64
 | VPSUBQ               | AVX2           | AVX2           |                | fb           | mm         | rrr        | VV1 0xFB  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:i64 REG1=YMM_N():r:qq:i64 MEM0:r:qq:i64
 | VPSUBQ               | AVX2           | AVX2           |                | fb           | 0b11       | rrr        | VV1 0xFB  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:i64 REG1=YMM_N():r:qq:i64 REG2=YMM_B():r:qq:i64
 | VPSUBQ               | AVX512         | AVX512EVEX     | AVX512F_128    | fb           | 0b11       | rrr        | EVV 0xFB V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                  | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
 | VPSUBQ               | AVX512         | AVX512EVEX     | AVX512F_128    | fb           | mm         | rrr        | EVV 0xFB V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPSUBQ               | AVX512         | AVX512EVEX     | AVX512F_256    | fb           | 0b11       | rrr        | EVV 0xFB V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                  | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
 | VPSUBQ               | AVX512         | AVX512EVEX     | AVX512F_256    | fb           | mm         | rrr        | EVV 0xFB V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPSUBQ               | AVX512         | AVX512EVEX     | AVX512F_512    | fb           | 0b11       | rrr        | EVV 0xFB V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                  | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
 | VPSUBQ               | AVX512         | AVX512EVEX     | AVX512F_512    | fb           | mm         | rrr        | EVV 0xFB V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPSUBRD              | KNC            | KNCE           |                | 6c           | mm         | rrr        | KVV 0x6C V0F38 V66 REXW=0  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT32()                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VPSUBRD              | KNC            | KNCE           |                | 6c           | 0b11       | rrr        | KVV 0x6C V0F38 V66 REXW=0  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VPSUBRD              | KNC            | KNCE           |                | 6c           | 0b11       | rrr        | KVV 0x6C V0F38 V66 REXW=0  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1                                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VPSUBRSETBD          | KNC            | KNCE           |                | 6f           | mm         | rrr        | KVV 0x6F  V0F38 V66  W0  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT32()                  | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  REG2=MASK_N():w:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VPSUBRSETBD          | KNC            | KNCE           |                | 6f           | 0b11       | rrr        | KVV 0x6F  V0F38 V66  W0  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                       | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  REG2=MASK_N():w:mskw  REG3=ZMM_B3():r:zd:TXT=REGSWIZ
 | VPSUBRSETBD          | KNC            | KNCE           |                | 6f           | 0b11       | rrr        | KVV 0x6F  V0F38 V66  W0  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 SWIZ=0                                | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  REG2=MASK_N():w:mskw  REG3=ZMM_B3():r:zd
 | VPSUBSB              | AVX            | AVX            |                | e8           | mm         | rrr        | VV1 0xE8  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 MEM0:r:dq:i8
 | VPSUBSB              | AVX            | AVX            |                | e8           | 0b11       | rrr        | VV1 0xE8  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 REG2=XMM_B():r:dq:i8
 | VPSUBSB              | AVX2           | AVX2           |                | e8           | mm         | rrr        | VV1 0xE8  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:i8 REG1=YMM_N():r:qq:i8 MEM0:r:qq:i8
 | VPSUBSB              | AVX2           | AVX2           |                | e8           | 0b11       | rrr        | VV1 0xE8  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:i8 REG1=YMM_N():r:qq:i8 REG2=YMM_B():r:qq:i8
 | VPSUBSB              | AVX512         | AVX512EVEX     | AVX512BW_128   | e8           | 0b11       | rrr        | EVV 0xE8 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                      | REG0=XMM_R3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i8 REG3=XMM_B3():r:dq:i8
 | VPSUBSB              | AVX512         | AVX512EVEX     | AVX512BW_128   | e8           | mm         | rrr        | EVV 0xE8 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i8 MEM0:r:dq:i8
 | VPSUBSB              | AVX512         | AVX512EVEX     | AVX512BW_256   | e8           | 0b11       | rrr        | EVV 0xE8 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                      | REG0=YMM_R3():w:qq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i8 REG3=YMM_B3():r:qq:i8
 | VPSUBSB              | AVX512         | AVX512EVEX     | AVX512BW_256   | e8           | mm         | rrr        | EVV 0xE8 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i8 MEM0:r:qq:i8
 | VPSUBSB              | AVX512         | AVX512EVEX     | AVX512BW_512   | e8           | 0b11       | rrr        | EVV 0xE8 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                      | REG0=ZMM_R3():w:zi8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi8 REG3=ZMM_B3():r:zi8
 | VPSUBSB              | AVX512         | AVX512EVEX     | AVX512BW_512   | e8           | mm         | rrr        | EVV 0xE8 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zi8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi8 MEM0:r:zd:i8
 | VPSUBSETBD           | KNC            | KNCE           |                | 5f           | mm         | rrr        | KVV 0x5F  V0F38 V66  W0  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT32()                  | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  REG2=MASK_N():w:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VPSUBSETBD           | KNC            | KNCE           |                | 5f           | 0b11       | rrr        | KVV 0x5F  V0F38 V66  W0  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                       | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  REG2=MASK_N():w:mskw  REG3=ZMM_B3():r:zd:TXT=REGSWIZ
 | VPSUBSETBD           | KNC            | KNCE           |                | 5f           | 0b11       | rrr        | KVV 0x5F  V0F38 V66  W0  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 SWIZ=0                                | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  REG2=MASK_N():w:mskw  REG3=ZMM_B3():r:zd
 | VPSUBSW              | AVX            | AVX            |                | e9           | mm         | rrr        | VV1 0xE9  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
 | VPSUBSW              | AVX            | AVX            |                | e9           | 0b11       | rrr        | VV1 0xE9  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
 | VPSUBSW              | AVX2           | AVX2           |                | e9           | mm         | rrr        | VV1 0xE9  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:i16 REG1=YMM_N():r:qq:i16 MEM0:r:qq:i16
 | VPSUBSW              | AVX2           | AVX2           |                | e9           | 0b11       | rrr        | VV1 0xE9  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:i16 REG1=YMM_N():r:qq:i16 REG2=YMM_B():r:qq:i16
 | VPSUBSW              | AVX512         | AVX512EVEX     | AVX512BW_128   | e9           | 0b11       | rrr        | EVV 0xE9 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                      | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 REG3=XMM_B3():r:dq:i16
 | VPSUBSW              | AVX512         | AVX512EVEX     | AVX512BW_128   | e9           | mm         | rrr        | EVV 0xE9 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 MEM0:r:dq:i16
 | VPSUBSW              | AVX512         | AVX512EVEX     | AVX512BW_256   | e9           | 0b11       | rrr        | EVV 0xE9 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                      | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 REG3=YMM_B3():r:qq:i16
 | VPSUBSW              | AVX512         | AVX512EVEX     | AVX512BW_256   | e9           | mm         | rrr        | EVV 0xE9 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 MEM0:r:qq:i16
 | VPSUBSW              | AVX512         | AVX512EVEX     | AVX512BW_512   | e9           | 0b11       | rrr        | EVV 0xE9 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                      | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 REG3=ZMM_B3():r:zi16
 | VPSUBSW              | AVX512         | AVX512EVEX     | AVX512BW_512   | e9           | mm         | rrr        | EVV 0xE9 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 MEM0:r:zd:i16
 | VPSUBUSB             | AVX            | AVX            |                | d8           | mm         | rrr        | VV1 0xD8  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 MEM0:r:dq:u8
 | VPSUBUSB             | AVX            | AVX            |                | d8           | 0b11       | rrr        | VV1 0xD8  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:u8
 | VPSUBUSB             | AVX2           | AVX2           |                | d8           | mm         | rrr        | VV1 0xD8  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u8 REG1=YMM_N():r:qq:u8 MEM0:r:qq:u8
 | VPSUBUSB             | AVX2           | AVX2           |                | d8           | 0b11       | rrr        | VV1 0xD8  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:u8 REG1=YMM_N():r:qq:u8 REG2=YMM_B():r:qq:u8
 | VPSUBUSB             | AVX512         | AVX512EVEX     | AVX512BW_128   | d8           | 0b11       | rrr        | EVV 0xD8 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                      | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
 | VPSUBUSB             | AVX512         | AVX512EVEX     | AVX512BW_128   | d8           | mm         | rrr        | EVV 0xD8 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
 | VPSUBUSB             | AVX512         | AVX512EVEX     | AVX512BW_256   | d8           | 0b11       | rrr        | EVV 0xD8 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                      | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
 | VPSUBUSB             | AVX512         | AVX512EVEX     | AVX512BW_256   | d8           | mm         | rrr        | EVV 0xD8 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
 | VPSUBUSB             | AVX512         | AVX512EVEX     | AVX512BW_512   | d8           | 0b11       | rrr        | EVV 0xD8 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                      | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
 | VPSUBUSB             | AVX512         | AVX512EVEX     | AVX512BW_512   | d8           | mm         | rrr        | EVV 0xD8 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
 | VPSUBUSW             | AVX            | AVX            |                | d9           | mm         | rrr        | VV1 0xD9  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 MEM0:r:dq:u16
 | VPSUBUSW             | AVX            | AVX            |                | d9           | 0b11       | rrr        | VV1 0xD9  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 REG2=XMM_B():r:dq:u16
 | VPSUBUSW             | AVX2           | AVX2           |                | d9           | mm         | rrr        | VV1 0xD9  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u16 REG1=YMM_N():r:qq:u16 MEM0:r:qq:u16
 | VPSUBUSW             | AVX2           | AVX2           |                | d9           | 0b11       | rrr        | VV1 0xD9  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:u16 REG1=YMM_N():r:qq:u16 REG2=YMM_B():r:qq:u16
 | VPSUBUSW             | AVX512         | AVX512EVEX     | AVX512BW_128   | d9           | 0b11       | rrr        | EVV 0xD9 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                      | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
 | VPSUBUSW             | AVX512         | AVX512EVEX     | AVX512BW_128   | d9           | mm         | rrr        | EVV 0xD9 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
 | VPSUBUSW             | AVX512         | AVX512EVEX     | AVX512BW_256   | d9           | 0b11       | rrr        | EVV 0xD9 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                      | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
 | VPSUBUSW             | AVX512         | AVX512EVEX     | AVX512BW_256   | d9           | mm         | rrr        | EVV 0xD9 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
 | VPSUBUSW             | AVX512         | AVX512EVEX     | AVX512BW_512   | d9           | 0b11       | rrr        | EVV 0xD9 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                      | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
 | VPSUBUSW             | AVX512         | AVX512EVEX     | AVX512BW_512   | d9           | mm         | rrr        | EVV 0xD9 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
 | VPSUBW               | AVX            | AVX            |                | f9           | mm         | rrr        | VV1 0xF9  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
 | VPSUBW               | AVX            | AVX            |                | f9           | 0b11       | rrr        | VV1 0xF9  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
 | VPSUBW               | AVX2           | AVX2           |                | f9           | mm         | rrr        | VV1 0xF9  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:i16 REG1=YMM_N():r:qq:i16 MEM0:r:qq:i16
 | VPSUBW               | AVX2           | AVX2           |                | f9           | 0b11       | rrr        | VV1 0xF9  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:i16 REG1=YMM_N():r:qq:i16 REG2=YMM_B():r:qq:i16
 | VPSUBW               | AVX512         | AVX512EVEX     | AVX512BW_128   | f9           | 0b11       | rrr        | EVV 0xF9 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                      | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
 | VPSUBW               | AVX512         | AVX512EVEX     | AVX512BW_128   | f9           | mm         | rrr        | EVV 0xF9 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
 | VPSUBW               | AVX512         | AVX512EVEX     | AVX512BW_256   | f9           | 0b11       | rrr        | EVV 0xF9 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                      | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
 | VPSUBW               | AVX512         | AVX512EVEX     | AVX512BW_256   | f9           | mm         | rrr        | EVV 0xF9 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
 | VPSUBW               | AVX512         | AVX512EVEX     | AVX512BW_512   | f9           | 0b11       | rrr        | EVV 0xF9 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                      | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
 | VPSUBW               | AVX512         | AVX512EVEX     | AVX512BW_512   | f9           | mm         | rrr        | EVV 0xF9 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
 | VPTERNLOGD           | LOGICAL        | AVX512EVEX     | AVX512F_128    | 25           | 0b11       | rrr        | EVV 0x25 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0   UIMM8()                      | REG0=XMM_R3():rw:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32 IMM0:r:b
 | VPTERNLOGD           | LOGICAL        | AVX512EVEX     | AVX512F_128    | 25           | mm         | rrr        | EVV 0x25 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
 | VPTERNLOGD           | LOGICAL        | AVX512EVEX     | AVX512F_256    | 25           | 0b11       | rrr        | EVV 0x25 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0   UIMM8()                      | REG0=YMM_R3():rw:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32 IMM0:r:b
 | VPTERNLOGD           | LOGICAL        | AVX512EVEX     | AVX512F_256    | 25           | mm         | rrr        | EVV 0x25 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
 | VPTERNLOGD           | LOGICAL        | AVX512EVEX     | AVX512F_512    | 25           | 0b11       | rrr        | EVV 0x25 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0   UIMM8()                      | REG0=ZMM_R3():rw:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32 IMM0:r:b
 | VPTERNLOGD           | LOGICAL        | AVX512EVEX     | AVX512F_512    | 25           | mm         | rrr        | EVV 0x25 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
 | VPTERNLOGQ           | LOGICAL        | AVX512EVEX     | AVX512F_128    | 25           | 0b11       | rrr        | EVV 0x25 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1   UIMM8()                      | REG0=XMM_R3():rw:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64 IMM0:r:b
 | VPTERNLOGQ           | LOGICAL        | AVX512EVEX     | AVX512F_128    | 25           | mm         | rrr        | EVV 0x25 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VPTERNLOGQ           | LOGICAL        | AVX512EVEX     | AVX512F_256    | 25           | 0b11       | rrr        | EVV 0x25 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1   UIMM8()                      | REG0=YMM_R3():rw:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64 IMM0:r:b
 | VPTERNLOGQ           | LOGICAL        | AVX512EVEX     | AVX512F_256    | 25           | mm         | rrr        | EVV 0x25 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VPTERNLOGQ           | LOGICAL        | AVX512EVEX     | AVX512F_512    | 25           | 0b11       | rrr        | EVV 0x25 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1   UIMM8()                      | REG0=ZMM_R3():rw:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64 IMM0:r:b
 | VPTERNLOGQ           | LOGICAL        | AVX512EVEX     | AVX512F_512    | 25           | mm         | rrr        | EVV 0x25 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VPTEST               | LOGICAL        | AVX            |                | 17           | mm         | rrr        | VV1 0x17  VL128 V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                              | REG0=XMM_R():r:dq MEM0:r:dq
 | VPTEST               | LOGICAL        | AVX            |                | 17           | 0b11       | rrr        | VV1 0x17  VL128 V66 V0F38 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=XMM_R():r:dq REG1=XMM_B():r:dq
 | VPTEST               | LOGICAL        | AVX            |                | 17           | mm         | rrr        | VV1 0x17  VL256 V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                              | REG0=YMM_R():r:qq MEM0:r:qq
 | VPTEST               | LOGICAL        | AVX            |                | 17           | 0b11       | rrr        | VV1 0x17  VL256 V66 V0F38 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=YMM_R():r:qq REG1=YMM_B():r:qq
 | VPTESTMB             | LOGICAL        | AVX512EVEX     | AVX512BW_128   | 26           | 0b11       | rrr        | EVV 0x26 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0    ZEROING=0                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
 | VPTESTMB             | LOGICAL        | AVX512EVEX     | AVX512BW_128   | 26           | mm         | rrr        | EVV 0x26 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0    ZEROING=0  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
 | VPTESTMB             | LOGICAL        | AVX512EVEX     | AVX512BW_256   | 26           | 0b11       | rrr        | EVV 0x26 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0    ZEROING=0                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
 | VPTESTMB             | LOGICAL        | AVX512EVEX     | AVX512BW_256   | 26           | mm         | rrr        | EVV 0x26 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0    ZEROING=0  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
 | VPTESTMB             | LOGICAL        | AVX512EVEX     | AVX512BW_512   | 26           | 0b11       | rrr        | EVV 0x26 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0    ZEROING=0                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
 | VPTESTMB             | LOGICAL        | AVX512EVEX     | AVX512BW_512   | 26           | mm         | rrr        | EVV 0x26 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0    ZEROING=0  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
 | VPTESTMD             | KNC            | KNCE           |                | 27           | mm         | rrr        | KVV 0x27 V66 V0F38  W0 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM()  UPCONVERT_INT32()                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zd MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VPTESTMD             | KNC            | KNCE           |                | 27           | 0b11       | rrr        | KVV 0x27 V66 V0F38  W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE64()                         | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zd REG3=ZMM_B3():r:zd:TXT=REGSWIZ
 | VPTESTMD             | KNC            | KNCE           |                | 27           | 0b11       | rrr        | KVV 0x27 V66 V0F38  W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1  SWIZ=0                                 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zd REG3=ZMM_B3():r:zd
 | VPTESTMD             | LOGICAL        | AVX512EVEX     | AVX512F_128    | 27           | 0b11       | rrr        | EVV 0x27 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0    ZEROING=0                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
 | VPTESTMD             | LOGICAL        | AVX512EVEX     | AVX512F_128    | 27           | mm         | rrr        | EVV 0x27 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ZEROING=0  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPTESTMD             | LOGICAL        | AVX512EVEX     | AVX512F_256    | 27           | 0b11       | rrr        | EVV 0x27 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0    ZEROING=0                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
 | VPTESTMD             | LOGICAL        | AVX512EVEX     | AVX512F_256    | 27           | mm         | rrr        | EVV 0x27 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ZEROING=0  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPTESTMD             | LOGICAL        | AVX512EVEX     | AVX512F_512    | 27           | 0b11       | rrr        | EVV 0x27 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0    ZEROING=0                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
 | VPTESTMD             | LOGICAL        | AVX512EVEX     | AVX512F_512    | 27           | mm         | rrr        | EVV 0x27 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ZEROING=0  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPTESTMQ             | LOGICAL        | AVX512EVEX     | AVX512F_128    | 27           | 0b11       | rrr        | EVV 0x27 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1    ZEROING=0                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
 | VPTESTMQ             | LOGICAL        | AVX512EVEX     | AVX512F_128    | 27           | mm         | rrr        | EVV 0x27 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ZEROING=0  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPTESTMQ             | LOGICAL        | AVX512EVEX     | AVX512F_256    | 27           | 0b11       | rrr        | EVV 0x27 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1    ZEROING=0                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
 | VPTESTMQ             | LOGICAL        | AVX512EVEX     | AVX512F_256    | 27           | mm         | rrr        | EVV 0x27 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ZEROING=0  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPTESTMQ             | LOGICAL        | AVX512EVEX     | AVX512F_512    | 27           | 0b11       | rrr        | EVV 0x27 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1    ZEROING=0                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
 | VPTESTMQ             | LOGICAL        | AVX512EVEX     | AVX512F_512    | 27           | mm         | rrr        | EVV 0x27 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ZEROING=0  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPTESTMW             | LOGICAL        | AVX512EVEX     | AVX512BW_128   | 26           | 0b11       | rrr        | EVV 0x26 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1    ZEROING=0                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
 | VPTESTMW             | LOGICAL        | AVX512EVEX     | AVX512BW_128   | 26           | mm         | rrr        | EVV 0x26 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1    ZEROING=0  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
 | VPTESTMW             | LOGICAL        | AVX512EVEX     | AVX512BW_256   | 26           | 0b11       | rrr        | EVV 0x26 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1    ZEROING=0                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
 | VPTESTMW             | LOGICAL        | AVX512EVEX     | AVX512BW_256   | 26           | mm         | rrr        | EVV 0x26 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1    ZEROING=0  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
 | VPTESTMW             | LOGICAL        | AVX512EVEX     | AVX512BW_512   | 26           | 0b11       | rrr        | EVV 0x26 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1    ZEROING=0                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
 | VPTESTMW             | LOGICAL        | AVX512EVEX     | AVX512BW_512   | 26           | mm         | rrr        | EVV 0x26 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1    ZEROING=0  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
 | VPTESTNMB            | LOGICAL        | AVX512EVEX     | AVX512BW_128   | 26           | 0b11       | rrr        | EVV 0x26 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0    ZEROING=0                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
 | VPTESTNMB            | LOGICAL        | AVX512EVEX     | AVX512BW_128   | 26           | mm         | rrr        | EVV 0x26 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0    ZEROING=0  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
 | VPTESTNMB            | LOGICAL        | AVX512EVEX     | AVX512BW_256   | 26           | 0b11       | rrr        | EVV 0x26 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0    ZEROING=0                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
 | VPTESTNMB            | LOGICAL        | AVX512EVEX     | AVX512BW_256   | 26           | mm         | rrr        | EVV 0x26 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0    ZEROING=0  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
 | VPTESTNMB            | LOGICAL        | AVX512EVEX     | AVX512BW_512   | 26           | 0b11       | rrr        | EVV 0x26 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0    ZEROING=0                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
 | VPTESTNMB            | LOGICAL        | AVX512EVEX     | AVX512BW_512   | 26           | mm         | rrr        | EVV 0x26 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0    ZEROING=0  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
 | VPTESTNMD            | LOGICAL        | AVX512EVEX     | AVX512F_128    | 27           | 0b11       | rrr        | EVV 0x27 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0    ZEROING=0                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
 | VPTESTNMD            | LOGICAL        | AVX512EVEX     | AVX512F_128    | 27           | mm         | rrr        | EVV 0x27 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ZEROING=0  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPTESTNMD            | LOGICAL        | AVX512EVEX     | AVX512F_256    | 27           | 0b11       | rrr        | EVV 0x27 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0    ZEROING=0                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
 | VPTESTNMD            | LOGICAL        | AVX512EVEX     | AVX512F_256    | 27           | mm         | rrr        | EVV 0x27 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ZEROING=0  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPTESTNMD            | LOGICAL        | AVX512EVEX     | AVX512F_512    | 27           | 0b11       | rrr        | EVV 0x27 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0    ZEROING=0                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
 | VPTESTNMD            | LOGICAL        | AVX512EVEX     | AVX512F_512    | 27           | mm         | rrr        | EVV 0x27 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ZEROING=0  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPTESTNMQ            | LOGICAL        | AVX512EVEX     | AVX512F_128    | 27           | 0b11       | rrr        | EVV 0x27 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1    ZEROING=0                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
 | VPTESTNMQ            | LOGICAL        | AVX512EVEX     | AVX512F_128    | 27           | mm         | rrr        | EVV 0x27 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ZEROING=0  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPTESTNMQ            | LOGICAL        | AVX512EVEX     | AVX512F_256    | 27           | 0b11       | rrr        | EVV 0x27 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1    ZEROING=0                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
 | VPTESTNMQ            | LOGICAL        | AVX512EVEX     | AVX512F_256    | 27           | mm         | rrr        | EVV 0x27 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ZEROING=0  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPTESTNMQ            | LOGICAL        | AVX512EVEX     | AVX512F_512    | 27           | 0b11       | rrr        | EVV 0x27 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1    ZEROING=0                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
 | VPTESTNMQ            | LOGICAL        | AVX512EVEX     | AVX512F_512    | 27           | mm         | rrr        | EVV 0x27 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ZEROING=0  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPTESTNMW            | LOGICAL        | AVX512EVEX     | AVX512BW_128   | 26           | 0b11       | rrr        | EVV 0x26 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1    ZEROING=0                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
 | VPTESTNMW            | LOGICAL        | AVX512EVEX     | AVX512BW_128   | 26           | mm         | rrr        | EVV 0x26 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1    ZEROING=0  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
 | VPTESTNMW            | LOGICAL        | AVX512EVEX     | AVX512BW_256   | 26           | 0b11       | rrr        | EVV 0x26 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1    ZEROING=0                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
 | VPTESTNMW            | LOGICAL        | AVX512EVEX     | AVX512BW_256   | 26           | mm         | rrr        | EVV 0x26 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1    ZEROING=0  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
 | VPTESTNMW            | LOGICAL        | AVX512EVEX     | AVX512BW_512   | 26           | 0b11       | rrr        | EVV 0x26 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1    ZEROING=0                   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
 | VPTESTNMW            | LOGICAL        | AVX512EVEX     | AVX512BW_512   | 26           | mm         | rrr        | EVV 0x26 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1    ZEROING=0  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
 | VPUNPCKHBW           | AVX            | AVX            |                | 68           | mm         | rrr        | VV1 0x68  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 MEM0:r:dq:u8
 | VPUNPCKHBW           | AVX            | AVX            |                | 68           | 0b11       | rrr        | VV1 0x68  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:u8
 | VPUNPCKHBW           | AVX2           | AVX2           |                | 68           | mm         | rrr        | VV1 0x68  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u8 REG1=YMM_N():r:qq:u8 MEM0:r:qq:u8
 | VPUNPCKHBW           | AVX2           | AVX2           |                | 68           | 0b11       | rrr        | VV1 0x68  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:u8 REG1=YMM_N():r:qq:u8 REG2=YMM_B():r:qq:u8
 | VPUNPCKHBW           | AVX512         | AVX512EVEX     | AVX512BW_128   | 68           | 0b11       | rrr        | EVV 0x68 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                      | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
 | VPUNPCKHBW           | AVX512         | AVX512EVEX     | AVX512BW_128   | 68           | mm         | rrr        | EVV 0x68 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
 | VPUNPCKHBW           | AVX512         | AVX512EVEX     | AVX512BW_256   | 68           | 0b11       | rrr        | EVV 0x68 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                      | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
 | VPUNPCKHBW           | AVX512         | AVX512EVEX     | AVX512BW_256   | 68           | mm         | rrr        | EVV 0x68 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
 | VPUNPCKHBW           | AVX512         | AVX512EVEX     | AVX512BW_512   | 68           | 0b11       | rrr        | EVV 0x68 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                      | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
 | VPUNPCKHBW           | AVX512         | AVX512EVEX     | AVX512BW_512   | 68           | mm         | rrr        | EVV 0x68 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
 | VPUNPCKHDQ           | AVX            | AVX            |                | 6a           | mm         | rrr        | VV1 0x6A  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 MEM0:r:dq:u32
 | VPUNPCKHDQ           | AVX            | AVX            |                | 6a           | 0b11       | rrr        | VV1 0x6A  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 REG2=XMM_B():r:dq:u32
 | VPUNPCKHDQ           | AVX2           | AVX2           |                | 6a           | mm         | rrr        | VV1 0x6A  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u32 REG1=YMM_N():r:qq:u32 MEM0:r:qq:u32
 | VPUNPCKHDQ           | AVX2           | AVX2           |                | 6a           | 0b11       | rrr        | VV1 0x6A  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:u32 REG1=YMM_N():r:qq:u32 REG2=YMM_B():r:qq:u32
 | VPUNPCKHDQ           | AVX512         | AVX512EVEX     | AVX512F_128    | 6a           | 0b11       | rrr        | EVV 0x6A V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                  | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
 | VPUNPCKHDQ           | AVX512         | AVX512EVEX     | AVX512F_128    | 6a           | mm         | rrr        | EVV 0x6A V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPUNPCKHDQ           | AVX512         | AVX512EVEX     | AVX512F_256    | 6a           | 0b11       | rrr        | EVV 0x6A V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                  | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
 | VPUNPCKHDQ           | AVX512         | AVX512EVEX     | AVX512F_256    | 6a           | mm         | rrr        | EVV 0x6A V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPUNPCKHDQ           | AVX512         | AVX512EVEX     | AVX512F_512    | 6a           | 0b11       | rrr        | EVV 0x6A V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                  | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
 | VPUNPCKHDQ           | AVX512         | AVX512EVEX     | AVX512F_512    | 6a           | mm         | rrr        | EVV 0x6A V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPUNPCKHQDQ          | AVX            | AVX            |                | 6d           | mm         | rrr        | VV1 0x6D  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 MEM0:r:dq:u64
 | VPUNPCKHQDQ          | AVX            | AVX            |                | 6d           | 0b11       | rrr        | VV1 0x6D  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 REG2=XMM_B():r:dq:u64
 | VPUNPCKHQDQ          | AVX2           | AVX2           |                | 6d           | mm         | rrr        | VV1 0x6D  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u64 REG1=YMM_N():r:qq:u64 MEM0:r:qq:u64
 | VPUNPCKHQDQ          | AVX2           | AVX2           |                | 6d           | 0b11       | rrr        | VV1 0x6D  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:u64 REG1=YMM_N():r:qq:u64 REG2=YMM_B():r:qq:u64
 | VPUNPCKHQDQ          | AVX512         | AVX512EVEX     | AVX512F_128    | 6d           | 0b11       | rrr        | EVV 0x6D V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                  | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
 | VPUNPCKHQDQ          | AVX512         | AVX512EVEX     | AVX512F_128    | 6d           | mm         | rrr        | EVV 0x6D V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPUNPCKHQDQ          | AVX512         | AVX512EVEX     | AVX512F_256    | 6d           | 0b11       | rrr        | EVV 0x6D V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                  | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
 | VPUNPCKHQDQ          | AVX512         | AVX512EVEX     | AVX512F_256    | 6d           | mm         | rrr        | EVV 0x6D V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPUNPCKHQDQ          | AVX512         | AVX512EVEX     | AVX512F_512    | 6d           | 0b11       | rrr        | EVV 0x6D V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                  | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
 | VPUNPCKHQDQ          | AVX512         | AVX512EVEX     | AVX512F_512    | 6d           | mm         | rrr        | EVV 0x6D V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPUNPCKHWD           | AVX            | AVX            |                | 69           | mm         | rrr        | VV1 0x69  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 MEM0:r:dq:u16
 | VPUNPCKHWD           | AVX            | AVX            |                | 69           | 0b11       | rrr        | VV1 0x69  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 REG2=XMM_B():r:dq:u16
 | VPUNPCKHWD           | AVX2           | AVX2           |                | 69           | mm         | rrr        | VV1 0x69  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u16 REG1=YMM_N():r:qq:u16 MEM0:r:qq:u16
 | VPUNPCKHWD           | AVX2           | AVX2           |                | 69           | 0b11       | rrr        | VV1 0x69  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:u16 REG1=YMM_N():r:qq:u16 REG2=YMM_B():r:qq:u16
 | VPUNPCKHWD           | AVX512         | AVX512EVEX     | AVX512BW_128   | 69           | 0b11       | rrr        | EVV 0x69 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                      | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
 | VPUNPCKHWD           | AVX512         | AVX512EVEX     | AVX512BW_128   | 69           | mm         | rrr        | EVV 0x69 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
 | VPUNPCKHWD           | AVX512         | AVX512EVEX     | AVX512BW_256   | 69           | 0b11       | rrr        | EVV 0x69 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                      | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
 | VPUNPCKHWD           | AVX512         | AVX512EVEX     | AVX512BW_256   | 69           | mm         | rrr        | EVV 0x69 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
 | VPUNPCKHWD           | AVX512         | AVX512EVEX     | AVX512BW_512   | 69           | 0b11       | rrr        | EVV 0x69 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                      | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
 | VPUNPCKHWD           | AVX512         | AVX512EVEX     | AVX512BW_512   | 69           | mm         | rrr        | EVV 0x69 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
 | VPUNPCKLBW           | AVX            | AVX            |                | 60           | mm         | rrr        | VV1 0x60  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 MEM0:r:dq:u8
 | VPUNPCKLBW           | AVX            | AVX            |                | 60           | 0b11       | rrr        | VV1 0x60  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:u8
 | VPUNPCKLBW           | AVX2           | AVX2           |                | 60           | mm         | rrr        | VV1 0x60  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u8 REG1=YMM_N():r:qq:u8 MEM0:r:qq:u8
 | VPUNPCKLBW           | AVX2           | AVX2           |                | 60           | 0b11       | rrr        | VV1 0x60  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:u8 REG1=YMM_N():r:qq:u8 REG2=YMM_B():r:qq:u8
 | VPUNPCKLBW           | AVX512         | AVX512EVEX     | AVX512BW_128   | 60           | 0b11       | rrr        | EVV 0x60 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                      | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
 | VPUNPCKLBW           | AVX512         | AVX512EVEX     | AVX512BW_128   | 60           | mm         | rrr        | EVV 0x60 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
 | VPUNPCKLBW           | AVX512         | AVX512EVEX     | AVX512BW_256   | 60           | 0b11       | rrr        | EVV 0x60 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                      | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
 | VPUNPCKLBW           | AVX512         | AVX512EVEX     | AVX512BW_256   | 60           | mm         | rrr        | EVV 0x60 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
 | VPUNPCKLBW           | AVX512         | AVX512EVEX     | AVX512BW_512   | 60           | 0b11       | rrr        | EVV 0x60 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                      | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
 | VPUNPCKLBW           | AVX512         | AVX512EVEX     | AVX512BW_512   | 60           | mm         | rrr        | EVV 0x60 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
 | VPUNPCKLDQ           | AVX            | AVX            |                | 62           | mm         | rrr        | VV1 0x62  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 MEM0:r:dq:u32
 | VPUNPCKLDQ           | AVX            | AVX            |                | 62           | 0b11       | rrr        | VV1 0x62  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 REG2=XMM_B():r:dq:u32
 | VPUNPCKLDQ           | AVX2           | AVX2           |                | 62           | mm         | rrr        | VV1 0x62  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u32 REG1=YMM_N():r:qq:u32 MEM0:r:qq:u32
 | VPUNPCKLDQ           | AVX2           | AVX2           |                | 62           | 0b11       | rrr        | VV1 0x62  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:u32 REG1=YMM_N():r:qq:u32 REG2=YMM_B():r:qq:u32
 | VPUNPCKLDQ           | AVX512         | AVX512EVEX     | AVX512F_128    | 62           | 0b11       | rrr        | EVV 0x62 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                  | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
 | VPUNPCKLDQ           | AVX512         | AVX512EVEX     | AVX512F_128    | 62           | mm         | rrr        | EVV 0x62 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPUNPCKLDQ           | AVX512         | AVX512EVEX     | AVX512F_256    | 62           | 0b11       | rrr        | EVV 0x62 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                  | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
 | VPUNPCKLDQ           | AVX512         | AVX512EVEX     | AVX512F_256    | 62           | mm         | rrr        | EVV 0x62 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPUNPCKLDQ           | AVX512         | AVX512EVEX     | AVX512F_512    | 62           | 0b11       | rrr        | EVV 0x62 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                  | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
 | VPUNPCKLDQ           | AVX512         | AVX512EVEX     | AVX512F_512    | 62           | mm         | rrr        | EVV 0x62 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPUNPCKLQDQ          | AVX            | AVX            |                | 6c           | mm         | rrr        | VV1 0x6C  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 MEM0:r:dq:u64
 | VPUNPCKLQDQ          | AVX            | AVX            |                | 6c           | 0b11       | rrr        | VV1 0x6C  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 REG2=XMM_B():r:dq:u64
 | VPUNPCKLQDQ          | AVX2           | AVX2           |                | 6c           | mm         | rrr        | VV1 0x6C  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u64 REG1=YMM_N():r:qq:u64 MEM0:r:qq:u64
 | VPUNPCKLQDQ          | AVX2           | AVX2           |                | 6c           | 0b11       | rrr        | VV1 0x6C  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:u64 REG1=YMM_N():r:qq:u64 REG2=YMM_B():r:qq:u64
 | VPUNPCKLQDQ          | AVX512         | AVX512EVEX     | AVX512F_128    | 6c           | 0b11       | rrr        | EVV 0x6C V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                  | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
 | VPUNPCKLQDQ          | AVX512         | AVX512EVEX     | AVX512F_128    | 6c           | mm         | rrr        | EVV 0x6C V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPUNPCKLQDQ          | AVX512         | AVX512EVEX     | AVX512F_256    | 6c           | 0b11       | rrr        | EVV 0x6C V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                  | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
 | VPUNPCKLQDQ          | AVX512         | AVX512EVEX     | AVX512F_256    | 6c           | mm         | rrr        | EVV 0x6C V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPUNPCKLQDQ          | AVX512         | AVX512EVEX     | AVX512F_512    | 6c           | 0b11       | rrr        | EVV 0x6C V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                  | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
 | VPUNPCKLQDQ          | AVX512         | AVX512EVEX     | AVX512F_512    | 6c           | mm         | rrr        | EVV 0x6C V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPUNPCKLWD           | AVX            | AVX            |                | 61           | mm         | rrr        | VV1 0x61  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 MEM0:r:dq:u16
 | VPUNPCKLWD           | AVX            | AVX            |                | 61           | 0b11       | rrr        | VV1 0x61  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 REG2=XMM_B():r:dq:u16
 | VPUNPCKLWD           | AVX2           | AVX2           |                | 61           | mm         | rrr        | VV1 0x61  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u16 REG1=YMM_N():r:qq:u16 MEM0:r:qq:u16
 | VPUNPCKLWD           | AVX2           | AVX2           |                | 61           | 0b11       | rrr        | VV1 0x61  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:u16 REG1=YMM_N():r:qq:u16 REG2=YMM_B():r:qq:u16
 | VPUNPCKLWD           | AVX512         | AVX512EVEX     | AVX512BW_128   | 61           | 0b11       | rrr        | EVV 0x61 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                                      | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
 | VPUNPCKLWD           | AVX512         | AVX512EVEX     | AVX512BW_128   | 61           | mm         | rrr        | EVV 0x61 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
 | VPUNPCKLWD           | AVX512         | AVX512EVEX     | AVX512BW_256   | 61           | 0b11       | rrr        | EVV 0x61 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                                      | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
 | VPUNPCKLWD           | AVX512         | AVX512EVEX     | AVX512BW_256   | 61           | mm         | rrr        | EVV 0x61 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
 | VPUNPCKLWD           | AVX512         | AVX512EVEX     | AVX512BW_512   | 61           | 0b11       | rrr        | EVV 0x61 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                                      | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
 | VPUNPCKLWD           | AVX512         | AVX512EVEX     | AVX512BW_512   | 61           | mm         | rrr        | EVV 0x61 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
 | VPXOR                | LOGICAL        | AVX            |                | ef           | mm         | rrr        | VV1 0xEF  VL128 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq:u128 REG1=XMM_N():r:dq:u128 MEM0:r:dq:u128
 | VPXOR                | LOGICAL        | AVX            |                | ef           | 0b11       | rrr        | VV1 0xEF  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u128 REG1=XMM_N():r:dq:u128 REG2=XMM_B():r:dq:u128
 | VPXOR                | LOGICAL        | AVX2           |                | ef           | mm         | rrr        | VV1 0xEF  VL256 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u256 REG1=YMM_N():r:qq:u256 MEM0:r:qq:u256
 | VPXOR                | LOGICAL        | AVX2           |                | ef           | 0b11       | rrr        | VV1 0xEF   VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=YMM_R():w:qq:u256 REG1=YMM_N():r:qq:u256 REG2=YMM_B():r:qq:u256
 | VPXORD               | KNC            | KNCE           |                | ef           | mm         | rrr        | KVV 0xEF V0F REXW=0 V66  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT32()                  | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VPXORD               | KNC            | KNCE           |                | ef           | 0b11       | rrr        | KVV 0xEF V0F REXW=0 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                       | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VPXORD               | KNC            | KNCE           |                | ef           | 0b11       | rrr        | KVV 0xEF V0F REXW=0 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1                                       | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VPXORD               | LOGICAL        | AVX512EVEX     | AVX512F_128    | ef           | 0b11       | rrr        | EVV 0xEF V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                  | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
 | VPXORD               | LOGICAL        | AVX512EVEX     | AVX512F_128    | ef           | mm         | rrr        | EVV 0xEF V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPXORD               | LOGICAL        | AVX512EVEX     | AVX512F_256    | ef           | 0b11       | rrr        | EVV 0xEF V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                  | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
 | VPXORD               | LOGICAL        | AVX512EVEX     | AVX512F_256    | ef           | mm         | rrr        | EVV 0xEF V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPXORD               | LOGICAL        | AVX512EVEX     | AVX512F_512    | ef           | 0b11       | rrr        | EVV 0xEF V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                  | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
 | VPXORD               | LOGICAL        | AVX512EVEX     | AVX512F_512    | ef           | mm         | rrr        | EVV 0xEF V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VPXORQ               | KNC            | KNCE           |                | ef           | mm         | rrr        | KVV 0xEF V0F REXW=1 V66  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT64()                  | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VPXORQ               | KNC            | KNCE           |                | ef           | 0b11       | rrr        | KVV 0xEF V0F REXW=1 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE64()                       | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=REGSWIZ
 | VPXORQ               | KNC            | KNCE           |                | ef           | 0b11       | rrr        | KVV 0xEF V0F REXW=1 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1                                       | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VPXORQ               | LOGICAL        | AVX512EVEX     | AVX512F_128    | ef           | 0b11       | rrr        | EVV 0xEF V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                  | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
 | VPXORQ               | LOGICAL        | AVX512EVEX     | AVX512F_128    | ef           | mm         | rrr        | EVV 0xEF V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPXORQ               | LOGICAL        | AVX512EVEX     | AVX512F_256    | ef           | 0b11       | rrr        | EVV 0xEF V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                  | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
 | VPXORQ               | LOGICAL        | AVX512EVEX     | AVX512F_256    | ef           | mm         | rrr        | EVV 0xEF V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VPXORQ               | LOGICAL        | AVX512EVEX     | AVX512F_512    | ef           | 0b11       | rrr        | EVV 0xEF V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                  | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
 | VPXORQ               | LOGICAL        | AVX512EVEX     | AVX512F_512    | ef           | mm         | rrr        | EVV 0xEF V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VRANGEPD             | AVX512         | AVX512EVEX     | AVX512DQ_128   | 50           | 0b11       | rrr        | EVV 0x50 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1   UIMM8()                      | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64 IMM0:r:b
 | VRANGEPD             | AVX512         | AVX512EVEX     | AVX512DQ_128   | 50           | mm         | rrr        | EVV 0x50 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
 | VRANGEPD             | AVX512         | AVX512EVEX     | AVX512DQ_256   | 50           | 0b11       | rrr        | EVV 0x50 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1   UIMM8()                      | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64 IMM0:r:b
 | VRANGEPD             | AVX512         | AVX512EVEX     | AVX512DQ_256   | 50           | mm         | rrr        | EVV 0x50 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
 | VRANGEPD             | AVX512         | AVX512EVEX     | AVX512DQ_512   | 50           | 0b11       | rrr        | EVV 0x50 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1   UIMM8()                      | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64 IMM0:r:b
 | VRANGEPD             | AVX512         | AVX512EVEX     | AVX512DQ_512   | 50           | 0b11       | rrr        | EVV 0x50 V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W1   UIMM8()    | REG0=ZMM_R3():w:zf64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64 IMM0:r:b
 | VRANGEPD             | AVX512         | AVX512EVEX     | AVX512DQ_512   | 50           | mm         | rrr        | EVV 0x50 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
 | VRANGEPS             | AVX512         | AVX512EVEX     | AVX512DQ_128   | 50           | 0b11       | rrr        | EVV 0x50 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0   UIMM8()                      | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32 IMM0:r:b
 | VRANGEPS             | AVX512         | AVX512EVEX     | AVX512DQ_128   | 50           | mm         | rrr        | EVV 0x50 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
 | VRANGEPS             | AVX512         | AVX512EVEX     | AVX512DQ_256   | 50           | 0b11       | rrr        | EVV 0x50 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0   UIMM8()                      | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32 IMM0:r:b
 | VRANGEPS             | AVX512         | AVX512EVEX     | AVX512DQ_256   | 50           | mm         | rrr        | EVV 0x50 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
 | VRANGEPS             | AVX512         | AVX512EVEX     | AVX512DQ_512   | 50           | 0b11       | rrr        | EVV 0x50 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0   UIMM8()                      | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32 IMM0:r:b
 | VRANGEPS             | AVX512         | AVX512EVEX     | AVX512DQ_512   | 50           | 0b11       | rrr        | EVV 0x50 V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0   UIMM8()    | REG0=ZMM_R3():w:zf32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32 IMM0:r:b
 | VRANGEPS             | AVX512         | AVX512EVEX     | AVX512DQ_512   | 50           | mm         | rrr        | EVV 0x50 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
 | VRANGESD             | AVX512         | AVX512EVEX     | AVX512DQ_SCALAR | 51           | 0b11       | rrr        | EVV 0x51 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1   UIMM8()                             | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64 IMM0:r:b
 | VRANGESD             | AVX512         | AVX512EVEX     | AVX512DQ_SCALAR | 51           | 0b11       | rrr        | EVV 0x51 V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W1   UIMM8()    | REG0=XMM_R3():w:dq:f64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64 IMM0:r:b
 | VRANGESD             | AVX512         | AVX512EVEX     | AVX512DQ_SCALAR | 51           | mm         | rrr        | EVV 0x51 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1   UIMM8()  ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64 IMM0:r:b
 | VRANGESS             | AVX512         | AVX512EVEX     | AVX512DQ_SCALAR | 51           | 0b11       | rrr        | EVV 0x51 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0   UIMM8()                             | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32 IMM0:r:b
 | VRANGESS             | AVX512         | AVX512EVEX     | AVX512DQ_SCALAR | 51           | 0b11       | rrr        | EVV 0x51 V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W0   UIMM8()    | REG0=XMM_R3():w:dq:f32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32 IMM0:r:b
 | VRANGESS             | AVX512         | AVX512EVEX     | AVX512DQ_SCALAR | 51           | mm         | rrr        | EVV 0x51 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0   UIMM8()  ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32 IMM0:r:b
 | VRCP14PD             | AVX512         | AVX512EVEX     | AVX512F_128    | 4c           | 0b11       | rrr        | EVV 0x4C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                        | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64
 | VRCP14PD             | AVX512         | AVX512EVEX     | AVX512F_128    | 4c           | mm         | rrr        | EVV 0x4C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VRCP14PD             | AVX512         | AVX512EVEX     | AVX512F_256    | 4c           | 0b11       | rrr        | EVV 0x4C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                        | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64
 | VRCP14PD             | AVX512         | AVX512EVEX     | AVX512F_256    | 4c           | mm         | rrr        | EVV 0x4C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VRCP14PD             | AVX512         | AVX512EVEX     | AVX512F_512    | 4c           | 0b11       | rrr        | EVV 0x4C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                        | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
 | VRCP14PD             | AVX512         | AVX512EVEX     | AVX512F_512    | 4c           | mm         | rrr        | EVV 0x4C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VRCP14PS             | AVX512         | AVX512EVEX     | AVX512F_128    | 4c           | 0b11       | rrr        | EVV 0x4C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
 | VRCP14PS             | AVX512         | AVX512EVEX     | AVX512F_128    | 4c           | mm         | rrr        | EVV 0x4C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VRCP14PS             | AVX512         | AVX512EVEX     | AVX512F_256    | 4c           | 0b11       | rrr        | EVV 0x4C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
 | VRCP14PS             | AVX512         | AVX512EVEX     | AVX512F_256    | 4c           | mm         | rrr        | EVV 0x4C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VRCP14PS             | AVX512         | AVX512EVEX     | AVX512F_512    | 4c           | 0b11       | rrr        | EVV 0x4C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
 | VRCP14PS             | AVX512         | AVX512EVEX     | AVX512F_512    | 4c           | mm         | rrr        | EVV 0x4C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VRCP14SD             | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 4d           | 0b11       | rrr        | EVV 0x4D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                                       | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VRCP14SD             | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 4d           | mm         | rrr        | EVV 0x4D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
 | VRCP14SS             | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 4d           | 0b11       | rrr        | EVV 0x4D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                                       | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VRCP14SS             | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 4d           | mm         | rrr        | EVV 0x4D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
 | VRCP23PS             | KNC            | KNCE           | KNCE           | ca           | mm         | rrr        | KVV 0xCA V0F38 V66  W0  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() NOSWIZF32()                  | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  MEM0:r:zd:TXT=NT
 | VRCP23PS             | KNC            | KNCE           | KNCE           | ca           | 0b11       | rrr        | KVV 0xCA V0F38 V66  W0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] NR=1 KNC_SAE()                      | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw REG2=ZMM_B3():r:zd:TXT=SAEC
 | VRCP23PS             | KNC            | KNCE           | KNCE           | ca           | 0b11       | rrr        | KVV 0xCA V0F38 V66  W0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] NR=0 SWIZ=0                         | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw REG2=ZMM_B3():r:zd
 | VRCP28PD             | AVX512         | AVX512EVEX     | AVX512ER_512   | ca           | 0b11       | rrr        | EVV 0xCA V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                        | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
 | VRCP28PD             | AVX512         | AVX512EVEX     | AVX512ER_512   | ca           | 0b11       | rrr        | EVV 0xCA V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W1  NOEVSR      | REG0=ZMM_R3():w:zf64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
 | VRCP28PD             | AVX512         | AVX512EVEX     | AVX512ER_512   | ca           | mm         | rrr        | EVV 0xCA V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VRCP28PS             | AVX512         | AVX512EVEX     | AVX512ER_512   | ca           | 0b11       | rrr        | EVV 0xCA V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
 | VRCP28PS             | AVX512         | AVX512EVEX     | AVX512ER_512   | ca           | 0b11       | rrr        | EVV 0xCA V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0  NOEVSR      | REG0=ZMM_R3():w:zf32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
 | VRCP28PS             | AVX512         | AVX512EVEX     | AVX512ER_512   | ca           | mm         | rrr        | EVV 0xCA V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VRCP28SD             | AVX512         | AVX512EVEX     | AVX512ER_SCALAR | cb           | 0b11       | rrr        | EVV 0xCB V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                                       | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VRCP28SD             | AVX512         | AVX512EVEX     | AVX512ER_SCALAR | cb           | 0b11       | rrr        | EVV 0xCB V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W1              | REG0=XMM_R3():w:dq:f64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VRCP28SD             | AVX512         | AVX512EVEX     | AVX512ER_SCALAR | cb           | mm         | rrr        | EVV 0xCB V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
 | VRCP28SS             | AVX512         | AVX512EVEX     | AVX512ER_SCALAR | cb           | 0b11       | rrr        | EVV 0xCB V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                                       | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VRCP28SS             | AVX512         | AVX512EVEX     | AVX512ER_SCALAR | cb           | 0b11       | rrr        | EVV 0xCB V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W0              | REG0=XMM_R3():w:dq:f32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VRCP28SS             | AVX512         | AVX512EVEX     | AVX512ER_SCALAR | cb           | mm         | rrr        | EVV 0xCB V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
 | VRCPPS               | AVX            | AVX            |                | 53           | mm         | rrr        | VV1 0x53  VNP VL128 NOVSR V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:f32  MEM0:r:dq:f32
 | VRCPPS               | AVX            | AVX            |                | 53           | 0b11       | rrr        | VV1 0x53  VNP VL128 NOVSR V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:f32  REG1=XMM_B():r:dq:f32
 | VRCPPS               | AVX            | AVX            |                | 53           | mm         | rrr        | VV1 0x53  VNP VL256 NOVSR V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=YMM_R():w:qq:f32  MEM0:r:qq:f32
 | VRCPPS               | AVX            | AVX            |                | 53           | 0b11       | rrr        | VV1 0x53  VNP VL256 NOVSR V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=YMM_R():w:qq:f32  REG1=YMM_B():r:qq:f32
 | VRCPSS               | AVX            | AVX            |                | 53           | mm         | rrr        | VV1 0x53  VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                            | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:d:f32
 | VRCPSS               | AVX            | AVX            |                | 53           | 0b11       | rrr        | VV1 0x53  VF3 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                   | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:d:f32
 | VREDUCEPD            | AVX512         | AVX512EVEX     | AVX512DQ_128   | 56           | 0b11       | rrr        | EVV 0x56 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR UIMM8()                | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64 IMM0:r:b
 | VREDUCEPD            | AVX512         | AVX512EVEX     | AVX512DQ_128   | 56           | mm         | rrr        | EVV 0x56 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
 | VREDUCEPD            | AVX512         | AVX512EVEX     | AVX512DQ_256   | 56           | 0b11       | rrr        | EVV 0x56 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR UIMM8()                | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64 IMM0:r:b
 | VREDUCEPD            | AVX512         | AVX512EVEX     | AVX512DQ_256   | 56           | mm         | rrr        | EVV 0x56 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
 | VREDUCEPD            | AVX512         | AVX512EVEX     | AVX512DQ_512   | 56           | 0b11       | rrr        | EVV 0x56 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR UIMM8()                | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64 IMM0:r:b
 | VREDUCEPD            | AVX512         | AVX512EVEX     | AVX512DQ_512   | 56           | 0b11       | rrr        | EVV 0x56 V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W1  NOEVSR UIMM8() | REG0=ZMM_R3():w:zf64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64 IMM0:r:b
 | VREDUCEPD            | AVX512         | AVX512EVEX     | AVX512DQ_512   | 56           | mm         | rrr        | EVV 0x56 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
 | VREDUCEPS            | AVX512         | AVX512EVEX     | AVX512DQ_128   | 56           | 0b11       | rrr        | EVV 0x56 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR UIMM8()                | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32 IMM0:r:b
 | VREDUCEPS            | AVX512         | AVX512EVEX     | AVX512DQ_128   | 56           | mm         | rrr        | EVV 0x56 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
 | VREDUCEPS            | AVX512         | AVX512EVEX     | AVX512DQ_256   | 56           | 0b11       | rrr        | EVV 0x56 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR UIMM8()                | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32 IMM0:r:b
 | VREDUCEPS            | AVX512         | AVX512EVEX     | AVX512DQ_256   | 56           | mm         | rrr        | EVV 0x56 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
 | VREDUCEPS            | AVX512         | AVX512EVEX     | AVX512DQ_512   | 56           | 0b11       | rrr        | EVV 0x56 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR UIMM8()                | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32 IMM0:r:b
 | VREDUCEPS            | AVX512         | AVX512EVEX     | AVX512DQ_512   | 56           | 0b11       | rrr        | EVV 0x56 V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0  NOEVSR UIMM8() | REG0=ZMM_R3():w:zf32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32 IMM0:r:b
 | VREDUCEPS            | AVX512         | AVX512EVEX     | AVX512DQ_512   | 56           | mm         | rrr        | EVV 0x56 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
 | VREDUCESD            | AVX512         | AVX512EVEX     | AVX512DQ_SCALAR | 57           | 0b11       | rrr        | EVV 0x57 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1   UIMM8()                             | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64 IMM0:r:b
 | VREDUCESD            | AVX512         | AVX512EVEX     | AVX512DQ_SCALAR | 57           | 0b11       | rrr        | EVV 0x57 V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W1   UIMM8()    | REG0=XMM_R3():w:dq:f64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64 IMM0:r:b
 | VREDUCESD            | AVX512         | AVX512EVEX     | AVX512DQ_SCALAR | 57           | mm         | rrr        | EVV 0x57 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1   UIMM8()  ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64 IMM0:r:b
 | VREDUCESS            | AVX512         | AVX512EVEX     | AVX512DQ_SCALAR | 57           | 0b11       | rrr        | EVV 0x57 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0   UIMM8()                             | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32 IMM0:r:b
 | VREDUCESS            | AVX512         | AVX512EVEX     | AVX512DQ_SCALAR | 57           | 0b11       | rrr        | EVV 0x57 V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W0   UIMM8()    | REG0=XMM_R3():w:dq:f32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32 IMM0:r:b
 | VREDUCESS            | AVX512         | AVX512EVEX     | AVX512DQ_SCALAR | 57           | mm         | rrr        | EVV 0x57 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0   UIMM8()  ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32 IMM0:r:b
 | VRNDFXPNTPD          | CONVERT        | KNCE           |                | 52           | mm         | rrr        | KVV 0x52 V0F3A V66  W1  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() UPCONVERT_FLT64()    | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw MEM0:r:zv:TXT=CONVERT:TXT=NT IMM0:r:b
 | VRNDFXPNTPD          | CONVERT        | KNCE           |                | 52           | 0b11       | rrr        | KVV 0x52 V0F3A V66  W1  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] UIMM8() NR=1 KNC_SAE()              | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_B3():r:zf64:TXT=SAEC  IMM0:r:b
 | VRNDFXPNTPD          | CONVERT        | KNCE           |                | 52           | 0b11       | rrr        | KVV 0x52 V0F3A V66  W1  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] UIMM8() NR=0 REG_SWIZZLE64()        | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_B3():r:zf64 IMM0:r:b
 | VRNDFXPNTPS          | KNC            | KNCE           | KNCE           | 52           | mm         | rrr        | KVV 0x52 V0F3A V66  W0  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() UPCONVERT_FLT32()    | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw  MEM0:r:zv:TXT=CONVERT:TXT=NT IMM0:r:b
 | VRNDFXPNTPS          | KNC            | KNCE           | KNCE           | 52           | 0b11       | rrr        | KVV 0x52 V0F3A V66  W0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] UIMM8() NR=1 KNC_SAE()              | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw  REG2=ZMM_B3():r:zf32:TXT=SAEC  IMM0:r:b
 | VRNDFXPNTPS          | KNC            | KNCE           | KNCE           | 52           | 0b11       | rrr        | KVV 0x52 V0F3A V66  W0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] UIMM8() NR=0 REG_SWIZZLE32()        | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw  REG2=ZMM_B3():r:zf32:TXT=REGSWIZ IMM0:r:b
 | VRNDSCALEPD          | AVX512         | AVX512EVEX     | AVX512F_128    | 09           | 0b11       | rrr        | EVV 0x09 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR UIMM8()                | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64 IMM0:r:b
 | VRNDSCALEPD          | AVX512         | AVX512EVEX     | AVX512F_128    | 09           | mm         | rrr        | EVV 0x09 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
 | VRNDSCALEPD          | AVX512         | AVX512EVEX     | AVX512F_256    | 09           | 0b11       | rrr        | EVV 0x09 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR UIMM8()                | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64 IMM0:r:b
 | VRNDSCALEPD          | AVX512         | AVX512EVEX     | AVX512F_256    | 09           | mm         | rrr        | EVV 0x09 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
 | VRNDSCALEPD          | AVX512         | AVX512EVEX     | AVX512F_512    | 09           | 0b11       | rrr        | EVV 0x09 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR UIMM8()                | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64 IMM0:r:b
 | VRNDSCALEPD          | AVX512         | AVX512EVEX     | AVX512F_512    | 09           | 0b11       | rrr        | EVV 0x09 V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W1  NOEVSR UIMM8() | REG0=ZMM_R3():w:zf64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64 IMM0:r:b
 | VRNDSCALEPD          | AVX512         | AVX512EVEX     | AVX512F_512    | 09           | mm         | rrr        | EVV 0x09 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
 | VRNDSCALEPS          | AVX512         | AVX512EVEX     | AVX512F_128    | 08           | 0b11       | rrr        | EVV 0x08 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR UIMM8()                | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32 IMM0:r:b
 | VRNDSCALEPS          | AVX512         | AVX512EVEX     | AVX512F_128    | 08           | mm         | rrr        | EVV 0x08 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
 | VRNDSCALEPS          | AVX512         | AVX512EVEX     | AVX512F_256    | 08           | 0b11       | rrr        | EVV 0x08 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR UIMM8()                | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32 IMM0:r:b
 | VRNDSCALEPS          | AVX512         | AVX512EVEX     | AVX512F_256    | 08           | mm         | rrr        | EVV 0x08 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
 | VRNDSCALEPS          | AVX512         | AVX512EVEX     | AVX512F_512    | 08           | 0b11       | rrr        | EVV 0x08 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR UIMM8()                | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32 IMM0:r:b
 | VRNDSCALEPS          | AVX512         | AVX512EVEX     | AVX512F_512    | 08           | 0b11       | rrr        | EVV 0x08 V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0  NOEVSR UIMM8() | REG0=ZMM_R3():w:zf32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32 IMM0:r:b
 | VRNDSCALEPS          | AVX512         | AVX512EVEX     | AVX512F_512    | 08           | mm         | rrr        | EVV 0x08 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
 | VRNDSCALESD          | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 0b           | 0b11       | rrr        | EVV 0x0B V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1   UIMM8()                             | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64 IMM0:r:b
 | VRNDSCALESD          | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 0b           | 0b11       | rrr        | EVV 0x0B V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W1   UIMM8()    | REG0=XMM_R3():w:dq:f64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64 IMM0:r:b
 | VRNDSCALESD          | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 0b           | mm         | rrr        | EVV 0x0B V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1   UIMM8()  ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64 IMM0:r:b
 | VRNDSCALESS          | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 0a           | 0b11       | rrr        | EVV 0x0A V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0   UIMM8()                             | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32 IMM0:r:b
 | VRNDSCALESS          | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 0a           | 0b11       | rrr        | EVV 0x0A V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W0   UIMM8()    | REG0=XMM_R3():w:dq:f32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32 IMM0:r:b
 | VRNDSCALESS          | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 0a           | mm         | rrr        | EVV 0x0A V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0   UIMM8()  ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32 IMM0:r:b
 | VROUNDPD             | AVX            | AVX            |                | 09           | mm         | rrr        | VV1 0x09  VL128 V66 V0F3A NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                      | REG0=XMM_R():w:dq:f64  MEM0:r:dq:f64 IMM0:r:b
 | VROUNDPD             | AVX            | AVX            |                | 09           | 0b11       | rrr        | VV1 0x09  VL128 V66 V0F3A NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                             | REG0=XMM_R():w:dq:f64 REG1=XMM_B():r:dq:f64 IMM0:r:b
 | VROUNDPD             | AVX            | AVX            |                | 09           | mm         | rrr        | VV1 0x09  VL256 V66 V0F3A NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                      | REG0=YMM_R():w:qq:f64  MEM0:r:qq:f64 IMM0:r:b
 | VROUNDPD             | AVX            | AVX            |                | 09           | 0b11       | rrr        | VV1 0x09  VL256 V66 V0F3A NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                             | REG0=YMM_R():w:qq:f64 REG1=YMM_B():r:qq:f64 IMM0:r:b
 | VROUNDPS             | AVX            | AVX            |                | 08           | mm         | rrr        | VV1 0x08  VL128 V66 V0F3A NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                      | REG0=XMM_R():w:dq:f32  MEM0:r:dq:f32 IMM0:r:b
 | VROUNDPS             | AVX            | AVX            |                | 08           | 0b11       | rrr        | VV1 0x08  VL128 V66 V0F3A NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                             | REG0=XMM_R():w:dq:f32 REG1=XMM_B():r:dq:f32 IMM0:r:b
 | VROUNDPS             | AVX            | AVX            |                | 08           | mm         | rrr        | VV1 0x08  VL256 V66 V0F3A NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                      | REG0=YMM_R():w:qq:f32  MEM0:r:qq:f32 IMM0:r:b
 | VROUNDPS             | AVX            | AVX            |                | 08           | 0b11       | rrr        | VV1 0x08  VL256 V66 V0F3A NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                             | REG0=YMM_R():w:qq:f32 REG1=YMM_B():r:qq:f32 IMM0:r:b
 | VROUNDSD             | AVX            | AVX            |                | 0b           | mm         | rrr        | VV1 0x0B  V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                                  | REG0=XMM_R():w:dq:f64  REG1=XMM_N():r:dq:f64  MEM0:r:q:f64         IMM0:r:b
 | VROUNDSD             | AVX            | AVX            |                | 0b           | 0b11       | rrr        | VV1 0x0B  V66 V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                         | REG0=XMM_R():w:dq:f64  REG1=XMM_N():r:dq:f64  REG2=XMM_B():r:q:f64 IMM0:r:b
 | VROUNDSS             | AVX            | AVX            |                | 0a           | mm         | rrr        | VV1 0x0A  V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                                  | REG0=XMM_R():w:dq:f32  REG1=XMM_N():r:dq:f32  MEM0:r:d:f32         IMM0:r:b
 | VROUNDSS             | AVX            | AVX            |                | 0a           | 0b11       | rrr        | VV1 0x0A  V66 V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                         | REG0=XMM_R():w:dq:f32  REG1=XMM_N():r:dq:f32  REG2=XMM_B():r:d:f32 IMM0:r:b
 | VRSQRT14PD           | AVX512         | AVX512EVEX     | AVX512F_128    | 4e           | 0b11       | rrr        | EVV 0x4E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                        | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64
 | VRSQRT14PD           | AVX512         | AVX512EVEX     | AVX512F_128    | 4e           | mm         | rrr        | EVV 0x4E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VRSQRT14PD           | AVX512         | AVX512EVEX     | AVX512F_256    | 4e           | 0b11       | rrr        | EVV 0x4E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                        | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64
 | VRSQRT14PD           | AVX512         | AVX512EVEX     | AVX512F_256    | 4e           | mm         | rrr        | EVV 0x4E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VRSQRT14PD           | AVX512         | AVX512EVEX     | AVX512F_512    | 4e           | 0b11       | rrr        | EVV 0x4E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                        | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
 | VRSQRT14PD           | AVX512         | AVX512EVEX     | AVX512F_512    | 4e           | mm         | rrr        | EVV 0x4E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VRSQRT14PS           | AVX512         | AVX512EVEX     | AVX512F_128    | 4e           | 0b11       | rrr        | EVV 0x4E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                        | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
 | VRSQRT14PS           | AVX512         | AVX512EVEX     | AVX512F_128    | 4e           | mm         | rrr        | EVV 0x4E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VRSQRT14PS           | AVX512         | AVX512EVEX     | AVX512F_256    | 4e           | 0b11       | rrr        | EVV 0x4E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                        | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
 | VRSQRT14PS           | AVX512         | AVX512EVEX     | AVX512F_256    | 4e           | mm         | rrr        | EVV 0x4E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VRSQRT14PS           | AVX512         | AVX512EVEX     | AVX512F_512    | 4e           | 0b11       | rrr        | EVV 0x4E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
 | VRSQRT14PS           | AVX512         | AVX512EVEX     | AVX512F_512    | 4e           | mm         | rrr        | EVV 0x4E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VRSQRT14SD           | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 4f           | 0b11       | rrr        | EVV 0x4F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                                       | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VRSQRT14SD           | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 4f           | mm         | rrr        | EVV 0x4F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
 | VRSQRT14SS           | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 4f           | 0b11       | rrr        | EVV 0x4F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                                       | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VRSQRT14SS           | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 4f           | mm         | rrr        | EVV 0x4F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
 | VRSQRT23PS           | KNC            | KNCE           | KNCE           | cb           | mm         | rrr        | KVV 0xCB V0F38 V66  W0  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() NOSWIZF32()                  | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  MEM0:r:zd:TXT=NT
 | VRSQRT23PS           | KNC            | KNCE           | KNCE           | cb           | 0b11       | rrr        | KVV 0xCB V0F38 V66  W0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] NR=1 KNC_SAE()                      | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw REG2=ZMM_B3():r:zd:TXT=SAEC
 | VRSQRT23PS           | KNC            | KNCE           | KNCE           | cb           | 0b11       | rrr        | KVV 0xCB V0F38 V66  W0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] NR=0 SWIZ=0                         | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw REG2=ZMM_B3():r:zd
 | VRSQRT28PD           | AVX512         | AVX512EVEX     | AVX512ER_512   | cc           | 0b11       | rrr        | EVV 0xCC V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                        | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
 | VRSQRT28PD           | AVX512         | AVX512EVEX     | AVX512ER_512   | cc           | 0b11       | rrr        | EVV 0xCC V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W1  NOEVSR      | REG0=ZMM_R3():w:zf64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
 | VRSQRT28PD           | AVX512         | AVX512EVEX     | AVX512ER_512   | cc           | mm         | rrr        | EVV 0xCC V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VRSQRT28PS           | AVX512         | AVX512EVEX     | AVX512ER_512   | cc           | 0b11       | rrr        | EVV 0xCC V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                        | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
 | VRSQRT28PS           | AVX512         | AVX512EVEX     | AVX512ER_512   | cc           | 0b11       | rrr        | EVV 0xCC V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0  NOEVSR      | REG0=ZMM_R3():w:zf32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
 | VRSQRT28PS           | AVX512         | AVX512EVEX     | AVX512ER_512   | cc           | mm         | rrr        | EVV 0xCC V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VRSQRT28SD           | AVX512         | AVX512EVEX     | AVX512ER_SCALAR | cd           | 0b11       | rrr        | EVV 0xCD V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                                       | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VRSQRT28SD           | AVX512         | AVX512EVEX     | AVX512ER_SCALAR | cd           | 0b11       | rrr        | EVV 0xCD V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W1              | REG0=XMM_R3():w:dq:f64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VRSQRT28SD           | AVX512         | AVX512EVEX     | AVX512ER_SCALAR | cd           | mm         | rrr        | EVV 0xCD V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
 | VRSQRT28SS           | AVX512         | AVX512EVEX     | AVX512ER_SCALAR | cd           | 0b11       | rrr        | EVV 0xCD V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                                       | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VRSQRT28SS           | AVX512         | AVX512EVEX     | AVX512ER_SCALAR | cd           | 0b11       | rrr        | EVV 0xCD V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W0              | REG0=XMM_R3():w:dq:f32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VRSQRT28SS           | AVX512         | AVX512EVEX     | AVX512ER_SCALAR | cd           | mm         | rrr        | EVV 0xCD V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
 | VRSQRTPS             | AVX            | AVX            |                | 52           | mm         | rrr        | VV1 0x52  VNP VL128 NOVSR V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:f32  MEM0:r:dq:f32
 | VRSQRTPS             | AVX            | AVX            |                | 52           | 0b11       | rrr        | VV1 0x52  VNP VL128 NOVSR V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:f32  REG1=XMM_B():r:dq:f32
 | VRSQRTPS             | AVX            | AVX            |                | 52           | mm         | rrr        | VV1 0x52  VNP VL256 NOVSR V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=YMM_R():w:qq:f32  MEM0:r:qq:f32
 | VRSQRTPS             | AVX            | AVX            |                | 52           | 0b11       | rrr        | VV1 0x52  VNP VL256 NOVSR V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=YMM_R():w:qq:f32  REG1=YMM_B():r:qq:f32
 | VRSQRTSS             | AVX            | AVX            |                | 52           | mm         | rrr        | VV1 0x52  VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                            | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:d:f32
 | VRSQRTSS             | AVX            | AVX            |                | 52           | 0b11       | rrr        | VV1 0x52  VF3 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                   | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:d:f32
 | VSCALEFPD            | AVX512         | AVX512EVEX     | AVX512F_128    | 2c           | 0b11       | rrr        | EVV 0x2C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VSCALEFPD            | AVX512         | AVX512EVEX     | AVX512F_128    | 2c           | mm         | rrr        | EVV 0x2C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VSCALEFPD            | AVX512         | AVX512EVEX     | AVX512F_256    | 2c           | 0b11       | rrr        | EVV 0x2C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
 | VSCALEFPD            | AVX512         | AVX512EVEX     | AVX512F_256    | 2c           | mm         | rrr        | EVV 0x2C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VSCALEFPD            | AVX512         | AVX512EVEX     | AVX512F_512    | 2c           | 0b11       | rrr        | EVV 0x2C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VSCALEFPD            | AVX512         | AVX512EVEX     | AVX512F_512    | 2c           | 0b11       | rrr        | EVV 0x2C V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1     | REG0=ZMM_R3():w:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VSCALEFPD            | AVX512         | AVX512EVEX     | AVX512F_512    | 2c           | mm         | rrr        | EVV 0x2C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VSCALEFPS            | AVX512         | AVX512EVEX     | AVX512F_128    | 2c           | 0b11       | rrr        | EVV 0x2C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VSCALEFPS            | AVX512         | AVX512EVEX     | AVX512F_128    | 2c           | mm         | rrr        | EVV 0x2C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VSCALEFPS            | AVX512         | AVX512EVEX     | AVX512F_256    | 2c           | 0b11       | rrr        | EVV 0x2C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
 | VSCALEFPS            | AVX512         | AVX512EVEX     | AVX512F_256    | 2c           | mm         | rrr        | EVV 0x2C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VSCALEFPS            | AVX512         | AVX512EVEX     | AVX512F_512    | 2c           | 0b11       | rrr        | EVV 0x2C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VSCALEFPS            | AVX512         | AVX512EVEX     | AVX512F_512    | 2c           | 0b11       | rrr        | EVV 0x2C V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0     | REG0=ZMM_R3():w:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VSCALEFPS            | AVX512         | AVX512EVEX     | AVX512F_512    | 2c           | mm         | rrr        | EVV 0x2C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VSCALEFSD            | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 2d           | 0b11       | rrr        | EVV 0x2D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                                       | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VSCALEFSD            | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 2d           | 0b11       | rrr        | EVV 0x2D V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1     | REG0=XMM_R3():w:dq:f64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VSCALEFSD            | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 2d           | mm         | rrr        | EVV 0x2D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
 | VSCALEFSS            | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 2d           | 0b11       | rrr        | EVV 0x2D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                                       | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VSCALEFSS            | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 2d           | 0b11       | rrr        | EVV 0x2D V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W0     | REG0=XMM_R3():w:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VSCALEFSS            | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 2d           | mm         | rrr        | EVV 0x2D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
 | VSCALEPS             | KNC            | KNCE           |                | 84           | mm         | rrr        | KVV 0x84 V0F38 REXW=0 V66  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_INT32()                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VSCALEPS             | KNC            | KNCE           |                | 84           | 0b11       | rrr        | KVV 0x84 V0F38 REXW=0 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VSCALEPS             | KNC            | KNCE           |                | 84           | 0b11       | rrr        | KVV 0x84 V0F38 REXW=0 V66  MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1                                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VSCATTERDPD          | KNC            | KNCE           | KNCE           | a2           | mm         | rrr        | KVV 0xA2 V66 V0F38  W1 NO_SPARSE_EVSR MOD[mm] MOD!=3 REG[rrr] RM[0b100] KNC_VMODRM() DNCONVERT_FLT64() | MEM0:w:zv:TXT=NT:TXT=CONVERT  REG1=MASK1():rw:mskw   REG0=ZMM_R3():r:zf64 NELEM=1:SUPP
 | VSCATTERDPD          | SCATTER        | AVX512EVEX     | AVX512F_128    | a2           | mm         | rrr        | EVV 0xA2 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL128  W1 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:w:q:f64 REG0=MASKNOT0():rw:mskw REG1=XMM_R3():r:dq:f64
 | VSCATTERDPD          | SCATTER        | AVX512EVEX     | AVX512F_256    | a2           | mm         | rrr        | EVV 0xA2 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL256  W1 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:w:q:f64 REG0=MASKNOT0():rw:mskw REG1=YMM_R3():r:qq:f64
 | VSCATTERDPD          | SCATTER        | AVX512EVEX     | AVX512F_512    | a2           | mm         | rrr        | EVV 0xA2 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL512  W1 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:w:q:f64 REG0=MASKNOT0():rw:mskw REG1=ZMM_R3():r:zf64
 | VSCATTERDPS          | KNC            | KNCE           | KNCE           | a2           | mm         | rrr        | KVV 0xA2 V66 V0F38  W0 NO_SPARSE_EVSR MOD[mm] MOD!=3 REG[rrr] RM[0b100] KNC_VMODRM() DNCONVERT_FLT32() | MEM0:w:zv:TXT=NT:TXT=CONVERT  REG1=MASK1():rw:mskw   REG0=ZMM_R3():r:zf32 NELEM=1:SUPP
 | VSCATTERDPS          | SCATTER        | AVX512EVEX     | AVX512F_128    | a2           | mm         | rrr        | EVV 0xA2 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL128  W0 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:w:d:f32 REG0=MASKNOT0():rw:mskw REG1=XMM_R3():r:dq:f32
 | VSCATTERDPS          | SCATTER        | AVX512EVEX     | AVX512F_256    | a2           | mm         | rrr        | EVV 0xA2 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL256  W0 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:w:d:f32 REG0=MASKNOT0():rw:mskw REG1=YMM_R3():r:qq:f32
 | VSCATTERDPS          | SCATTER        | AVX512EVEX     | AVX512F_512    | a2           | mm         | rrr        | EVV 0xA2 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL512  W0 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:w:d:f32 REG0=MASKNOT0():rw:mskw REG1=ZMM_R3():r:zf32
 | VSCATTERPF0DPD       | SCATTER        | AVX512EVEX     | AVX512PF_512   | c6           | mm         | 0b101      | EVV 0xC6 V66 V0F38 MOD[mm] MOD!=3 REG[0b101] RM[nnn] BCRC=0   VL512  W1 RM=4 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:r:q:f64 REG0=MASKNOT0():rw:mskw
 | VSCATTERPF0DPS       | PREFETCH       | KNCE           |                | c6           | mm         | 0b101      | KVV 0xC6 V0F38 V66  REXW=0 NO_SPARSE_EVSR MOD[mm] MOD!=3 REG[0b101] RM[0b100] KNC_VMODRM() UPCONVERT_FLT32_LOAD() | MEM0:w:zv:TXT=NT REG0=MASK1():rw:mskw
 | VSCATTERPF0DPS       | SCATTER        | AVX512EVEX     | AVX512PF_512   | c6           | mm         | 0b101      | EVV 0xC6 V66 V0F38 MOD[mm] MOD!=3 REG[0b101] RM[nnn] BCRC=0   VL512  W0 RM=4 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:r:d:f32 REG0=MASKNOT0():rw:mskw
 | VSCATTERPF0HINTDPD   | PREFETCH       | KNCE           | KNC_PF_HINT    | c6           | mm         | 0b100      | KVV 0xC6 V0F38 V66  REXW=1 NO_SPARSE_EVSR MOD[mm] MOD!=3 REG[0b100] RM[0b100] KNC_VMODRM() UPCONVERT_FLT32_LOAD() | MEM0:w:zv:TXT=NT REG0=MASK1():rw:mskw
 | VSCATTERPF0HINTDPS   | PREFETCH       | KNCE           | KNC_PF_HINT    | c6           | mm         | 0b100      | KVV 0xC6 V0F38 V66  REXW=0 NO_SPARSE_EVSR MOD[mm] MOD!=3 REG[0b100] RM[0b100] KNC_VMODRM() UPCONVERT_FLT32_LOAD() | MEM0:w:zv:TXT=NT REG0=MASK1():rw:mskw
 | VSCATTERPF0QPD       | SCATTER        | AVX512EVEX     | AVX512PF_512   | c7           | mm         | 0b101      | EVV 0xC7 V66 V0F38 MOD[mm] MOD!=3 REG[0b101] RM[nnn] BCRC=0   VL512  W1 RM=4 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:r:q:f64 REG0=MASKNOT0():rw:mskw
 | VSCATTERPF0QPS       | SCATTER        | AVX512EVEX     | AVX512PF_512   | c7           | mm         | 0b101      | EVV 0xC7 V66 V0F38 MOD[mm] MOD!=3 REG[0b101] RM[nnn] BCRC=0   VL512  W0 RM=4 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:r:d:f32 REG0=MASKNOT0():rw:mskw
 | VSCATTERPF1DPD       | SCATTER        | AVX512EVEX     | AVX512PF_512   | c6           | mm         | 0b110      | EVV 0xC6 V66 V0F38 MOD[mm] MOD!=3 REG[0b110] RM[nnn] BCRC=0   VL512  W1 RM=4 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:r:q:f64 REG0=MASKNOT0():rw:mskw
 | VSCATTERPF1DPS       | PREFETCH       | KNCE           |                | c6           | mm         | 0b110      | KVV 0xC6 V0F38 V66  REXW=0 NO_SPARSE_EVSR MOD[mm] MOD!=3 REG[0b110] RM[0b100] KNC_VMODRM() UPCONVERT_FLT32_LOAD() | MEM0:w:zv:TXT=NT REG0=MASK1():rw:mskw
 | VSCATTERPF1DPS       | SCATTER        | AVX512EVEX     | AVX512PF_512   | c6           | mm         | 0b110      | EVV 0xC6 V66 V0F38 MOD[mm] MOD!=3 REG[0b110] RM[nnn] BCRC=0   VL512  W0 RM=4 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:r:d:f32 REG0=MASKNOT0():rw:mskw
 | VSCATTERPF1QPD       | SCATTER        | AVX512EVEX     | AVX512PF_512   | c7           | mm         | 0b110      | EVV 0xC7 V66 V0F38 MOD[mm] MOD!=3 REG[0b110] RM[nnn] BCRC=0   VL512  W1 RM=4 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:r:q:f64 REG0=MASKNOT0():rw:mskw
 | VSCATTERPF1QPS       | SCATTER        | AVX512EVEX     | AVX512PF_512   | c7           | mm         | 0b110      | EVV 0xC7 V66 V0F38 MOD[mm] MOD!=3 REG[0b110] RM[nnn] BCRC=0   VL512  W0 RM=4 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:r:d:f32 REG0=MASKNOT0():rw:mskw
 | VSCATTERQPD          | SCATTER        | AVX512EVEX     | AVX512F_128    | a3           | mm         | rrr        | EVV 0xA3 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL128  W1 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:w:q:f64 REG0=MASKNOT0():rw:mskw REG1=XMM_R3():r:dq:f64
 | VSCATTERQPD          | SCATTER        | AVX512EVEX     | AVX512F_256    | a3           | mm         | rrr        | EVV 0xA3 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL256  W1 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:w:q:f64 REG0=MASKNOT0():rw:mskw REG1=YMM_R3():r:qq:f64
 | VSCATTERQPD          | SCATTER        | AVX512EVEX     | AVX512F_512    | a3           | mm         | rrr        | EVV 0xA3 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL512  W1 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:w:q:f64 REG0=MASKNOT0():rw:mskw REG1=ZMM_R3():r:zf64
 | VSCATTERQPS          | SCATTER        | AVX512EVEX     | AVX512F_128    | a3           | mm         | rrr        | EVV 0xA3 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL128  W0 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:w:d:f32 REG0=MASKNOT0():rw:mskw REG1=XMM_R3():r:dq:f32
 | VSCATTERQPS          | SCATTER        | AVX512EVEX     | AVX512F_256    | a3           | mm         | rrr        | EVV 0xA3 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL256  W0 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:w:d:f32 REG0=MASKNOT0():rw:mskw REG1=XMM_R3():r:dq:f32
 | VSCATTERQPS          | SCATTER        | AVX512EVEX     | AVX512F_512    | a3           | mm         | rrr        | EVV 0xA3 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL512  W0 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:w:d:f32 REG0=MASKNOT0():rw:mskw REG1=YMM_R3():r:qq:f32
 | VSHUFF32X4           | AVX512         | AVX512EVEX     | AVX512F_256    | 23           | 0b11       | rrr        | EVV 0x23 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0   UIMM8()                      | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32 IMM0:r:b
 | VSHUFF32X4           | AVX512         | AVX512EVEX     | AVX512F_256    | 23           | mm         | rrr        | EVV 0x23 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
 | VSHUFF32X4           | AVX512         | AVX512EVEX     | AVX512F_512    | 23           | 0b11       | rrr        | EVV 0x23 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0   UIMM8()                      | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32 IMM0:r:b
 | VSHUFF32X4           | AVX512         | AVX512EVEX     | AVX512F_512    | 23           | mm         | rrr        | EVV 0x23 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
 | VSHUFF64X2           | AVX512         | AVX512EVEX     | AVX512F_256    | 23           | 0b11       | rrr        | EVV 0x23 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1   UIMM8()                      | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64 IMM0:r:b
 | VSHUFF64X2           | AVX512         | AVX512EVEX     | AVX512F_256    | 23           | mm         | rrr        | EVV 0x23 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
 | VSHUFF64X2           | AVX512         | AVX512EVEX     | AVX512F_512    | 23           | 0b11       | rrr        | EVV 0x23 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1   UIMM8()                      | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64 IMM0:r:b
 | VSHUFF64X2           | AVX512         | AVX512EVEX     | AVX512F_512    | 23           | mm         | rrr        | EVV 0x23 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
 | VSHUFI32X4           | AVX512         | AVX512EVEX     | AVX512F_256    | 43           | 0b11       | rrr        | EVV 0x43 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0   UIMM8()                      | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32 IMM0:r:b
 | VSHUFI32X4           | AVX512         | AVX512EVEX     | AVX512F_256    | 43           | mm         | rrr        | EVV 0x43 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
 | VSHUFI32X4           | AVX512         | AVX512EVEX     | AVX512F_512    | 43           | 0b11       | rrr        | EVV 0x43 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0   UIMM8()                      | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32 IMM0:r:b
 | VSHUFI32X4           | AVX512         | AVX512EVEX     | AVX512F_512    | 43           | mm         | rrr        | EVV 0x43 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
 | VSHUFI64X2           | AVX512         | AVX512EVEX     | AVX512F_256    | 43           | 0b11       | rrr        | EVV 0x43 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1   UIMM8()                      | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64 IMM0:r:b
 | VSHUFI64X2           | AVX512         | AVX512EVEX     | AVX512F_256    | 43           | mm         | rrr        | EVV 0x43 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VSHUFI64X2           | AVX512         | AVX512EVEX     | AVX512F_512    | 43           | 0b11       | rrr        | EVV 0x43 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1   UIMM8()                      | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64 IMM0:r:b
 | VSHUFI64X2           | AVX512         | AVX512EVEX     | AVX512F_512    | 43           | mm         | rrr        | EVV 0x43 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
 | VSHUFPD              | AVX            | AVX            |                | c6           | mm         | rrr        | VV1 0xC6  VL128 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                              | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64 IMM0:r:b
 | VSHUFPD              | AVX            | AVX            |                | c6           | 0b11       | rrr        | VV1 0xC6  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                     | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64 IMM0:r:b
 | VSHUFPD              | AVX            | AVX            |                | c6           | mm         | rrr        | VV1 0xC6  VL256 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                              | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64 IMM0:r:b
 | VSHUFPD              | AVX            | AVX            |                | c6           | 0b11       | rrr        | VV1 0xC6  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                     | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64 IMM0:r:b
 | VSHUFPD              | AVX512         | AVX512EVEX     | AVX512F_128    | c6           | 0b11       | rrr        | EVV 0xC6 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1   UIMM8()                        | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64 IMM0:r:b
 | VSHUFPD              | AVX512         | AVX512EVEX     | AVX512F_128    | c6           | mm         | rrr        | EVV 0xC6 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
 | VSHUFPD              | AVX512         | AVX512EVEX     | AVX512F_256    | c6           | 0b11       | rrr        | EVV 0xC6 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1   UIMM8()                        | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64 IMM0:r:b
 | VSHUFPD              | AVX512         | AVX512EVEX     | AVX512F_256    | c6           | mm         | rrr        | EVV 0xC6 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
 | VSHUFPD              | AVX512         | AVX512EVEX     | AVX512F_512    | c6           | 0b11       | rrr        | EVV 0xC6 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1   UIMM8()                        | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64 IMM0:r:b
 | VSHUFPD              | AVX512         | AVX512EVEX     | AVX512F_512    | c6           | mm         | rrr        | EVV 0xC6 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
 | VSHUFPS              | AVX            | AVX            |                | c6           | mm         | rrr        | VV1 0xC6  VL128 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                              | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32 IMM0:r:b
 | VSHUFPS              | AVX            | AVX            |                | c6           | 0b11       | rrr        | VV1 0xC6  VL128 VNP V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                     | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32 IMM0:r:b
 | VSHUFPS              | AVX            | AVX            |                | c6           | mm         | rrr        | VV1 0xC6  VL256 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                              | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32 IMM0:r:b
 | VSHUFPS              | AVX            | AVX            |                | c6           | 0b11       | rrr        | VV1 0xC6  VL256 VNP V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                     | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32 IMM0:r:b
 | VSHUFPS              | AVX512         | AVX512EVEX     | AVX512F_128    | c6           | 0b11       | rrr        | EVV 0xC6 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0   UIMM8()                        | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32 IMM0:r:b
 | VSHUFPS              | AVX512         | AVX512EVEX     | AVX512F_128    | c6           | mm         | rrr        | EVV 0xC6 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
 | VSHUFPS              | AVX512         | AVX512EVEX     | AVX512F_256    | c6           | 0b11       | rrr        | EVV 0xC6 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0   UIMM8()                        | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32 IMM0:r:b
 | VSHUFPS              | AVX512         | AVX512EVEX     | AVX512F_256    | c6           | mm         | rrr        | EVV 0xC6 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
 | VSHUFPS              | AVX512         | AVX512EVEX     | AVX512F_512    | c6           | 0b11       | rrr        | EVV 0xC6 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0   UIMM8()                        | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32 IMM0:r:b
 | VSHUFPS              | AVX512         | AVX512EVEX     | AVX512F_512    | c6           | mm         | rrr        | EVV 0xC6 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
 | VSQRTPD              | AVX            | AVX            |                | 51           | mm         | rrr        | VV1 0x51  VL128 V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:f64 MEM0:r:dq:f64
 | VSQRTPD              | AVX            | AVX            |                | 51           | 0b11       | rrr        | VV1 0x51  VL128 V66 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:f64  REG1=XMM_B():r:dq:f64
 | VSQRTPD              | AVX            | AVX            |                | 51           | mm         | rrr        | VV1 0x51  VL256 V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=YMM_R():w:qq:f64  MEM0:r:qq:f64
 | VSQRTPD              | AVX            | AVX            |                | 51           | 0b11       | rrr        | VV1 0x51  VL256 V66 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=YMM_R():w:qq:f64  REG1=YMM_B():r:qq:f64
 | VSQRTPD              | AVX512         | AVX512EVEX     | AVX512F_128    | 51           | 0b11       | rrr        | EVV 0x51 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR                          | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64
 | VSQRTPD              | AVX512         | AVX512EVEX     | AVX512F_128    | 51           | mm         | rrr        | EVV 0x51 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VSQRTPD              | AVX512         | AVX512EVEX     | AVX512F_256    | 51           | 0b11       | rrr        | EVV 0x51 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR                          | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64
 | VSQRTPD              | AVX512         | AVX512EVEX     | AVX512F_256    | 51           | mm         | rrr        | EVV 0x51 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VSQRTPD              | AVX512         | AVX512EVEX     | AVX512F_512    | 51           | 0b11       | rrr        | EVV 0x51 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR                          | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
 | VSQRTPD              | AVX512         | AVX512EVEX     | AVX512F_512    | 51           | 0b11       | rrr        | EVV 0x51 V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1  NOEVSR | REG0=ZMM_R3():w:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
 | VSQRTPD              | AVX512         | AVX512EVEX     | AVX512F_512    | 51           | mm         | rrr        | EVV 0x51 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
 | VSQRTPS              | AVX            | AVX            |                | 51           | mm         | rrr        | VV1 0x51  VL128 VNP NOVSR V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=XMM_R():w:dq:f32  MEM0:r:dq:f32
 | VSQRTPS              | AVX            | AVX            |                | 51           | 0b11       | rrr        | VV1 0x51  VL128 VNP NOVSR V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=XMM_R():w:dq:f32  REG1=XMM_B():r:dq:f32
 | VSQRTPS              | AVX            | AVX            |                | 51           | mm         | rrr        | VV1 0x51  VL256 VNP NOVSR V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=YMM_R():w:qq:f32  MEM0:r:qq:f32
 | VSQRTPS              | AVX            | AVX            |                | 51           | 0b11       | rrr        | VV1 0x51  VL256 VNP NOVSR V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=YMM_R():w:qq:f32  REG1=YMM_B():r:qq:f32
 | VSQRTPS              | AVX512         | AVX512EVEX     | AVX512F_128    | 51           | 0b11       | rrr        | EVV 0x51 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR                          | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
 | VSQRTPS              | AVX512         | AVX512EVEX     | AVX512F_128    | 51           | mm         | rrr        | EVV 0x51 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VSQRTPS              | AVX512         | AVX512EVEX     | AVX512F_256    | 51           | 0b11       | rrr        | EVV 0x51 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR                          | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
 | VSQRTPS              | AVX512         | AVX512EVEX     | AVX512F_256    | 51           | mm         | rrr        | EVV 0x51 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VSQRTPS              | AVX512         | AVX512EVEX     | AVX512F_512    | 51           | 0b11       | rrr        | EVV 0x51 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR                          | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
 | VSQRTPS              | AVX512         | AVX512EVEX     | AVX512F_512    | 51           | 0b11       | rrr        | EVV 0x51 VNP V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0  NOEVSR | REG0=ZMM_R3():w:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
 | VSQRTPS              | AVX512         | AVX512EVEX     | AVX512F_512    | 51           | mm         | rrr        | EVV 0x51 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
 | VSQRTSD              | AVX            | AVX            |                | 51           | mm         | rrr        | VV1 0x51  VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                            | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:q:f64
 | VSQRTSD              | AVX            | AVX            |                | 51           | 0b11       | rrr        | VV1 0x51  VF2 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                   | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:q:f64
 | VSQRTSD              | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 51           | 0b11       | rrr        | EVV 0x51 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                                         | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VSQRTSD              | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 51           | 0b11       | rrr        | EVV 0x51 VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1       | REG0=XMM_R3():w:dq:f64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VSQRTSD              | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 51           | mm         | rrr        | EVV 0x51 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
 | VSQRTSS              | AVX            | AVX            |                | 51           | mm         | rrr        | VV1 0x51  VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                            | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:d:f32
 | VSQRTSS              | AVX            | AVX            |                | 51           | 0b11       | rrr        | VV1 0x51  VF3 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                   | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:d:f32
 | VSQRTSS              | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 51           | 0b11       | rrr        | EVV 0x51 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                                         | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VSQRTSS              | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 51           | 0b11       | rrr        | EVV 0x51 VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W0       | REG0=XMM_R3():w:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VSQRTSS              | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 51           | mm         | rrr        | EVV 0x51 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
 | VSTMXCSR             | AVX            | AVX            |                | ae           | mm         | 0b011      | VV1 0xAE VL128 VNP V0F NOVSR MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM()                               | MEM0:w:d REG0=XED_REG_MXCSR:r:SUPP
 | VSUBPD               | AVX            | AVX            |                | 5c           | mm         | rrr        | VV1 0x5C  V66 V0F VL128 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
 | VSUBPD               | AVX            | AVX            |                | 5c           | 0b11       | rrr        | VV1 0x5C  V66 V0F VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
 | VSUBPD               | AVX            | AVX            |                | 5c           | mm         | rrr        | VV1 0x5C  V66 V0F VL256 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
 | VSUBPD               | AVX            | AVX            |                | 5c           | 0b11       | rrr        | VV1 0x5C  V66 V0F VL256 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
 | VSUBPD               | AVX512         | AVX512EVEX     | AVX512F_128    | 5c           | 0b11       | rrr        | EVV 0x5C V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                  | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VSUBPD               | AVX512         | AVX512EVEX     | AVX512F_128    | 5c           | mm         | rrr        | EVV 0x5C V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VSUBPD               | AVX512         | AVX512EVEX     | AVX512F_256    | 5c           | 0b11       | rrr        | EVV 0x5C V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                  | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
 | VSUBPD               | AVX512         | AVX512EVEX     | AVX512F_256    | 5c           | mm         | rrr        | EVV 0x5C V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VSUBPD               | AVX512         | AVX512EVEX     | AVX512F_512    | 5c           | 0b11       | rrr        | EVV 0x5C V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                  | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VSUBPD               | AVX512         | AVX512EVEX     | AVX512F_512    | 5c           | 0b11       | rrr        | EVV 0x5C V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1       | REG0=ZMM_R3():w:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VSUBPD               | AVX512         | AVX512EVEX     | AVX512F_512    | 5c           | mm         | rrr        | EVV 0x5C V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VSUBPD               | KNC            | KNCE           |                | 5c           | mm         | rrr        | KVV 0x5C V0F V66  REXW=1 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT64()                  | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VSUBPD               | KNC            | KNCE           |                | 5c           | 0b11       | rrr        | KVV 0x5C V0F V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE64()                       | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=REGSWIZ
 | VSUBPD               | KNC            | KNCE           |                | 5c           | 0b11       | rrr        | KVV 0x5C V0F V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                     | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=ROUNDC:TXT=SAEC
 | VSUBPS               | AVX            | AVX            |                | 5c           | mm         | rrr        | VV1 0x5C  VL128 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
 | VSUBPS               | AVX            | AVX            |                | 5c           | 0b11       | rrr        | VV1 0x5C  VL128 VNP V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
 | VSUBPS               | AVX            | AVX            |                | 5c           | mm         | rrr        | VV1 0x5C  VL256 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
 | VSUBPS               | AVX            | AVX            |                | 5c           | 0b11       | rrr        | VV1 0x5C  VL256 VNP V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
 | VSUBPS               | AVX512         | AVX512EVEX     | AVX512F_128    | 5c           | 0b11       | rrr        | EVV 0x5C VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                  | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VSUBPS               | AVX512         | AVX512EVEX     | AVX512F_128    | 5c           | mm         | rrr        | EVV 0x5C VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VSUBPS               | AVX512         | AVX512EVEX     | AVX512F_256    | 5c           | 0b11       | rrr        | EVV 0x5C VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                  | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
 | VSUBPS               | AVX512         | AVX512EVEX     | AVX512F_256    | 5c           | mm         | rrr        | EVV 0x5C VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VSUBPS               | AVX512         | AVX512EVEX     | AVX512F_512    | 5c           | 0b11       | rrr        | EVV 0x5C VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                  | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VSUBPS               | AVX512         | AVX512EVEX     | AVX512F_512    | 5c           | 0b11       | rrr        | EVV 0x5C VNP V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0       | REG0=ZMM_R3():w:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VSUBPS               | AVX512         | AVX512EVEX     | AVX512F_512    | 5c           | mm         | rrr        | EVV 0x5C VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VSUBPS               | KNC            | KNCE           |                | 5c           | mm         | rrr        | KVV 0x5C V0F VNP  REXW=0 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT32()                  | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VSUBPS               | KNC            | KNCE           |                | 5c           | 0b11       | rrr        | KVV 0x5C V0F VNP  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                       | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VSUBPS               | KNC            | KNCE           |                | 5c           | 0b11       | rrr        | KVV 0x5C V0F VNP  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=ROUNDC:TXT=SAEC
 | VSUBRPD              | KNC            | KNCE           |                | 6d           | mm         | rrr        | KVV 0x6D V0F38 V66  REXW=1 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT64()                | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VSUBRPD              | KNC            | KNCE           |                | 6d           | 0b11       | rrr        | KVV 0x6D V0F38 V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE64()                     | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=REGSWIZ
 | VSUBRPD              | KNC            | KNCE           |                | 6d           | 0b11       | rrr        | KVV 0x6D V0F38 V66  REXW=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                   | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64:TXT=ROUNDC:TXT=SAEC
 | VSUBRPS              | KNC            | KNCE           |                | 6d           | mm         | rrr        | KVV 0x6D V0F38 V66  REXW=0 MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UPCONVERT_FLT32()                | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:zv:TXT=CONVERT:TXT=NT
 | VSUBRPS              | KNC            | KNCE           |                | 6d           | 0b11       | rrr        | KVV 0x6D V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=0 REG_SWIZZLE32()                     | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=REGSWIZ
 | VSUBRPS              | KNC            | KNCE           |                | 6d           | 0b11       | rrr        | KVV 0x6D V0F38 V66  REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] NR=1 ROUND() KNC_SAE()                   | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32:TXT=ROUNDC:TXT=SAEC
 | VSUBSD               | AVX            | AVX            |                | 5c           | mm         | rrr        | VV1 0x5C  VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                            | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:q:f64
 | VSUBSD               | AVX            | AVX            |                | 5c           | 0b11       | rrr        | VV1 0x5C  VF2 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                   | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:q:f64
 | VSUBSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 5c           | 0b11       | rrr        | EVV 0x5C VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                                         | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VSUBSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 5c           | 0b11       | rrr        | EVV 0x5C VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1       | REG0=XMM_R3():w:dq:f64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VSUBSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 5c           | mm         | rrr        | EVV 0x5C VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
 | VSUBSS               | AVX            | AVX            |                | 5c           | mm         | rrr        | VV1 0x5C  VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                            | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:d:f32
 | VSUBSS               | AVX            | AVX            |                | 5c           | 0b11       | rrr        | VV1 0x5C  VF3 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                   | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:d:f32
 | VSUBSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 5c           | 0b11       | rrr        | EVV 0x5C VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                                         | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VSUBSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 5c           | 0b11       | rrr        | EVV 0x5C VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W0       | REG0=XMM_R3():w:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VSUBSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 5c           | mm         | rrr        | EVV 0x5C VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
 | VTESTPD              | LOGICAL_FP     | AVX            |                | 0f           | mm         | rrr        | VV1 0x0F  VL128 V66 V0F38 norexw_prefix NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                | REG0=XMM_R():r:dq:f64 MEM0:r:dq:f64
 | VTESTPD              | LOGICAL_FP     | AVX            |                | 0f           | 0b11       | rrr        | VV1 0x0F VL128 V66 V0F38 norexw_prefix NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                        | REG0=XMM_R():r:dq:f64 REG1=XMM_B():r:dq:f64
 | VTESTPD              | LOGICAL_FP     | AVX            |                | 0f           | mm         | rrr        | VV1 0x0F VL256 V66 V0F38  norexw_prefix NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                | REG0=YMM_R():r:qq:f64 MEM0:r:qq:f64
 | VTESTPD              | LOGICAL_FP     | AVX            |                | 0f           | 0b11       | rrr        | VV1 0x0F VL256 V66 V0F38 norexw_prefix NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                        | REG0=YMM_R():r:qq:f64 REG1=YMM_B():r:qq:f64
 | VTESTPS              | LOGICAL_FP     | AVX            |                | 0e           | mm         | rrr        | VV1 0x0E VL128 V66 V0F38 norexw_prefix  NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                | REG0=XMM_R():r:dq:f32 MEM0:r:dq:f32
 | VTESTPS              | LOGICAL_FP     | AVX            |                | 0e           | 0b11       | rrr        | VV1 0x0E  VL128 V66 V0F38 norexw_prefix NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                       | REG0=XMM_R():r:dq:f32 REG1=XMM_B():r:dq:f32
 | VTESTPS              | LOGICAL_FP     | AVX            |                | 0e           | mm         | rrr        | VV1 0x0E VL256 V66 V0F38  norexw_prefix NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                | REG0=YMM_R():r:qq:f32 MEM0:r:qq:f32
 | VTESTPS              | LOGICAL_FP     | AVX            |                | 0e           | 0b11       | rrr        | VV1 0x0E VL256 V66 V0F38 norexw_prefix NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                        | REG0=YMM_R():r:qq:f32 REG1=YMM_B():r:qq:f32
 | VUCOMISD             | AVX            | AVX            |                | 2e           | mm         | rrr        | VV1 0x2E V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                       | REG0=XMM_R():r:dq:f64  MEM0:r:q:f64
 | VUCOMISD             | AVX            | AVX            |                | 2e           | 0b11       | rrr        | VV1 0x2E V66 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                              | REG0=XMM_R():r:dq:f64  REG1=XMM_B():r:q:f64
 | VUCOMISD             | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 2e           | 0b11       | rrr        | EVV 0x2E V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1  NOEVSR  ZEROING=0 MASK=0  FIX_ROUND_LEN128() | REG0=XMM_R3():r:dq:f64 REG1=XMM_B3():r:dq:f64
 | VUCOMISD             | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 2e           | 0b11       | rrr        | EVV 0x2E V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W1  NOEVSR  ZEROING=0 MASK=0 | REG0=XMM_R3():r:dq:f64:TXT=SAESTR REG1=XMM_B3():r:dq:f64
 | VUCOMISD             | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 2e           | mm         | rrr        | EVV 0x2E V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_SCALAR() FIX_ROUND_LEN128() | REG0=XMM_R3():r:dq:f64 MEM0:r:q:f64
 | VUCOMISS             | AVX            | AVX            |                | 2e           | mm         | rrr        | VV1 0x2E VNP V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                       | REG0=XMM_R():r:dq:f32  MEM0:r:d:f32
 | VUCOMISS             | AVX            | AVX            |                | 2e           | 0b11       | rrr        | VV1 0x2E VNP V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                              | REG0=XMM_R():r:dq:f32  REG1=XMM_B():r:d:f32
 | VUCOMISS             | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 2e           | 0b11       | rrr        | EVV 0x2E VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0  NOEVSR  ZEROING=0 MASK=0  FIX_ROUND_LEN128() | REG0=XMM_R3():r:dq:f32 REG1=XMM_B3():r:dq:f32
 | VUCOMISS             | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 2e           | 0b11       | rrr        | EVV 0x2E VNP V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W0  NOEVSR  ZEROING=0 MASK=0 | REG0=XMM_R3():r:dq:f32:TXT=SAESTR REG1=XMM_B3():r:dq:f32
 | VUCOMISS             | AVX512         | AVX512EVEX     | AVX512F_SCALAR | 2e           | mm         | rrr        | EVV 0x2E VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_SCALAR() FIX_ROUND_LEN128() | REG0=XMM_R3():r:dq:f32 MEM0:r:d:f32
 | VUNPCKHPD            | AVX            | AVX            |                | 15           | mm         | rrr        | VV1 0x15  VL128 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
 | VUNPCKHPD            | AVX            | AVX            |                | 15           | 0b11       | rrr        | VV1 0x15  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
 | VUNPCKHPD            | AVX            | AVX            |                | 15           | mm         | rrr        | VV1 0x15  VL256 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
 | VUNPCKHPD            | AVX            | AVX            |                | 15           | 0b11       | rrr        | VV1 0x15  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
 | VUNPCKHPD            | AVX512         | AVX512EVEX     | AVX512F_128    | 15           | 0b11       | rrr        | EVV 0x15 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                  | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VUNPCKHPD            | AVX512         | AVX512EVEX     | AVX512F_128    | 15           | mm         | rrr        | EVV 0x15 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VUNPCKHPD            | AVX512         | AVX512EVEX     | AVX512F_256    | 15           | 0b11       | rrr        | EVV 0x15 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                  | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
 | VUNPCKHPD            | AVX512         | AVX512EVEX     | AVX512F_256    | 15           | mm         | rrr        | EVV 0x15 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VUNPCKHPD            | AVX512         | AVX512EVEX     | AVX512F_512    | 15           | 0b11       | rrr        | EVV 0x15 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                  | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VUNPCKHPD            | AVX512         | AVX512EVEX     | AVX512F_512    | 15           | mm         | rrr        | EVV 0x15 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VUNPCKHPS            | AVX            | AVX            |                | 15           | mm         | rrr        | VV1 0x15  VL128 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
 | VUNPCKHPS            | AVX            | AVX            |                | 15           | 0b11       | rrr        | VV1 0x15  VL128 VNP V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
 | VUNPCKHPS            | AVX            | AVX            |                | 15           | mm         | rrr        | VV1 0x15  VL256 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
 | VUNPCKHPS            | AVX            | AVX            |                | 15           | 0b11       | rrr        | VV1 0x15  VL256 VNP V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
 | VUNPCKHPS            | AVX512         | AVX512EVEX     | AVX512F_128    | 15           | 0b11       | rrr        | EVV 0x15 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                  | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VUNPCKHPS            | AVX512         | AVX512EVEX     | AVX512F_128    | 15           | mm         | rrr        | EVV 0x15 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VUNPCKHPS            | AVX512         | AVX512EVEX     | AVX512F_256    | 15           | 0b11       | rrr        | EVV 0x15 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                  | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
 | VUNPCKHPS            | AVX512         | AVX512EVEX     | AVX512F_256    | 15           | mm         | rrr        | EVV 0x15 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VUNPCKHPS            | AVX512         | AVX512EVEX     | AVX512F_512    | 15           | 0b11       | rrr        | EVV 0x15 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                  | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VUNPCKHPS            | AVX512         | AVX512EVEX     | AVX512F_512    | 15           | mm         | rrr        | EVV 0x15 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VUNPCKLPD            | AVX            | AVX            |                | 14           | mm         | rrr        | VV1 0x14  VL128 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
 | VUNPCKLPD            | AVX            | AVX            |                | 14           | 0b11       | rrr        | VV1 0x14  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
 | VUNPCKLPD            | AVX            | AVX            |                | 14           | mm         | rrr        | VV1 0x14  VL256 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
 | VUNPCKLPD            | AVX            | AVX            |                | 14           | 0b11       | rrr        | VV1 0x14  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
 | VUNPCKLPD            | AVX512         | AVX512EVEX     | AVX512F_128    | 14           | 0b11       | rrr        | EVV 0x14 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                  | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
 | VUNPCKLPD            | AVX512         | AVX512EVEX     | AVX512F_128    | 14           | mm         | rrr        | EVV 0x14 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VUNPCKLPD            | AVX512         | AVX512EVEX     | AVX512F_256    | 14           | 0b11       | rrr        | EVV 0x14 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                  | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
 | VUNPCKLPD            | AVX512         | AVX512EVEX     | AVX512F_256    | 14           | mm         | rrr        | EVV 0x14 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VUNPCKLPD            | AVX512         | AVX512EVEX     | AVX512F_512    | 14           | 0b11       | rrr        | EVV 0x14 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                  | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
 | VUNPCKLPD            | AVX512         | AVX512EVEX     | AVX512F_512    | 14           | mm         | rrr        | EVV 0x14 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
 | VUNPCKLPS            | AVX            | AVX            |                | 14           | mm         | rrr        | VV1 0x14  VL128 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
 | VUNPCKLPS            | AVX            | AVX            |                | 14           | 0b11       | rrr        | VV1 0x14  VL128 VNP V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
 | VUNPCKLPS            | AVX            | AVX            |                | 14           | mm         | rrr        | VV1 0x14  VL256 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
 | VUNPCKLPS            | AVX            | AVX            |                | 14           | 0b11       | rrr        | VV1 0x14  VL256 VNP V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
 | VUNPCKLPS            | AVX512         | AVX512EVEX     | AVX512F_128    | 14           | 0b11       | rrr        | EVV 0x14 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                  | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
 | VUNPCKLPS            | AVX512         | AVX512EVEX     | AVX512F_128    | 14           | mm         | rrr        | EVV 0x14 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VUNPCKLPS            | AVX512         | AVX512EVEX     | AVX512F_256    | 14           | 0b11       | rrr        | EVV 0x14 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                  | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
 | VUNPCKLPS            | AVX512         | AVX512EVEX     | AVX512F_256    | 14           | mm         | rrr        | EVV 0x14 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VUNPCKLPS            | AVX512         | AVX512EVEX     | AVX512F_512    | 14           | 0b11       | rrr        | EVV 0x14 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                  | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
 | VUNPCKLPS            | AVX512         | AVX512EVEX     | AVX512F_512    | 14           | mm         | rrr        | EVV 0x14 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
 | VXORPD               | LOGICAL_FP     | AVX            |                | 57           | mm         | rrr        | VV1 0x57  V66 V0F VL128 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 MEM0:r:dq:u64
 | VXORPD               | LOGICAL_FP     | AVX            |                | 57           | 0b11       | rrr        | VV1 0x57  V66 V0F VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 REG2=XMM_B():r:dq:u64
 | VXORPD               | LOGICAL_FP     | AVX            |                | 57           | mm         | rrr        | VV1 0x57  V66 V0F VL256 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=YMM_R():w:qq:u64 REG1=YMM_N():r:qq:u64 MEM0:r:qq:u64
 | VXORPD               | LOGICAL_FP     | AVX            |                | 57           | 0b11       | rrr        | VV1 0x57  V66 V0F VL256 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq:u64 REG1=YMM_N():r:qq:u64 REG2=YMM_B():r:qq:u64
 | VXORPD               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_128   | 57           | 0b11       | rrr        | EVV 0x57 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1                                  | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
 | VXORPD               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_128   | 57           | mm         | rrr        | EVV 0x57 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VXORPD               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_256   | 57           | 0b11       | rrr        | EVV 0x57 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1                                  | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
 | VXORPD               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_256   | 57           | mm         | rrr        | EVV 0x57 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VXORPD               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_512   | 57           | 0b11       | rrr        | EVV 0x57 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1                                  | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
 | VXORPD               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_512   | 57           | mm         | rrr        | EVV 0x57 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
 | VXORPS               | LOGICAL_FP     | AVX            |                | 57           | mm         | rrr        | VV1 0x57  VNP V0F VL128 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():w:dq REG1=XMM_N():r:dq MEM0:r:dq
 | VXORPS               | LOGICAL_FP     | AVX            |                | 57           | 0b11       | rrr        | VV1 0x57  VNP V0F VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():w:dq REG1=XMM_N():r:dq REG2=XMM_B():r:dq
 | VXORPS               | LOGICAL_FP     | AVX            |                | 57           | mm         | rrr        | VV1 0x57  VNP V0F VL256 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=YMM_R():w:qq REG1=YMM_N():r:qq MEM0:r:qq
 | VXORPS               | LOGICAL_FP     | AVX            |                | 57           | 0b11       | rrr        | VV1 0x57  VNP V0F VL256 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=YMM_R():w:qq REG1=YMM_N():r:qq REG2=YMM_B():r:qq
 | VXORPS               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_128   | 57           | 0b11       | rrr        | EVV 0x57 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0                                  | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
 | VXORPS               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_128   | 57           | mm         | rrr        | EVV 0x57 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VXORPS               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_256   | 57           | 0b11       | rrr        | EVV 0x57 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0                                  | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
 | VXORPS               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_256   | 57           | mm         | rrr        | EVV 0x57 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VXORPS               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_512   | 57           | 0b11       | rrr        | EVV 0x57 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0                                  | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
 | VXORPS               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_512   | 57           | mm         | rrr        | EVV 0x57 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
 | VZEROALL             | AVX            | AVX            |                | 77           |            |            | VV1 0x77 VNP  V0F VL256  NOVSR                                                                       | 
 | VZEROUPPER           | AVX            | AVX            |                | 77           |            |            | VV1 0x77 VNP  V0F VL128 NOVSR                                                                        | 
 | WBINVD               | SYSTEM         | BASE           | I486REAL       | 0f 09        |            |            | 0x0F 0x09                                                                                            | 
 | WBINVD               | SYSTEM         | BASE           | I486REAL       | 0f 09        |            |            | 0x0F 0x09 WBNOINVD=0                                                                                 | 
 | WBINVD               | SYSTEM         | BASE           | I486REAL       | 0f 09        |            |            | 0x0F 0x09 WBNOINVD=1 REP!=3                                                                          | 
 | WBNOINVD             | SYSTEM         | WBNOINVD       | WBNOINVD       | 0f 09        |            |            | 0x0F 0x09 WBNOINVD=1 f3_refining_prefix                                                              | 
 | WRFSBASE             | RDWRFSGS       | RDWRFSGS       |                | 0f ae        | 0b11       | 0b010      | 0x0F 0xAE MOD[0b11] MOD=3 REG[0b010] RM[nnn] mode64 f3_refining_prefix                               | REG0=GPRy_B():r   REG1=XED_REG_FSBASE:w:SUPP:y
 | WRGSBASE             | RDWRFSGS       | RDWRFSGS       |                | 0f ae        | 0b11       | 0b011      | 0x0F 0xAE MOD[0b11] MOD=3 REG[0b011] RM[nnn] mode64 f3_refining_prefix                               | REG0=GPRy_B():r   REG1=XED_REG_GSBASE:w:SUPP:y
 | WRMSR                | SYSTEM         | BASE           | PENTIUMREAL    | 0f 30        |            |            | 0x0F 0x30                                                                                            | REG0=XED_REG_EAX:r:SUPP REG1=XED_REG_EDX:r:SUPP REG2=XED_REG_ECX:r:SUPP REG3=XED_REG_MSRS:w:SUPP
 | WRPKRU               | PKU            | PKU            | PKU            | 0f 01        | 0b11       | 0b101      | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b101] RM[0b111]  no_refining_prefix                                   | REG0=XED_REG_EDX:r:SUPP REG1=XED_REG_EAX:r:SUPP REG2=XED_REG_ECX:r:SUPP
 | WRSSD                | CET            | CET            | CET            | 0f 38 f6     | mm         | rrr        | 0x0F 0x38 0xF6 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  no_refining_prefix    W0                    | MEM0:w:d:u32 REG0=GPR32_R():r:d:u32
 | WRSSQ                | CET            | CET            | CET            | 0f 38 f6     | mm         | rrr        | 0x0F 0x38 0xF6 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  no_refining_prefix    W1  mode64            | MEM0:w:q:u64 REG0=GPR64_R():r:q:u64
 | WRUSSD               | CET            | CET            | CET            | 0f 38 f5     | mm         | rrr        | 0x0F 0x38 0xF5 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  osz_refining_prefix    W0                   | MEM0:w:d:u32 REG0=GPR32_R():r:d:u32
 | WRUSSQ               | CET            | CET            | CET            | 0f 38 f5     | mm         | rrr        | 0x0F 0x38 0xF5 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  osz_refining_prefix    W1  mode64           | MEM0:w:q:u64 REG0=GPR64_R():r:q:u64
 | XABORT               | UNCOND_BR      | RTM            |                | c6           | 0b11       | 0b111      | 0xC6 MOD[0b11] MOD=3 REG[0b111] RM[0b000]  UIMM8()                                                   | REG0=XED_REG_EAX:rcw:SUPP IMM0:r:b
 | XADD                 | SEMAPHORE      | BASE           | I486REAL       | 0f c0        | mm         | rrr        | 0x0F 0xC0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                                      | MEM0:rw:b REG0=GPR8_R():rw
 | XADD                 | SEMAPHORE      | BASE           | I486REAL       | 0f c0        | 0b11       | rrr        | 0x0F 0xC0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPR8_B():rw REG1=GPR8_R():rw
 | XADD                 | SEMAPHORE      | BASE           | I486REAL       | 0f c1        | mm         | rrr        | 0x0F 0xC1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                                      | MEM0:rw:v REG0=GPRv_R():rw
 | XADD                 | SEMAPHORE      | BASE           | I486REAL       | 0f c1        | 0b11       | rrr        | 0x0F 0xC1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                           | REG0=GPRv_B():rw REG1=GPRv_R():rw
 | XADD_LOCK            | SEMAPHORE      | BASE           | I486REAL       | 0f c0        | mm         | rrr        | 0x0F 0xC0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                                        | MEM0:rw:b REG0=GPR8_R():rw
 | XADD_LOCK            | SEMAPHORE      | BASE           | I486REAL       | 0f c1        | mm         | rrr        | 0x0F 0xC1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                                        | MEM0:rw:v REG0=GPRv_R():rw
 | XBEGIN               | COND_BR        | RTM            |                | c7           | 0b11       | 0b111      | 0xC7 MOD[0b11] MOD=3 REG[0b111] RM[0b000] BRDISPz()                                                  | RELBR:r:z REG0=rIP():rw:SUPP REG1=XED_REG_EAX:cw:SUPP
 | XCHG                 | DATAXFER       | BASE           | I86            | 86           | mm         | rrr        | 0x86 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                                             | MEM0:rw:b REG0=GPR8_R():rw
 | XCHG                 | DATAXFER       | BASE           | I86            | 86           | mm         | rrr        | 0x86 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                                           | MEM0:rw:b REG0=GPR8_R():rw
 | XCHG                 | DATAXFER       | BASE           | I86            | 86           | 0b11       | rrr        | 0x86 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPR8_B():rw REG1=GPR8_R():rw
 | XCHG                 | DATAXFER       | BASE           | I86            | 87           | mm         | rrr        | 0x87 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                                             | MEM0:rw:v REG0=GPRv_R():rw
 | XCHG                 | DATAXFER       | BASE           | I86            | 87           | mm         | rrr        | 0x87 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                                           | MEM0:rw:v REG0=GPRv_R():rw
 | XCHG                 | DATAXFER       | BASE           | I86            | 87           | 0b11       | rrr        | 0x87 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPRv_B():rw REG1=GPRv_R():rw
 | XCHG                 | DATAXFER       | BASE           | I86            |              |            |            | 0b1001_0 SRM[rrr] SRM!=0                                                                             | REG0=GPRv_SB():rw REG1=OrAX():rw:IMPL
 | XCHG                 | DATAXFER       | BASE           | I86            |              |            |            | 0b1001_0 SRM[rrr] SRM=0 not_refining_f3 rexb_prefix                                                  | REG0=GPRv_SB():rw REG1=OrAX():rw:IMPL
 | XEND                 | COND_BR        | RTM            |                | 0f 01        | 0b11       | 0b010      | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b010] RM[0b101]  no_refining_prefix                                   | 
 | XGETBV               | XSAVE          | XSAVE          |                | 0f 01        | 0b11       | 0b010      | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b010] RM[0b000] no_refining_prefix                                    | REG0=XED_REG_ECX:r:SUPP REG1=XED_REG_EDX:w:SUPP  REG2=XED_REG_EAX:w:SUPP REG3=XED_REG_XCR0:r:SUPP
 | XLAT                 | MISC           | BASE           | I86            | d7           |            |            | 0xD7 OVERRIDE_SEG0()                                                                                 | MEM0:r:SUPP:b BASE0=ArBX():r:SUPP  INDEX=XED_REG_AL:r:SUPP  REG0=XED_REG_AL:w:SUPP SEG0=FINAL_DSEG():r:SUPP SCALE=1:r:SUPP
 | XOR                  | LOGICAL        | BASE           | I86            | 80           | mm         | 0b110      | 0x80 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM() UIMM8() nolock_prefix                                 | MEM0:rw:b IMM0:r:b
 | XOR                  | LOGICAL        | BASE           | I86            | 80           | 0b11       | 0b110      | 0x80 MOD[0b11] MOD=3 REG[0b110] RM[nnn] UIMM8()                                                      | REG0=GPR8_B():rw IMM0:r:b
 | XOR                  | LOGICAL        | BASE           | I86            | 81           | mm         | 0b110      | 0x81 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM() SIMMz() nolock_prefix                                 | MEM0:rw:v IMM0:r:z
 | XOR                  | LOGICAL        | BASE           | I86            | 81           | 0b11       | 0b110      | 0x81 MOD[0b11] MOD=3 REG[0b110] RM[nnn] SIMMz()                                                      | REG0=GPRv_B():rw IMM0:r:z
 | XOR                  | LOGICAL        | BASE           | I86            | 82           | mm         | 0b110      | 0x82 MOD[mm] MOD!=3 REG[0b110] RM[nnn] not64 MODRM() UIMM8() nolock_prefix                           | MEM0:rw:b IMM0:r:b
 | XOR                  | LOGICAL        | BASE           | I86            | 82           | 0b11       | 0b110      | 0x82 MOD[0b11] MOD=3 REG[0b110] RM[nnn] not64 UIMM8()                                                | REG0=GPR8_B():rw IMM0:r:b
 | XOR                  | LOGICAL        | BASE           | I86            | 83           | mm         | 0b110      | 0x83 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM() SIMM8() nolock_prefix                                 | MEM0:rw:v IMM0:r:b:i8
 | XOR                  | LOGICAL        | BASE           | I86            | 83           | 0b11       | 0b110      | 0x83 MOD[0b11] MOD=3 REG[0b110] RM[nnn] SIMM8()                                                      | REG0=GPRv_B():rw IMM0:r:b:i8
 | XOR                  | LOGICAL        | BASE           | I86            | 30           | mm         | rrr        | 0x30 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                                           | MEM0:rw:b REG0=GPR8_R():r
 | XOR                  | LOGICAL        | BASE           | I86            | 30           | 0b11       | rrr        | 0x30 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPR8_B():rw REG1=GPR8_R():r
 | XOR                  | LOGICAL        | BASE           | I86            | 31           | mm         | rrr        | 0x31 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                                           | MEM0:rw:v REG0=GPRv_R():r
 | XOR                  | LOGICAL        | BASE           | I86            | 31           | 0b11       | rrr        | 0x31 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPRv_B():rw REG1=GPRv_R():r
 | XOR                  | LOGICAL        | BASE           | I86            | 32           | 0b11       | rrr        | 0x32 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPR8_R():rw REG1=GPR8_B():r
 | XOR                  | LOGICAL        | BASE           | I86            | 32           | mm         | rrr        | 0x32 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                         | REG0=GPR8_R():rw MEM0:r:b
 | XOR                  | LOGICAL        | BASE           | I86            | 33           | 0b11       | rrr        | 0x33 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                                | REG0=GPRv_R():rw REG1=GPRv_B():r
 | XOR                  | LOGICAL        | BASE           | I86            | 33           | mm         | rrr        | 0x33 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                                         | REG0=GPRv_R():rw MEM0:r:v
 | XOR                  | LOGICAL        | BASE           | I86            | 34           |            |            | 0x34 UIMM8()                                                                                         | REG0=XED_REG_AL:rw:IMPL IMM0:r:b
 | XOR                  | LOGICAL        | BASE           | I86            | 35           |            |            | 0x35 SIMMz()                                                                                         | REG0=OrAX():rw:IMPL IMM0:r:z
 | XOR_LOCK             | LOGICAL        | BASE           | I86            | 80           | mm         | 0b110      | 0x80 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM() UIMM8() lock_prefix                                   | MEM0:rw:b IMM0:r:b
 | XOR_LOCK             | LOGICAL        | BASE           | I86            | 81           | mm         | 0b110      | 0x81 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM() SIMMz() lock_prefix                                   | MEM0:rw:v IMM0:r:z
 | XOR_LOCK             | LOGICAL        | BASE           | I86            | 82           | mm         | 0b110      | 0x82 MOD[mm] MOD!=3 REG[0b110] RM[nnn] not64 MODRM() UIMM8() lock_prefix                             | MEM0:rw:b IMM0:r:b
 | XOR_LOCK             | LOGICAL        | BASE           | I86            | 83           | mm         | 0b110      | 0x83 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM() SIMM8() lock_prefix                                   | MEM0:rw:v IMM0:r:b:i8
 | XOR_LOCK             | LOGICAL        | BASE           | I86            | 30           | mm         | rrr        | 0x30 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                                             | MEM0:rw:b REG0=GPR8_R():r
 | XOR_LOCK             | LOGICAL        | BASE           | I86            | 31           | mm         | rrr        | 0x31 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                                             | MEM0:rw:v REG0=GPRv_R():r
 | XORPD                | LOGICAL_FP     | SSE2           |                | 0f 57        | mm         | rrr        | 0x0F 0x57 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM()                  | REG0=XMM_R():rw:xuq MEM0:r:xuq
 | XORPD                | LOGICAL_FP     | SSE2           |                | 0f 57        | 0b11       | rrr        | 0x0F 0x57 osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66()                         | REG0=XMM_R():rw:xuq REG1=XMM_B():r:xuq
 | XORPS                | LOGICAL_FP     | SSE            |                | 0f 57        | mm         | rrr        | 0x0F 0x57 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                                | REG0=XMM_R():rw:xud MEM0:r:xud
 | XORPS                | LOGICAL_FP     | SSE            |                | 0f 57        | 0b11       | rrr        | 0x0F 0x57 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                        | REG0=XMM_R():rw:xud REG1=XMM_B():r:xud
 | XRESLDTRK            | TSX_LDTRK      | TSX_LDTRK      | TSX_LDTRK      | 0f 01        | 0b11       | 0b101      | 0x0F 0x01 MOD[0b11] MOD=3  REG[0b101] RM[0b001]  f2_refining_prefix                                  | 
 | XRSTOR               | XSAVE          | XSAVE          |                | 0f ae        | mm         | 0b101      | 0x0F 0xAE MOD[mm]  MOD!=3 REG[0b101] RM[nnn] no_refining_prefix norexw_prefix MODRM()                | MEM0:r:mxsave REG0=XED_REG_EDX:r:SUPP REG1=XED_REG_EAX:r:SUPP REG2=XED_REG_XCR0:r:SUPP
 | XRSTOR64             | XSAVE          | XSAVE          |                | 0f ae        | mm         | 0b101      | 0x0F 0xAE MOD[mm]  MOD!=3 REG[0b101] RM[nnn] no_refining_prefix rexw_prefix MODRM()                  | MEM0:r:mxsave REG0=XED_REG_EDX:r:SUPP REG1=XED_REG_EAX:r:SUPP REG2=XED_REG_XCR0:r:SUPP
 | XRSTORS              | XSAVE          | XSAVES         |                | 0f c7        | mm         | 0b011      | 0x0F 0xC7 MOD[mm]  MOD!=3 REG[0b011] RM[nnn] MODRM() norexw_prefix no_refining_prefix                | MEM0:r:mxsave REG0=XED_REG_EDX:r:SUPP REG1=XED_REG_EAX:r:SUPP REG2=XED_REG_XCR0:r:SUPP
 | XRSTORS64            | XSAVE          | XSAVES         |                | 0f c7        | mm         | 0b011      | 0x0F 0xC7 MOD[mm]  MOD!=3 REG[0b011] RM[nnn] MODRM() rexw_prefix no_refining_prefix                  | MEM0:r:mxsave REG0=XED_REG_EDX:r:SUPP REG1=XED_REG_EAX:r:SUPP REG2=XED_REG_XCR0:r:SUPP
 | XSAVE                | XSAVE          | XSAVE          |                | 0f ae        | mm         | 0b100      | 0x0F 0xAE MOD[mm]  MOD!=3 REG[0b100] RM[nnn] no_refining_prefix norexw_prefix MODRM()                | MEM0:rw:mxsave REG0=XED_REG_EDX:r:SUPP REG1=XED_REG_EAX:r:SUPP REG2=XED_REG_XCR0:r:SUPP
 | XSAVE64              | XSAVE          | XSAVE          |                | 0f ae        | mm         | 0b100      | 0x0F 0xAE MOD[mm]  MOD!=3 REG[0b100] RM[nnn] no_refining_prefix rexw_prefix MODRM()                  | MEM0:rw:mxsave REG0=XED_REG_EDX:r:SUPP REG1=XED_REG_EAX:r:SUPP REG2=XED_REG_XCR0:r:SUPP
 | XSAVEC               | XSAVE          | XSAVEC         |                | 0f c7        | mm         | 0b100      | 0x0F 0xC7 MOD[mm]  MOD!=3 REG[0b100] RM[nnn] MODRM() norexw_prefix no_refining_prefix                | MEM0:w:mxsave REG0=XED_REG_EDX:r:SUPP REG1=XED_REG_EAX:r:SUPP REG2=XED_REG_XCR0:r:SUPP
 | XSAVEC64             | XSAVE          | XSAVEC         |                | 0f c7        | mm         | 0b100      | 0x0F 0xC7 MOD[mm]  MOD!=3 REG[0b100] RM[nnn] MODRM() rexw_prefix no_refining_prefix                  | MEM0:w:mxsave REG0=XED_REG_EDX:r:SUPP REG1=XED_REG_EAX:r:SUPP REG2=XED_REG_XCR0:r:SUPP
 | XSAVEOPT             | XSAVEOPT       | XSAVEOPT       |                | 0f ae        | mm         | 0b110      | 0x0F 0xAE MOD[mm] MOD!=3 REG[0b110] RM[nnn]  no_refining_prefix norexw_prefix MODRM()                | MEM0:rw:mxsave REG0=XED_REG_EDX:r:SUPP REG1=XED_REG_EAX:r:SUPP REG2=XED_REG_XCR0:r:SUPP
 | XSAVEOPT64           | XSAVEOPT       | XSAVEOPT       |                | 0f ae        | mm         | 0b110      | 0x0F 0xAE MOD[mm] MOD!=3 REG[0b110] RM[nnn] no_refining_prefix rexw_prefix MODRM()                   | MEM0:rw:mxsave REG0=XED_REG_EDX:r:SUPP REG1=XED_REG_EAX:r:SUPP REG2=XED_REG_XCR0:r:SUPP
 | XSAVES               | XSAVE          | XSAVES         |                | 0f c7        | mm         | 0b101      | 0x0F 0xC7 MOD[mm]  MOD!=3 REG[0b101] RM[nnn] MODRM() norexw_prefix no_refining_prefix                | MEM0:w:mxsave REG0=XED_REG_EDX:r:SUPP REG1=XED_REG_EAX:r:SUPP REG2=XED_REG_XCR0:r:SUPP
 | XSAVES64             | XSAVE          | XSAVES         |                | 0f c7        | mm         | 0b101      | 0x0F 0xC7 MOD[mm]  MOD!=3 REG[0b101] RM[nnn] MODRM() rexw_prefix no_refining_prefix                  | MEM0:w:mxsave REG0=XED_REG_EDX:r:SUPP REG1=XED_REG_EAX:r:SUPP REG2=XED_REG_XCR0:r:SUPP
 | XSETBV               | XSAVE          | XSAVE          |                | 0f 01        | 0b11       | 0b010      | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b010] RM[0b001] no_refining_prefix                                    | REG0=XED_REG_ECX:r:SUPP REG1=XED_REG_EDX:r:SUPP  REG2=XED_REG_EAX:r:SUPP REG3=XED_REG_XCR0:w:SUPP
 | XSUSLDTRK            | TSX_LDTRK      | TSX_LDTRK      | TSX_LDTRK      | 0f 01        | 0b11       | 0b101      | 0x0F 0x01 MOD[0b11] MOD=3  REG[0b101] RM[0b000]  f2_refining_prefix                                  | 
 | XTEST                | LOGICAL        | RTM            |                | 0f 01        | 0b11       | 0b010      | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b010] RM[0b110]  no_refining_prefix                                   | 
