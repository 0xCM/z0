| Class               | Category      | Extension     | IsaSet        | BaseCode    | Mod       | Reg       | Pattern                                                                                             | Operands
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
| VADDPD              | AVX           | AVX           |               | 58          | mm        | rrr       | VV1 0x58  V66 VL128 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
| VADDPD              | AVX           | AVX           |               | 58          | 0b11      | rrr       | VV1 0x58  V66 VL128 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
| VADDPD              | AVX           | AVX           |               | 58          | mm        | rrr       | VV1 0x58  V66 VL256 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
| VADDPD              | AVX           | AVX           |               | 58          | 0b11      | rrr       | VV1 0x58  V66 VL256 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
| VADDPS              | AVX           | AVX           |               | 58          | mm        | rrr       | VV1 0x58  VNP VL128 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
| VADDPS              | AVX           | AVX           |               | 58          | 0b11      | rrr       | VV1 0x58  VNP VL128 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
| VADDPS              | AVX           | AVX           |               | 58          | mm        | rrr       | VV1 0x58  VNP VL256 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
| VADDPS              | AVX           | AVX           |               | 58          | 0b11      | rrr       | VV1 0x58  VNP VL256 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
| VADDSD              | AVX           | AVX           |               | 58          | mm        | rrr       | VV1 0x58  VF2  V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                         | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:q:f64
| VADDSD              | AVX           | AVX           |               | 58          | 0b11      | rrr       | VV1 0x58  VF2  V0F  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:q:f64
| VADDSS              | AVX           | AVX           |               | 58          | mm        | rrr       | VV1 0x58  VF3  V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                         | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:d:f32
| VADDSS              | AVX           | AVX           |               | 58          | 0b11      | rrr       | VV1 0x58  VF3  V0F  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:d:f32
| VADDSUBPD           | AVX           | AVX           |               | d0          | mm        | rrr       | VV1 0xD0  VL128 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
| VADDSUBPD           | AVX           | AVX           |               | d0          | 0b11      | rrr       | VV1 0xD0  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
| VADDSUBPD           | AVX           | AVX           |               | d0          | mm        | rrr       | VV1 0xD0  VL256 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
| VADDSUBPD           | AVX           | AVX           |               | d0          | 0b11      | rrr       | VV1 0xD0  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
| VADDSUBPS           | AVX           | AVX           |               | d0          | mm        | rrr       | VV1 0xD0  VL128 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
| VADDSUBPS           | AVX           | AVX           |               | d0          | 0b11      | rrr       | VV1 0xD0  VL128 VF2 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
| VADDSUBPS           | AVX           | AVX           |               | d0          | mm        | rrr       | VV1 0xD0  VL256 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
| VADDSUBPS           | AVX           | AVX           |               | d0          | 0b11      | rrr       | VV1 0xD0  VL256 VF2 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
| VANDNPD             | LOGICAL_FP    | AVX           |               | 55          | mm        | rrr       | VV1 0x55  VL128 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 MEM0:r:dq:u64
| VANDNPD             | LOGICAL_FP    | AVX           |               | 55          | 0b11      | rrr       | VV1 0x55  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 REG2=XMM_B():r:dq:u64
| VANDNPD             | LOGICAL_FP    | AVX           |               | 55          | mm        | rrr       | VV1 0x55  VL256 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u64 REG1=YMM_N():r:qq:u64 MEM0:r:qq:u64
| VANDNPD             | LOGICAL_FP    | AVX           |               | 55          | 0b11      | rrr       | VV1 0x55  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=YMM_R():w:qq:u64 REG1=YMM_N():r:qq:u64 REG2=YMM_B():r:qq:u64
| VANDNPS             | LOGICAL_FP    | AVX           |               | 55          | mm        | rrr       | VV1 0x55  VL128 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq REG1=XMM_N():r:dq MEM0:r:dq
| VANDNPS             | LOGICAL_FP    | AVX           |               | 55          | 0b11      | rrr       | VV1 0x55  VL128 VNP V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq REG1=XMM_N():r:dq REG2=XMM_B():r:dq
| VANDNPS             | LOGICAL_FP    | AVX           |               | 55          | mm        | rrr       | VV1 0x55  VL256 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq REG1=YMM_N():r:qq MEM0:r:qq
| VANDNPS             | LOGICAL_FP    | AVX           |               | 55          | 0b11      | rrr       | VV1 0x55  VL256 VNP V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=YMM_R():w:qq REG1=YMM_N():r:qq REG2=YMM_B():r:qq
| VANDPD              | LOGICAL_FP    | AVX           |               | 54          | mm        | rrr       | VV1 0x54  VL128 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 MEM0:r:dq:u64
| VANDPD              | LOGICAL_FP    | AVX           |               | 54          | 0b11      | rrr       | VV1 0x54  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 REG2=XMM_B():r:dq:u64
| VANDPD              | LOGICAL_FP    | AVX           |               | 54          | mm        | rrr       | VV1 0x54  VL256 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u64 REG1=YMM_N():r:qq:u64 MEM0:r:qq:u64
| VANDPD              | LOGICAL_FP    | AVX           |               | 54          | 0b11      | rrr       | VV1 0x54  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=YMM_R():w:qq:u64 REG1=YMM_N():r:qq:u64 REG2=YMM_B():r:qq:u64
| VANDPS              | LOGICAL_FP    | AVX           |               | 54          | mm        | rrr       | VV1 0x54  VL128 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq REG1=XMM_N():r:dq MEM0:r:dq
| VANDPS              | LOGICAL_FP    | AVX           |               | 54          | 0b11      | rrr       | VV1 0x54  VL128 VNP V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq REG1=XMM_N():r:dq REG2=XMM_B():r:dq
| VANDPS              | LOGICAL_FP    | AVX           |               | 54          | mm        | rrr       | VV1 0x54  VL256 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq REG1=YMM_N():r:qq MEM0:r:qq
| VANDPS              | LOGICAL_FP    | AVX           |               | 54          | 0b11      | rrr       | VV1 0x54  VL256 VNP V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=YMM_R():w:qq REG1=YMM_N():r:qq REG2=YMM_B():r:qq
| VBLENDPD            | AVX           | AVX           |               | 0d          | mm        | rrr       | VV1 0x0D  VL128 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                           | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64 IMM0:r:b
| VBLENDPD            | AVX           | AVX           |               | 0d          | 0b11      | rrr       | VV1 0x0D  VL128 V66 V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                  | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64 IMM0:r:b
| VBLENDPD            | AVX           | AVX           |               | 0d          | mm        | rrr       | VV1 0x0D  VL256 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                           | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64 IMM0:r:b
| VBLENDPD            | AVX           | AVX           |               | 0d          | 0b11      | rrr       | VV1 0x0D  VL256 V66 V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                  | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64 IMM0:r:b
| VBLENDPS            | AVX           | AVX           |               | 0c          | mm        | rrr       | VV1 0x0C  VL128 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                           | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32 IMM0:r:b
| VBLENDPS            | AVX           | AVX           |               | 0c          | 0b11      | rrr       | VV1 0x0C  VL128 V66 V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                  | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32 IMM0:r:b
| VBLENDPS            | AVX           | AVX           |               | 0c          | mm        | rrr       | VV1 0x0C  VL256 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                           | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32 IMM0:r:b
| VBLENDPS            | AVX           | AVX           |               | 0c          | 0b11      | rrr       | VV1 0x0C  VL256 V66 V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                  | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32 IMM0:r:b
| VBLENDVPD           | AVX           | AVX           |               | 4b          | mm        | rrr       | VV1 0x4B   V66 V0F3A VL128 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()          | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64 REG2=XMM_SE():r:dq:u64
| VBLENDVPD           | AVX           | AVX           |               | 4b          | 0b11      | rrr       | VV1 0x4B   V66 V0F3A VL128 norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                 | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64 REG3=XMM_SE():r:dq:u64
| VBLENDVPD           | AVX           | AVX           |               | 4b          | mm        | rrr       | VV1 0x4B   V66 V0F3A VL256 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()          | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64 REG2=YMM_SE():r:qq:u64
| VBLENDVPD           | AVX           | AVX           |               | 4b          | 0b11      | rrr       | VV1 0x4B   V66 V0F3A VL256 norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                 | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64 REG3=YMM_SE():r:qq:u64
| VBLENDVPS           | AVX           | AVX           |               | 4a          | mm        | rrr       | VV1 0x4A   V66 V0F3A VL128 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()          | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32 REG2=XMM_SE():r:dq:u32
| VBLENDVPS           | AVX           | AVX           |               | 4a          | 0b11      | rrr       | VV1 0x4A   V66 V0F3A VL128 norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                 | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32 REG3=XMM_SE():r:dq:u32
| VBLENDVPS           | AVX           | AVX           |               | 4a          | mm        | rrr       | VV1 0x4A   V66 V0F3A VL256 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()          | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32 REG2=YMM_SE():r:qq:u32
| VBLENDVPS           | AVX           | AVX           |               | 4a          | 0b11      | rrr       | VV1 0x4A   V66 V0F3A VL256 norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                 | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32 REG3=YMM_SE():r:qq:u32
| VBROADCASTF128      | BROADCAST     | AVX           |               | 1a          | mm        | rrr       | VV1 0x1A norexw_prefix VL256 V66 V0F38 NOVSR  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()               | REG0=YMM_R():w:qq:f64 MEM0:r:dq:f64 EMX_BROADCAST_2TO4_64
| VBROADCASTSD        | BROADCAST     | AVX           |               | 19          | mm        | rrr       | VV1 0x19  norexw_prefix VL256 V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()               | REG0=YMM_R():w:qq:f64 MEM0:r:q:f64 EMX_BROADCAST_1TO4_64
| VBROADCASTSS        | BROADCAST     | AVX           |               | 18          | mm        | rrr       | VV1 0x18  norexw_prefix VL128 V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()               | REG0=XMM_R():w:dq:f32 MEM0:r:d:f32 EMX_BROADCAST_1TO4_32
| VBROADCASTSS        | BROADCAST     | AVX           |               | 18          | mm        | rrr       | VV1 0x18  norexw_prefix VL256 V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()               | REG0=YMM_R():w:qq:f32 MEM0:r:d:f32 EMX_BROADCAST_1TO8_32
| VCMPPD              | AVX           | AVX           |               | c2          | mm        | rrr       | VV1 0xC2  V66 VL128 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                             | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64 IMM0:r:b
| VCMPPD              | AVX           | AVX           |               | c2          | 0b11      | rrr       | VV1 0xC2  V66 VL128 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                    | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64 IMM0:r:b
| VCMPPD              | AVX           | AVX           |               | c2          | mm        | rrr       | VV1 0xC2  V66 VL256 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                             | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64 IMM0:r:b
| VCMPPD              | AVX           | AVX           |               | c2          | 0b11      | rrr       | VV1 0xC2  V66 VL256 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                    | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64 IMM0:r:b
| VCMPPS              | AVX           | AVX           |               | c2          | mm        | rrr       | VV1 0xC2  VNP VL128 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                             | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32 IMM0:r:b
| VCMPPS              | AVX           | AVX           |               | c2          | 0b11      | rrr       | VV1 0xC2  VNP VL128 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                    | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32 IMM0:r:b
| VCMPPS              | AVX           | AVX           |               | c2          | mm        | rrr       | VV1 0xC2  VNP VL256 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                             | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32 IMM0:r:b
| VCMPPS              | AVX           | AVX           |               | c2          | 0b11      | rrr       | VV1 0xC2  VNP VL256 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                    | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32 IMM0:r:b
| VCMPSD              | AVX           | AVX           |               | c2          | mm        | rrr       | VV1 0xC2   VF2 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                                 | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:q:f64 IMM0:r:b
| VCMPSD              | AVX           | AVX           |               | c2          | 0b11      | rrr       | VV1 0xC2   VF2 V0F  MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                        | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:q:f64 IMM0:r:b
| VCMPSS              | AVX           | AVX           |               | c2          | mm        | rrr       | VV1 0xC2   VF3 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                                 | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:d:f32 IMM0:r:b
| VCMPSS              | AVX           | AVX           |               | c2          | 0b11      | rrr       | VV1 0xC2   VF3 V0F  MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                        | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:d:f32 IMM0:r:b
| VCOMISD             | AVX           | AVX           |               | 2f          | mm        | rrr       | VV1 0x2F   V66 V0F  NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=XMM_R():r:q:f64 MEM0:r:q:f64
| VCOMISD             | AVX           | AVX           |               | 2f          | 0b11      | rrr       | VV1 0x2F   V66 V0F  NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                          | REG0=XMM_R():r:q:f64 REG1=XMM_B():r:q:f64
| VCOMISS             | AVX           | AVX           |               | 2f          | mm        | rrr       | VV1 0x2F   VNP V0F  NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                   | REG0=XMM_R():r:d:f32 MEM0:r:d:f32
| VCOMISS             | AVX           | AVX           |               | 2f          | 0b11      | rrr       | VV1 0x2F   VNP V0F  NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                          | REG0=XMM_R():r:d:f32 REG1=XMM_B():r:d:f32
| VCVTDQ2PD           | CONVERT       | AVX           |               | e6          | mm        | rrr       | VV1 0xE6  VL128 VF3 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=XMM_R():w:dq:f64 MEM0:r:q:i32
| VCVTDQ2PD           | CONVERT       | AVX           |               | e6          | 0b11      | rrr       | VV1 0xE6  VL128 VF3 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=XMM_R():w:dq:f64 REG1=XMM_B():r:q:i32
| VCVTDQ2PD           | CONVERT       | AVX           |               | e6          | mm        | rrr       | VV1 0xE6  VL256 VF3 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=YMM_R():w:qq:f64 MEM0:r:dq:i32
| VCVTDQ2PD           | CONVERT       | AVX           |               | e6          | 0b11      | rrr       | VV1 0xE6  VL256 VF3 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=YMM_R():w:qq:f64 REG1=XMM_B():r:dq:i32
| VCVTDQ2PS           | CONVERT       | AVX           |               | 5b          | mm        | rrr       | VV1 0x5B  VL128 VNP V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=XMM_R():w:dq:f32 MEM0:r:dq:i32
| VCVTDQ2PS           | CONVERT       | AVX           |               | 5b          | 0b11      | rrr       | VV1 0x5B  VL128 VNP V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=XMM_R():w:dq:f32 REG1=XMM_B():r:dq:i32
| VCVTDQ2PS           | CONVERT       | AVX           |               | 5b          | mm        | rrr       | VV1 0x5B  VL256 VNP V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=YMM_R():w:qq:f32 MEM0:r:qq:i32
| VCVTDQ2PS           | CONVERT       | AVX           |               | 5b          | 0b11      | rrr       | VV1 0x5B  VL256 VNP V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=YMM_R():w:qq:f32 REG1=YMM_B():r:qq:i32
| VCVTPD2DQ           | CONVERT       | AVX           |               | e6          | mm        | rrr       | VV1 0xE6  VL128 VF2 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=XMM_R():w:dq:i32 MEM0:r:dq:f64
| VCVTPD2DQ           | CONVERT       | AVX           |               | e6          | 0b11      | rrr       | VV1 0xE6  VL128 VF2 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=XMM_R():w:dq:i32 REG1=XMM_B():r:dq:f64
| VCVTPD2DQ           | CONVERT       | AVX           |               | e6          | mm        | rrr       | VV1 0xE6  VL256 VF2 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=XMM_R():w:dq:i32 MEM0:r:qq:f64
| VCVTPD2DQ           | CONVERT       | AVX           |               | e6          | 0b11      | rrr       | VV1 0xE6  VL256 VF2 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=XMM_R():w:dq:i32 REG1=YMM_B():r:qq:f64
| VCVTPD2PS           | CONVERT       | AVX           |               | 5a          | mm        | rrr       | VV1 0x5A  V66 VL128 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=XMM_R():w:dq:f32 MEM0:r:dq:f64
| VCVTPD2PS           | CONVERT       | AVX           |               | 5a          | 0b11      | rrr       | VV1 0x5A  V66 VL128 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=XMM_R():w:dq:f32 REG1=XMM_B():r:dq:f64
| VCVTPD2PS           | CONVERT       | AVX           |               | 5a          | mm        | rrr       | VV1 0x5A  V66 VL256 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=XMM_R():w:dq:f32 MEM0:r:qq:f64
| VCVTPD2PS           | CONVERT       | AVX           |               | 5a          | 0b11      | rrr       | VV1 0x5A  V66 VL256 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=XMM_R():w:dq:f32 REG1=YMM_B():r:qq:f64
| VCVTPS2DQ           | CONVERT       | AVX           |               | 5b          | mm        | rrr       | VV1 0x5B  VL128 V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=XMM_R():w:dq:i32 MEM0:r:dq:f32
| VCVTPS2DQ           | CONVERT       | AVX           |               | 5b          | 0b11      | rrr       | VV1 0x5B  VL128 V66 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=XMM_R():w:dq:i32 REG1=XMM_B():r:dq:f32
| VCVTPS2DQ           | CONVERT       | AVX           |               | 5b          | mm        | rrr       | VV1 0x5B  VL256 V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=YMM_R():w:qq:i32 MEM0:r:qq:f32
| VCVTPS2DQ           | CONVERT       | AVX           |               | 5b          | 0b11      | rrr       | VV1 0x5B  VL256 V66 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=YMM_R():w:qq:i32 REG1=YMM_B():r:qq:f32
| VCVTPS2PD           | CONVERT       | AVX           |               | 5a          | mm        | rrr       | VV1 0x5A  VNP VL128 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=XMM_R():w:dq:f64 MEM0:r:q:f32
| VCVTPS2PD           | CONVERT       | AVX           |               | 5a          | 0b11      | rrr       | VV1 0x5A  VNP VL128 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=XMM_R():w:dq:f64 REG1=XMM_B():r:q:f32
| VCVTPS2PD           | CONVERT       | AVX           |               | 5a          | mm        | rrr       | VV1 0x5A  VNP VL256 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=YMM_R():w:qq:f64 MEM0:r:dq:f32
| VCVTPS2PD           | CONVERT       | AVX           |               | 5a          | 0b11      | rrr       | VV1 0x5A  VNP VL256 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=YMM_R():w:qq:f64 REG1=XMM_B():r:dq:f32
| VCVTSD2SI           | CONVERT       | AVX           |               | 2d          | mm        | rrr       | VV1 0x2D   VF2 V0F  NOVSR not64 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                             | REG0=GPR32_R():w:d:i32 MEM0:r:q:f64
| VCVTSD2SI           | CONVERT       | AVX           |               | 2d          | 0b11      | rrr       | VV1 0x2D   VF2 V0F  NOVSR not64 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                    | REG0=GPR32_R():w:d:i32 REG1=XMM_B():r:q:f64
| VCVTSD2SI           | CONVERT       | AVX           |               | 2d          | mm        | rrr       | VV1 0x2D   VF2 V0F  NOVSR mode64 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()              | REG0=GPR32_R():w:d:i32 MEM0:r:q:f64
| VCVTSD2SI           | CONVERT       | AVX           |               | 2d          | 0b11      | rrr       | VV1 0x2D   VF2 V0F  NOVSR mode64 norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                     | REG0=GPR32_R():w:d:i32 REG1=XMM_B():r:q:f64
| VCVTSD2SI           | CONVERT       | AVX           |               | 2d          | mm        | rrr       | VV1 0x2D   VF2 V0F  NOVSR mode64 rexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                | REG0=GPR64_R():w:q:i64 MEM0:r:q:f64
| VCVTSD2SI           | CONVERT       | AVX           |               | 2d          | 0b11      | rrr       | VV1 0x2D   VF2 V0F  NOVSR mode64 rexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                       | REG0=GPR64_R():w:q:i64 REG1=XMM_B():r:q:f64
| VCVTSD2SS           | CONVERT       | AVX           |               | 5a          | mm        | rrr       | VV1 0x5A  VF2 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                          | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:q:f64
| VCVTSD2SS           | CONVERT       | AVX           |               | 5a          | 0b11      | rrr       | VV1 0x5A  VF2 V0F  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                 | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:q:f64
| VCVTSI2SD           | CONVERT       | AVX           |               | 2a          | mm        | rrr       | VV1 0x2A  VF2 V0F not64 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:d:i32
| VCVTSI2SD           | CONVERT       | AVX           |               | 2a          | 0b11      | rrr       | VV1 0x2A  VF2 V0F not64 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=GPR32_B():r:d:i32
| VCVTSI2SD           | CONVERT       | AVX           |               | 2a          | mm        | rrr       | VV1 0x2A  VF2 V0F mode64 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                      | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:d:i32
| VCVTSI2SD           | CONVERT       | AVX           |               | 2a          | 0b11      | rrr       | VV1 0x2A  VF2 V0F mode64 norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                             | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=GPR32_B():r:d:i32
| VCVTSI2SD           | CONVERT       | AVX           |               | 2a          | mm        | rrr       | VV1 0x2A  VF2 V0F mode64 rexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                        | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:q:i64
| VCVTSI2SD           | CONVERT       | AVX           |               | 2a          | 0b11      | rrr       | VV1 0x2A  VF2 V0F mode64 rexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                               | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=GPR64_B():r:q:i64
| VCVTSI2SS           | CONVERT       | AVX           |               | 2a          | mm        | rrr       | VV1 0x2A   VF3 V0F not64 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:d:i32
| VCVTSI2SS           | CONVERT       | AVX           |               | 2a          | 0b11      | rrr       | VV1 0x2A   VF3 V0F not64 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                           | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=GPR32_B():r:d:i32
| VCVTSI2SS           | CONVERT       | AVX           |               | 2a          | mm        | rrr       | VV1 0x2A   VF3 V0F mode64 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                     | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:d:i32
| VCVTSI2SS           | CONVERT       | AVX           |               | 2a          | 0b11      | rrr       | VV1 0x2A   VF3 V0F mode64 norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                            | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=GPR32_B():r:d:i32
| VCVTSI2SS           | CONVERT       | AVX           |               | 2a          | mm        | rrr       | VV1 0x2A   VF3 V0F mode64 rexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                       | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:q:i64
| VCVTSI2SS           | CONVERT       | AVX           |               | 2a          | 0b11      | rrr       | VV1 0x2A   VF3 V0F mode64 rexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                              | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=GPR64_B():r:q:i64
| VCVTSS2SD           | CONVERT       | AVX           |               | 5a          | mm        | rrr       | VV1 0x5A  VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                           | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:d:f32
| VCVTSS2SD           | CONVERT       | AVX           |               | 5a          | 0b11      | rrr       | VV1 0x5A  VF3 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                  | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:d:f32
| VCVTSS2SI           | CONVERT       | AVX           |               | 2d          | mm        | rrr       | VV1 0x2D   VF3 V0F  NOVSR not64 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                             | REG0=GPR32_R():w:d:i32 MEM0:r:d:f32
| VCVTSS2SI           | CONVERT       | AVX           |               | 2d          | 0b11      | rrr       | VV1 0x2D   VF3 V0F  NOVSR not64 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                    | REG0=GPR32_R():w:d:i32 REG1=XMM_B():r:d:f32
| VCVTSS2SI           | CONVERT       | AVX           |               | 2d          | mm        | rrr       | VV1 0x2D   VF3 V0F  NOVSR mode64 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()              | REG0=GPR32_R():w:d:i32 MEM0:r:d:f32
| VCVTSS2SI           | CONVERT       | AVX           |               | 2d          | 0b11      | rrr       | VV1 0x2D   VF3 V0F  NOVSR mode64 norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                     | REG0=GPR32_R():w:d:i32 REG1=XMM_B():r:d:f32
| VCVTSS2SI           | CONVERT       | AVX           |               | 2d          | mm        | rrr       | VV1 0x2D   VF3 V0F  NOVSR mode64 rexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                | REG0=GPR64_R():w:q:i64 MEM0:r:d:f32
| VCVTSS2SI           | CONVERT       | AVX           |               | 2d          | 0b11      | rrr       | VV1 0x2D   VF3 V0F  NOVSR mode64 rexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                       | REG0=GPR64_R():w:q:i64 REG1=XMM_B():r:d:f32
| VCVTTPD2DQ          | CONVERT       | AVX           |               | e6          | mm        | rrr       | VV1 0xE6  VL128 V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=XMM_R():w:dq:i32 MEM0:r:dq:f64
| VCVTTPD2DQ          | CONVERT       | AVX           |               | e6          | 0b11      | rrr       | VV1 0xE6  VL128 V66 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=XMM_R():w:dq:i32 REG1=XMM_B():r:dq:f64
| VCVTTPD2DQ          | CONVERT       | AVX           |               | e6          | mm        | rrr       | VV1 0xE6  VL256 V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=XMM_R():w:dq:i32 MEM0:r:qq:f64
| VCVTTPD2DQ          | CONVERT       | AVX           |               | e6          | 0b11      | rrr       | VV1 0xE6  VL256 V66 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=XMM_R():w:dq:i32 REG1=YMM_B():r:qq:f64
| VCVTTPS2DQ          | CONVERT       | AVX           |               | 5b          | mm        | rrr       | VV1 0x5B  VL128 VF3 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=XMM_R():w:dq:i32 MEM0:r:dq:f32
| VCVTTPS2DQ          | CONVERT       | AVX           |               | 5b          | 0b11      | rrr       | VV1 0x5B  VL128 VF3 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=XMM_R():w:dq:i32 REG1=XMM_B():r:dq:f32
| VCVTTPS2DQ          | CONVERT       | AVX           |               | 5b          | mm        | rrr       | VV1 0x5B  VL256 VF3 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=YMM_R():w:qq:i32 MEM0:r:qq:f32
| VCVTTPS2DQ          | CONVERT       | AVX           |               | 5b          | 0b11      | rrr       | VV1 0x5B  VL256 VF3 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=YMM_R():w:qq:i32 REG1=YMM_B():r:qq:f32
| VCVTTSD2SI          | CONVERT       | AVX           |               | 2c          | mm        | rrr       | VV1 0x2C   VF2 V0F  NOVSR not64 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                             | REG0=GPR32_R():w:d:i32 MEM0:r:q:f64
| VCVTTSD2SI          | CONVERT       | AVX           |               | 2c          | 0b11      | rrr       | VV1 0x2C   VF2 V0F  NOVSR not64 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                    | REG0=GPR32_R():w:d:i32 REG1=XMM_B():r:q:f64
| VCVTTSD2SI          | CONVERT       | AVX           |               | 2c          | mm        | rrr       | VV1 0x2C   VF2 V0F  NOVSR mode64 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()              | REG0=GPR32_R():w:d:i32 MEM0:r:q:f64
| VCVTTSD2SI          | CONVERT       | AVX           |               | 2c          | 0b11      | rrr       | VV1 0x2C   VF2 V0F  NOVSR mode64 norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                     | REG0=GPR32_R():w:d:i32 REG1=XMM_B():r:q:f64
| VCVTTSD2SI          | CONVERT       | AVX           |               | 2c          | mm        | rrr       | VV1 0x2C   VF2 V0F  NOVSR mode64 rexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                | REG0=GPR64_R():w:q:i64 MEM0:r:q:f64
| VCVTTSD2SI          | CONVERT       | AVX           |               | 2c          | 0b11      | rrr       | VV1 0x2C   VF2 V0F  NOVSR mode64 rexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                       | REG0=GPR64_R():w:q:i64 REG1=XMM_B():r:q:f64
| VCVTTSS2SI          | CONVERT       | AVX           |               | 2c          | mm        | rrr       | VV1 0x2C   VF3 V0F  NOVSR not64 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                             | REG0=GPR32_R():w:d:i32 MEM0:r:d:f32
| VCVTTSS2SI          | CONVERT       | AVX           |               | 2c          | 0b11      | rrr       | VV1 0x2C   VF3 V0F  NOVSR not64 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                    | REG0=GPR32_R():w:d:i32 REG1=XMM_B():r:d:f32
| VCVTTSS2SI          | CONVERT       | AVX           |               | 2c          | mm        | rrr       | VV1 0x2C   VF3 V0F  NOVSR mode64 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()              | REG0=GPR32_R():w:d:i32 MEM0:r:d:f32
| VCVTTSS2SI          | CONVERT       | AVX           |               | 2c          | 0b11      | rrr       | VV1 0x2C   VF3 V0F  NOVSR mode64 norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                     | REG0=GPR32_R():w:d:i32 REG1=XMM_B():r:d:f32
| VCVTTSS2SI          | CONVERT       | AVX           |               | 2c          | mm        | rrr       | VV1 0x2C   VF3 V0F  NOVSR mode64 rexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                | REG0=GPR64_R():w:q:i64 MEM0:r:d:f32
| VCVTTSS2SI          | CONVERT       | AVX           |               | 2c          | 0b11      | rrr       | VV1 0x2C   VF3 V0F  NOVSR mode64 rexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                       | REG0=GPR64_R():w:q:i64 REG1=XMM_B():r:d:f32
| VDIVPD              | AVX           | AVX           |               | 5e          | mm        | rrr       | VV1 0x5E  V66 V0F VL128 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
| VDIVPD              | AVX           | AVX           |               | 5e          | 0b11      | rrr       | VV1 0x5E  V66 V0F VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
| VDIVPD              | AVX           | AVX           |               | 5e          | mm        | rrr       | VV1 0x5E  V66 V0F VL256 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
| VDIVPD              | AVX           | AVX           |               | 5e          | 0b11      | rrr       | VV1 0x5E  V66 V0F VL256 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
| VDIVPS              | AVX           | AVX           |               | 5e          | mm        | rrr       | VV1 0x5E  VNP V0F VL128 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
| VDIVPS              | AVX           | AVX           |               | 5e          | 0b11      | rrr       | VV1 0x5E  VNP V0F VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
| VDIVPS              | AVX           | AVX           |               | 5e          | mm        | rrr       | VV1 0x5E  VNP V0F VL256 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
| VDIVPS              | AVX           | AVX           |               | 5e          | 0b11      | rrr       | VV1 0x5E  VNP V0F VL256 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
| VDIVSD              | AVX           | AVX           |               | 5e          | mm        | rrr       | VV1 0x5E  VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                           | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:q:f64
| VDIVSD              | AVX           | AVX           |               | 5e          | 0b11      | rrr       | VV1 0x5E  VF2 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                  | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:q:f64
| VDIVSS              | AVX           | AVX           |               | 5e          | mm        | rrr       | VV1 0x5E  VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                           | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:d:f32
| VDIVSS              | AVX           | AVX           |               | 5e          | 0b11      | rrr       | VV1 0x5E  VF3 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                  | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:d:f32
| VDPPD               | AVX           | AVX           |               | 41          | mm        | rrr       | VV1 0x41  VL128 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                           | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64 IMM0:r:b
| VDPPD               | AVX           | AVX           |               | 41          | 0b11      | rrr       | VV1 0x41  VL128 V66 V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                  | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64 IMM0:r:b
| VDPPS               | AVX           | AVX           |               | 40          | mm        | rrr       | VV1 0x40  VL128 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                           | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32 IMM0:r:b
| VDPPS               | AVX           | AVX           |               | 40          | 0b11      | rrr       | VV1 0x40  VL128 V66 V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                  | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32 IMM0:r:b
| VDPPS               | AVX           | AVX           |               | 40          | mm        | rrr       | VV1 0x40  VL256 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                           | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32 IMM0:r:b
| VDPPS               | AVX           | AVX           |               | 40          | 0b11      | rrr       | VV1 0x40  VL256 V66 V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                  | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32 IMM0:r:b
| VEXTRACTF128        | AVX           | AVX           |               | 19          | mm        | rrr       | VV1 0x19  norexw_prefix VL256 V66 V0F3A NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()       | MEM0:w:dq:f64 REG0=YMM_R():r:dq:f64  IMM0:r:b
| VEXTRACTF128        | AVX           | AVX           |               | 19          | 0b11      | rrr       | VV1 0x19  norexw_prefix VL256 V66 V0F3A NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()              | REG0=XMM_B():w:dq:f64 REG1=YMM_R():r:dq:f64 IMM0:r:b
| VEXTRACTPS          | AVX           | AVX           |               | 17          | mm        | rrr       | VV1 0x17  VL128 V66 V0F3A  NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                    | MEM0:w:d:f32  REG0=XMM_R():r:dq:f32  IMM0:r:b
| VEXTRACTPS          | AVX           | AVX           |               | 17          | 0b11      | rrr       | VV1 0x17  VL128 V66 V0F3A  NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                           | REG0=GPR32_B():w  REG1=XMM_R():r:dq:f32  IMM0:r:b
| VHADDPD             | AVX           | AVX           |               | 7c          | mm        | rrr       | VV1 0x7C  VL128 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
| VHADDPD             | AVX           | AVX           |               | 7c          | 0b11      | rrr       | VV1 0x7C  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
| VHADDPD             | AVX           | AVX           |               | 7c          | mm        | rrr       | VV1 0x7C  VL256 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
| VHADDPD             | AVX           | AVX           |               | 7c          | 0b11      | rrr       | VV1 0x7C  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
| VHADDPS             | AVX           | AVX           |               | 7c          | mm        | rrr       | VV1 0x7C  VL128 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
| VHADDPS             | AVX           | AVX           |               | 7c          | 0b11      | rrr       | VV1 0x7C  VL128 VF2 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
| VHADDPS             | AVX           | AVX           |               | 7c          | mm        | rrr       | VV1 0x7C  VL256 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
| VHADDPS             | AVX           | AVX           |               | 7c          | 0b11      | rrr       | VV1 0x7C  VL256 VF2 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
| VHSUBPD             | AVX           | AVX           |               | 7d          | mm        | rrr       | VV1 0x7D  VL128 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
| VHSUBPD             | AVX           | AVX           |               | 7d          | 0b11      | rrr       | VV1 0x7D  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
| VHSUBPD             | AVX           | AVX           |               | 7d          | mm        | rrr       | VV1 0x7D  VL256 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
| VHSUBPD             | AVX           | AVX           |               | 7d          | 0b11      | rrr       | VV1 0x7D  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
| VHSUBPS             | AVX           | AVX           |               | 7d          | mm        | rrr       | VV1 0x7D  VL128 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
| VHSUBPS             | AVX           | AVX           |               | 7d          | 0b11      | rrr       | VV1 0x7D  VL128 VF2 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
| VHSUBPS             | AVX           | AVX           |               | 7d          | mm        | rrr       | VV1 0x7D  VL256 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
| VHSUBPS             | AVX           | AVX           |               | 7d          | 0b11      | rrr       | VV1 0x7D  VL256 VF2 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
| VINSERTF128         | AVX           | AVX           |               | 18          | mm        | rrr       | VV1 0x18  norexw_prefix VL256 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()             | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:dq:f64 IMM0:r:b EMX_BROADCAST_2TO4_64
| VINSERTF128         | AVX           | AVX           |               | 18          | 0b11      | rrr       | VV1 0x18  norexw_prefix  VL256 V66 V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                   | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=XMM_B():r:dq:f64 IMM0:r:b EMX_BROADCAST_2TO4_64
| VINSERTPS           | AVX           | AVX           |               | 21          | mm        | rrr       | VV1 0x21  VL128 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                           | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:d:f32 IMM0:r:b
| VINSERTPS           | AVX           | AVX           |               | 21          | 0b11      | rrr       | VV1 0x21  VL128 V66 V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                  | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32 IMM0:r:b
| VLDDQU              | AVX           | AVX           |               | f0          | mm        | rrr       | VV1 0xF0  VL128 VF2 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=XMM_R():w:dq  MEM0:r:dq
| VLDDQU              | AVX           | AVX           |               | f0          | mm        | rrr       | VV1 0xF0  VL256 VF2 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=YMM_R():w:qq MEM0:r:qq
| VLDMXCSR            | AVX           | AVX           |               | ae          | mm        | 0b010     | VV1 0xAE VL128 VNP V0F NOVSR MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM()                              | MEM0:r:d REG0=XED_REG_MXCSR:w:SUPP
| VMASKMOVDQU         | AVX           | AVX           |               | f7          | 0b11      | rrr       | VV1 0xF7 V0F V66 VL128  NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=XMM_R():r:dq:u8 REG1=XMM_B():r:dq:u8 MEM0:w:SUPP:dq:u8 BASE0=ArDI():r:SUPP SEG0=FINAL_DSEG():r:SUPP
| VMASKMOVPD          | AVX           | AVX           |               | 2d          | mm        | rrr       | VV1 0x2D  V66 VL128 V0F38  norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                    | REG0=XMM_R():w:dq:f64   REG1=XMM_N():r:dq:u64 MEM0:r:dq:f64
| VMASKMOVPD          | AVX           | AVX           |               | 2d          | mm        | rrr       | VV1 0x2D  V66 VL256 V0F38 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                     | REG0=YMM_R():w:qq:f64   REG1=YMM_N():r:qq:u64 MEM0:r:qq:f64
| VMASKMOVPD          | AVX           | AVX           |               | 2f          | mm        | rrr       | VV1 0x2F   V66 V0F38 VL128 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                    | MEM0:w:dq:f64  REG0=XMM_N():r:dq:u64  REG1=XMM_R():r:dq:f64
| VMASKMOVPD          | AVX           | AVX           |               | 2f          | mm        | rrr       | VV1 0x2F   V66 V0F38 VL256 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                    | MEM0:w:qq:f64  REG0=YMM_N():r:qq:u64   REG1=YMM_R():r:qq:f64
| VMASKMOVPS          | AVX           | AVX           |               | 2c          | mm        | rrr       | VV1 0x2C V66 VL128 V0F38 norexw_prefix  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                     | REG0=XMM_R():w:dq:f32   REG1=XMM_N():r:dq MEM0:r:dq:f32
| VMASKMOVPS          | AVX           | AVX           |               | 2c          | mm        | rrr       | VV1 0x2C V66 VL256 V0F38    norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                   | REG0=YMM_R():w:qq:f32   REG1=YMM_N():r:qq MEM0:r:qq:f32
| VMASKMOVPS          | AVX           | AVX           |               | 2e          | mm        | rrr       | VV1 0x2E V66 V0F38 VL128  norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                     | MEM0:w:dq:f32  REG0=XMM_N():r:dq   REG1=XMM_R():r:dq:f32
| VMASKMOVPS          | AVX           | AVX           |               | 2e          | mm        | rrr       | VV1 0x2E V66 V0F38 VL256 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                      | MEM0:w:qq:f32   REG0=YMM_N():r:qq  REG1=YMM_R():r:qq:f32
| VMAXPD              | AVX           | AVX           |               | 5f          | mm        | rrr       | VV1 0x5F  V66 V0F VL128 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
| VMAXPD              | AVX           | AVX           |               | 5f          | 0b11      | rrr       | VV1 0x5F  V66 V0F VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
| VMAXPD              | AVX           | AVX           |               | 5f          | mm        | rrr       | VV1 0x5F  V66 V0F VL256 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
| VMAXPD              | AVX           | AVX           |               | 5f          | 0b11      | rrr       | VV1 0x5F  V66 V0F VL256 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
| VMAXPS              | AVX           | AVX           |               | 5f          | mm        | rrr       | VV1 0x5F  VNP V0F VL128 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
| VMAXPS              | AVX           | AVX           |               | 5f          | 0b11      | rrr       | VV1 0x5F  VNP V0F VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
| VMAXPS              | AVX           | AVX           |               | 5f          | mm        | rrr       | VV1 0x5F  VNP V0F VL256 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
| VMAXPS              | AVX           | AVX           |               | 5f          | 0b11      | rrr       | VV1 0x5F  VNP V0F VL256 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
| VMAXSD              | AVX           | AVX           |               | 5f          | mm        | rrr       | VV1 0x5F  VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                           | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:q:f64
| VMAXSD              | AVX           | AVX           |               | 5f          | 0b11      | rrr       | VV1 0x5F  VF2 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                  | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:q:f64
| VMAXSS              | AVX           | AVX           |               | 5f          | mm        | rrr       | VV1 0x5F  VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                           | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:d:f32
| VMAXSS              | AVX           | AVX           |               | 5f          | 0b11      | rrr       | VV1 0x5F  VF3 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                  | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:d:f32
| VMINPD              | AVX           | AVX           |               | 5d          | mm        | rrr       | VV1 0x5D  V66 V0F VL128 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
| VMINPD              | AVX           | AVX           |               | 5d          | 0b11      | rrr       | VV1 0x5D  V66 V0F VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
| VMINPD              | AVX           | AVX           |               | 5d          | mm        | rrr       | VV1 0x5D  V66 V0F VL256 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
| VMINPD              | AVX           | AVX           |               | 5d          | 0b11      | rrr       | VV1 0x5D  V66 V0F VL256 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
| VMINPS              | AVX           | AVX           |               | 5d          | mm        | rrr       | VV1 0x5D  VNP V0F VL128 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
| VMINPS              | AVX           | AVX           |               | 5d          | 0b11      | rrr       | VV1 0x5D  VNP V0F VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
| VMINPS              | AVX           | AVX           |               | 5d          | mm        | rrr       | VV1 0x5D  VNP V0F VL256 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
| VMINPS              | AVX           | AVX           |               | 5d          | 0b11      | rrr       | VV1 0x5D  VNP V0F VL256 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
| VMINSD              | AVX           | AVX           |               | 5d          | mm        | rrr       | VV1 0x5D  VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                           | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:q:f64
| VMINSD              | AVX           | AVX           |               | 5d          | 0b11      | rrr       | VV1 0x5D  VF2 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                  | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:q:f64
| VMINSS              | AVX           | AVX           |               | 5d          | mm        | rrr       | VV1 0x5D  VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                           | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:d:f32
| VMINSS              | AVX           | AVX           |               | 5d          | 0b11      | rrr       | VV1 0x5D  VF3 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                  | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:d:f32
| VMOVAPD             | DATAXFER      | AVX           |               | 28          | mm        | rrr       | VV1 0x28  VL128 V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=XMM_R():w:dq:f64  MEM0:r:dq:f64
| VMOVAPD             | DATAXFER      | AVX           |               | 28          | 0b11      | rrr       | VV1 0x28  VL128 V66 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=XMM_R():w:dq:f64  REG1=XMM_B():r:dq:f64
| VMOVAPD             | DATAXFER      | AVX           |               | 29          | mm        | rrr       | VV1 0x29  VL128 V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | MEM0:w:dq:f64 REG0=XMM_R():r:dq:f64
| VMOVAPD             | DATAXFER      | AVX           |               | 29          | 0b11      | rrr       | VV1 0x29  VL128 V66 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=XMM_B():w:dq:f64 REG1=XMM_R():r:dq:f64
| VMOVAPD             | DATAXFER      | AVX           |               | 28          | mm        | rrr       | VV1 0x28  VL256 V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=YMM_R():w:qq:f64  MEM0:r:qq:f64
| VMOVAPD             | DATAXFER      | AVX           |               | 28          | 0b11      | rrr       | VV1 0x28  VL256 V66 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=YMM_R():w:qq:f64  REG1=YMM_B():r:qq:f64
| VMOVAPD             | DATAXFER      | AVX           |               | 29          | mm        | rrr       | VV1 0x29  VL256 V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | MEM0:w:qq:f64 REG0=YMM_R():r:qq:f64
| VMOVAPD             | DATAXFER      | AVX           |               | 29          | 0b11      | rrr       | VV1 0x29  VL256 V66 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=YMM_B():w:qq:f64 REG1=YMM_R():r:qq:f64
| VMOVAPS             | DATAXFER      | AVX           |               | 28          | mm        | rrr       | VV1 0x28  VL128 VNP V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=XMM_R():w:dq:f32  MEM0:r:dq:f32
| VMOVAPS             | DATAXFER      | AVX           |               | 28          | 0b11      | rrr       | VV1 0x28  VL128 VNP V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=XMM_R():w:dq:f32  REG1=XMM_B():r:dq:f32
| VMOVAPS             | DATAXFER      | AVX           |               | 29          | mm        | rrr       | VV1 0x29  VL128 VNP V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | MEM0:w:dq:f32 REG0=XMM_R():r:dq:f32
| VMOVAPS             | DATAXFER      | AVX           |               | 29          | 0b11      | rrr       | VV1 0x29  VL128 VNP V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=XMM_B():w:dq:f32 REG1=XMM_R():r:dq:f32
| VMOVAPS             | DATAXFER      | AVX           |               | 28          | mm        | rrr       | VV1 0x28  VL256 VNP V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=YMM_R():w:qq:f32  MEM0:r:qq:f32
| VMOVAPS             | DATAXFER      | AVX           |               | 28          | 0b11      | rrr       | VV1 0x28  VL256 VNP V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=YMM_R():w:qq:f32  REG1=YMM_B():r:qq:f32
| VMOVAPS             | DATAXFER      | AVX           |               | 29          | mm        | rrr       | VV1 0x29  VL256 VNP V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | MEM0:w:qq:f32 REG0=YMM_R():r:qq:f32
| VMOVAPS             | DATAXFER      | AVX           |               | 29          | 0b11      | rrr       | VV1 0x29  VL256 VNP V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=YMM_B():w:qq:f32 REG1=YMM_R():r:qq:f32
| VMOVD               | DATAXFER      | AVX           |               | 6e          | mm        | rrr       | VV1 0x6E  VL128 V66 V0F not64 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                         | REG0=XMM_R():w:dq  MEM0:r:d
| VMOVD               | DATAXFER      | AVX           |               | 6e          | 0b11      | rrr       | VV1 0x6E  VL128 V66 V0F not64  NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                               | REG0=XMM_R():w:dq  REG1=GPR32_B():r:d
| VMOVD               | DATAXFER      | AVX           |               | 7e          | mm        | rrr       | VV1 0x7E  VL128 V66 V0F not64  NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                        | MEM0:w:d           REG0=XMM_R():r:d
| VMOVD               | DATAXFER      | AVX           |               | 7e          | 0b11      | rrr       | VV1 0x7E  VL128 V66 V0F not64  NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                               | REG0=GPR32_B():w:d REG1=XMM_R():r:d
| VMOVD               | DATAXFER      | AVX           |               | 6e          | mm        | rrr       | VV1 0x6E  VL128 V66 V0F mode64 norexw_prefix NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()          | REG0=XMM_R():w:dq  MEM0:r:d
| VMOVD               | DATAXFER      | AVX           |               | 6e          | 0b11      | rrr       | VV1 0x6E  VL128 V66 V0F mode64 norexw_prefix NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                 | REG0=XMM_R():w:dq  REG1=GPR32_B():r:d
| VMOVD               | DATAXFER      | AVX           |               | 7e          | mm        | rrr       | VV1 0x7E  VL128 V66 V0F mode64 norexw_prefix NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()          | MEM0:w:d           REG0=XMM_R():r:d
| VMOVD               | DATAXFER      | AVX           |               | 7e          | 0b11      | rrr       | VV1 0x7E  VL128 V66 V0F mode64 norexw_prefix NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                 | REG0=GPR32_B():w:d REG1=XMM_R():r:d
| VMOVDDUP            | DATAXFER      | AVX           |               | 12          | mm        | rrr       | VV1 0x12  VL128 VF2 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=XMM_R():w:dq:f64  MEM0:r:q:f64
| VMOVDDUP            | DATAXFER      | AVX           |               | 12          | 0b11      | rrr       | VV1 0x12  VL128 VF2 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=XMM_R():w:dq:f64  REG1=XMM_B():r:dq:f64
| VMOVDDUP            | DATAXFER      | AVX           |               | 12          | mm        | rrr       | VV1 0x12  VL256 VF2 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=YMM_R():w:qq:f64  MEM0:r:qq:f64
| VMOVDDUP            | DATAXFER      | AVX           |               | 12          | 0b11      | rrr       | VV1 0x12  VL256 VF2 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=YMM_R():w:qq:f64  REG1=YMM_B():r:qq:f64
| VMOVDQA             | DATAXFER      | AVX           |               | 6f          | mm        | rrr       | VV1 0x6F  VL128 V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=XMM_R():w:dq  MEM0:r:dq
| VMOVDQA             | DATAXFER      | AVX           |               | 6f          | 0b11      | rrr       | VV1 0x6F  VL128 V66 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=XMM_R():w:dq  REG1=XMM_B():r:dq
| VMOVDQA             | DATAXFER      | AVX           |               | 7f          | mm        | rrr       | VV1 0x7F  VL128 V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | MEM0:w:dq REG0=XMM_R():r:dq
| VMOVDQA             | DATAXFER      | AVX           |               | 7f          | 0b11      | rrr       | VV1 0x7F  VL128 V66 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=XMM_B():w:dq REG1=XMM_R():r:dq
| VMOVDQA             | DATAXFER      | AVX           |               | 6f          | mm        | rrr       | VV1 0x6F  VL256 V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=YMM_R():w:qq  MEM0:r:qq
| VMOVDQA             | DATAXFER      | AVX           |               | 6f          | 0b11      | rrr       | VV1 0x6F  VL256 V66 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=YMM_R():w:qq  REG1=YMM_B():r:qq
| VMOVDQA             | DATAXFER      | AVX           |               | 7f          | mm        | rrr       | VV1 0x7F  VL256 V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | MEM0:w:qq REG0=YMM_R():r:qq
| VMOVDQA             | DATAXFER      | AVX           |               | 7f          | 0b11      | rrr       | VV1 0x7F  VL256 V66 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=YMM_B():w:qq REG1=YMM_R():r:qq
| VMOVDQU             | DATAXFER      | AVX           |               | 6f          | mm        | rrr       | VV1 0x6F  VL128 VF3 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=XMM_R():w:dq  MEM0:r:dq
| VMOVDQU             | DATAXFER      | AVX           |               | 6f          | 0b11      | rrr       | VV1 0x6F  VL128 VF3 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=XMM_R():w:dq  REG1=XMM_B():r:dq
| VMOVDQU             | DATAXFER      | AVX           |               | 6f          | mm        | rrr       | VV1 0x6F  VL256 VF3 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=YMM_R():w:qq  MEM0:r:qq
| VMOVDQU             | DATAXFER      | AVX           |               | 6f          | 0b11      | rrr       | VV1 0x6F  VL256 VF3 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=YMM_R():w:qq  REG1=YMM_B():r:qq
| VMOVDQU             | DATAXFER      | AVX           |               | 7f          | mm        | rrr       | VV1 0x7F  VL128 VF3 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | MEM0:w:dq REG0=XMM_R():r:dq
| VMOVDQU             | DATAXFER      | AVX           |               | 7f          | 0b11      | rrr       | VV1 0x7F  VL128 VF3 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=XMM_B():w:dq REG1=XMM_R():r:dq
| VMOVDQU             | DATAXFER      | AVX           |               | 7f          | mm        | rrr       | VV1 0x7F  VL256 VF3 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | MEM0:w:qq REG0=YMM_R():r:qq
| VMOVDQU             | DATAXFER      | AVX           |               | 7f          | 0b11      | rrr       | VV1 0x7F  VL256 VF3 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=YMM_B():w:qq REG1=YMM_R():r:qq
| VMOVHLPS            | DATAXFER      | AVX           |               | 12          | 0b11      | rrr       | VV1 0x12  VL128 VNP V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
| VMOVHPD             | DATAXFER      | AVX           |               | 16          | mm        | rrr       | VV1 0x16  VL128 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:f64   REG1=XMM_N():r:q:f64   MEM0:r:q:f64
| VMOVHPD             | DATAXFER      | AVX           |               | 17          | mm        | rrr       | VV1 0x17  VL128 V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | MEM0:w:q:f64            REG0=XMM_R():r:dq:f64
| VMOVHPS             | DATAXFER      | AVX           |               | 16          | mm        | rrr       | VV1 0x16  VL128 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:f32   REG1=XMM_N():r:q:f32   MEM0:r:q:f32
| VMOVHPS             | DATAXFER      | AVX           |               | 17          | mm        | rrr       | VV1 0x17  VL128 VNP V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | MEM0:w:q:f32            REG0=XMM_R():r:dq:f32
| VMOVLHPS            | DATAXFER      | AVX           |               | 16          | 0b11      | rrr       | VV1 0x16  VL128 VNP V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:q:f32 REG2=XMM_B():r:q:f32
| VMOVLPD             | DATAXFER      | AVX           |               | 12          | mm        | rrr       | VV1 0x12  VL128 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:f64   REG1=XMM_N():r:dq:f64   MEM0:r:q:f64
| VMOVLPD             | DATAXFER      | AVX           |               | 13          | mm        | rrr       | VV1 0x13  VL128 V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | MEM0:w:q:f64            REG0=XMM_R():r:q:f64
| VMOVLPS             | DATAXFER      | AVX           |               | 12          | mm        | rrr       | VV1 0x12  VL128 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:f32   REG1=XMM_N():r:dq:f32   MEM0:r:q:f32
| VMOVLPS             | DATAXFER      | AVX           |               | 13          | mm        | rrr       | VV1 0x13  VL128 VNP V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | MEM0:w:q:f32            REG0=XMM_R():r:q:f32
| VMOVMSKPD           | DATAXFER      | AVX           |               | 50          | 0b11      | rrr       | VV1 0x50  VL128 V66 V0F  NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=GPR32_R():w:d   REG1=XMM_B():r:dq:f64
| VMOVMSKPD           | DATAXFER      | AVX           |               | 50          | 0b11      | rrr       | VV1 0x50  VL256 V66 V0F  NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=GPR32_R():w:d   REG1=YMM_B():r:qq:f64
| VMOVMSKPS           | DATAXFER      | AVX           |               | 50          | 0b11      | rrr       | VV1 0x50  VL128 VNP V0F  NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=GPR32_R():w:d   REG1=XMM_B():r:dq:f32
| VMOVMSKPS           | DATAXFER      | AVX           |               | 50          | 0b11      | rrr       | VV1 0x50  VL256 VNP V0F  NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=GPR32_R():w:d   REG1=YMM_B():r:qq:f32
| VMOVNTDQ            | DATAXFER      | AVX           |               | e7          | mm        | rrr       | VV1 0xE7  V66 V0F VL128 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | MEM0:w:dq:i32  REG0=XMM_R():r:dq:i32
| VMOVNTDQ            | DATAXFER      | AVX           |               | e7          | mm        | rrr       | VV1 0xE7  V66 V0F VL256 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | MEM0:w:qq:i32  REG0=YMM_R():r:qq:i32
| VMOVNTDQA           | DATAXFER      | AVX           |               | 2a          | mm        | rrr       | VV1 0x2A  V66 V0F38 VL128 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                             | REG0=XMM_R():w:dq MEM0:r:dq
| VMOVNTPD            | DATAXFER      | AVX           |               | 2b          | mm        | rrr       | VV1 0x2B  V66 V0F VL128 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | MEM0:w:dq:f64  REG0=XMM_R():r:dq:f64
| VMOVNTPD            | DATAXFER      | AVX           |               | 2b          | mm        | rrr       | VV1 0x2B  V66 V0F VL256 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | MEM0:w:qq:f64  REG0=YMM_R():r:qq:f64
| VMOVNTPS            | DATAXFER      | AVX           |               | 2b          | mm        | rrr       | VV1 0x2B  VNP V0F VL128 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | MEM0:w:dq:f32  REG0=XMM_R():r:dq:f32
| VMOVNTPS            | DATAXFER      | AVX           |               | 2b          | mm        | rrr       | VV1 0x2B  VNP V0F VL256 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | MEM0:w:qq:f32  REG0=YMM_R():r:qq:f32
| VMOVQ               | DATAXFER      | AVX           |               | 6e          | mm        | rrr       | VV1 0x6E  VL128 V66 V0F mode64 rexw_prefix NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()            | REG0=XMM_R():w:dq  MEM0:r:q
| VMOVQ               | DATAXFER      | AVX           |               | 6e          | 0b11      | rrr       | VV1 0x6E  VL128 V66 V0F mode64 rexw_prefix NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                   | REG0=XMM_R():w:dq  REG1=GPR64_B():r:q
| VMOVQ               | DATAXFER      | AVX           |               | 7e          | mm        | rrr       | VV1 0x7E  VL128 V66 V0F mode64 rexw_prefix NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()            | MEM0:w:q           REG0=XMM_R():r:q
| VMOVQ               | DATAXFER      | AVX           |               | 7e          | 0b11      | rrr       | VV1 0x7E  VL128 V66 V0F mode64 rexw_prefix NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                   | REG0=GPR64_B():w:q REG1=XMM_R():r:q
| VMOVQ               | DATAXFER      | AVX           |               | 7e          | mm        | rrr       | VV1 0x7E  VL128 VF3 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=XMM_R():w:dq   MEM0:r:q
| VMOVQ               | DATAXFER      | AVX           |               | 7e          | 0b11      | rrr       | VV1 0x7E  VL128 VF3 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=XMM_R():w:dq   REG1=XMM_B():r:q
| VMOVQ               | DATAXFER      | AVX           |               | d6          | mm        | rrr       | VV1 0xD6  VL128 V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | MEM0:w:q   REG0=XMM_R():r:q
| VMOVQ               | DATAXFER      | AVX           |               | d6          | 0b11      | rrr       | VV1 0xD6  VL128 V66 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=XMM_B():w:dq  REG1=XMM_R():r:q
| VMOVSD              | DATAXFER      | AVX           |               | 10          | mm        | rrr       | VV1 0x10  VF2 V0F MOD[mm] MOD!=3  NOVSR REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:f64   MEM0:r:q:f64
| VMOVSD              | DATAXFER      | AVX           |               | 10          | 0b11      | rrr       | VV1 0x10  VF2 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                  | REG0=XMM_R():w:dq:f64  REG1=XMM_N():r:dq:f64    REG2=XMM_B():r:q:f64
| VMOVSD              | DATAXFER      | AVX           |               | 11          | mm        | rrr       | VV1 0x11  VF2 V0F MOD[mm] MOD!=3 NOVSR REG[rrr] RM[nnn] MODRM()                                     | MEM0:w:q:f64           REG0=XMM_R():r:q:f64
| VMOVSD              | DATAXFER      | AVX           |               | 11          | 0b11      | rrr       | VV1 0x11  VF2 V0F  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                 | REG0=XMM_B():w:dq:f64   REG1=XMM_N():r:dq:f64  REG2=XMM_R():r:q:f64
| VMOVSHDUP           | DATAXFER      | AVX           |               | 16          | mm        | rrr       | VV1 0x16  VL128 VF3 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=XMM_R():w:dq:f32 MEM0:r:dq:f32
| VMOVSHDUP           | DATAXFER      | AVX           |               | 16          | 0b11      | rrr       | VV1 0x16  VL128 VF3 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=XMM_R():w:dq:f32 REG1=XMM_B():r:dq:f32
| VMOVSHDUP           | DATAXFER      | AVX           |               | 16          | mm        | rrr       | VV1 0x16  VL256 VF3 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=YMM_R():w:qq:f32 MEM0:r:qq:f32
| VMOVSHDUP           | DATAXFER      | AVX           |               | 16          | 0b11      | rrr       | VV1 0x16  VL256 VF3 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=YMM_R():w:qq:f32 REG1=YMM_B():r:qq:f32
| VMOVSLDUP           | DATAXFER      | AVX           |               | 12          | mm        | rrr       | VV1 0x12  VL128 VF3 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=XMM_R():w:dq:f32 MEM0:r:dq:f32
| VMOVSLDUP           | DATAXFER      | AVX           |               | 12          | 0b11      | rrr       | VV1 0x12  VL128 VF3 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=XMM_R():w:dq:f32 REG1=XMM_B():r:dq:f32
| VMOVSLDUP           | DATAXFER      | AVX           |               | 12          | mm        | rrr       | VV1 0x12  VL256 VF3 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=YMM_R():w:qq:f32 MEM0:r:qq:f32
| VMOVSLDUP           | DATAXFER      | AVX           |               | 12          | 0b11      | rrr       | VV1 0x12  VL256 VF3 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=YMM_R():w:qq:f32 REG1=YMM_B():r:qq:f32
| VMOVSS              | DATAXFER      | AVX           |               | 10          | mm        | rrr       | VV1 0x10  VF3 V0F MOD[mm] MOD!=3  NOVSR REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:f32  MEM0:r:d:f32
| VMOVSS              | DATAXFER      | AVX           |               | 10          | 0b11      | rrr       | VV1 0x10  VF3 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                  | REG0=XMM_R():w:dq:f32  REG1=XMM_N():r:dq:f32    REG2=XMM_B():r:d:f32
| VMOVSS              | DATAXFER      | AVX           |               | 11          | mm        | rrr       | VV1 0x11  VF3 V0F  MOD[mm] MOD!=3 NOVSR  REG[rrr] RM[nnn] MODRM()                                   | MEM0:w:d:f32          REG0=XMM_R():r:d:f32
| VMOVSS              | DATAXFER      | AVX           |               | 11          | 0b11      | rrr       | VV1 0x11  VF3 V0F  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                 | REG0=XMM_B():w:dq:f32   REG1=XMM_N():r:dq:f32   REG2=XMM_R():r:d:f32
| VMOVUPD             | DATAXFER      | AVX           |               | 10          | mm        | rrr       | VV1 0x10  V66 VL128 V0F NOVSR  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                              | REG0=XMM_R():w:dq:f64   MEM0:r:dq:f64
| VMOVUPD             | DATAXFER      | AVX           |               | 10          | 0b11      | rrr       | VV1 0x10  V66 VL128 V0F NOVSR  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=XMM_R():w:dq:f64   REG1=XMM_B():r:dq:f64
| VMOVUPD             | DATAXFER      | AVX           |               | 11          | mm        | rrr       | VV1 0x11  V66 VL128 V0F NOVSR  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                              | MEM0:w:dq:f64           REG0=XMM_R():r:dq:f64
| VMOVUPD             | DATAXFER      | AVX           |               | 11          | 0b11      | rrr       | VV1 0x11  V66 VL128 V0F NOVSR  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=XMM_B():w:dq:f64   REG1=XMM_R():r:dq:f64
| VMOVUPD             | DATAXFER      | AVX           |               | 10          | mm        | rrr       | VV1 0x10  V66 VL256 V0F NOVSR  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                              | REG0=YMM_R():w:qq:f64      MEM0:r:qq:f64
| VMOVUPD             | DATAXFER      | AVX           |               | 10          | 0b11      | rrr       | VV1 0x10  V66 VL256 V0F NOVSR  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=YMM_R():w:qq:f64      REG1=YMM_B():r:qq:f64
| VMOVUPD             | DATAXFER      | AVX           |               | 11          | mm        | rrr       | VV1 0x11  V66 VL256 V0F NOVSR  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                              | MEM0:w:qq:f64              REG0=YMM_R():r:qq:f64
| VMOVUPD             | DATAXFER      | AVX           |               | 11          | 0b11      | rrr       | VV1 0x11  V66 VL256 V0F NOVSR  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=YMM_B():w:qq:f64      REG1=YMM_R():r:qq:f64
| VMOVUPS             | DATAXFER      | AVX           |               | 10          | mm        | rrr       | VV1 0x10  VNP VL128 V0F NOVSR  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                              | REG0=XMM_R():w:dq:f32   MEM0:r:dq:f32
| VMOVUPS             | DATAXFER      | AVX           |               | 10          | 0b11      | rrr       | VV1 0x10  VNP VL128 V0F NOVSR  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=XMM_R():w:dq:f32   REG1=XMM_B():r:dq:f32
| VMOVUPS             | DATAXFER      | AVX           |               | 11          | mm        | rrr       | VV1 0x11  VNP VL128 V0F NOVSR  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                              | MEM0:w:dq:f32           REG0=XMM_R():r:dq:f32
| VMOVUPS             | DATAXFER      | AVX           |               | 11          | 0b11      | rrr       | VV1 0x11  VNP VL128 V0F NOVSR  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=XMM_B():w:dq:f32   REG1=XMM_R():r:dq:f32
| VMOVUPS             | DATAXFER      | AVX           |               | 10          | mm        | rrr       | VV1 0x10  VNP VL256 V0F NOVSR  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                              | REG0=YMM_R():w:qq:f32      MEM0:r:qq:f32
| VMOVUPS             | DATAXFER      | AVX           |               | 10          | 0b11      | rrr       | VV1 0x10  VNP VL256 V0F NOVSR  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=YMM_R():w:qq:f32      REG1=YMM_B():r:qq:f32
| VMOVUPS             | DATAXFER      | AVX           |               | 11          | mm        | rrr       | VV1 0x11  VNP VL256 V0F NOVSR  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                              | MEM0:w:qq:f32              REG0=YMM_R():r:qq:f32
| VMOVUPS             | DATAXFER      | AVX           |               | 11          | 0b11      | rrr       | VV1 0x11  VNP VL256 V0F NOVSR  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=YMM_B():w:qq:f32      REG1=YMM_R():r:qq:f32
| VMPSADBW            | AVX           | AVX           |               | 42          | mm        | rrr       | VV1 0x42  VL128 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                           | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u8 MEM0:r:dq:u8 IMM0:r:b
| VMPSADBW            | AVX           | AVX           |               | 42          | 0b11      | rrr       | VV1 0x42  VL128 V66 V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                  | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:u8 IMM0:r:b
| VMULPD              | AVX           | AVX           |               | 59          | mm        | rrr       | VV1 0x59  VL128 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
| VMULPD              | AVX           | AVX           |               | 59          | 0b11      | rrr       | VV1 0x59  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
| VMULPD              | AVX           | AVX           |               | 59          | mm        | rrr       | VV1 0x59  VL256 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
| VMULPD              | AVX           | AVX           |               | 59          | 0b11      | rrr       | VV1 0x59  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
| VMULPS              | AVX           | AVX           |               | 59          | mm        | rrr       | VV1 0x59  VL128 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
| VMULPS              | AVX           | AVX           |               | 59          | 0b11      | rrr       | VV1 0x59  VL128 VNP V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
| VMULPS              | AVX           | AVX           |               | 59          | mm        | rrr       | VV1 0x59  VL256 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
| VMULPS              | AVX           | AVX           |               | 59          | 0b11      | rrr       | VV1 0x59  VL256 VNP V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
| VMULSD              | AVX           | AVX           |               | 59          | mm        | rrr       | VV1 0x59  VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                           | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:q:f64
| VMULSD              | AVX           | AVX           |               | 59          | 0b11      | rrr       | VV1 0x59  VF2 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                  | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:q:f64
| VMULSS              | AVX           | AVX           |               | 59          | mm        | rrr       | VV1 0x59  VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                           | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:d:f32
| VMULSS              | AVX           | AVX           |               | 59          | 0b11      | rrr       | VV1 0x59  VF3 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                  | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:d:f32
| VORPD               | LOGICAL_FP    | AVX           |               | 56          | mm        | rrr       | VV1 0x56  VL128 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 MEM0:r:dq:u64
| VORPD               | LOGICAL_FP    | AVX           |               | 56          | 0b11      | rrr       | VV1 0x56  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 REG2=XMM_B():r:dq:u64
| VORPD               | LOGICAL_FP    | AVX           |               | 56          | mm        | rrr       | VV1 0x56  VL256 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u64 REG1=YMM_N():r:qq:u64 MEM0:r:qq:u64
| VORPD               | LOGICAL_FP    | AVX           |               | 56          | 0b11      | rrr       | VV1 0x56  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=YMM_R():w:qq:u64 REG1=YMM_N():r:qq:u64 REG2=YMM_B():r:qq:u64
| VORPS               | LOGICAL_FP    | AVX           |               | 56          | mm        | rrr       | VV1 0x56  VNP V0F VL128 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 MEM0:r:dq:u32
| VORPS               | LOGICAL_FP    | AVX           |               | 56          | 0b11      | rrr       | VV1 0x56  VNP V0F VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 REG2=XMM_B():r:dq:u32
| VORPS               | LOGICAL_FP    | AVX           |               | 56          | mm        | rrr       | VV1 0x56  VNP V0F VL256 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u32 REG1=YMM_N():r:qq:u32 MEM0:r:qq:u32
| VORPS               | LOGICAL_FP    | AVX           |               | 56          | 0b11      | rrr       | VV1 0x56  VNP V0F VL256 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=YMM_R():w:qq:u32 REG1=YMM_N():r:qq:u32 REG2=YMM_B():r:qq:u32
| VPABSB              | AVX           | AVX           |               | 1c          | mm        | rrr       | VV1 0x1C   V66 V0F38 VL128 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                            | REG0=XMM_R():w:dq:u8 MEM0:r:dq:i8
| VPABSB              | AVX           | AVX           |               | 1c          | 0b11      | rrr       | VV1 0x1C  V66 V0F38 VL128 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                    | REG0=XMM_R():w:dq:u8  REG1=XMM_B():r:dq:i8
| VPABSD              | AVX           | AVX           |               | 1e          | mm        | rrr       | VV1 0x1E   V66 V0F38 VL128 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                            | REG0=XMM_R():w:dq:u32 MEM0:r:dq:i32
| VPABSD              | AVX           | AVX           |               | 1e          | 0b11      | rrr       | VV1 0x1E  V66 V0F38 VL128 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                    | REG0=XMM_R():w:dq:u32  REG1=XMM_B():r:dq:i32
| VPABSW              | AVX           | AVX           |               | 1d          | mm        | rrr       | VV1 0x1D   V66 V0F38 VL128 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                            | REG0=XMM_R():w:dq:u16 MEM0:r:dq:i16
| VPABSW              | AVX           | AVX           |               | 1d          | 0b11      | rrr       | VV1 0x1D  V66 V0F38 VL128 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                    | REG0=XMM_R():w:dq:u16  REG1=XMM_B():r:dq:i16
| VPACKSSDW           | AVX           | AVX           |               | 6b          | mm        | rrr       | VV1 0x6B  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32
| VPACKSSDW           | AVX           | AVX           |               | 6b          | 0b11      | rrr       | VV1 0x6B  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32
| VPACKSSWB           | AVX           | AVX           |               | 63          | mm        | rrr       | VV1 0x63  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
| VPACKSSWB           | AVX           | AVX           |               | 63          | 0b11      | rrr       | VV1 0x63  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
| VPACKUSDW           | AVX           | AVX           |               | 2b          | mm        | rrr       | VV1 0x2B  V66 V0F38 VL128  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32
| VPACKUSDW           | AVX           | AVX           |               | 2b          | 0b11      | rrr       | VV1 0x2B  V66 V0F38 VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                          | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32
| VPACKUSWB           | AVX           | AVX           |               | 67          | mm        | rrr       | VV1 0x67  V66 V0F VL128  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
| VPACKUSWB           | AVX           | AVX           |               | 67          | 0b11      | rrr       | VV1 0x67  V66 V0F VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
| VPADDB              | AVX           | AVX           |               | fc          | mm        | rrr       | VV1 0xFC  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 MEM0:r:dq:i8
| VPADDB              | AVX           | AVX           |               | fc          | 0b11      | rrr       | VV1 0xFC  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 REG2=XMM_B():r:dq:i8
| VPADDD              | AVX           | AVX           |               | fe          | mm        | rrr       | VV1 0xFE  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32
| VPADDD              | AVX           | AVX           |               | fe          | 0b11      | rrr       | VV1 0xFE  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32
| VPADDQ              | AVX           | AVX           |               | d4          | mm        | rrr       | VV1 0xD4  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:i64 REG1=XMM_N():r:dq:i64 MEM0:r:dq:i64
| VPADDQ              | AVX           | AVX           |               | d4          | 0b11      | rrr       | VV1 0xD4  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:i64 REG1=XMM_N():r:dq:i64 REG2=XMM_B():r:dq:i64
| VPADDSB             | AVX           | AVX           |               | ec          | mm        | rrr       | VV1 0xEC  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 MEM0:r:dq:i8
| VPADDSB             | AVX           | AVX           |               | ec          | 0b11      | rrr       | VV1 0xEC  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 REG2=XMM_B():r:dq:i8
| VPADDSW             | AVX           | AVX           |               | ed          | mm        | rrr       | VV1 0xED  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
| VPADDSW             | AVX           | AVX           |               | ed          | 0b11      | rrr       | VV1 0xED  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
| VPADDUSB            | AVX           | AVX           |               | dc          | mm        | rrr       | VV1 0xDC  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 MEM0:r:dq:u8
| VPADDUSB            | AVX           | AVX           |               | dc          | 0b11      | rrr       | VV1 0xDC  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:u8
| VPADDUSW            | AVX           | AVX           |               | dd          | mm        | rrr       | VV1 0xDD  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 MEM0:r:dq:u16
| VPADDUSW            | AVX           | AVX           |               | dd          | 0b11      | rrr       | VV1 0xDD  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 REG2=XMM_B():r:dq:u16
| VPADDW              | AVX           | AVX           |               | fd          | mm        | rrr       | VV1 0xFD  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
| VPADDW              | AVX           | AVX           |               | fd          | 0b11      | rrr       | VV1 0xFD  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
| VPALIGNR            | AVX           | AVX           |               | 0f          | mm        | rrr       | VV1 0x0F  VL128 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                           | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 MEM0:r:dq:u8 IMM0:r:b
| VPALIGNR            | AVX           | AVX           |               | 0f          | 0b11      | rrr       | VV1 0x0F  VL128 V66 V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                  | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:u8 IMM0:r:b
| VPAND               | LOGICAL       | AVX           |               | db          | mm        | rrr       | VV1 0xDB  VL128 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u128 REG1=XMM_N():r:dq:u128 MEM0:r:dq:u128
| VPAND               | LOGICAL       | AVX           |               | db          | 0b11      | rrr       | VV1 0xDB  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u128 REG1=XMM_N():r:dq:u128 REG2=XMM_B():r:dq:u128
| VPANDN              | LOGICAL       | AVX           |               | df          | mm        | rrr       | VV1 0xDF  VL128 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u128 REG1=XMM_N():r:dq:u128 MEM0:r:dq:u128
| VPANDN              | LOGICAL       | AVX           |               | df          | 0b11      | rrr       | VV1 0xDF  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u128 REG1=XMM_N():r:dq:u128 REG2=XMM_B():r:dq:u128
| VPAVGB              | AVX           | AVX           |               | e0          | mm        | rrr       | VV1 0xE0  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 MEM0:r:dq:u8
| VPAVGB              | AVX           | AVX           |               | e0          | 0b11      | rrr       | VV1 0xE0  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:u8
| VPAVGW              | AVX           | AVX           |               | e3          | mm        | rrr       | VV1 0xE3  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 MEM0:r:dq:u16
| VPAVGW              | AVX           | AVX           |               | e3          | 0b11      | rrr       | VV1 0xE3  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 REG2=XMM_B():r:dq:u16
| VPBLENDVB           | AVX           | AVX           |               | 4c          | mm        | rrr       | VV1 0x4C   VL128 V66 V0F3A norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()          | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 MEM0:r:dq:i8 REG2=XMM_SE():r:dq:i8
| VPBLENDVB           | AVX           | AVX           |               | 4c          | 0b11      | rrr       | VV1 0x4C   VL128 V66 V0F3A norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                 | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 REG2=XMM_B():r:dq:i8 REG3=XMM_SE():r:dq:i8
| VPBLENDW            | AVX           | AVX           |               | 0e          | mm        | rrr       | VV1 0x0E  VL128 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                           | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 MEM0:r:dq:u16 IMM0:r:b
| VPBLENDW            | AVX           | AVX           |               | 0e          | 0b11      | rrr       | VV1 0x0E  VL128 V66 V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                  | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 REG2=XMM_B():r:dq:u16 IMM0:r:b
| VPCLMULQDQ          | AVX           | AVX           |               | 44          | 0b11      | rrr       | VV1 0x44  V66 V0F3A  MOD[0b11]  MOD=3  REG[rrr] RM[nnn] VL128 UIMM8()                               | REG0=XMM_R():w:dq:u128  REG1=XMM_N():r:dq:u64 REG2=XMM_B():r:dq:u64 IMM0:r:b
| VPCLMULQDQ          | AVX           | AVX           |               | 44          | mm        | rrr       | VV1 0x44  V66 V0F3A  MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() VL128 UIMM8()                         | REG0=XMM_R():w:dq:u128  REG1=XMM_N():r:dq:u64 MEM0:r:dq:u64 IMM0:r:b
| VPCMPEQB            | AVX           | AVX           |               | 74          | mm        | rrr       | VV1 0x74  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 MEM0:r:dq:u8
| VPCMPEQB            | AVX           | AVX           |               | 74          | 0b11      | rrr       | VV1 0x74  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:u8
| VPCMPEQD            | AVX           | AVX           |               | 76          | mm        | rrr       | VV1 0x76  V66 V0F VL128  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 MEM0:r:dq:u32
| VPCMPEQD            | AVX           | AVX           |               | 76          | 0b11      | rrr       | VV1 0x76  V66 V0F VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 REG2=XMM_B():r:dq:u32
| VPCMPEQQ            | AVX           | AVX           |               | 29          | mm        | rrr       | VV1 0x29  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 MEM0:r:dq:u64
| VPCMPEQQ            | AVX           | AVX           |               | 29          | 0b11      | rrr       | VV1 0x29  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                          | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 REG2=XMM_B():r:dq:u64
| VPCMPEQW            | AVX           | AVX           |               | 75          | mm        | rrr       | VV1 0x75  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 MEM0:r:dq:u16
| VPCMPEQW            | AVX           | AVX           |               | 75          | 0b11      | rrr       | VV1 0x75  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 REG2=XMM_B():r:dq:u16
| VPCMPESTRI          | STTNI         | AVX           |               | 61          | mm        | rrr       | VV1 0x61  VL128 V66 V0F3A NOVSR not64 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()               | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_EAX:r:SUPP REG2=XED_REG_EDX:r:SUPP REG3=XED_REG_ECX:w:SUPP
| VPCMPESTRI          | STTNI         | AVX           |               | 61          | 0b11      | rrr       | VV1 0x61  VL128 V66 V0F3A NOVSR not64 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                      | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_EAX:r:SUPP REG3=XED_REG_EDX:r:SUPP REG4=XED_REG_ECX:w:SUPP
| VPCMPESTRI          | STTNI         | AVX           |               | 61          | mm        | rrr       | VV1 0x61  VL128 V66 V0F3A NOVSR mode64 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()           | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_EAX:r:SUPP REG2=XED_REG_EDX:r:SUPP REG3=XED_REG_ECX:w:SUPP
| VPCMPESTRI          | STTNI         | AVX           |               | 61          | 0b11      | rrr       | VV1 0x61  VL128 V66 V0F3A NOVSR mode64 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                  | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_EAX:r:SUPP REG3=XED_REG_EDX:r:SUPP REG4=XED_REG_ECX:w:SUPP
| VPCMPESTRI64        | STTNI         | AVX           |               | 61          | mm        | rrr       | VV1 0x61  VL128 V66 V0F3A NOVSR mode64 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()           | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_RAX:r:SUPP REG2=XED_REG_RDX:r:SUPP REG3=XED_REG_RCX:w:SUPP
| VPCMPESTRI64        | STTNI         | AVX           |               | 61          | 0b11      | rrr       | VV1 0x61  VL128 V66 V0F3A NOVSR mode64 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                  | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_RAX:r:SUPP REG3=XED_REG_RDX:r:SUPP REG4=XED_REG_RCX:w:SUPP
| VPCMPESTRM          | STTNI         | AVX           |               | 60          | mm        | rrr       | VV1 0x60  VL128 V66 V0F3A NOVSR not64 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()               | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_EAX:r:SUPP REG2=XED_REG_EDX:r:SUPP REG3=XED_REG_XMM0:w:dq:SUPP
| VPCMPESTRM          | STTNI         | AVX           |               | 60          | 0b11      | rrr       | VV1 0x60  VL128 V66 V0F3A NOVSR not64 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                      | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_EAX:r:SUPP REG3=XED_REG_EDX:r:SUPP REG4=XED_REG_XMM0:w:dq:SUPP
| VPCMPESTRM          | STTNI         | AVX           |               | 60          | mm        | rrr       | VV1 0x60  VL128 V66 V0F3A NOVSR mode64 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()           | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_EAX:r:SUPP REG2=XED_REG_EDX:r:SUPP REG3=XED_REG_XMM0:w:dq:SUPP
| VPCMPESTRM          | STTNI         | AVX           |               | 60          | 0b11      | rrr       | VV1 0x60  VL128 V66 V0F3A NOVSR mode64 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                  | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_EAX:r:SUPP REG3=XED_REG_EDX:r:SUPP REG4=XED_REG_XMM0:w:dq:SUPP
| VPCMPESTRM64        | STTNI         | AVX           |               | 60          | mm        | rrr       | VV1 0x60  VL128 V66 V0F3A NOVSR mode64 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()           | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_RAX:r:SUPP REG2=XED_REG_RDX:r:SUPP REG3=XED_REG_XMM0:w:dq:SUPP
| VPCMPESTRM64        | STTNI         | AVX           |               | 60          | 0b11      | rrr       | VV1 0x60  VL128 V66 V0F3A NOVSR mode64 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                  | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_RAX:r:SUPP REG3=XED_REG_RDX:r:SUPP REG4=XED_REG_XMM0:w:dq:SUPP
| VPCMPGTB            | AVX           | AVX           |               | 64          | mm        | rrr       | VV1 0x64  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 MEM0:r:dq:i8
| VPCMPGTB            | AVX           | AVX           |               | 64          | 0b11      | rrr       | VV1 0x64  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 REG2=XMM_B():r:dq:i8
| VPCMPGTD            | AVX           | AVX           |               | 66          | mm        | rrr       | VV1 0x66  V66 V0F VL128  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32
| VPCMPGTD            | AVX           | AVX           |               | 66          | 0b11      | rrr       | VV1 0x66  V66 V0F VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32
| VPCMPGTQ            | AVX           | AVX           |               | 37          | mm        | rrr       | VV1 0x37  V66 V0F38 VL128  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i64 MEM0:r:dq:i64
| VPCMPGTQ            | AVX           | AVX           |               | 37          | 0b11      | rrr       | VV1 0x37  V66 V0F38 VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                          | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i64 REG2=XMM_B():r:dq:i64
| VPCMPGTW            | AVX           | AVX           |               | 65          | mm        | rrr       | VV1 0x65  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
| VPCMPGTW            | AVX           | AVX           |               | 65          | 0b11      | rrr       | VV1 0x65  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
| VPCMPISTRI          | STTNI         | AVX           |               | 63          | mm        | rrr       | VV1 0x63  VL128 V66 V0F3A NOVSR  not64  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()             | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_ECX:w:SUPP
| VPCMPISTRI          | STTNI         | AVX           |               | 63          | 0b11      | rrr       | VV1 0x63  VL128 V66 V0F3A NOVSR  not64  MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                    | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_ECX:w:SUPP
| VPCMPISTRI          | STTNI         | AVX           |               | 63          | mm        | rrr       | VV1 0x63  VL128 V66 V0F3A NOVSR mode64 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()           | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_ECX:w:SUPP
| VPCMPISTRI          | STTNI         | AVX           |               | 63          | 0b11      | rrr       | VV1 0x63  VL128 V66 V0F3A NOVSR mode64 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                  | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_ECX:w:SUPP
| VPCMPISTRI64        | STTNI         | AVX           |               | 63          | mm        | rrr       | VV1 0x63  VL128 V66 V0F3A NOVSR mode64 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()           | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_RCX:w:SUPP
| VPCMPISTRI64        | STTNI         | AVX           |               | 63          | 0b11      | rrr       | VV1 0x63  VL128 V66 V0F3A NOVSR mode64 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                  | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_RCX:w:SUPP
| VPCMPISTRM          | STTNI         | AVX           |               | 62          | mm        | rrr       | VV1 0x62  VL128 V66 V0F3A NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                     | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_XMM0:w:dq:SUPP
| VPCMPISTRM          | STTNI         | AVX           |               | 62          | 0b11      | rrr       | VV1 0x62  VL128 V66 V0F3A NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                            | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_XMM0:w:dq:SUPP
| VPERM2F128          | AVX           | AVX           |               | 06          | mm        | rrr       | VV1 0x06 VL256 V66 V0F3A norexw_prefix  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()             | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64 IMM0:r:b
| VPERM2F128          | AVX           | AVX           |               | 06          | 0b11      | rrr       | VV1 0x06 VL256 V66 V0F3A norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                     | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64 IMM0:r:b
| VPERMILPD           | AVX           | AVX           |               | 0d          | mm        | rrr       | VV1 0x0D VL128 V66 V0F38 norexw_prefix  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                     | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:u64
| VPERMILPD           | AVX           | AVX           |               | 0d          | 0b11      | rrr       | VV1 0x0D  VL128 V66 V0F38 norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                            | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:u64
| VPERMILPD           | AVX           | AVX           |               | 0d          | mm        | rrr       | VV1 0x0D  VL256 V66 V0F38 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                     | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:u64
| VPERMILPD           | AVX           | AVX           |               | 0d          | 0b11      | rrr       | VV1 0x0D  VL256 V66 V0F38 norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                            | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:u64
| VPERMILPD           | AVX           | AVX           |               | 05          | mm        | rrr       | VV1 0x05  VL128 V66 V0F3A norexw_prefix NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()       | REG0=XMM_R():w:dq:f64 MEM0:r:dq:f64 IMM0:r:b
| VPERMILPD           | AVX           | AVX           |               | 05          | 0b11      | rrr       | VV1 0x05  VL128 V66 V0F3A norexw_prefix NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()              | REG0=XMM_R():w:dq:f64 REG1=XMM_B():r:dq:f64 IMM0:r:b
| VPERMILPD           | AVX           | AVX           |               | 05          | mm        | rrr       | VV1 0x05  VL256 V66 V0F3A norexw_prefix NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()       | REG0=YMM_R():w:qq:f64 MEM0:r:qq:f64 IMM0:r:b
| VPERMILPD           | AVX           | AVX           |               | 05          | 0b11      | rrr       | VV1 0x05  VL256 V66 V0F3A norexw_prefix NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()              | REG0=YMM_R():w:qq:f64 REG1=YMM_B():r:qq:f64 IMM0:r:b
| VPERMILPS           | AVX           | AVX           |               | 0c          | mm        | rrr       | VV1 0x0C VL128 V66 V0F38 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                      | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:u32
| VPERMILPS           | AVX           | AVX           |               | 0c          | 0b11      | rrr       | VV1 0x0C  VL128 V66 V0F38 norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                            | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:u32
| VPERMILPS           | AVX           | AVX           |               | 0c          | mm        | rrr       | VV1 0x0C  VL256 V66 V0F38  norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                    | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:u32
| VPERMILPS           | AVX           | AVX           |               | 0c          | 0b11      | rrr       | VV1 0x0C  VL256 V66 V0F38  norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                           | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:u32
| VPERMILPS           | AVX           | AVX           |               | 04          | mm        | rrr       | VV1 0x04 VL128 V66 V0F3A norexw_prefix NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()        | REG0=XMM_R():w:dq:f32 MEM0:r:dq:f32 IMM0:r:b
| VPERMILPS           | AVX           | AVX           |               | 04          | 0b11      | rrr       | VV1 0x04 VL128 V66 V0F3A norexw_prefix NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()               | REG0=XMM_R():w:dq:f32 REG1=XMM_B():r:dq:f32 IMM0:r:b
| VPERMILPS           | AVX           | AVX           |               | 04          | mm        | rrr       | VV1 0x04 VL256 V66 V0F3A norexw_prefix NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()        | REG0=YMM_R():w:qq:f32 MEM0:r:qq:f32 IMM0:r:b
| VPERMILPS           | AVX           | AVX           |               | 04          | 0b11      | rrr       | VV1 0x04 VL256 V66 V0F3A norexw_prefix NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()               | REG0=YMM_R():w:qq:f32 REG1=YMM_B():r:qq:f32 IMM0:r:b
| VPEXTRB             | AVX           | AVX           |               | 14          | mm        | rrr       | VV1 0x14  VL128 V66 V0F3A NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                     | MEM0:w:b           REG0=XMM_R():r:dq:u8 IMM0:r:b
| VPEXTRB             | AVX           | AVX           |               | 14          | 0b11      | rrr       | VV1 0x14  VL128 V66 V0F3A NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                            | REG0=GPR32_B():w:d REG1=XMM_R():r:dq:u8 IMM0:r:b
| VPEXTRD             | AVX           | AVX           |               | 16          | mm        | rrr       | VV1 0x16 VL128 V66 V0F3A mode64 norexw_prefix NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() | MEM0:w:d REG0=XMM_R():r:dq:u32 IMM0:r:b
| VPEXTRD             | AVX           | AVX           |               | 16          | 0b11      | rrr       | VV1 0x16 VL128 V66 V0F3A mode64 norexw_prefix NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()        | REG0=GPR32_B():w:d REG1=XMM_R():r:dq:u32 IMM0:r:b
| VPEXTRD             | AVX           | AVX           |               | 16          | mm        | rrr       | VV1 0x16 VL128 V66 V0F3A not64  NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()               | MEM0:w:d REG0=XMM_R():r:dq:u32 IMM0:r:b
| VPEXTRD             | AVX           | AVX           |               | 16          | 0b11      | rrr       | VV1 0x16 VL128 V66 V0F3A not64  NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                      | REG0=GPR32_B():w:d REG1=XMM_R():r:dq:u32 IMM0:r:b
| VPEXTRQ             | AVX           | AVX           |               | 16          | mm        | rrr       | VV1 0x16  VL128 V66 V0F3A mode64 rexw_prefix  NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() | MEM0:w:q              REG0=XMM_R():r:dq:u64 IMM0:r:b
| VPEXTRQ             | AVX           | AVX           |               | 16          | 0b11      | rrr       | VV1 0x16  VL128 V66 V0F3A mode64 rexw_prefix NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()         | REG0=GPR64_B():w:q    REG1=XMM_R():r:dq:u64 IMM0:r:b
| VPEXTRW             | AVX           | AVX           |               | 15          | mm        | rrr       | VV1 0x15  VL128 V66 V0F3A NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                     | MEM0:w:w           REG0=XMM_R():r:dq:u16 IMM0:r:b
| VPEXTRW             | AVX           | AVX           |               | 15          | 0b11      | rrr       | VV1 0x15  VL128 V66 V0F3A NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                            | REG0=GPR32_B():w:d REG1=XMM_R():r:dq:u16 IMM0:r:b
| VPEXTRW             | AVX           | AVX           |               | c5          | 0b11      | rrr       | VV1 0xC5  VL128 V66 V0F  NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                             | REG0=GPR32_R():w:d    REG1=XMM_B():r:dq:u16 IMM0:r:b
| VPHADDD             | AVX           | AVX           |               | 02          | mm        | rrr       | VV1 0x02  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32
| VPHADDD             | AVX           | AVX           |               | 02          | 0b11      | rrr       | VV1 0x02  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                          | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32
| VPHADDSW            | AVX           | AVX           |               | 03          | mm        | rrr       | VV1 0x03  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
| VPHADDSW            | AVX           | AVX           |               | 03          | 0b11      | rrr       | VV1 0x03  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                          | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
| VPHADDW             | AVX           | AVX           |               | 01          | mm        | rrr       | VV1 0x01  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
| VPHADDW             | AVX           | AVX           |               | 01          | 0b11      | rrr       | VV1 0x01  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                          | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
| VPHMINPOSUW         | AVX           | AVX           |               | 41          | mm        | rrr       | VV1 0x41   V66 V0F38 VL128 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                            | REG0=XMM_R():w:dq:u16 MEM0:r:dq:u16
| VPHMINPOSUW         | AVX           | AVX           |               | 41          | 0b11      | rrr       | VV1 0x41  V66 V0F38 VL128 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                    | REG0=XMM_R():w:dq:u16  REG1=XMM_B():r:dq:u16
| VPHSUBD             | AVX           | AVX           |               | 06          | mm        | rrr       | VV1 0x06  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32
| VPHSUBD             | AVX           | AVX           |               | 06          | 0b11      | rrr       | VV1 0x06  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                          | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32
| VPHSUBSW            | AVX           | AVX           |               | 07          | mm        | rrr       | VV1 0x07  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
| VPHSUBSW            | AVX           | AVX           |               | 07          | 0b11      | rrr       | VV1 0x07  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                          | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
| VPHSUBW             | AVX           | AVX           |               | 05          | mm        | rrr       | VV1 0x05  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
| VPHSUBW             | AVX           | AVX           |               | 05          | 0b11      | rrr       | VV1 0x05  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                          | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
| VPINSRB             | AVX           | AVX           |               | 20          | mm        | rrr       | VV1 0x20  VL128 V66 V0F3A  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                          | REG0=XMM_R():w:dq:u8     REG1=XMM_N():r:dq:u8  MEM0:r:b:u8            IMM0:r:b
| VPINSRB             | AVX           | AVX           |               | 20          | 0b11      | rrr       | VV1 0x20  VL128 V66 V0F3A  MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                 | REG0=XMM_R():w:dq:u8     REG1=XMM_N():r:dq:u8  REG2=GPR32_B():r:d:u8  IMM0:r:b
| VPINSRD             | AVX           | AVX           |               | 22          | mm        | rrr       | VV1 0x22  VL128 V66 V0F3A mode64 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()      | REG0=XMM_R():w:dq:u32     REG1=XMM_N():r:dq:u32  MEM0:r:d:u32            IMM0:r:b
| VPINSRD             | AVX           | AVX           |               | 22          | 0b11      | rrr       | VV1 0x22  VL128 V66 V0F3A mode64 norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()             | REG0=XMM_R():w:dq:u32     REG1=XMM_N():r:dq:u32  REG2=GPR32_B():r:d:u32  IMM0:r:b
| VPINSRD             | AVX           | AVX           |               | 22          | mm        | rrr       | VV1 0x22  VL128 V66 V0F3A not64 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                     | REG0=XMM_R():w:dq:u32     REG1=XMM_N():r:dq:u32  MEM0:r:d:u32            IMM0:r:b
| VPINSRD             | AVX           | AVX           |               | 22          | 0b11      | rrr       | VV1 0x22  VL128 V66 V0F3A not64 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                            | REG0=XMM_R():w:dq:u32     REG1=XMM_N():r:dq:u32  REG2=GPR32_B():r:d:u32  IMM0:r:b
| VPINSRQ             | AVX           | AVX           |               | 22          | mm        | rrr       | VV1 0x22  VL128 V66 V0F3A mode64 rexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()        | REG0=XMM_R():w:dq:u64     REG1=XMM_N():r:dq:u64  MEM0:r:q:u64            IMM0:r:b
| VPINSRQ             | AVX           | AVX           |               | 22          | 0b11      | rrr       | VV1 0x22  VL128 V66 V0F3A mode64 rexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()               | REG0=XMM_R():w:dq:u64     REG1=XMM_N():r:dq:u64  REG2=GPR64_B():r:q:u64  IMM0:r:b
| VPINSRW             | AVX           | AVX           |               | c4          | mm        | rrr       | VV1 0xC4  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                            | REG0=XMM_R():w:dq:u16     REG1=XMM_N():r:dq:u16  MEM0:r:w:u16           IMM0:r:b
| VPINSRW             | AVX           | AVX           |               | c4          | 0b11      | rrr       | VV1 0xC4  VL128 V66 V0F  MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                   | REG0=XMM_R():w:dq:u16     REG1=XMM_N():r:dq:u16  REG2=GPR32_B():r:d:u16  IMM0:r:b
| VPMADDUBSW          | AVX           | AVX           |               | 04          | mm        | rrr       | VV1 0x04  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:u8 MEM0:r:dq:i8
| VPMADDUBSW          | AVX           | AVX           |               | 04          | 0b11      | rrr       | VV1 0x04  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                          | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:i8
| VPMADDWD            | AVX           | AVX           |               | f5          | mm        | rrr       | VV1 0xF5  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
| VPMADDWD            | AVX           | AVX           |               | f5          | 0b11      | rrr       | VV1 0xF5  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
| VPMAXSB             | AVX           | AVX           |               | 3c          | mm        | rrr       | VV1 0x3C  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 MEM0:r:dq:i8
| VPMAXSB             | AVX           | AVX           |               | 3c          | 0b11      | rrr       | VV1 0x3C  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                          | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 REG2=XMM_B():r:dq:i8
| VPMAXSD             | AVX           | AVX           |               | 3d          | mm        | rrr       | VV1 0x3D  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32
| VPMAXSD             | AVX           | AVX           |               | 3d          | 0b11      | rrr       | VV1 0x3D  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                          | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32
| VPMAXSW             | AVX           | AVX           |               | ee          | mm        | rrr       | VV1 0xEE  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
| VPMAXSW             | AVX           | AVX           |               | ee          | 0b11      | rrr       | VV1 0xEE  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
| VPMAXUB             | AVX           | AVX           |               | de          | mm        | rrr       | VV1 0xDE  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 MEM0:r:dq:u8
| VPMAXUB             | AVX           | AVX           |               | de          | 0b11      | rrr       | VV1 0xDE  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:u8
| VPMAXUD             | AVX           | AVX           |               | 3f          | mm        | rrr       | VV1 0x3F  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 MEM0:r:dq:u32
| VPMAXUD             | AVX           | AVX           |               | 3f          | 0b11      | rrr       | VV1 0x3F  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                          | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 REG2=XMM_B():r:dq:u32
| VPMAXUW             | AVX           | AVX           |               | 3e          | mm        | rrr       | VV1 0x3E  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 MEM0:r:dq:u16
| VPMAXUW             | AVX           | AVX           |               | 3e          | 0b11      | rrr       | VV1 0x3E  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                          | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 REG2=XMM_B():r:dq:u16
| VPMINSB             | AVX           | AVX           |               | 38          | mm        | rrr       | VV1 0x38  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 MEM0:r:dq:i8
| VPMINSB             | AVX           | AVX           |               | 38          | 0b11      | rrr       | VV1 0x38  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                          | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 REG2=XMM_B():r:dq:i8
| VPMINSD             | AVX           | AVX           |               | 39          | mm        | rrr       | VV1 0x39  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32
| VPMINSD             | AVX           | AVX           |               | 39          | 0b11      | rrr       | VV1 0x39  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                          | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32
| VPMINSW             | AVX           | AVX           |               | ea          | mm        | rrr       | VV1 0xEA  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
| VPMINSW             | AVX           | AVX           |               | ea          | 0b11      | rrr       | VV1 0xEA  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
| VPMINUB             | AVX           | AVX           |               | da          | mm        | rrr       | VV1 0xDA  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 MEM0:r:dq:u8
| VPMINUB             | AVX           | AVX           |               | da          | 0b11      | rrr       | VV1 0xDA  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:u8
| VPMINUD             | AVX           | AVX           |               | 3b          | mm        | rrr       | VV1 0x3B  V66 V0F38 VL128  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 MEM0:r:dq:u32
| VPMINUD             | AVX           | AVX           |               | 3b          | 0b11      | rrr       | VV1 0x3B  V66 V0F38 VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                          | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 REG2=XMM_B():r:dq:u32
| VPMINUW             | AVX           | AVX           |               | 3a          | mm        | rrr       | VV1 0x3A  V66 V0F38 VL128  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 MEM0:r:dq:u16
| VPMINUW             | AVX           | AVX           |               | 3a          | 0b11      | rrr       | VV1 0x3A  V66 V0F38 VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                          | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 REG2=XMM_B():r:dq:u16
| VPMOVMSKB           | AVX           | AVX           |               | d7          | 0b11      | rrr       | VV1 0xD7  VL128 V66 V0F  NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                     | REG0=GPR32_R():w:d:u32   REG1=XMM_B():r:dq:i8
| VPMOVSXBD           | AVX           | AVX           |               | 21          | 0b11      | rrr       | VV1 0x21  VL128 V66 V0F38 NOVSR  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                   | REG0=XMM_R():w:dq:i32   REG1=XMM_B():r:d:i8
| VPMOVSXBD           | AVX           | AVX           |               | 21          | mm        | rrr       | VV1 0x21  VL128 V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                             | REG0=XMM_R():w:dq:i32   MEM0:r:d:i8
| VPMOVSXBQ           | AVX           | AVX           |               | 22          | 0b11      | rrr       | VV1 0x22  VL128 V66 V0F38 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                    | REG0=XMM_R():w:dq:i64   REG1=XMM_B():r:w:i8
| VPMOVSXBQ           | AVX           | AVX           |               | 22          | mm        | rrr       | VV1 0x22  VL128 V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                             | REG0=XMM_R():w:dq:i64   MEM0:r:w:i8
| VPMOVSXBW           | AVX           | AVX           |               | 20          | 0b11      | rrr       | VV1 0x20  VL128 V66 V0F38 NOVSR  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                   | REG0=XMM_R():w:dq:i16   REG1=XMM_B():r:q:i8
| VPMOVSXBW           | AVX           | AVX           |               | 20          | mm        | rrr       | VV1 0x20  VL128 V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                             | REG0=XMM_R():w:dq:i16  MEM0:r:q:i8
| VPMOVSXDQ           | AVX           | AVX           |               | 25          | 0b11      | rrr       | VV1 0x25  VL128 V66 V0F38 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                    | REG0=XMM_R():w:dq:i64   REG1=XMM_B():r:q:i32
| VPMOVSXDQ           | AVX           | AVX           |               | 25          | mm        | rrr       | VV1 0x25  VL128 V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                             | REG0=XMM_R():w:dq:i64   MEM0:r:q:i32
| VPMOVSXWD           | AVX           | AVX           |               | 23          | 0b11      | rrr       | VV1 0x23  VL128 V66 V0F38 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                    | REG0=XMM_R():w:dq:i32   REG1=XMM_B():r:q:i16
| VPMOVSXWD           | AVX           | AVX           |               | 23          | mm        | rrr       | VV1 0x23  VL128 V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                             | REG0=XMM_R():w:dq:i32   MEM0:r:q:i16
| VPMOVSXWQ           | AVX           | AVX           |               | 24          | 0b11      | rrr       | VV1 0x24  VL128 V66 V0F38 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                    | REG0=XMM_R():w:dq:i64   REG1=XMM_B():r:d:i16
| VPMOVSXWQ           | AVX           | AVX           |               | 24          | mm        | rrr       | VV1 0x24  VL128 V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                             | REG0=XMM_R():w:dq:i64   MEM0:r:d:i16
| VPMOVZXBD           | AVX           | AVX           |               | 31          | 0b11      | rrr       | VV1 0x31  VL128 V66 V0F38 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                    | REG0=XMM_R():w:dq:u32   REG1=XMM_B():r:d:u8
| VPMOVZXBD           | AVX           | AVX           |               | 31          | mm        | rrr       | VV1 0x31  VL128 V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                             | REG0=XMM_R():w:dq:u32   MEM0:r:d:u8
| VPMOVZXBQ           | AVX           | AVX           |               | 32          | 0b11      | rrr       | VV1 0x32  V66 V0F38 VL128 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                    | REG0=XMM_R():w:dq:u64   REG1=XMM_B():r:w:u8
| VPMOVZXBQ           | AVX           | AVX           |               | 32          | mm        | rrr       | VV1 0x32  V66 V0F38 VL128 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                             | REG0=XMM_R():w:dq:u64   MEM0:r:w:u8
| VPMOVZXBW           | AVX           | AVX           |               | 30          | 0b11      | rrr       | VV1 0x30  VL128 V66 V0F38 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                    | REG0=XMM_R():w:dq:u16   REG1=XMM_B():r:q:u8
| VPMOVZXBW           | AVX           | AVX           |               | 30          | mm        | rrr       | VV1 0x30  VL128 V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                             | REG0=XMM_R():w:dq:u16   MEM0:r:q:u8
| VPMOVZXDQ           | AVX           | AVX           |               | 35          | 0b11      | rrr       | VV1 0x35  VL128 V66 V0F38 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                    | REG0=XMM_R():w:dq:u64   REG1=XMM_B():r:q:u32
| VPMOVZXDQ           | AVX           | AVX           |               | 35          | mm        | rrr       | VV1 0x35  VL128 V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                             | REG0=XMM_R():w:dq:u64   MEM0:r:q:u32
| VPMOVZXWD           | AVX           | AVX           |               | 33          | 0b11      | rrr       | VV1 0x33  V66 V0F38 VL128 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                    | REG0=XMM_R():w:dq:u32   REG1=XMM_B():r:q:u16
| VPMOVZXWD           | AVX           | AVX           |               | 33          | mm        | rrr       | VV1 0x33  V66 V0F38 VL128 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                             | REG0=XMM_R():w:dq:u32   MEM0:r:q:u16
| VPMOVZXWQ           | AVX           | AVX           |               | 34          | 0b11      | rrr       | VV1 0x34  VL128 V66 V0F38 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                    | REG0=XMM_R():w:dq:u64   REG1=XMM_B():r:d:u16
| VPMOVZXWQ           | AVX           | AVX           |               | 34          | mm        | rrr       | VV1 0x34  VL128 V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                             | REG0=XMM_R():w:dq:u64   MEM0:r:d:u16
| VPMULDQ             | AVX           | AVX           |               | 28          | mm        | rrr       | VV1 0x28  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():w:dq:i64 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32
| VPMULDQ             | AVX           | AVX           |               | 28          | 0b11      | rrr       | VV1 0x28  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                          | REG0=XMM_R():w:dq:i64 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32
| VPMULHRSW           | AVX           | AVX           |               | 0b          | mm        | rrr       | VV1 0x0B  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
| VPMULHRSW           | AVX           | AVX           |               | 0b          | 0b11      | rrr       | VV1 0x0B  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                          | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
| VPMULHUW            | AVX           | AVX           |               | e4          | mm        | rrr       | VV1 0xE4  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 MEM0:r:dq:u16
| VPMULHUW            | AVX           | AVX           |               | e4          | 0b11      | rrr       | VV1 0xE4  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 REG2=XMM_B():r:dq:u16
| VPMULHW             | AVX           | AVX           |               | e5          | mm        | rrr       | VV1 0xE5  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
| VPMULHW             | AVX           | AVX           |               | e5          | 0b11      | rrr       | VV1 0xE5  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
| VPMULLD             | AVX           | AVX           |               | 40          | mm        | rrr       | VV1 0x40  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32
| VPMULLD             | AVX           | AVX           |               | 40          | 0b11      | rrr       | VV1 0x40  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                          | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32
| VPMULLW             | AVX           | AVX           |               | d5          | mm        | rrr       | VV1 0xD5  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
| VPMULLW             | AVX           | AVX           |               | d5          | 0b11      | rrr       | VV1 0xD5  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
| VPMULUDQ            | AVX           | AVX           |               | f4          | mm        | rrr       | VV1 0xF4  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u32 MEM0:r:dq:u32
| VPMULUDQ            | AVX           | AVX           |               | f4          | 0b11      | rrr       | VV1 0xF4  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u32 REG2=XMM_B():r:dq:u32
| VPOR                | LOGICAL       | AVX           |               | eb          | mm        | rrr       | VV1 0xEB  VL128 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u128 REG1=XMM_N():r:dq:u128 MEM0:r:dq:u128
| VPOR                | LOGICAL       | AVX           |               | eb          | 0b11      | rrr       | VV1 0xEB  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u128 REG1=XMM_N():r:dq:u128 REG2=XMM_B():r:dq:u128
| VPSADBW             | AVX           | AVX           |               | f6          | mm        | rrr       | VV1 0xF6  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u8 MEM0:r:dq:u8
| VPSADBW             | AVX           | AVX           |               | f6          | 0b11      | rrr       | VV1 0xF6  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:u8
| VPSHUFB             | AVX           | AVX           |               | 00          | mm        | rrr       | VV1 0x00  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 MEM0:r:dq:u8
| VPSHUFB             | AVX           | AVX           |               | 00          | 0b11      | rrr       | VV1 0x00  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                          | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:u8
| VPSHUFD             | AVX           | AVX           |               | 70          | mm        | rrr       | VV1 0x70  VL128 V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                       | REG0=XMM_R():w:dq MEM0:r:dq  IMM0:r:b
| VPSHUFD             | AVX           | AVX           |               | 70          | 0b11      | rrr       | VV1 0x70  VL128 V66 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                              | REG0=XMM_R():w:dq REG1=XMM_B():r:dq IMM0:r:b
| VPSHUFHW            | AVX           | AVX           |               | 70          | mm        | rrr       | VV1 0x70  VL128 VF3 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                       | REG0=XMM_R():w:dq MEM0:r:dq  IMM0:r:b
| VPSHUFHW            | AVX           | AVX           |               | 70          | 0b11      | rrr       | VV1 0x70  VL128 VF3 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                              | REG0=XMM_R():w:dq REG1=XMM_B():r:dq IMM0:r:b
| VPSHUFLW            | AVX           | AVX           |               | 70          | mm        | rrr       | VV1 0x70  VL128 VF2 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                       | REG0=XMM_R():w:dq MEM0:r:dq  IMM0:r:b
| VPSHUFLW            | AVX           | AVX           |               | 70          | 0b11      | rrr       | VV1 0x70  VL128 VF2 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                              | REG0=XMM_R():w:dq REG1=XMM_B():r:dq IMM0:r:b
| VPSIGNB             | AVX           | AVX           |               | 08          | mm        | rrr       | VV1 0x08  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 MEM0:r:dq:i8
| VPSIGNB             | AVX           | AVX           |               | 08          | 0b11      | rrr       | VV1 0x08  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                          | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 REG2=XMM_B():r:dq:i8
| VPSIGND             | AVX           | AVX           |               | 0a          | mm        | rrr       | VV1 0x0A  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32
| VPSIGND             | AVX           | AVX           |               | 0a          | 0b11      | rrr       | VV1 0x0A  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                          | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32
| VPSIGNW             | AVX           | AVX           |               | 09          | mm        | rrr       | VV1 0x09  VL128 V66 V0F38  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                  | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
| VPSIGNW             | AVX           | AVX           |               | 09          | 0b11      | rrr       | VV1 0x09  VL128 V66 V0F38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                          | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
| VPSLLD              | AVX           | AVX           |               | f2          | mm        | rrr       | VV1 0xF2  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 MEM0:r:dq:u64
| VPSLLD              | AVX           | AVX           |               | f2          | 0b11      | rrr       | VV1 0xF2  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 REG2=XMM_B():r:dq:u64
| VPSLLD              | AVX           | AVX           |               | 72          | 0b11      | 0b110     | VV1 0x72  VL128 V66 V0F MOD[0b11] MOD=3 REG[0b110] RM[nnn] UIMM8()                                  | REG0=XMM_N():w:dq:u32 REG1=XMM_B():r:dq:u32 IMM0:r:b  #NDD
| VPSLLDQ             | AVX           | AVX           |               | 73          | 0b11      | 0b111     | VV1 0x73  VL128 V66 V0F MOD[0b11] MOD=3 REG[0b111] RM[nnn] UIMM8()                                  | REG0=XMM_N():w:dq:u128 REG1=XMM_B():r:dq:u128 IMM0:r:b   # NDD
| VPSLLQ              | AVX           | AVX           |               | f3          | mm        | rrr       | VV1 0xF3  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 MEM0:r:dq:u64
| VPSLLQ              | AVX           | AVX           |               | f3          | 0b11      | rrr       | VV1 0xF3  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 REG2=XMM_B():r:dq:u64
| VPSLLQ              | AVX           | AVX           |               | 73          | 0b11      | 0b110     | VV1 0x73  VL128 V66 V0F MOD[0b11] MOD=3 REG[0b110] RM[nnn] UIMM8()                                  | REG0=XMM_N():w:dq:u64 REG1=XMM_B():r:dq:u64 IMM0:r:b # NDD
| VPSLLW              | AVX           | AVX           |               | f1          | mm        | rrr       | VV1 0xF1  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 MEM0:r:dq:u64
| VPSLLW              | AVX           | AVX           |               | f1          | 0b11      | rrr       | VV1 0xF1  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 REG2=XMM_B():r:dq:u64
| VPSLLW              | AVX           | AVX           |               | 71          | 0b11      | 0b110     | VV1 0x71  VL128 V66 V0F MOD[0b11] MOD=3 REG[0b110] RM[nnn] UIMM8()                                  | REG0=XMM_N():w:dq:u16 REG1=XMM_B():r:dq:u16 IMM0:r:b # NDD
| VPSRAD              | AVX           | AVX           |               | e2          | mm        | rrr       | VV1 0xE2  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 MEM0:r:dq:u64
| VPSRAD              | AVX           | AVX           |               | e2          | 0b11      | rrr       | VV1 0xE2  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:u64
| VPSRAD              | AVX           | AVX           |               | 72          | 0b11      | 0b100     | VV1 0x72  VL128 V66 V0F MOD[0b11] MOD=3 REG[0b100] RM[nnn] UIMM8()                                  | REG0=XMM_N():w:dq:i32 REG1=XMM_B():r:dq:i32 IMM0:r:b # NDD
| VPSRAW              | AVX           | AVX           |               | e1          | mm        | rrr       | VV1 0xE1  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:u64
| VPSRAW              | AVX           | AVX           |               | e1          | 0b11      | rrr       | VV1 0xE1  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:u64
| VPSRAW              | AVX           | AVX           |               | 71          | 0b11      | 0b100     | VV1 0x71  VL128 V66 V0F MOD[0b11] MOD=3 REG[0b100] RM[nnn] UIMM8()                                  | REG0=XMM_N():w:dq:i16 REG1=XMM_B():r:dq:i16 IMM0:r:b # NDD
| VPSRLD              | AVX           | AVX           |               | d2          | mm        | rrr       | VV1 0xD2  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 MEM0:r:dq:u64
| VPSRLD              | AVX           | AVX           |               | d2          | 0b11      | rrr       | VV1 0xD2  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 REG2=XMM_B():r:dq:u64
| VPSRLD              | AVX           | AVX           |               | 72          | 0b11      | 0b010     | VV1 0x72  VL128 V66 V0F MOD[0b11] MOD=3 REG[0b010] RM[nnn] UIMM8()                                  | REG0=XMM_N():w:dq:u32 REG1=XMM_B():r:dq:u32 IMM0:r:b # NDD
| VPSRLDQ             | AVX           | AVX           |               | 73          | 0b11      | 0b011     | VV1 0x73  VL128 V66 V0F MOD[0b11] MOD=3 REG[0b011] RM[nnn] UIMM8()                                  | REG0=XMM_N():w:dq:u128 REG1=XMM_B():r:dq:u128 IMM0:r:b   # NDD
| VPSRLQ              | AVX           | AVX           |               | d3          | mm        | rrr       | VV1 0xD3  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 MEM0:r:dq:u64
| VPSRLQ              | AVX           | AVX           |               | d3          | 0b11      | rrr       | VV1 0xD3  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 REG2=XMM_B():r:dq:u64
| VPSRLQ              | AVX           | AVX           |               | 73          | 0b11      | 0b010     | VV1 0x73  VL128 V66 V0F MOD[0b11] MOD=3 REG[0b010] RM[nnn] UIMM8()                                  | REG0=XMM_N():w:dq:u64 REG1=XMM_B():r:dq:u64 IMM0:r:b  # NDD
| VPSRLW              | AVX           | AVX           |               | d1          | mm        | rrr       | VV1 0xD1  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 MEM0:r:dq:u64
| VPSRLW              | AVX           | AVX           |               | d1          | 0b11      | rrr       | VV1 0xD1  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 REG2=XMM_B():r:dq:u64
| VPSRLW              | AVX           | AVX           |               | 71          | 0b11      | 0b010     | VV1 0x71  VL128 V66 V0F MOD[0b11] MOD=3 REG[0b010] RM[nnn] UIMM8()                                  | REG0=XMM_N():w:dq:u16 REG1=XMM_B():r:dq:u16 IMM0:r:b # NDD
| VPSUBB              | AVX           | AVX           |               | f8          | mm        | rrr       | VV1 0xF8  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 MEM0:r:dq:i8
| VPSUBB              | AVX           | AVX           |               | f8          | 0b11      | rrr       | VV1 0xF8  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 REG2=XMM_B():r:dq:i8
| VPSUBD              | AVX           | AVX           |               | fa          | mm        | rrr       | VV1 0xFA  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32
| VPSUBD              | AVX           | AVX           |               | fa          | 0b11      | rrr       | VV1 0xFA  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32
| VPSUBQ              | AVX           | AVX           |               | fb          | mm        | rrr       | VV1 0xFB  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:i64 REG1=XMM_N():r:dq:i64 MEM0:r:dq:i64
| VPSUBQ              | AVX           | AVX           |               | fb          | 0b11      | rrr       | VV1 0xFB  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:i64 REG1=XMM_N():r:dq:i64 REG2=XMM_B():r:dq:i64
| VPSUBSB             | AVX           | AVX           |               | e8          | mm        | rrr       | VV1 0xE8  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 MEM0:r:dq:i8
| VPSUBSB             | AVX           | AVX           |               | e8          | 0b11      | rrr       | VV1 0xE8  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 REG2=XMM_B():r:dq:i8
| VPSUBSW             | AVX           | AVX           |               | e9          | mm        | rrr       | VV1 0xE9  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
| VPSUBSW             | AVX           | AVX           |               | e9          | 0b11      | rrr       | VV1 0xE9  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
| VPSUBUSB            | AVX           | AVX           |               | d8          | mm        | rrr       | VV1 0xD8  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 MEM0:r:dq:u8
| VPSUBUSB            | AVX           | AVX           |               | d8          | 0b11      | rrr       | VV1 0xD8  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:u8
| VPSUBUSW            | AVX           | AVX           |               | d9          | mm        | rrr       | VV1 0xD9  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 MEM0:r:dq:u16
| VPSUBUSW            | AVX           | AVX           |               | d9          | 0b11      | rrr       | VV1 0xD9  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 REG2=XMM_B():r:dq:u16
| VPSUBW              | AVX           | AVX           |               | f9          | mm        | rrr       | VV1 0xF9  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
| VPSUBW              | AVX           | AVX           |               | f9          | 0b11      | rrr       | VV1 0xF9  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
| VPTEST              | LOGICAL       | AVX           |               | 17          | mm        | rrr       | VV1 0x17  VL128 V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                             | REG0=XMM_R():r:dq MEM0:r:dq
| VPTEST              | LOGICAL       | AVX           |               | 17          | 0b11      | rrr       | VV1 0x17  VL128 V66 V0F38 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                    | REG0=XMM_R():r:dq REG1=XMM_B():r:dq
| VPTEST              | LOGICAL       | AVX           |               | 17          | mm        | rrr       | VV1 0x17  VL256 V66 V0F38 NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                             | REG0=YMM_R():r:qq MEM0:r:qq
| VPTEST              | LOGICAL       | AVX           |               | 17          | 0b11      | rrr       | VV1 0x17  VL256 V66 V0F38 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                    | REG0=YMM_R():r:qq REG1=YMM_B():r:qq
| VPUNPCKHBW          | AVX           | AVX           |               | 68          | mm        | rrr       | VV1 0x68  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 MEM0:r:dq:u8
| VPUNPCKHBW          | AVX           | AVX           |               | 68          | 0b11      | rrr       | VV1 0x68  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:u8
| VPUNPCKHDQ          | AVX           | AVX           |               | 6a          | mm        | rrr       | VV1 0x6A  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 MEM0:r:dq:u32
| VPUNPCKHDQ          | AVX           | AVX           |               | 6a          | 0b11      | rrr       | VV1 0x6A  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 REG2=XMM_B():r:dq:u32
| VPUNPCKHQDQ         | AVX           | AVX           |               | 6d          | mm        | rrr       | VV1 0x6D  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 MEM0:r:dq:u64
| VPUNPCKHQDQ         | AVX           | AVX           |               | 6d          | 0b11      | rrr       | VV1 0x6D  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 REG2=XMM_B():r:dq:u64
| VPUNPCKHWD          | AVX           | AVX           |               | 69          | mm        | rrr       | VV1 0x69  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 MEM0:r:dq:u16
| VPUNPCKHWD          | AVX           | AVX           |               | 69          | 0b11      | rrr       | VV1 0x69  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 REG2=XMM_B():r:dq:u16
| VPUNPCKLBW          | AVX           | AVX           |               | 60          | mm        | rrr       | VV1 0x60  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 MEM0:r:dq:u8
| VPUNPCKLBW          | AVX           | AVX           |               | 60          | 0b11      | rrr       | VV1 0x60  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:u8
| VPUNPCKLDQ          | AVX           | AVX           |               | 62          | mm        | rrr       | VV1 0x62  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 MEM0:r:dq:u32
| VPUNPCKLDQ          | AVX           | AVX           |               | 62          | 0b11      | rrr       | VV1 0x62  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 REG2=XMM_B():r:dq:u32
| VPUNPCKLQDQ         | AVX           | AVX           |               | 6c          | mm        | rrr       | VV1 0x6C  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 MEM0:r:dq:u64
| VPUNPCKLQDQ         | AVX           | AVX           |               | 6c          | 0b11      | rrr       | VV1 0x6C  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 REG2=XMM_B():r:dq:u64
| VPUNPCKLWD          | AVX           | AVX           |               | 61          | mm        | rrr       | VV1 0x61  VL128 V66 V0F  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                    | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 MEM0:r:dq:u16
| VPUNPCKLWD          | AVX           | AVX           |               | 61          | 0b11      | rrr       | VV1 0x61  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 REG2=XMM_B():r:dq:u16
| VPXOR               | LOGICAL       | AVX           |               | ef          | mm        | rrr       | VV1 0xEF  VL128 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u128 REG1=XMM_N():r:dq:u128 MEM0:r:dq:u128
| VPXOR               | LOGICAL       | AVX           |               | ef          | 0b11      | rrr       | VV1 0xEF  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u128 REG1=XMM_N():r:dq:u128 REG2=XMM_B():r:dq:u128
| VRCPPS              | AVX           | AVX           |               | 53          | mm        | rrr       | VV1 0x53  VNP VL128 NOVSR V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=XMM_R():w:dq:f32  MEM0:r:dq:f32
| VRCPPS              | AVX           | AVX           |               | 53          | 0b11      | rrr       | VV1 0x53  VNP VL128 NOVSR V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=XMM_R():w:dq:f32  REG1=XMM_B():r:dq:f32
| VRCPPS              | AVX           | AVX           |               | 53          | mm        | rrr       | VV1 0x53  VNP VL256 NOVSR V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=YMM_R():w:qq:f32  MEM0:r:qq:f32
| VRCPPS              | AVX           | AVX           |               | 53          | 0b11      | rrr       | VV1 0x53  VNP VL256 NOVSR V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=YMM_R():w:qq:f32  REG1=YMM_B():r:qq:f32
| VRCPSS              | AVX           | AVX           |               | 53          | mm        | rrr       | VV1 0x53  VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                           | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:d:f32
| VRCPSS              | AVX           | AVX           |               | 53          | 0b11      | rrr       | VV1 0x53  VF3 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                  | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:d:f32
| VROUNDPD            | AVX           | AVX           |               | 09          | mm        | rrr       | VV1 0x09  VL128 V66 V0F3A NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                     | REG0=XMM_R():w:dq:f64  MEM0:r:dq:f64 IMM0:r:b
| VROUNDPD            | AVX           | AVX           |               | 09          | 0b11      | rrr       | VV1 0x09  VL128 V66 V0F3A NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                            | REG0=XMM_R():w:dq:f64 REG1=XMM_B():r:dq:f64 IMM0:r:b
| VROUNDPD            | AVX           | AVX           |               | 09          | mm        | rrr       | VV1 0x09  VL256 V66 V0F3A NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                     | REG0=YMM_R():w:qq:f64  MEM0:r:qq:f64 IMM0:r:b
| VROUNDPD            | AVX           | AVX           |               | 09          | 0b11      | rrr       | VV1 0x09  VL256 V66 V0F3A NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                            | REG0=YMM_R():w:qq:f64 REG1=YMM_B():r:qq:f64 IMM0:r:b
| VROUNDPS            | AVX           | AVX           |               | 08          | mm        | rrr       | VV1 0x08  VL128 V66 V0F3A NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                     | REG0=XMM_R():w:dq:f32  MEM0:r:dq:f32 IMM0:r:b
| VROUNDPS            | AVX           | AVX           |               | 08          | 0b11      | rrr       | VV1 0x08  VL128 V66 V0F3A NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                            | REG0=XMM_R():w:dq:f32 REG1=XMM_B():r:dq:f32 IMM0:r:b
| VROUNDPS            | AVX           | AVX           |               | 08          | mm        | rrr       | VV1 0x08  VL256 V66 V0F3A NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                     | REG0=YMM_R():w:qq:f32  MEM0:r:qq:f32 IMM0:r:b
| VROUNDPS            | AVX           | AVX           |               | 08          | 0b11      | rrr       | VV1 0x08  VL256 V66 V0F3A NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                            | REG0=YMM_R():w:qq:f32 REG1=YMM_B():r:qq:f32 IMM0:r:b
| VROUNDSD            | AVX           | AVX           |               | 0b          | mm        | rrr       | VV1 0x0B  V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                                 | REG0=XMM_R():w:dq:f64  REG1=XMM_N():r:dq:f64  MEM0:r:q:f64         IMM0:r:b
| VROUNDSD            | AVX           | AVX           |               | 0b          | 0b11      | rrr       | VV1 0x0B  V66 V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                        | REG0=XMM_R():w:dq:f64  REG1=XMM_N():r:dq:f64  REG2=XMM_B():r:q:f64 IMM0:r:b
| VROUNDSS            | AVX           | AVX           |               | 0a          | mm        | rrr       | VV1 0x0A  V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                                 | REG0=XMM_R():w:dq:f32  REG1=XMM_N():r:dq:f32  MEM0:r:d:f32         IMM0:r:b
| VROUNDSS            | AVX           | AVX           |               | 0a          | 0b11      | rrr       | VV1 0x0A  V66 V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                        | REG0=XMM_R():w:dq:f32  REG1=XMM_N():r:dq:f32  REG2=XMM_B():r:d:f32 IMM0:r:b
| VRSQRTPS            | AVX           | AVX           |               | 52          | mm        | rrr       | VV1 0x52  VNP VL128 NOVSR V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=XMM_R():w:dq:f32  MEM0:r:dq:f32
| VRSQRTPS            | AVX           | AVX           |               | 52          | 0b11      | rrr       | VV1 0x52  VNP VL128 NOVSR V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=XMM_R():w:dq:f32  REG1=XMM_B():r:dq:f32
| VRSQRTPS            | AVX           | AVX           |               | 52          | mm        | rrr       | VV1 0x52  VNP VL256 NOVSR V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=YMM_R():w:qq:f32  MEM0:r:qq:f32
| VRSQRTPS            | AVX           | AVX           |               | 52          | 0b11      | rrr       | VV1 0x52  VNP VL256 NOVSR V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=YMM_R():w:qq:f32  REG1=YMM_B():r:qq:f32
| VRSQRTSS            | AVX           | AVX           |               | 52          | mm        | rrr       | VV1 0x52  VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                           | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:d:f32
| VRSQRTSS            | AVX           | AVX           |               | 52          | 0b11      | rrr       | VV1 0x52  VF3 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                  | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:d:f32
| VSHUFPD             | AVX           | AVX           |               | c6          | mm        | rrr       | VV1 0xC6  VL128 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                             | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64 IMM0:r:b
| VSHUFPD             | AVX           | AVX           |               | c6          | 0b11      | rrr       | VV1 0xC6  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                    | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64 IMM0:r:b
| VSHUFPD             | AVX           | AVX           |               | c6          | mm        | rrr       | VV1 0xC6  VL256 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                             | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64 IMM0:r:b
| VSHUFPD             | AVX           | AVX           |               | c6          | 0b11      | rrr       | VV1 0xC6  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                    | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64 IMM0:r:b
| VSHUFPS             | AVX           | AVX           |               | c6          | mm        | rrr       | VV1 0xC6  VL128 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                             | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32 IMM0:r:b
| VSHUFPS             | AVX           | AVX           |               | c6          | 0b11      | rrr       | VV1 0xC6  VL128 VNP V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                    | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32 IMM0:r:b
| VSHUFPS             | AVX           | AVX           |               | c6          | mm        | rrr       | VV1 0xC6  VL256 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                             | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32 IMM0:r:b
| VSHUFPS             | AVX           | AVX           |               | c6          | 0b11      | rrr       | VV1 0xC6  VL256 VNP V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                                    | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32 IMM0:r:b
| VSQRTPD             | AVX           | AVX           |               | 51          | mm        | rrr       | VV1 0x51  VL128 V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=XMM_R():w:dq:f64 MEM0:r:dq:f64
| VSQRTPD             | AVX           | AVX           |               | 51          | 0b11      | rrr       | VV1 0x51  VL128 V66 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=XMM_R():w:dq:f64  REG1=XMM_B():r:dq:f64
| VSQRTPD             | AVX           | AVX           |               | 51          | mm        | rrr       | VV1 0x51  VL256 V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=YMM_R():w:qq:f64  MEM0:r:qq:f64
| VSQRTPD             | AVX           | AVX           |               | 51          | 0b11      | rrr       | VV1 0x51  VL256 V66 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=YMM_R():w:qq:f64  REG1=YMM_B():r:qq:f64
| VSQRTPS             | AVX           | AVX           |               | 51          | mm        | rrr       | VV1 0x51  VL128 VNP NOVSR V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=XMM_R():w:dq:f32  MEM0:r:dq:f32
| VSQRTPS             | AVX           | AVX           |               | 51          | 0b11      | rrr       | VV1 0x51  VL128 VNP NOVSR V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=XMM_R():w:dq:f32  REG1=XMM_B():r:dq:f32
| VSQRTPS             | AVX           | AVX           |               | 51          | mm        | rrr       | VV1 0x51  VL256 VNP NOVSR V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=YMM_R():w:qq:f32  MEM0:r:qq:f32
| VSQRTPS             | AVX           | AVX           |               | 51          | 0b11      | rrr       | VV1 0x51  VL256 VNP NOVSR V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=YMM_R():w:qq:f32  REG1=YMM_B():r:qq:f32
| VSQRTSD             | AVX           | AVX           |               | 51          | mm        | rrr       | VV1 0x51  VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                           | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:q:f64
| VSQRTSD             | AVX           | AVX           |               | 51          | 0b11      | rrr       | VV1 0x51  VF2 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                  | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:q:f64
| VSQRTSS             | AVX           | AVX           |               | 51          | mm        | rrr       | VV1 0x51  VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                           | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:d:f32
| VSQRTSS             | AVX           | AVX           |               | 51          | 0b11      | rrr       | VV1 0x51  VF3 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                  | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:d:f32
| VSTMXCSR            | AVX           | AVX           |               | ae          | mm        | 0b011     | VV1 0xAE VL128 VNP V0F NOVSR MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM()                              | MEM0:w:d REG0=XED_REG_MXCSR:r:SUPP
| VSUBPD              | AVX           | AVX           |               | 5c          | mm        | rrr       | VV1 0x5C  V66 V0F VL128 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
| VSUBPD              | AVX           | AVX           |               | 5c          | 0b11      | rrr       | VV1 0x5C  V66 V0F VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
| VSUBPD              | AVX           | AVX           |               | 5c          | mm        | rrr       | VV1 0x5C  V66 V0F VL256 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
| VSUBPD              | AVX           | AVX           |               | 5c          | 0b11      | rrr       | VV1 0x5C  V66 V0F VL256 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
| VSUBPS              | AVX           | AVX           |               | 5c          | mm        | rrr       | VV1 0x5C  VL128 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
| VSUBPS              | AVX           | AVX           |               | 5c          | 0b11      | rrr       | VV1 0x5C  VL128 VNP V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
| VSUBPS              | AVX           | AVX           |               | 5c          | mm        | rrr       | VV1 0x5C  VL256 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
| VSUBPS              | AVX           | AVX           |               | 5c          | 0b11      | rrr       | VV1 0x5C  VL256 VNP V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
| VSUBSD              | AVX           | AVX           |               | 5c          | mm        | rrr       | VV1 0x5C  VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                           | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:q:f64
| VSUBSD              | AVX           | AVX           |               | 5c          | 0b11      | rrr       | VV1 0x5C  VF2 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                  | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:q:f64
| VSUBSS              | AVX           | AVX           |               | 5c          | mm        | rrr       | VV1 0x5C  VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                           | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:d:f32
| VSUBSS              | AVX           | AVX           |               | 5c          | 0b11      | rrr       | VV1 0x5C  VF3 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                                  | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:d:f32
| VTESTPD             | LOGICAL_FP    | AVX           |               | 0f          | mm        | rrr       | VV1 0x0F  VL128 V66 V0F38 norexw_prefix NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()               | REG0=XMM_R():r:dq:f64 MEM0:r:dq:f64
| VTESTPD             | LOGICAL_FP    | AVX           |               | 0f          | 0b11      | rrr       | VV1 0x0F VL128 V66 V0F38 norexw_prefix NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                       | REG0=XMM_R():r:dq:f64 REG1=XMM_B():r:dq:f64
| VTESTPD             | LOGICAL_FP    | AVX           |               | 0f          | mm        | rrr       | VV1 0x0F VL256 V66 V0F38  norexw_prefix NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()               | REG0=YMM_R():r:qq:f64 MEM0:r:qq:f64
| VTESTPD             | LOGICAL_FP    | AVX           |               | 0f          | 0b11      | rrr       | VV1 0x0F VL256 V66 V0F38 norexw_prefix NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                       | REG0=YMM_R():r:qq:f64 REG1=YMM_B():r:qq:f64
| VTESTPS             | LOGICAL_FP    | AVX           |               | 0e          | mm        | rrr       | VV1 0x0E VL128 V66 V0F38 norexw_prefix  NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()               | REG0=XMM_R():r:dq:f32 MEM0:r:dq:f32
| VTESTPS             | LOGICAL_FP    | AVX           |               | 0e          | 0b11      | rrr       | VV1 0x0E  VL128 V66 V0F38 norexw_prefix NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                      | REG0=XMM_R():r:dq:f32 REG1=XMM_B():r:dq:f32
| VTESTPS             | LOGICAL_FP    | AVX           |               | 0e          | mm        | rrr       | VV1 0x0E VL256 V66 V0F38  norexw_prefix NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()               | REG0=YMM_R():r:qq:f32 MEM0:r:qq:f32
| VTESTPS             | LOGICAL_FP    | AVX           |               | 0e          | 0b11      | rrr       | VV1 0x0E VL256 V66 V0F38 norexw_prefix NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                       | REG0=YMM_R():r:qq:f32 REG1=YMM_B():r:qq:f32
| VUCOMISD            | AVX           | AVX           |               | 2e          | mm        | rrr       | VV1 0x2E V66 V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():r:dq:f64  MEM0:r:q:f64
| VUCOMISD            | AVX           | AVX           |               | 2e          | 0b11      | rrr       | VV1 0x2E V66 V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():r:dq:f64  REG1=XMM_B():r:q:f64
| VUCOMISS            | AVX           | AVX           |               | 2e          | mm        | rrr       | VV1 0x2E VNP V0F NOVSR MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                      | REG0=XMM_R():r:dq:f32  MEM0:r:d:f32
| VUCOMISS            | AVX           | AVX           |               | 2e          | 0b11      | rrr       | VV1 0x2E VNP V0F NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                             | REG0=XMM_R():r:dq:f32  REG1=XMM_B():r:d:f32
| VUNPCKHPD           | AVX           | AVX           |               | 15          | mm        | rrr       | VV1 0x15  VL128 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
| VUNPCKHPD           | AVX           | AVX           |               | 15          | 0b11      | rrr       | VV1 0x15  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
| VUNPCKHPD           | AVX           | AVX           |               | 15          | mm        | rrr       | VV1 0x15  VL256 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
| VUNPCKHPD           | AVX           | AVX           |               | 15          | 0b11      | rrr       | VV1 0x15  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
| VUNPCKHPS           | AVX           | AVX           |               | 15          | mm        | rrr       | VV1 0x15  VL128 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
| VUNPCKHPS           | AVX           | AVX           |               | 15          | 0b11      | rrr       | VV1 0x15  VL128 VNP V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
| VUNPCKHPS           | AVX           | AVX           |               | 15          | mm        | rrr       | VV1 0x15  VL256 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
| VUNPCKHPS           | AVX           | AVX           |               | 15          | 0b11      | rrr       | VV1 0x15  VL256 VNP V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
| VUNPCKLPD           | AVX           | AVX           |               | 14          | mm        | rrr       | VV1 0x14  VL128 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
| VUNPCKLPD           | AVX           | AVX           |               | 14          | 0b11      | rrr       | VV1 0x14  VL128 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64
| VUNPCKLPD           | AVX           | AVX           |               | 14          | mm        | rrr       | VV1 0x14  VL256 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
| VUNPCKLPD           | AVX           | AVX           |               | 14          | 0b11      | rrr       | VV1 0x14  VL256 V66 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
| VUNPCKLPS           | AVX           | AVX           |               | 14          | mm        | rrr       | VV1 0x14  VL128 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
| VUNPCKLPS           | AVX           | AVX           |               | 14          | 0b11      | rrr       | VV1 0x14  VL128 VNP V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32
| VUNPCKLPS           | AVX           | AVX           |               | 14          | mm        | rrr       | VV1 0x14  VL256 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
| VUNPCKLPS           | AVX           | AVX           |               | 14          | 0b11      | rrr       | VV1 0x14  VL256 VNP V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
| VXORPD              | LOGICAL_FP    | AVX           |               | 57          | mm        | rrr       | VV1 0x57  V66 V0F VL128 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 MEM0:r:dq:u64
| VXORPD              | LOGICAL_FP    | AVX           |               | 57          | 0b11      | rrr       | VV1 0x57  V66 V0F VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 REG2=XMM_B():r:dq:u64
| VXORPD              | LOGICAL_FP    | AVX           |               | 57          | mm        | rrr       | VV1 0x57  V66 V0F VL256 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq:u64 REG1=YMM_N():r:qq:u64 MEM0:r:qq:u64
| VXORPD              | LOGICAL_FP    | AVX           |               | 57          | 0b11      | rrr       | VV1 0x57  V66 V0F VL256 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=YMM_R():w:qq:u64 REG1=YMM_N():r:qq:u64 REG2=YMM_B():r:qq:u64
| VXORPS              | LOGICAL_FP    | AVX           |               | 57          | mm        | rrr       | VV1 0x57  VNP V0F VL128 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=XMM_R():w:dq REG1=XMM_N():r:dq MEM0:r:dq
| VXORPS              | LOGICAL_FP    | AVX           |               | 57          | 0b11      | rrr       | VV1 0x57  VNP V0F VL128 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=XMM_R():w:dq REG1=XMM_N():r:dq REG2=XMM_B():r:dq
| VXORPS              | LOGICAL_FP    | AVX           |               | 57          | mm        | rrr       | VV1 0x57  VNP V0F VL256 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=YMM_R():w:qq REG1=YMM_N():r:qq MEM0:r:qq
| VXORPS              | LOGICAL_FP    | AVX           |               | 57          | 0b11      | rrr       | VV1 0x57  VNP V0F VL256 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=YMM_R():w:qq REG1=YMM_N():r:qq REG2=YMM_B():r:qq
| VZEROALL            | AVX           | AVX           |               | 77          |           |           | VV1 0x77 VNP  V0F VL256  NOVSR                                                                      | 
| VZEROUPPER          | AVX           | AVX           |               | 77          |           |           | VV1 0x77 VNP  V0F VL128 NOVSR                                                                       | 
