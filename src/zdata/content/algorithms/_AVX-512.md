# _AVX-512

## KUNPCKDQ - _mm512_kunpackd

| KUNPCKDQ_MASKmskw_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 32 bits from masks "a" and "b", and store the 64-bit result in "dst".

[algorithm]

dst[31:0] := b[31:0]
dst[63:32] := a[31:0]
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## KUNPCKWD - _mm512_kunpackw

| KUNPCKWD_MASKmskw_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 16 bits from masks "a" and "b", and store the 32-bit result in "dst".

[algorithm]

dst[15:0] := b[15:0]
dst[31:16] := a[15:0]
dst[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VDBPSADBW - _mm256_dbsad_epu8

| VDBPSADBW_YMMu16_MASKmskw_YMMu8_YMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the sum of absolute differences (SADs) of quadruplets of unsigned 8-bit integers in "a" compared to
those in "b", and store the 16-bit results in "dst".
	Four SADs are performed on four 8-bit quadruplets for
each 64-bit lane. The first two SADs use the lower 8-bit quadruplet of the lane from "a", and the last two SADs
use the uppper 8-bit quadruplet of the lane from "a". Quadruplets from "b" are selected from within 128-bit
lanes according to the control in "imm8", and each SAD in each 64-bit lane uses the selected quadruplet at
8-bit offsets.

[algorithm]

FOR i := 0 to 1
    tmp.m128[i].dword[0] := b.m128[i].dword[ imm8[1:0] ]
    tmp.m128[i].dword[1] := b.m128[i].dword[ imm8[3:2] ]
    tmp.m128[i].dword[2] := b.m128[i].dword[ imm8[5:4] ]
    tmp.m128[i].dword[3] := b.m128[i].dword[ imm8[7:6] ]
ENDFOR
FOR j := 0 to 3
    i := j*64
    dst[i+15:i] := ABS(a[i+7:i] - tmp[i+7:i]) + ABS(a[i+15:i+8] - tmp[i+15:i+8]) +\
                   ABS(a[i+23:i+16] - tmp[i+23:i+16]) + ABS(a[i+31:i+24] - tmp[i+31:i+24])
    
    dst[i+31:i+16] := ABS(a[i+7:i] - tmp[i+15:i+8]) + ABS(a[i+15:i+8] - tmp[i+23:i+16]) +\
                      ABS(a[i+23:i+16] - tmp[i+31:i+24]) + ABS(a[i+31:i+24] - tmp[i+39:i+32])
    
    dst[i+47:i+32] := ABS(a[i+39:i+32] - tmp[i+23:i+16]) + ABS(a[i+47:i+40] - tmp[i+31:i+24]) +\
                      ABS(a[i+55:i+48] - tmp[i+39:i+32]) + ABS(a[i+63:i+56] - tmp[i+47:i+40])
    
    dst[i+63:i+48] := ABS(a[i+39:i+32] - tmp[i+31:i+24]) + ABS(a[i+47:i+40] - tmp[i+39:i+32]) +\
                      ABS(a[i+55:i+48] - tmp[i+47:i+40]) + ABS(a[i+63:i+56] - tmp[i+55:i+48])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VDBPSADBW - _mm256_mask_dbsad_epu8

| VDBPSADBW_YMMu16_MASKmskw_YMMu8_YMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the sum of absolute differences (SADs) of quadruplets of unsigned 8-bit integers in "a" compared to
those in "b", and store the 16-bit results in "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set).
	Four SADs are performed on four 8-bit quadruplets for each 64-bit
lane. The first two SADs use the lower 8-bit quadruplet of the lane from "a", and the last two SADs use the
uppper 8-bit quadruplet of the lane from "a". Quadruplets from "b" are selected from within 128-bit lanes
according to the control in "imm8", and each SAD in each 64-bit lane uses the selected quadruplet at 8-bit
offsets.

[algorithm]

FOR i := 0 to 1
    tmp.m128[i].dword[0] := b.m128[i].dword[ imm8[1:0] ]
    tmp.m128[i].dword[1] := b.m128[i].dword[ imm8[3:2] ]
    tmp.m128[i].dword[2] := b.m128[i].dword[ imm8[5:4] ]
    tmp.m128[i].dword[3] := b.m128[i].dword[ imm8[7:6] ]
ENDFOR
FOR j := 0 to 3
    i := j*64
    tmp_dst[i+15:i] := ABS(a[i+7:i] - tmp[i+7:i]) + ABS(a[i+15:i+8] - tmp[i+15:i+8]) +\
                       ABS(a[i+23:i+16] - tmp[i+23:i+16]) + ABS(a[i+31:i+24] - tmp[i+31:i+24])
    
    tmp_dst[i+31:i+16] := ABS(a[i+7:i] - tmp[i+15:i+8]) + ABS(a[i+15:i+8] - tmp[i+23:i+16]) +\
                          ABS(a[i+23:i+16] - tmp[i+31:i+24]) + ABS(a[i+31:i+24] - tmp[i+39:i+32])
    
    tmp_dst[i+47:i+32] := ABS(a[i+39:i+32] - tmp[i+23:i+16]) + ABS(a[i+47:i+40] - tmp[i+31:i+24]) +\
                          ABS(a[i+55:i+48] - tmp[i+39:i+32]) + ABS(a[i+63:i+56] - tmp[i+47:i+40])
    
    tmp_dst[i+63:i+48] := ABS(a[i+39:i+32] - tmp[i+31:i+24]) + ABS(a[i+47:i+40] - tmp[i+39:i+32]) +\
                          ABS(a[i+55:i+48] - tmp[i+47:i+40]) + ABS(a[i+63:i+56] - tmp[i+55:i+48])
ENDFOR
FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VDBPSADBW - _mm256_maskz_dbsad_epu8

| VDBPSADBW_YMMu16_MASKmskw_YMMu8_YMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the sum of absolute differences (SADs) of quadruplets of unsigned 8-bit integers in "a" compared to
those in "b", and store the 16-bit results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).
	Four SADs are performed on four 8-bit quadruplets for each 64-bit lane.
The first two SADs use the lower 8-bit quadruplet of the lane from "a", and the last two SADs use the uppper
8-bit quadruplet of the lane from "a". Quadruplets from "b" are selected from within 128-bit lanes according to
the control in "imm8", and each SAD in each 64-bit lane uses the selected quadruplet at 8-bit offsets.

[algorithm]

FOR i := 0 to 1
    tmp.m128[i].dword[0] := b.m128[i].dword[ imm8[1:0] ]
    tmp.m128[i].dword[1] := b.m128[i].dword[ imm8[3:2] ]
    tmp.m128[i].dword[2] := b.m128[i].dword[ imm8[5:4] ]
    tmp.m128[i].dword[3] := b.m128[i].dword[ imm8[7:6] ]
ENDFOR
FOR j := 0 to 3
    i := j*64
    tmp_dst[i+15:i] := ABS(a[i+7:i] - tmp[i+7:i]) + ABS(a[i+15:i+8] - tmp[i+15:i+8]) +\
                       ABS(a[i+23:i+16] - tmp[i+23:i+16]) + ABS(a[i+31:i+24] - tmp[i+31:i+24])
    
    tmp_dst[i+31:i+16] := ABS(a[i+7:i] - tmp[i+15:i+8]) + ABS(a[i+15:i+8] - tmp[i+23:i+16]) +\
                          ABS(a[i+23:i+16] - tmp[i+31:i+24]) + ABS(a[i+31:i+24] - tmp[i+39:i+32])
    
    tmp_dst[i+47:i+32] := ABS(a[i+39:i+32] - tmp[i+23:i+16]) + ABS(a[i+47:i+40] - tmp[i+31:i+24]) +\
                          ABS(a[i+55:i+48] - tmp[i+39:i+32]) + ABS(a[i+63:i+56] - tmp[i+47:i+40])
    
    tmp_dst[i+63:i+48] := ABS(a[i+39:i+32] - tmp[i+31:i+24]) + ABS(a[i+47:i+40] - tmp[i+39:i+32]) +\
                          ABS(a[i+55:i+48] - tmp[i+47:i+40]) + ABS(a[i+63:i+56] - tmp[i+55:i+48])
ENDFOR
FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VDBPSADBW - _mm512_dbsad_epu8

| VDBPSADBW_ZMMu16_MASKmskw_ZMMu8_ZMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the sum of absolute differences (SADs) of quadruplets of unsigned 8-bit integers in "a" compared to
those in "b", and store the 16-bit results in "dst".
	Four SADs are performed on four 8-bit quadruplets for
each 64-bit lane. The first two SADs use the lower 8-bit quadruplet of the lane from "a", and the last two SADs
use the uppper 8-bit quadruplet of the lane from "a". Quadruplets from "b" are selected from within 128-bit
lanes according to the control in "imm8", and each SAD in each 64-bit lane uses the selected quadruplet at
8-bit offsets.

[algorithm]

FOR i := 0 to 3
    tmp.m128[i].dword[0] := b.m128[i].dword[ imm8[1:0] ]
    tmp.m128[i].dword[1] := b.m128[i].dword[ imm8[3:2] ]
    tmp.m128[i].dword[2] := b.m128[i].dword[ imm8[5:4] ]
    tmp.m128[i].dword[3] := b.m128[i].dword[ imm8[7:6] ]
ENDFOR
FOR j := 0 to 7
    i := j*64
    dst[i+15:i] := ABS(a[i+7:i] - tmp[i+7:i]) + ABS(a[i+15:i+8] - tmp[i+15:i+8]) +\
                   ABS(a[i+23:i+16] - tmp[i+23:i+16]) + ABS(a[i+31:i+24] - tmp[i+31:i+24])
    
    dst[i+31:i+16] := ABS(a[i+7:i] - tmp[i+15:i+8]) + ABS(a[i+15:i+8] - tmp[i+23:i+16]) +\
                      ABS(a[i+23:i+16] - tmp[i+31:i+24]) + ABS(a[i+31:i+24] - tmp[i+39:i+32])
    
    dst[i+47:i+32] := ABS(a[i+39:i+32] - tmp[i+23:i+16]) + ABS(a[i+47:i+40] - tmp[i+31:i+24]) +\
                      ABS(a[i+55:i+48] - tmp[i+39:i+32]) + ABS(a[i+63:i+56] - tmp[i+47:i+40])
    
    dst[i+63:i+48] := ABS(a[i+39:i+32] - tmp[i+31:i+24]) + ABS(a[i+47:i+40] - tmp[i+39:i+32]) +\
                      ABS(a[i+55:i+48] - tmp[i+47:i+40]) + ABS(a[i+63:i+56] - tmp[i+55:i+48])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VDBPSADBW - _mm512_mask_dbsad_epu8

| VDBPSADBW_ZMMu16_MASKmskw_ZMMu8_ZMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the sum of absolute differences (SADs) of quadruplets of unsigned 8-bit integers in "a" compared to
those in "b", and store the 16-bit results in "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set).
	Four SADs are performed on four 8-bit quadruplets for each 64-bit
lane. The first two SADs use the lower 8-bit quadruplet of the lane from "a", and the last two SADs use the
uppper 8-bit quadruplet of the lane from "a". Quadruplets from "b" are selected from within 128-bit lanes
according to the control in "imm8", and each SAD in each 64-bit lane uses the selected quadruplet at 8-bit
offsets.

[algorithm]

FOR i := 0 to 3
    tmp.m128[i].dword[0] := b.m128[i].dword[ imm8[1:0] ]
    tmp.m128[i].dword[1] := b.m128[i].dword[ imm8[3:2] ]
    tmp.m128[i].dword[2] := b.m128[i].dword[ imm8[5:4] ]
    tmp.m128[i].dword[3] := b.m128[i].dword[ imm8[7:6] ]
ENDFOR
FOR j := 0 to 7
    i := j*64
    tmp_dst[i+15:i] := ABS(a[i+7:i] - tmp[i+7:i]) + ABS(a[i+15:i+8] - tmp[i+15:i+8]) +\
                       ABS(a[i+23:i+16] - tmp[i+23:i+16]) + ABS(a[i+31:i+24] - tmp[i+31:i+24])
    
    tmp_dst[i+31:i+16] := ABS(a[i+7:i] - tmp[i+15:i+8]) + ABS(a[i+15:i+8] - tmp[i+23:i+16]) +\
                          ABS(a[i+23:i+16] - tmp[i+31:i+24]) + ABS(a[i+31:i+24] - tmp[i+39:i+32])
    
    tmp_dst[i+47:i+32] := ABS(a[i+39:i+32] - tmp[i+23:i+16]) + ABS(a[i+47:i+40] - tmp[i+31:i+24]) +\
                          ABS(a[i+55:i+48] - tmp[i+39:i+32]) + ABS(a[i+63:i+56] - tmp[i+47:i+40])
    
    tmp_dst[i+63:i+48] := ABS(a[i+39:i+32] - tmp[i+31:i+24]) + ABS(a[i+47:i+40] - tmp[i+39:i+32]) +\
                          ABS(a[i+55:i+48] - tmp[i+47:i+40]) + ABS(a[i+63:i+56] - tmp[i+55:i+48])
ENDFOR
FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VDBPSADBW - _mm512_maskz_dbsad_epu8

| VDBPSADBW_ZMMu16_MASKmskw_ZMMu8_ZMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the sum of absolute differences (SADs) of quadruplets of unsigned 8-bit integers in "a" compared to
those in "b", and store the 16-bit results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).
	Four SADs are performed on four 8-bit quadruplets for each 64-bit lane.
The first two SADs use the lower 8-bit quadruplet of the lane from "a", and the last two SADs use the uppper
8-bit quadruplet of the lane from "a". Quadruplets from "b" are selected from within 128-bit lanes according to
the control in "imm8", and each SAD in each 64-bit lane uses the selected quadruplet at 8-bit offsets.

[algorithm]

FOR i := 0 to 3
    tmp.m128[i].dword[0] := b.m128[i].dword[ imm8[1:0] ]
    tmp.m128[i].dword[1] := b.m128[i].dword[ imm8[3:2] ]
    tmp.m128[i].dword[2] := b.m128[i].dword[ imm8[5:4] ]
    tmp.m128[i].dword[3] := b.m128[i].dword[ imm8[7:6] ]
ENDFOR
FOR j := 0 to 7
    i := j*64
    tmp_dst[i+15:i] := ABS(a[i+7:i] - tmp[i+7:i]) + ABS(a[i+15:i+8] - tmp[i+15:i+8]) +\
                       ABS(a[i+23:i+16] - tmp[i+23:i+16]) + ABS(a[i+31:i+24] - tmp[i+31:i+24])
    
    tmp_dst[i+31:i+16] := ABS(a[i+7:i] - tmp[i+15:i+8]) + ABS(a[i+15:i+8] - tmp[i+23:i+16]) +\
                          ABS(a[i+23:i+16] - tmp[i+31:i+24]) + ABS(a[i+31:i+24] - tmp[i+39:i+32])
    
    tmp_dst[i+47:i+32] := ABS(a[i+39:i+32] - tmp[i+23:i+16]) + ABS(a[i+47:i+40] - tmp[i+31:i+24]) +\
                          ABS(a[i+55:i+48] - tmp[i+39:i+32]) + ABS(a[i+63:i+56] - tmp[i+47:i+40])
    
    tmp_dst[i+63:i+48] := ABS(a[i+39:i+32] - tmp[i+31:i+24]) + ABS(a[i+47:i+40] - tmp[i+39:i+32]) +\
                          ABS(a[i+55:i+48] - tmp[i+47:i+40]) + ABS(a[i+63:i+56] - tmp[i+55:i+48])
ENDFOR
FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VDBPSADBW - _mm_dbsad_epu8

| VDBPSADBW_XMMu16_MASKmskw_XMMu8_XMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the sum of absolute differences (SADs) of quadruplets of unsigned 8-bit integers in "a" compared to
those in "b", and store the 16-bit results in "dst".
	Four SADs are performed on four 8-bit quadruplets for
each 64-bit lane. The first two SADs use the lower 8-bit quadruplet of the lane from "a", and the last two SADs
use the uppper 8-bit quadruplet of the lane from "a". Quadruplets from "b" are selected according to the
control in "imm8", and each SAD in each 64-bit lane uses the selected quadruplet at 8-bit offsets.

[algorithm]

tmp.dword[0] := b.dword[ imm8[1:0] ]
tmp.dword[1] := b.dword[ imm8[3:2] ]
tmp.dword[2] := b.dword[ imm8[5:4] ]
tmp.dword[3] := b.dword[ imm8[7:6] ]
FOR j := 0 to 1
    i := j*64
    dst[i+15:i] := ABS(a[i+7:i] - tmp[i+7:i]) + ABS(a[i+15:i+8] - tmp[i+15:i+8]) +\
                   ABS(a[i+23:i+16] - tmp[i+23:i+16]) + ABS(a[i+31:i+24] - tmp[i+31:i+24])
    
    dst[i+31:i+16] := ABS(a[i+7:i] - tmp[i+15:i+8]) + ABS(a[i+15:i+8] - tmp[i+23:i+16]) +\
                      ABS(a[i+23:i+16] - tmp[i+31:i+24]) + ABS(a[i+31:i+24] - tmp[i+39:i+32])
    
    dst[i+47:i+32] := ABS(a[i+39:i+32] - tmp[i+23:i+16]) + ABS(a[i+47:i+40] - tmp[i+31:i+24]) +\
                      ABS(a[i+55:i+48] - tmp[i+39:i+32]) + ABS(a[i+63:i+56] - tmp[i+47:i+40])
    
    dst[i+63:i+48] := ABS(a[i+39:i+32] - tmp[i+31:i+24]) + ABS(a[i+47:i+40] - tmp[i+39:i+32]) +\
                      ABS(a[i+55:i+48] - tmp[i+47:i+40]) + ABS(a[i+63:i+56] - tmp[i+55:i+48])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VDBPSADBW - _mm_mask_dbsad_epu8

| VDBPSADBW_XMMu16_MASKmskw_XMMu8_XMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the sum of absolute differences (SADs) of quadruplets of unsigned 8-bit integers in "a" compared to
those in "b", and store the 16-bit results in "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set).
	Four SADs are performed on four 8-bit quadruplets for each 64-bit
lane. The first two SADs use the lower 8-bit quadruplet of the lane from "a", and the last two SADs use the
uppper 8-bit quadruplet of the lane from "a". Quadruplets from "b" are selected according to the control in
"imm8", and each SAD in each 64-bit lane uses the selected quadruplet at 8-bit offsets.

[algorithm]

tmp.dword[0] := b.dword[ imm8[1:0] ]
tmp.dword[1] := b.dword[ imm8[3:2] ]
tmp.dword[2] := b.dword[ imm8[5:4] ]
tmp.dword[3] := b.dword[ imm8[7:6] ]
FOR j := 0 to 1
    i := j*64
    tmp_dst[i+15:i] := ABS(a[i+7:i] - tmp[i+7:i]) + ABS(a[i+15:i+8] - tmp[i+15:i+8]) +\
                       ABS(a[i+23:i+16] - tmp[i+23:i+16]) + ABS(a[i+31:i+24] - tmp[i+31:i+24])
    
    tmp_dst[i+31:i+16] := ABS(a[i+7:i] - tmp[i+15:i+8]) + ABS(a[i+15:i+8] - tmp[i+23:i+16]) +\
                          ABS(a[i+23:i+16] - tmp[i+31:i+24]) + ABS(a[i+31:i+24] - tmp[i+39:i+32])
    
    tmp_dst[i+47:i+32] := ABS(a[i+39:i+32] - tmp[i+23:i+16]) + ABS(a[i+47:i+40] - tmp[i+31:i+24]) +\
                          ABS(a[i+55:i+48] - tmp[i+39:i+32]) + ABS(a[i+63:i+56] - tmp[i+47:i+40])
    
    tmp_dst[i+63:i+48] := ABS(a[i+39:i+32] - tmp[i+31:i+24]) + ABS(a[i+47:i+40] - tmp[i+39:i+32]) +\
                          ABS(a[i+55:i+48] - tmp[i+47:i+40]) + ABS(a[i+63:i+56] - tmp[i+55:i+48])
ENDFOR
FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VDBPSADBW - _mm_maskz_dbsad_epu8

| VDBPSADBW_XMMu16_MASKmskw_XMMu8_XMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the sum of absolute differences (SADs) of quadruplets of unsigned 8-bit integers in "a" compared to
those in "b", and store the 16-bit results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).
	Four SADs are performed on four 8-bit quadruplets for each 64-bit lane.
The first two SADs use the lower 8-bit quadruplet of the lane from "a", and the last two SADs use the uppper
8-bit quadruplet of the lane from "a". Quadruplets from "b" are selected according to the control in "imm8",
and each SAD in each 64-bit lane uses the selected quadruplet at 8-bit offsets.

[algorithm]

tmp.dword[0] := b.dword[ imm8[1:0] ]
tmp.dword[1] := b.dword[ imm8[3:2] ]
tmp.dword[2] := b.dword[ imm8[5:4] ]
tmp.dword[3] := b.dword[ imm8[7:6] ]
FOR j := 0 to 1
    i := j*64
    tmp_dst[i+15:i] := ABS(a[i+7:i] - tmp[i+7:i]) + ABS(a[i+15:i+8] - tmp[i+15:i+8]) +\
                       ABS(a[i+23:i+16] - tmp[i+23:i+16]) + ABS(a[i+31:i+24] - tmp[i+31:i+24])
    
    tmp_dst[i+31:i+16] := ABS(a[i+7:i] - tmp[i+15:i+8]) + ABS(a[i+15:i+8] - tmp[i+23:i+16]) +\
                          ABS(a[i+23:i+16] - tmp[i+31:i+24]) + ABS(a[i+31:i+24] - tmp[i+39:i+32])
    
    tmp_dst[i+47:i+32] := ABS(a[i+39:i+32] - tmp[i+23:i+16]) + ABS(a[i+47:i+40] - tmp[i+31:i+24]) +\
                          ABS(a[i+55:i+48] - tmp[i+39:i+32]) + ABS(a[i+63:i+56] - tmp[i+47:i+40])
    
    tmp_dst[i+63:i+48] := ABS(a[i+39:i+32] - tmp[i+31:i+24]) + ABS(a[i+47:i+40] - tmp[i+39:i+32]) +\
                          ABS(a[i+55:i+48] - tmp[i+47:i+40]) + ABS(a[i+63:i+56] - tmp[i+55:i+48])
ENDFOR
FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU16 - _mm256_mask_loadu_epi16

| VMOVDQU16_YMMu16_MASKmskw_MEMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed 16-bit integers from memory into "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set). 
	"mem_addr" does not need to be aligned on any particular boundary.

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := MEM[mem_addr+i+15:mem_addr+i]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU16 - _mm256_mask_mov_epi16

| VMOVDQU16_YMMu16_MASKmskw_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Move packed 16-bit integers from "a" into "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := a[i+15:i]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU16 - _mm256_mask_storeu_epi16

| VMOVDQU16_MEMu16_MASKmskw_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Store packed 16-bit integers from "a" into memory using writemask "k".
	"mem_addr" does not need to be aligned
on any particular boundary.

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        MEM[mem_addr+i+15:mem_addr+i] := a[i+15:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VMOVDQU16 - _mm256_maskz_loadu_epi16

| VMOVDQU16_YMMu16_MASKmskw_MEMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed 16-bit integers from memory into "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).
	"mem_addr" does not need to be aligned on any particular boundary.

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := MEM[mem_addr+i+15:mem_addr+i]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU16 - _mm256_maskz_mov_epi16

| VMOVDQU16_YMMu16_MASKmskw_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Move packed 16-bit integers from "a" into "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := a[i+15:i]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU16 - _mm512_mask_loadu_epi16

| VMOVDQU16_ZMMu16_MASKmskw_MEMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed 16-bit integers from memory into "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set). 
	"mem_addr" does not need to be aligned on any particular boundary.

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := MEM[mem_addr+i+15:mem_addr+i]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU16 - _mm512_mask_mov_epi16

| VMOVDQU16_ZMMu16_MASKmskw_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Move packed 16-bit integers from "a" into "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := a[i+15:i]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU16 - _mm512_mask_storeu_epi16

| VMOVDQU16_MEMu16_MASKmskw_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Store packed 16-bit integers from "a" into memory using writemask "k".
	"mem_addr" does not need to be aligned
on any particular boundary.

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        MEM[mem_addr+i+15:mem_addr+i] := a[i+15:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VMOVDQU16 - _mm512_maskz_loadu_epi16

| VMOVDQU16_ZMMu16_MASKmskw_MEMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed 16-bit integers from memory into "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).
	"mem_addr" does not need to be aligned on any particular boundary.

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := MEM[mem_addr+i+15:mem_addr+i]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU16 - _mm512_maskz_mov_epi16

| VMOVDQU16_ZMMu16_MASKmskw_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Move packed 16-bit integers from "a" into "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := a[i+15:i]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU16 - _mm_mask_loadu_epi16

| VMOVDQU16_XMMu16_MASKmskw_MEMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed 16-bit integers from memory into "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set). 
	"mem_addr" does not need to be aligned on any particular boundary.

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := MEM[mem_addr+i+15:mem_addr+i]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU16 - _mm_mask_mov_epi16

| VMOVDQU16_XMMu16_MASKmskw_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Move packed 16-bit integers from "a" into "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := a[i+15:i]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU16 - _mm_mask_storeu_epi16

| VMOVDQU16_MEMu16_MASKmskw_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Store packed 16-bit integers from "a" into memory using writemask "k".
	"mem_addr" does not need to be aligned
on any particular boundary.

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        MEM[mem_addr+i+15:mem_addr+i] := a[i+15:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VMOVDQU16 - _mm_maskz_loadu_epi16

| VMOVDQU16_XMMu16_MASKmskw_MEMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed 16-bit integers from memory into "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).
	"mem_addr" does not need to be aligned on any particular boundary.

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := MEM[mem_addr+i+15:mem_addr+i]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU16 - _mm_maskz_mov_epi16

| VMOVDQU16_XMMu16_MASKmskw_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Move packed 16-bit integers from "a" into "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := a[i+15:i]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU8 - _mm256_mask_loadu_epi8

| VMOVDQU8_YMMu8_MASKmskw_MEMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed 8-bit integers from memory into "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set). 
	"mem_addr" does not need to be aligned on any particular boundary.

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := MEM[mem_addr+i+7:mem_addr+i]
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU8 - _mm256_mask_mov_epi8

| VMOVDQU8_YMMu8_MASKmskw_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Move packed 8-bit integers from "a" into "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := a[i+7:i]
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU8 - _mm256_mask_storeu_epi8

| VMOVDQU8_MEMu8_MASKmskw_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Store packed 8-bit integers from "a" into memory using writemask "k".
	"mem_addr" does not need to be aligned
on any particular boundary.

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        MEM[mem_addr+i+7:mem_addr+i] := a[i+7:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VMOVDQU8 - _mm256_maskz_loadu_epi8

| VMOVDQU8_YMMu8_MASKmskw_MEMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed 8-bit integers from memory into "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).
	"mem_addr" does not need to be aligned on any particular boundary.

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := MEM[mem_addr+i+7:mem_addr+i]
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU8 - _mm256_maskz_mov_epi8

| VMOVDQU8_YMMu8_MASKmskw_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Move packed 8-bit integers from "a" into "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := a[i+7:i]
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU8 - _mm512_mask_loadu_epi8

| VMOVDQU8_ZMMu8_MASKmskw_MEMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed 8-bit integers from memory into "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set). 
	"mem_addr" does not need to be aligned on any particular boundary.

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := MEM[mem_addr+i+7:mem_addr+i]
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU8 - _mm512_mask_mov_epi8

| VMOVDQU8_ZMMu8_MASKmskw_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Move packed 8-bit integers from "a" into "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := a[i+7:i]
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU8 - _mm512_mask_storeu_epi8

| VMOVDQU8_MEMu8_MASKmskw_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Store packed 8-bit integers from "a" into memory using writemask "k".
	"mem_addr" does not need to be aligned
on any particular boundary.

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        MEM[mem_addr+i+7:mem_addr+i] := a[i+7:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VMOVDQU8 - _mm512_maskz_loadu_epi8

| VMOVDQU8_ZMMu8_MASKmskw_MEMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed 8-bit integers from memory into "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).
	"mem_addr" does not need to be aligned on any particular boundary.

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := MEM[mem_addr+i+7:mem_addr+i]
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU8 - _mm512_maskz_mov_epi8

| VMOVDQU8_ZMMu8_MASKmskw_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Move packed 8-bit integers from "a" into "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := a[i+7:i]
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU8 - _mm_mask_loadu_epi8

| VMOVDQU8_XMMu8_MASKmskw_MEMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed 8-bit integers from memory into "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set). 
	"mem_addr" does not need to be aligned on any particular boundary.

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := MEM[mem_addr+i+7:mem_addr+i]
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU8 - _mm_mask_mov_epi8

| VMOVDQU8_XMMu8_MASKmskw_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Move packed 8-bit integers from "a" into "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := a[i+7:i]
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU8 - _mm_mask_storeu_epi8

| VMOVDQU8_MEMu8_MASKmskw_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Store packed 8-bit integers from "a" into memory using writemask "k".
	"mem_addr" does not need to be aligned
on any particular boundary.

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        MEM[mem_addr+i+7:mem_addr+i] := a[i+7:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VMOVDQU8 - _mm_maskz_loadu_epi8

| VMOVDQU8_XMMu8_MASKmskw_MEMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed 8-bit integers from memory into "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).
	"mem_addr" does not need to be aligned on any particular boundary.

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := MEM[mem_addr+i+7:mem_addr+i]
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU8 - _mm_maskz_mov_epi8

| VMOVDQU8_XMMu8_MASKmskw_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Move packed 8-bit integers from "a" into "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := a[i+7:i]
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPABSB - _mm256_mask_abs_epi8

| VPABSB_YMMi8_MASKmskw_YMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the absolute value of packed signed 8-bit integers in "a", and store the unsigned results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := ABS(a[i+7:i])
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPABSB - _mm256_maskz_abs_epi8

| VPABSB_YMMi8_MASKmskw_YMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the absolute value of packed signed 8-bit integers in "a", and store the unsigned results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := ABS(a[i+7:i])
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPABSB - _mm512_abs_epi8

| VPABSB_ZMMi8_MASKmskw_ZMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the absolute value of packed signed 8-bit integers in "a", and store the unsigned results in "dst".

[algorithm]

FOR j := 0 to 63
    i := j*8
    dst[i+7:i] := ABS(a[i+7:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPABSB - _mm512_mask_abs_epi8

| VPABSB_ZMMi8_MASKmskw_ZMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the absolute value of packed signed 8-bit integers in "a", and store the unsigned results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := ABS(a[i+7:i])
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPABSB - _mm512_maskz_abs_epi8

| VPABSB_ZMMi8_MASKmskw_ZMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the absolute value of packed signed 8-bit integers in "a", and store the unsigned results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := ABS(a[i+7:i])
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPABSB - _mm_mask_abs_epi8

| VPABSB_XMMi8_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the absolute value of packed signed 8-bit integers in "a", and store the unsigned results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := ABS(a[i+7:i])
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPABSB - _mm_maskz_abs_epi8

| VPABSB_XMMi8_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the absolute value of packed signed 8-bit integers in "a", and store the unsigned results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := ABS(a[i+7:i])
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPABSW - _mm256_mask_abs_epi16

| VPABSW_YMMi16_MASKmskw_YMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the absolute value of packed signed 16-bit integers in "a", and store the unsigned results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := ABS(a[i+15:i])
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPABSW - _mm256_maskz_abs_epi16

| VPABSW_YMMi16_MASKmskw_YMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the absolute value of packed signed 16-bit integers in "a", and store the unsigned results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := ABS(a[i+15:i])
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPABSW - _mm512_abs_epi16

| VPABSW_ZMMi16_MASKmskw_ZMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the absolute value of packed signed 16-bit integers in "a", and store the unsigned results in "dst".

[algorithm]

FOR j := 0 to 31
    i := j*16
    dst[i+15:i] := ABS(a[i+15:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPABSW - _mm512_mask_abs_epi16

| VPABSW_ZMMi16_MASKmskw_ZMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the absolute value of packed signed 16-bit integers in "a", and store the unsigned results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := ABS(a[i+15:i])
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPABSW - _mm512_maskz_abs_epi16

| VPABSW_ZMMi16_MASKmskw_ZMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the absolute value of packed signed 16-bit integers in "a", and store the unsigned results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := ABS(a[i+15:i])
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPABSW - _mm_mask_abs_epi16

| VPABSW_XMMi16_MASKmskw_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the absolute value of packed signed 16-bit integers in "a", and store the unsigned results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := ABS(a[i+15:i])
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPABSW - _mm_maskz_abs_epi16

| VPABSW_XMMi16_MASKmskw_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the absolute value of packed signed 16-bit integers in "a", and store the unsigned results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := ABS(a[i+15:i])
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPACKSSDW - _mm256_mask_packs_epi32

| VPACKSSDW_YMMi16_MASKmskw_YMMi32_YMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers from "a" and "b" to packed 16-bit integers using signed saturation, and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

tmp_dst[15:0] := Saturate16(a[31:0])
tmp_dst[31:16] := Saturate16(a[63:32])
tmp_dst[47:32] := Saturate16(a[95:64])
tmp_dst[63:48] := Saturate16(a[127:96])
tmp_dst[79:64] := Saturate16(b[31:0])
tmp_dst[95:80] := Saturate16(b[63:32])
tmp_dst[111:96] := Saturate16(b[95:64])
tmp_dst[127:112] := Saturate16(b[127:96])
tmp_dst[143:128] := Saturate16(a[159:128])
tmp_dst[159:144] := Saturate16(a[191:160])
tmp_dst[175:160] := Saturate16(a[223:192])
tmp_dst[191:176] := Saturate16(a[255:224])
tmp_dst[207:192] := Saturate16(b[159:128])
tmp_dst[223:208] := Saturate16(b[191:160])
tmp_dst[239:224] := Saturate16(b[223:192])
tmp_dst[255:240] := Saturate16(b[255:224])
FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPACKSSDW - _mm256_maskz_packs_epi32

| VPACKSSDW_YMMi16_MASKmskw_YMMi32_YMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers from "a" and "b" to packed 16-bit integers using signed saturation, and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

tmp_dst[15:0] := Saturate16(a[31:0])
tmp_dst[31:16] := Saturate16(a[63:32])
tmp_dst[47:32] := Saturate16(a[95:64])
tmp_dst[63:48] := Saturate16(a[127:96])
tmp_dst[79:64] := Saturate16(b[31:0])
tmp_dst[95:80] := Saturate16(b[63:32])
tmp_dst[111:96] := Saturate16(b[95:64])
tmp_dst[127:112] := Saturate16(b[127:96])
tmp_dst[143:128] := Saturate16(a[159:128])
tmp_dst[159:144] := Saturate16(a[191:160])
tmp_dst[175:160] := Saturate16(a[223:192])
tmp_dst[191:176] := Saturate16(a[255:224])
tmp_dst[207:192] := Saturate16(b[159:128])
tmp_dst[223:208] := Saturate16(b[191:160])
tmp_dst[239:224] := Saturate16(b[223:192])
tmp_dst[255:240] := Saturate16(b[255:224])
FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPACKSSDW - _mm512_mask_packs_epi32

| VPACKSSDW_ZMMi16_MASKmskw_ZMMi32_ZMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers from "a" and "b" to packed 16-bit integers using signed saturation, and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

tmp_dst[15:0] := Saturate16(a[31:0])
tmp_dst[31:16] := Saturate16(a[63:32])
tmp_dst[47:32] := Saturate16(a[95:64])
tmp_dst[63:48] := Saturate16(a[127:96])
tmp_dst[79:64] := Saturate16(b[31:0])
tmp_dst[95:80] := Saturate16(b[63:32])
tmp_dst[111:96] := Saturate16(b[95:64])
tmp_dst[127:112] := Saturate16(b[127:96])
tmp_dst[143:128] := Saturate16(a[159:128])
tmp_dst[159:144] := Saturate16(a[191:160])
tmp_dst[175:160] := Saturate16(a[223:192])
tmp_dst[191:176] := Saturate16(a[255:224])
tmp_dst[207:192] := Saturate16(b[159:128])
tmp_dst[223:208] := Saturate16(b[191:160])
tmp_dst[239:224] := Saturate16(b[223:192])
tmp_dst[255:240] := Saturate16(b[255:224])
tmp_dst[271:256] := Saturate16(a[287:256])
tmp_dst[287:272] := Saturate16(a[319:288])
tmp_dst[303:288] := Saturate16(a[351:320])
tmp_dst[319:304] := Saturate16(a[383:352])
tmp_dst[335:320] := Saturate16(b[287:256])
tmp_dst[351:336] := Saturate16(b[319:288])
tmp_dst[367:352] := Saturate16(b[351:320])
tmp_dst[383:368] := Saturate16(b[383:352])
tmp_dst[399:384] := Saturate16(a[415:384])
tmp_dst[415:400] := Saturate16(a[447:416])
tmp_dst[431:416] := Saturate16(a[479:448])
tmp_dst[447:432] := Saturate16(a[511:480])
tmp_dst[463:448] := Saturate16(b[415:384])
tmp_dst[479:464] := Saturate16(b[447:416])
tmp_dst[495:480] := Saturate16(b[479:448])
tmp_dst[511:496] := Saturate16(b[511:480])
FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPACKSSDW - _mm512_maskz_packs_epi32

| VPACKSSDW_ZMMi16_MASKmskw_ZMMi32_ZMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers from "a" and "b" to packed 16-bit integers using signed saturation, and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

tmp_dst[15:0] := Saturate16(a[31:0])
tmp_dst[31:16] := Saturate16(a[63:32])
tmp_dst[47:32] := Saturate16(a[95:64])
tmp_dst[63:48] := Saturate16(a[127:96])
tmp_dst[79:64] := Saturate16(b[31:0])
tmp_dst[95:80] := Saturate16(b[63:32])
tmp_dst[111:96] := Saturate16(b[95:64])
tmp_dst[127:112] := Saturate16(b[127:96])
tmp_dst[143:128] := Saturate16(a[159:128])
tmp_dst[159:144] := Saturate16(a[191:160])
tmp_dst[175:160] := Saturate16(a[223:192])
tmp_dst[191:176] := Saturate16(a[255:224])
tmp_dst[207:192] := Saturate16(b[159:128])
tmp_dst[223:208] := Saturate16(b[191:160])
tmp_dst[239:224] := Saturate16(b[223:192])
tmp_dst[255:240] := Saturate16(b[255:224])
tmp_dst[271:256] := Saturate16(a[287:256])
tmp_dst[287:272] := Saturate16(a[319:288])
tmp_dst[303:288] := Saturate16(a[351:320])
tmp_dst[319:304] := Saturate16(a[383:352])
tmp_dst[335:320] := Saturate16(b[287:256])
tmp_dst[351:336] := Saturate16(b[319:288])
tmp_dst[367:352] := Saturate16(b[351:320])
tmp_dst[383:368] := Saturate16(b[383:352])
tmp_dst[399:384] := Saturate16(a[415:384])
tmp_dst[415:400] := Saturate16(a[447:416])
tmp_dst[431:416] := Saturate16(a[479:448])
tmp_dst[447:432] := Saturate16(a[511:480])
tmp_dst[463:448] := Saturate16(b[415:384])
tmp_dst[479:464] := Saturate16(b[447:416])
tmp_dst[495:480] := Saturate16(b[479:448])
tmp_dst[511:496] := Saturate16(b[511:480])
FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPACKSSDW - _mm512_packs_epi32

| VPACKSSDW_ZMMi16_MASKmskw_ZMMi32_ZMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers from "a" and "b" to packed 16-bit integers using signed saturation, and
store the results in "dst".

[algorithm]

dst[15:0] := Saturate16(a[31:0])
dst[31:16] := Saturate16(a[63:32])
dst[47:32] := Saturate16(a[95:64])
dst[63:48] := Saturate16(a[127:96])
dst[79:64] := Saturate16(b[31:0])
dst[95:80] := Saturate16(b[63:32])
dst[111:96] := Saturate16(b[95:64])
dst[127:112] := Saturate16(b[127:96])
dst[143:128] := Saturate16(a[159:128])
dst[159:144] := Saturate16(a[191:160])
dst[175:160] := Saturate16(a[223:192])
dst[191:176] := Saturate16(a[255:224])
dst[207:192] := Saturate16(b[159:128])
dst[223:208] := Saturate16(b[191:160])
dst[239:224] := Saturate16(b[223:192])
dst[255:240] := Saturate16(b[255:224])
dst[271:256] := Saturate16(a[287:256])
dst[287:272] := Saturate16(a[319:288])
dst[303:288] := Saturate16(a[351:320])
dst[319:304] := Saturate16(a[383:352])
dst[335:320] := Saturate16(b[287:256])
dst[351:336] := Saturate16(b[319:288])
dst[367:352] := Saturate16(b[351:320])
dst[383:368] := Saturate16(b[383:352])
dst[399:384] := Saturate16(a[415:384])
dst[415:400] := Saturate16(a[447:416])
dst[431:416] := Saturate16(a[479:448])
dst[447:432] := Saturate16(a[511:480])
dst[463:448] := Saturate16(b[415:384])
dst[479:464] := Saturate16(b[447:416])
dst[495:480] := Saturate16(b[479:448])
dst[511:496] := Saturate16(b[511:480])
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPACKSSDW - _mm_mask_packs_epi32

| VPACKSSDW_XMMi16_MASKmskw_XMMi32_XMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers from "a" and "b" to packed 16-bit integers using signed saturation, and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

tmp_dst[15:0] := Saturate16(a[31:0])
tmp_dst[31:16] := Saturate16(a[63:32])
tmp_dst[47:32] := Saturate16(a[95:64])
tmp_dst[63:48] := Saturate16(a[127:96])
tmp_dst[79:64] := Saturate16(b[31:0])
tmp_dst[95:80] := Saturate16(b[63:32])
tmp_dst[111:96] := Saturate16(b[95:64])
tmp_dst[127:112] := Saturate16(b[127:96])
FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPACKSSDW - _mm_maskz_packs_epi32

| VPACKSSDW_XMMi16_MASKmskw_XMMi32_XMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers from "a" and "b" to packed 16-bit integers using signed saturation, and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

tmp_dst[15:0] := Saturate16(a[31:0])
tmp_dst[31:16] := Saturate16(a[63:32])
tmp_dst[47:32] := Saturate16(a[95:64])
tmp_dst[63:48] := Saturate16(a[127:96])
tmp_dst[79:64] := Saturate16(b[31:0])
tmp_dst[95:80] := Saturate16(b[63:32])
tmp_dst[111:96] := Saturate16(b[95:64])
tmp_dst[127:112] := Saturate16(b[127:96])
FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPACKSSWB - _mm256_mask_packs_epi16

| VPACKSSWB_YMMi8_MASKmskw_YMMi16_YMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 16-bit integers from "a" and "b" to packed 8-bit integers using signed saturation, and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

tmp_dst[7:0] := Saturate8(a[15:0])
tmp_dst[15:8] := Saturate8(a[31:16])
tmp_dst[23:16] := Saturate8(a[47:32])
tmp_dst[31:24] := Saturate8(a[63:48])
tmp_dst[39:32] := Saturate8(a[79:64])
tmp_dst[47:40] := Saturate8(a[95:80])
tmp_dst[55:48] := Saturate8(a[111:96])
tmp_dst[63:56] := Saturate8(a[127:112])
tmp_dst[71:64] := Saturate8(b[15:0])
tmp_dst[79:72] := Saturate8(b[31:16])
tmp_dst[87:80] := Saturate8(b[47:32])
tmp_dst[95:88] := Saturate8(b[63:48])
tmp_dst[103:96] := Saturate8(b[79:64])
tmp_dst[111:104] := Saturate8(b[95:80])
tmp_dst[119:112] := Saturate8(b[111:96])
tmp_dst[127:120] := Saturate8(b[127:112])
tmp_dst[135:128] := Saturate8(a[143:128])
tmp_dst[143:136] := Saturate8(a[159:144])
tmp_dst[151:144] := Saturate8(a[175:160])
tmp_dst[159:152] := Saturate8(a[191:176])
tmp_dst[167:160] := Saturate8(a[207:192])
tmp_dst[175:168] := Saturate8(a[223:208])
tmp_dst[183:176] := Saturate8(a[239:224])
tmp_dst[191:184] := Saturate8(a[255:240])
tmp_dst[199:192] := Saturate8(b[143:128])
tmp_dst[207:200] := Saturate8(b[159:144])
tmp_dst[215:208] := Saturate8(b[175:160])
tmp_dst[223:216] := Saturate8(b[191:176])
tmp_dst[231:224] := Saturate8(b[207:192])
tmp_dst[239:232] := Saturate8(b[223:208])
tmp_dst[247:240] := Saturate8(b[239:224])
tmp_dst[255:248] := Saturate8(b[255:240])
FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := tmp_dst[i+7:i]
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPACKSSWB - _mm256_maskz_packs_epi16

| VPACKSSWB_YMMi8_MASKmskw_YMMi16_YMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 16-bit integers from "a" and "b" to packed 8-bit integers using signed saturation, and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

tmp_dst[7:0] := Saturate8(a[15:0])
tmp_dst[15:8] := Saturate8(a[31:16])
tmp_dst[23:16] := Saturate8(a[47:32])
tmp_dst[31:24] := Saturate8(a[63:48])
tmp_dst[39:32] := Saturate8(a[79:64])
tmp_dst[47:40] := Saturate8(a[95:80])
tmp_dst[55:48] := Saturate8(a[111:96])
tmp_dst[63:56] := Saturate8(a[127:112])
tmp_dst[71:64] := Saturate8(b[15:0])
tmp_dst[79:72] := Saturate8(b[31:16])
tmp_dst[87:80] := Saturate8(b[47:32])
tmp_dst[95:88] := Saturate8(b[63:48])
tmp_dst[103:96] := Saturate8(b[79:64])
tmp_dst[111:104] := Saturate8(b[95:80])
tmp_dst[119:112] := Saturate8(b[111:96])
tmp_dst[127:120] := Saturate8(b[127:112])
tmp_dst[135:128] := Saturate8(a[143:128])
tmp_dst[143:136] := Saturate8(a[159:144])
tmp_dst[151:144] := Saturate8(a[175:160])
tmp_dst[159:152] := Saturate8(a[191:176])
tmp_dst[167:160] := Saturate8(a[207:192])
tmp_dst[175:168] := Saturate8(a[223:208])
tmp_dst[183:176] := Saturate8(a[239:224])
tmp_dst[191:184] := Saturate8(a[255:240])
tmp_dst[199:192] := Saturate8(b[143:128])
tmp_dst[207:200] := Saturate8(b[159:144])
tmp_dst[215:208] := Saturate8(b[175:160])
tmp_dst[223:216] := Saturate8(b[191:176])
tmp_dst[231:224] := Saturate8(b[207:192])
tmp_dst[239:232] := Saturate8(b[223:208])
tmp_dst[247:240] := Saturate8(b[239:224])
tmp_dst[255:248] := Saturate8(b[255:240])
FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := tmp_dst[i+7:i]
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPACKSSWB - _mm512_mask_packs_epi16

| VPACKSSWB_ZMMi8_MASKmskw_ZMMi16_ZMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 16-bit integers from "a" and "b" to packed 8-bit integers using signed saturation, and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

tmp_dst[7:0] := Saturate8(a[15:0])
tmp_dst[15:8] := Saturate8(a[31:16])
tmp_dst[23:16] := Saturate8(a[47:32])
tmp_dst[31:24] := Saturate8(a[63:48])
tmp_dst[39:32] := Saturate8(a[79:64])
tmp_dst[47:40] := Saturate8(a[95:80])
tmp_dst[55:48] := Saturate8(a[111:96])
tmp_dst[63:56] := Saturate8(a[127:112])
tmp_dst[71:64] := Saturate8(b[15:0])
tmp_dst[79:72] := Saturate8(b[31:16])
tmp_dst[87:80] := Saturate8(b[47:32])
tmp_dst[95:88] := Saturate8(b[63:48])
tmp_dst[103:96] := Saturate8(b[79:64])
tmp_dst[111:104] := Saturate8(b[95:80])
tmp_dst[119:112] := Saturate8(b[111:96])
tmp_dst[127:120] := Saturate8(b[127:112])
tmp_dst[135:128] := Saturate8(a[143:128])
tmp_dst[143:136] := Saturate8(a[159:144])
tmp_dst[151:144] := Saturate8(a[175:160])
tmp_dst[159:152] := Saturate8(a[191:176])
tmp_dst[167:160] := Saturate8(a[207:192])
tmp_dst[175:168] := Saturate8(a[223:208])
tmp_dst[183:176] := Saturate8(a[239:224])
tmp_dst[191:184] := Saturate8(a[255:240])
tmp_dst[199:192] := Saturate8(b[143:128])
tmp_dst[207:200] := Saturate8(b[159:144])
tmp_dst[215:208] := Saturate8(b[175:160])
tmp_dst[223:216] := Saturate8(b[191:176])
tmp_dst[231:224] := Saturate8(b[207:192])
tmp_dst[239:232] := Saturate8(b[223:208])
tmp_dst[247:240] := Saturate8(b[239:224])
tmp_dst[255:248] := Saturate8(b[255:240])
tmp_dst[263:256] := Saturate8(a[271:256])
tmp_dst[271:264] := Saturate8(a[287:272])
tmp_dst[279:272] := Saturate8(a[303:288])
tmp_dst[287:280] := Saturate8(a[319:304])
tmp_dst[295:288] := Saturate8(a[335:320])
tmp_dst[303:296] := Saturate8(a[351:336])
tmp_dst[311:304] := Saturate8(a[367:352])
tmp_dst[319:312] := Saturate8(a[383:368])
tmp_dst[327:320] := Saturate8(b[271:256])
tmp_dst[335:328] := Saturate8(b[287:272])
tmp_dst[343:336] := Saturate8(b[303:288])
tmp_dst[351:344] := Saturate8(b[319:304])
tmp_dst[359:352] := Saturate8(b[335:320])
tmp_dst[367:360] := Saturate8(b[351:336])
tmp_dst[375:368] := Saturate8(b[367:352])
tmp_dst[383:376] := Saturate8(b[383:368])
tmp_dst[391:384] := Saturate8(a[399:384])
tmp_dst[399:392] := Saturate8(a[415:400])
tmp_dst[407:400] := Saturate8(a[431:416])
tmp_dst[415:408] := Saturate8(a[447:432])
tmp_dst[423:416] := Saturate8(a[463:448])
tmp_dst[431:424] := Saturate8(a[479:464])
tmp_dst[439:432] := Saturate8(a[495:480])
tmp_dst[447:440] := Saturate8(a[511:496])
tmp_dst[455:448] := Saturate8(b[399:384])
tmp_dst[463:456] := Saturate8(b[415:400])
tmp_dst[471:464] := Saturate8(b[431:416])
tmp_dst[479:472] := Saturate8(b[447:432])
tmp_dst[487:480] := Saturate8(b[463:448])
tmp_dst[495:488] := Saturate8(b[479:464])
tmp_dst[503:496] := Saturate8(b[495:480])
tmp_dst[511:504] := Saturate8(b[511:496])
FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := tmp_dst[i+7:i]
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPACKSSWB - _mm512_maskz_packs_epi16

| VPACKSSWB_ZMMi8_MASKmskw_ZMMi16_ZMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 16-bit integers from "a" and "b" to packed 8-bit integers using signed saturation, and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

tmp_dst[7:0] := Saturate8(a[15:0])
tmp_dst[15:8] := Saturate8(a[31:16])
tmp_dst[23:16] := Saturate8(a[47:32])
tmp_dst[31:24] := Saturate8(a[63:48])
tmp_dst[39:32] := Saturate8(a[79:64])
tmp_dst[47:40] := Saturate8(a[95:80])
tmp_dst[55:48] := Saturate8(a[111:96])
tmp_dst[63:56] := Saturate8(a[127:112])
tmp_dst[71:64] := Saturate8(b[15:0])
tmp_dst[79:72] := Saturate8(b[31:16])
tmp_dst[87:80] := Saturate8(b[47:32])
tmp_dst[95:88] := Saturate8(b[63:48])
tmp_dst[103:96] := Saturate8(b[79:64])
tmp_dst[111:104] := Saturate8(b[95:80])
tmp_dst[119:112] := Saturate8(b[111:96])
tmp_dst[127:120] := Saturate8(b[127:112])
tmp_dst[135:128] := Saturate8(a[143:128])
tmp_dst[143:136] := Saturate8(a[159:144])
tmp_dst[151:144] := Saturate8(a[175:160])
tmp_dst[159:152] := Saturate8(a[191:176])
tmp_dst[167:160] := Saturate8(a[207:192])
tmp_dst[175:168] := Saturate8(a[223:208])
tmp_dst[183:176] := Saturate8(a[239:224])
tmp_dst[191:184] := Saturate8(a[255:240])
tmp_dst[199:192] := Saturate8(b[143:128])
tmp_dst[207:200] := Saturate8(b[159:144])
tmp_dst[215:208] := Saturate8(b[175:160])
tmp_dst[223:216] := Saturate8(b[191:176])
tmp_dst[231:224] := Saturate8(b[207:192])
tmp_dst[239:232] := Saturate8(b[223:208])
tmp_dst[247:240] := Saturate8(b[239:224])
tmp_dst[255:248] := Saturate8(b[255:240])
tmp_dst[263:256] := Saturate8(a[271:256])
tmp_dst[271:264] := Saturate8(a[287:272])
tmp_dst[279:272] := Saturate8(a[303:288])
tmp_dst[287:280] := Saturate8(a[319:304])
tmp_dst[295:288] := Saturate8(a[335:320])
tmp_dst[303:296] := Saturate8(a[351:336])
tmp_dst[311:304] := Saturate8(a[367:352])
tmp_dst[319:312] := Saturate8(a[383:368])
tmp_dst[327:320] := Saturate8(b[271:256])
tmp_dst[335:328] := Saturate8(b[287:272])
tmp_dst[343:336] := Saturate8(b[303:288])
tmp_dst[351:344] := Saturate8(b[319:304])
tmp_dst[359:352] := Saturate8(b[335:320])
tmp_dst[367:360] := Saturate8(b[351:336])
tmp_dst[375:368] := Saturate8(b[367:352])
tmp_dst[383:376] := Saturate8(b[383:368])
tmp_dst[391:384] := Saturate8(a[399:384])
tmp_dst[399:392] := Saturate8(a[415:400])
tmp_dst[407:400] := Saturate8(a[431:416])
tmp_dst[415:408] := Saturate8(a[447:432])
tmp_dst[423:416] := Saturate8(a[463:448])
tmp_dst[431:424] := Saturate8(a[479:464])
tmp_dst[439:432] := Saturate8(a[495:480])
tmp_dst[447:440] := Saturate8(a[511:496])
tmp_dst[455:448] := Saturate8(b[399:384])
tmp_dst[463:456] := Saturate8(b[415:400])
tmp_dst[471:464] := Saturate8(b[431:416])
tmp_dst[479:472] := Saturate8(b[447:432])
tmp_dst[487:480] := Saturate8(b[463:448])
tmp_dst[495:488] := Saturate8(b[479:464])
tmp_dst[503:496] := Saturate8(b[495:480])
tmp_dst[511:504] := Saturate8(b[511:496])
FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := tmp_dst[i+7:i]
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPACKSSWB - _mm512_packs_epi16

| VPACKSSWB_ZMMi8_MASKmskw_ZMMi16_ZMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 16-bit integers from "a" and "b" to packed 8-bit integers using signed saturation, and
store the results in "dst".

[algorithm]

dst[7:0] := Saturate8(a[15:0])
dst[15:8] := Saturate8(a[31:16])
dst[23:16] := Saturate8(a[47:32])
dst[31:24] := Saturate8(a[63:48])
dst[39:32] := Saturate8(a[79:64])
dst[47:40] := Saturate8(a[95:80])
dst[55:48] := Saturate8(a[111:96])
dst[63:56] := Saturate8(a[127:112])
dst[71:64] := Saturate8(b[15:0])
dst[79:72] := Saturate8(b[31:16])
dst[87:80] := Saturate8(b[47:32])
dst[95:88] := Saturate8(b[63:48])
dst[103:96] := Saturate8(b[79:64])
dst[111:104] := Saturate8(b[95:80])
dst[119:112] := Saturate8(b[111:96])
dst[127:120] := Saturate8(b[127:112])
dst[135:128] := Saturate8(a[143:128])
dst[143:136] := Saturate8(a[159:144])
dst[151:144] := Saturate8(a[175:160])
dst[159:152] := Saturate8(a[191:176])
dst[167:160] := Saturate8(a[207:192])
dst[175:168] := Saturate8(a[223:208])
dst[183:176] := Saturate8(a[239:224])
dst[191:184] := Saturate8(a[255:240])
dst[199:192] := Saturate8(b[143:128])
dst[207:200] := Saturate8(b[159:144])
dst[215:208] := Saturate8(b[175:160])
dst[223:216] := Saturate8(b[191:176])
dst[231:224] := Saturate8(b[207:192])
dst[239:232] := Saturate8(b[223:208])
dst[247:240] := Saturate8(b[239:224])
dst[255:248] := Saturate8(b[255:240])
dst[263:256] := Saturate8(a[271:256])
dst[271:264] := Saturate8(a[287:272])
dst[279:272] := Saturate8(a[303:288])
dst[287:280] := Saturate8(a[319:304])
dst[295:288] := Saturate8(a[335:320])
dst[303:296] := Saturate8(a[351:336])
dst[311:304] := Saturate8(a[367:352])
dst[319:312] := Saturate8(a[383:368])
dst[327:320] := Saturate8(b[271:256])
dst[335:328] := Saturate8(b[287:272])
dst[343:336] := Saturate8(b[303:288])
dst[351:344] := Saturate8(b[319:304])
dst[359:352] := Saturate8(b[335:320])
dst[367:360] := Saturate8(b[351:336])
dst[375:368] := Saturate8(b[367:352])
dst[383:376] := Saturate8(b[383:368])
dst[391:384] := Saturate8(a[399:384])
dst[399:392] := Saturate8(a[415:400])
dst[407:400] := Saturate8(a[431:416])
dst[415:408] := Saturate8(a[447:432])
dst[423:416] := Saturate8(a[463:448])
dst[431:424] := Saturate8(a[479:464])
dst[439:432] := Saturate8(a[495:480])
dst[447:440] := Saturate8(a[511:496])
dst[455:448] := Saturate8(b[399:384])
dst[463:456] := Saturate8(b[415:400])
dst[471:464] := Saturate8(b[431:416])
dst[479:472] := Saturate8(b[447:432])
dst[487:480] := Saturate8(b[463:448])
dst[495:488] := Saturate8(b[479:464])
dst[503:496] := Saturate8(b[495:480])
dst[511:504] := Saturate8(b[511:496])
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPACKSSWB - _mm_mask_packs_epi16

| VPACKSSWB_XMMi8_MASKmskw_XMMi16_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 16-bit integers from "a" and "b" to packed 8-bit integers using signed saturation, and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

tmp_dst[7:0] := Saturate8(a[15:0])
tmp_dst[15:8] := Saturate8(a[31:16])
tmp_dst[23:16] := Saturate8(a[47:32])
tmp_dst[31:24] := Saturate8(a[63:48])
tmp_dst[39:32] := Saturate8(a[79:64])
tmp_dst[47:40] := Saturate8(a[95:80])
tmp_dst[55:48] := Saturate8(a[111:96])
tmp_dst[63:56] := Saturate8(a[127:112])
tmp_dst[71:64] := Saturate8(b[15:0])
tmp_dst[79:72] := Saturate8(b[31:16])
tmp_dst[87:80] := Saturate8(b[47:32])
tmp_dst[95:88] := Saturate8(b[63:48])
tmp_dst[103:96] := Saturate8(b[79:64])
tmp_dst[111:104] := Saturate8(b[95:80])
tmp_dst[119:112] := Saturate8(b[111:96])
tmp_dst[127:120] := Saturate8(b[127:112])
FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := tmp_dst[i+7:i]
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPACKSSWB - _mm_maskz_packs_epi16

| VPACKSSWB_XMMi8_MASKmskw_XMMi16_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 16-bit integers from "a" and "b" to packed 8-bit integers using signed saturation, and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

tmp_dst[7:0] := Saturate8(a[15:0])
tmp_dst[15:8] := Saturate8(a[31:16])
tmp_dst[23:16] := Saturate8(a[47:32])
tmp_dst[31:24] := Saturate8(a[63:48])
tmp_dst[39:32] := Saturate8(a[79:64])
tmp_dst[47:40] := Saturate8(a[95:80])
tmp_dst[55:48] := Saturate8(a[111:96])
tmp_dst[63:56] := Saturate8(a[127:112])
tmp_dst[71:64] := Saturate8(b[15:0])
tmp_dst[79:72] := Saturate8(b[31:16])
tmp_dst[87:80] := Saturate8(b[47:32])
tmp_dst[95:88] := Saturate8(b[63:48])
tmp_dst[103:96] := Saturate8(b[79:64])
tmp_dst[111:104] := Saturate8(b[95:80])
tmp_dst[119:112] := Saturate8(b[111:96])
tmp_dst[127:120] := Saturate8(b[127:112])
FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := tmp_dst[i+7:i]
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPACKUSDW - _mm256_mask_packus_epi32

| VPACKUSDW_YMMu16_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers from "a" and "b" to packed 16-bit integers using unsigned saturation,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

tmp_dst[15:0] := SaturateU16(a[31:0])
tmp_dst[31:16] := SaturateU16(a[63:32])
tmp_dst[47:32] := SaturateU16(a[95:64])
tmp_dst[63:48] := SaturateU16(a[127:96])
tmp_dst[79:64] := SaturateU16(b[31:0])
tmp_dst[95:80] := SaturateU16(b[63:32])
tmp_dst[111:96] := SaturateU16(b[95:64])
tmp_dst[127:112] := SaturateU16(b[127:96])
tmp_dst[143:128] := SaturateU16(a[159:128])
tmp_dst[159:144] := SaturateU16(a[191:160])
tmp_dst[175:160] := SaturateU16(a[223:192])
tmp_dst[191:176] := SaturateU16(a[255:224])
tmp_dst[207:192] := SaturateU16(b[159:128])
tmp_dst[223:208] := SaturateU16(b[191:160])
tmp_dst[239:224] := SaturateU16(b[223:192])
tmp_dst[255:240] := SaturateU16(b[255:224])
FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPACKUSDW - _mm256_maskz_packus_epi32

| VPACKUSDW_YMMu16_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers from "a" and "b" to packed 16-bit integers using unsigned saturation,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

tmp_dst[15:0] := SaturateU16(a[31:0])
tmp_dst[31:16] := SaturateU16(a[63:32])
tmp_dst[47:32] := SaturateU16(a[95:64])
tmp_dst[63:48] := SaturateU16(a[127:96])
tmp_dst[79:64] := SaturateU16(b[31:0])
tmp_dst[95:80] := SaturateU16(b[63:32])
tmp_dst[111:96] := SaturateU16(b[95:64])
tmp_dst[127:112] := SaturateU16(b[127:96])
tmp_dst[143:128] := SaturateU16(a[159:128])
tmp_dst[159:144] := SaturateU16(a[191:160])
tmp_dst[175:160] := SaturateU16(a[223:192])
tmp_dst[191:176] := SaturateU16(a[255:224])
tmp_dst[207:192] := SaturateU16(b[159:128])
tmp_dst[223:208] := SaturateU16(b[191:160])
tmp_dst[239:224] := SaturateU16(b[223:192])
tmp_dst[255:240] := SaturateU16(b[255:224])
FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPACKUSDW - _mm512_mask_packus_epi32

| VPACKUSDW_ZMMu16_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers from "a" and "b" to packed 16-bit integers using unsigned saturation,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

tmp_dst[15:0] := SaturateU16(a[31:0])
tmp_dst[31:16] := SaturateU16(a[63:32])
tmp_dst[47:32] := SaturateU16(a[95:64])
tmp_dst[63:48] := SaturateU16(a[127:96])
tmp_dst[79:64] := SaturateU16(b[31:0])
tmp_dst[95:80] := SaturateU16(b[63:32])
tmp_dst[111:96] := SaturateU16(b[95:64])
tmp_dst[127:112] := SaturateU16(b[127:96])
tmp_dst[143:128] := SaturateU16(a[159:128])
tmp_dst[159:144] := SaturateU16(a[191:160])
tmp_dst[175:160] := SaturateU16(a[223:192])
tmp_dst[191:176] := SaturateU16(a[255:224])
tmp_dst[207:192] := SaturateU16(b[159:128])
tmp_dst[223:208] := SaturateU16(b[191:160])
tmp_dst[239:224] := SaturateU16(b[223:192])
tmp_dst[255:240] := SaturateU16(b[255:224])
tmp_dst[271:256] := SaturateU16(a[287:256])
tmp_dst[287:272] := SaturateU16(a[319:288])
tmp_dst[303:288] := SaturateU16(a[351:320])
tmp_dst[319:304] := SaturateU16(a[383:352])
tmp_dst[335:320] := SaturateU16(b[287:256])
tmp_dst[351:336] := SaturateU16(b[319:288])
tmp_dst[367:352] := SaturateU16(b[351:320])
tmp_dst[383:368] := SaturateU16(b[383:352])
tmp_dst[399:384] := SaturateU16(a[415:384])
tmp_dst[415:400] := SaturateU16(a[447:416])
tmp_dst[431:416] := SaturateU16(a[479:448])
tmp_dst[447:432] := SaturateU16(a[511:480])
tmp_dst[463:448] := SaturateU16(b[415:384])
tmp_dst[479:464] := SaturateU16(b[447:416])
tmp_dst[495:480] := SaturateU16(b[479:448])
tmp_dst[511:496] := SaturateU16(b[511:480])
FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPACKUSDW - _mm512_maskz_packus_epi32

| VPACKUSDW_ZMMu16_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers from "a" and "b" to packed 16-bit integers using unsigned saturation,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

tmp_dst[15:0] := SaturateU16(a[31:0])
tmp_dst[31:16] := SaturateU16(a[63:32])
tmp_dst[47:32] := SaturateU16(a[95:64])
tmp_dst[63:48] := SaturateU16(a[127:96])
tmp_dst[79:64] := SaturateU16(b[31:0])
tmp_dst[95:80] := SaturateU16(b[63:32])
tmp_dst[111:96] := SaturateU16(b[95:64])
tmp_dst[127:112] := SaturateU16(b[127:96])
tmp_dst[143:128] := SaturateU16(a[159:128])
tmp_dst[159:144] := SaturateU16(a[191:160])
tmp_dst[175:160] := SaturateU16(a[223:192])
tmp_dst[191:176] := SaturateU16(a[255:224])
tmp_dst[207:192] := SaturateU16(b[159:128])
tmp_dst[223:208] := SaturateU16(b[191:160])
tmp_dst[239:224] := SaturateU16(b[223:192])
tmp_dst[255:240] := SaturateU16(b[255:224])
tmp_dst[271:256] := SaturateU16(a[287:256])
tmp_dst[287:272] := SaturateU16(a[319:288])
tmp_dst[303:288] := SaturateU16(a[351:320])
tmp_dst[319:304] := SaturateU16(a[383:352])
tmp_dst[335:320] := SaturateU16(b[287:256])
tmp_dst[351:336] := SaturateU16(b[319:288])
tmp_dst[367:352] := SaturateU16(b[351:320])
tmp_dst[383:368] := SaturateU16(b[383:352])
tmp_dst[399:384] := SaturateU16(a[415:384])
tmp_dst[415:400] := SaturateU16(a[447:416])
tmp_dst[431:416] := SaturateU16(a[479:448])
tmp_dst[447:432] := SaturateU16(a[511:480])
tmp_dst[463:448] := SaturateU16(b[415:384])
tmp_dst[479:464] := SaturateU16(b[447:416])
tmp_dst[495:480] := SaturateU16(b[479:448])
tmp_dst[511:496] := SaturateU16(b[511:480])
FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPACKUSDW - _mm512_packus_epi32

| VPACKUSDW_ZMMu16_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers from "a" and "b" to packed 16-bit integers using unsigned saturation,
and store the results in "dst".

[algorithm]

dst[15:0] := SaturateU16(a[31:0])
dst[31:16] := SaturateU16(a[63:32])
dst[47:32] := SaturateU16(a[95:64])
dst[63:48] := SaturateU16(a[127:96])
dst[79:64] := SaturateU16(b[31:0])
dst[95:80] := SaturateU16(b[63:32])
dst[111:96] := SaturateU16(b[95:64])
dst[127:112] := SaturateU16(b[127:96])
dst[143:128] := SaturateU16(a[159:128])
dst[159:144] := SaturateU16(a[191:160])
dst[175:160] := SaturateU16(a[223:192])
dst[191:176] := SaturateU16(a[255:224])
dst[207:192] := SaturateU16(b[159:128])
dst[223:208] := SaturateU16(b[191:160])
dst[239:224] := SaturateU16(b[223:192])
dst[255:240] := SaturateU16(b[255:224])
dst[271:256] := SaturateU16(a[287:256])
dst[287:272] := SaturateU16(a[319:288])
dst[303:288] := SaturateU16(a[351:320])
dst[319:304] := SaturateU16(a[383:352])
dst[335:320] := SaturateU16(b[287:256])
dst[351:336] := SaturateU16(b[319:288])
dst[367:352] := SaturateU16(b[351:320])
dst[383:368] := SaturateU16(b[383:352])
dst[399:384] := SaturateU16(a[415:384])
dst[415:400] := SaturateU16(a[447:416])
dst[431:416] := SaturateU16(a[479:448])
dst[447:432] := SaturateU16(a[511:480])
dst[463:448] := SaturateU16(b[415:384])
dst[479:464] := SaturateU16(b[447:416])
dst[495:480] := SaturateU16(b[479:448])
dst[511:496] := SaturateU16(b[511:480])
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPACKUSDW - _mm_mask_packus_epi32

| VPACKUSDW_XMMu16_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers from "a" and "b" to packed 16-bit integers using unsigned saturation,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

tmp_dst[15:0] := SaturateU16(a[31:0])
tmp_dst[31:16] := SaturateU16(a[63:32])
tmp_dst[47:32] := SaturateU16(a[95:64])
tmp_dst[63:48] := SaturateU16(a[127:96])
tmp_dst[79:64] := SaturateU16(b[31:0])
tmp_dst[95:80] := SaturateU16(b[63:32])
tmp_dst[111:96] := SaturateU16(b[95:64])
tmp_dst[127:112] := SaturateU16(b[127:96])
FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPACKUSDW - _mm_maskz_packus_epi32

| VPACKUSDW_XMMu16_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers from "a" and "b" to packed 16-bit integers using unsigned saturation,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

tmp_dst[15:0] := SaturateU16(a[31:0])
tmp_dst[31:16] := SaturateU16(a[63:32])
tmp_dst[47:32] := SaturateU16(a[95:64])
tmp_dst[63:48] := SaturateU16(a[127:96])
tmp_dst[79:64] := SaturateU16(b[31:0])
tmp_dst[95:80] := SaturateU16(b[63:32])
tmp_dst[111:96] := SaturateU16(b[95:64])
tmp_dst[127:112] := SaturateU16(b[127:96])
FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPACKUSWB - _mm256_mask_packus_epi16

| VPACKUSWB_YMMu8_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 16-bit integers from "a" and "b" to packed 8-bit integers using unsigned saturation, and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

tmp_dst[7:0] := SaturateU8(a[15:0])
tmp_dst[15:8] := SaturateU8(a[31:16])
tmp_dst[23:16] := SaturateU8(a[47:32])
tmp_dst[31:24] := SaturateU8(a[63:48])
tmp_dst[39:32] := SaturateU8(a[79:64])
tmp_dst[47:40] := SaturateU8(a[95:80])
tmp_dst[55:48] := SaturateU8(a[111:96])
tmp_dst[63:56] := SaturateU8(a[127:112])
tmp_dst[71:64] := SaturateU8(b[15:0])
tmp_dst[79:72] := SaturateU8(b[31:16])
tmp_dst[87:80] := SaturateU8(b[47:32])
tmp_dst[95:88] := SaturateU8(b[63:48])
tmp_dst[103:96] := SaturateU8(b[79:64])
tmp_dst[111:104] := SaturateU8(b[95:80])
tmp_dst[119:112] := SaturateU8(b[111:96])
tmp_dst[127:120] := SaturateU8(b[127:112])
tmp_dst[135:128] := SaturateU8(a[143:128])
tmp_dst[143:136] := SaturateU8(a[159:144])
tmp_dst[151:144] := SaturateU8(a[175:160])
tmp_dst[159:152] := SaturateU8(a[191:176])
tmp_dst[167:160] := SaturateU8(a[207:192])
tmp_dst[175:168] := SaturateU8(a[223:208])
tmp_dst[183:176] := SaturateU8(a[239:224])
tmp_dst[191:184] := SaturateU8(a[255:240])
tmp_dst[199:192] := SaturateU8(b[143:128])
tmp_dst[207:200] := SaturateU8(b[159:144])
tmp_dst[215:208] := SaturateU8(b[175:160])
tmp_dst[223:216] := SaturateU8(b[191:176])
tmp_dst[231:224] := SaturateU8(b[207:192])
tmp_dst[239:232] := SaturateU8(b[223:208])
tmp_dst[247:240] := SaturateU8(b[239:224])
tmp_dst[255:248] := SaturateU8(b[255:240])
FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := tmp_dst[i+7:i]
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPACKUSWB - _mm256_maskz_packus_epi16

| VPACKUSWB_YMMu8_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 16-bit integers from "a" and "b" to packed 8-bit integers using unsigned saturation, and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

tmp_dst[7:0] := SaturateU8(a[15:0])
tmp_dst[15:8] := SaturateU8(a[31:16])
tmp_dst[23:16] := SaturateU8(a[47:32])
tmp_dst[31:24] := SaturateU8(a[63:48])
tmp_dst[39:32] := SaturateU8(a[79:64])
tmp_dst[47:40] := SaturateU8(a[95:80])
tmp_dst[55:48] := SaturateU8(a[111:96])
tmp_dst[63:56] := SaturateU8(a[127:112])
tmp_dst[71:64] := SaturateU8(b[15:0])
tmp_dst[79:72] := SaturateU8(b[31:16])
tmp_dst[87:80] := SaturateU8(b[47:32])
tmp_dst[95:88] := SaturateU8(b[63:48])
tmp_dst[103:96] := SaturateU8(b[79:64])
tmp_dst[111:104] := SaturateU8(b[95:80])
tmp_dst[119:112] := SaturateU8(b[111:96])
tmp_dst[127:120] := SaturateU8(b[127:112])
tmp_dst[135:128] := SaturateU8(a[143:128])
tmp_dst[143:136] := SaturateU8(a[159:144])
tmp_dst[151:144] := SaturateU8(a[175:160])
tmp_dst[159:152] := SaturateU8(a[191:176])
tmp_dst[167:160] := SaturateU8(a[207:192])
tmp_dst[175:168] := SaturateU8(a[223:208])
tmp_dst[183:176] := SaturateU8(a[239:224])
tmp_dst[191:184] := SaturateU8(a[255:240])
tmp_dst[199:192] := SaturateU8(b[143:128])
tmp_dst[207:200] := SaturateU8(b[159:144])
tmp_dst[215:208] := SaturateU8(b[175:160])
tmp_dst[223:216] := SaturateU8(b[191:176])
tmp_dst[231:224] := SaturateU8(b[207:192])
tmp_dst[239:232] := SaturateU8(b[223:208])
tmp_dst[247:240] := SaturateU8(b[239:224])
tmp_dst[255:248] := SaturateU8(b[255:240])
FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := tmp_dst[i+7:i]
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPACKUSWB - _mm512_mask_packus_epi16

| VPACKUSWB_ZMMu8_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 16-bit integers from "a" and "b" to packed 8-bit integers using unsigned saturation, and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

tmp_dst[7:0] := SaturateU8(a[15:0])
tmp_dst[15:8] := SaturateU8(a[31:16])
tmp_dst[23:16] := SaturateU8(a[47:32])
tmp_dst[31:24] := SaturateU8(a[63:48])
tmp_dst[39:32] := SaturateU8(a[79:64])
tmp_dst[47:40] := SaturateU8(a[95:80])
tmp_dst[55:48] := SaturateU8(a[111:96])
tmp_dst[63:56] := SaturateU8(a[127:112])
tmp_dst[71:64] := SaturateU8(b[15:0])
tmp_dst[79:72] := SaturateU8(b[31:16])
tmp_dst[87:80] := SaturateU8(b[47:32])
tmp_dst[95:88] := SaturateU8(b[63:48])
tmp_dst[103:96] := SaturateU8(b[79:64])
tmp_dst[111:104] := SaturateU8(b[95:80])
tmp_dst[119:112] := SaturateU8(b[111:96])
tmp_dst[127:120] := SaturateU8(b[127:112])
tmp_dst[135:128] := SaturateU8(a[143:128])
tmp_dst[143:136] := SaturateU8(a[159:144])
tmp_dst[151:144] := SaturateU8(a[175:160])
tmp_dst[159:152] := SaturateU8(a[191:176])
tmp_dst[167:160] := SaturateU8(a[207:192])
tmp_dst[175:168] := SaturateU8(a[223:208])
tmp_dst[183:176] := SaturateU8(a[239:224])
tmp_dst[191:184] := SaturateU8(a[255:240])
tmp_dst[199:192] := SaturateU8(b[143:128])
tmp_dst[207:200] := SaturateU8(b[159:144])
tmp_dst[215:208] := SaturateU8(b[175:160])
tmp_dst[223:216] := SaturateU8(b[191:176])
tmp_dst[231:224] := SaturateU8(b[207:192])
tmp_dst[239:232] := SaturateU8(b[223:208])
tmp_dst[247:240] := SaturateU8(b[239:224])
tmp_dst[255:248] := SaturateU8(b[255:240])
tmp_dst[263:256] := SaturateU8(a[271:256])
tmp_dst[271:264] := SaturateU8(a[287:272])
tmp_dst[279:272] := SaturateU8(a[303:288])
tmp_dst[287:280] := SaturateU8(a[319:304])
tmp_dst[295:288] := SaturateU8(a[335:320])
tmp_dst[303:296] := SaturateU8(a[351:336])
tmp_dst[311:304] := SaturateU8(a[367:352])
tmp_dst[319:312] := SaturateU8(a[383:368])
tmp_dst[327:320] := SaturateU8(b[271:256])
tmp_dst[335:328] := SaturateU8(b[287:272])
tmp_dst[343:336] := SaturateU8(b[303:288])
tmp_dst[351:344] := SaturateU8(b[319:304])
tmp_dst[359:352] := SaturateU8(b[335:320])
tmp_dst[367:360] := SaturateU8(b[351:336])
tmp_dst[375:368] := SaturateU8(b[367:352])
tmp_dst[383:376] := SaturateU8(b[383:368])
tmp_dst[391:384] := SaturateU8(a[399:384])
tmp_dst[399:392] := SaturateU8(a[415:400])
tmp_dst[407:400] := SaturateU8(a[431:416])
tmp_dst[415:408] := SaturateU8(a[447:432])
tmp_dst[423:416] := SaturateU8(a[463:448])
tmp_dst[431:424] := SaturateU8(a[479:464])
tmp_dst[439:432] := SaturateU8(a[495:480])
tmp_dst[447:440] := SaturateU8(a[511:496])
tmp_dst[455:448] := SaturateU8(b[399:384])
tmp_dst[463:456] := SaturateU8(b[415:400])
tmp_dst[471:464] := SaturateU8(b[431:416])
tmp_dst[479:472] := SaturateU8(b[447:432])
tmp_dst[487:480] := SaturateU8(b[463:448])
tmp_dst[495:488] := SaturateU8(b[479:464])
tmp_dst[503:496] := SaturateU8(b[495:480])
tmp_dst[511:504] := SaturateU8(b[511:496])
FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := tmp_dst[i+7:i]
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPACKUSWB - _mm512_maskz_packus_epi16

| VPACKUSWB_ZMMu8_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 16-bit integers from "a" and "b" to packed 8-bit integers using unsigned saturation, and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

tmp_dst[7:0] := SaturateU8(a[15:0])
tmp_dst[15:8] := SaturateU8(a[31:16])
tmp_dst[23:16] := SaturateU8(a[47:32])
tmp_dst[31:24] := SaturateU8(a[63:48])
tmp_dst[39:32] := SaturateU8(a[79:64])
tmp_dst[47:40] := SaturateU8(a[95:80])
tmp_dst[55:48] := SaturateU8(a[111:96])
tmp_dst[63:56] := SaturateU8(a[127:112])
tmp_dst[71:64] := SaturateU8(b[15:0])
tmp_dst[79:72] := SaturateU8(b[31:16])
tmp_dst[87:80] := SaturateU8(b[47:32])
tmp_dst[95:88] := SaturateU8(b[63:48])
tmp_dst[103:96] := SaturateU8(b[79:64])
tmp_dst[111:104] := SaturateU8(b[95:80])
tmp_dst[119:112] := SaturateU8(b[111:96])
tmp_dst[127:120] := SaturateU8(b[127:112])
tmp_dst[135:128] := SaturateU8(a[143:128])
tmp_dst[143:136] := SaturateU8(a[159:144])
tmp_dst[151:144] := SaturateU8(a[175:160])
tmp_dst[159:152] := SaturateU8(a[191:176])
tmp_dst[167:160] := SaturateU8(a[207:192])
tmp_dst[175:168] := SaturateU8(a[223:208])
tmp_dst[183:176] := SaturateU8(a[239:224])
tmp_dst[191:184] := SaturateU8(a[255:240])
tmp_dst[199:192] := SaturateU8(b[143:128])
tmp_dst[207:200] := SaturateU8(b[159:144])
tmp_dst[215:208] := SaturateU8(b[175:160])
tmp_dst[223:216] := SaturateU8(b[191:176])
tmp_dst[231:224] := SaturateU8(b[207:192])
tmp_dst[239:232] := SaturateU8(b[223:208])
tmp_dst[247:240] := SaturateU8(b[239:224])
tmp_dst[255:248] := SaturateU8(b[255:240])
tmp_dst[263:256] := SaturateU8(a[271:256])
tmp_dst[271:264] := SaturateU8(a[287:272])
tmp_dst[279:272] := SaturateU8(a[303:288])
tmp_dst[287:280] := SaturateU8(a[319:304])
tmp_dst[295:288] := SaturateU8(a[335:320])
tmp_dst[303:296] := SaturateU8(a[351:336])
tmp_dst[311:304] := SaturateU8(a[367:352])
tmp_dst[319:312] := SaturateU8(a[383:368])
tmp_dst[327:320] := SaturateU8(b[271:256])
tmp_dst[335:328] := SaturateU8(b[287:272])
tmp_dst[343:336] := SaturateU8(b[303:288])
tmp_dst[351:344] := SaturateU8(b[319:304])
tmp_dst[359:352] := SaturateU8(b[335:320])
tmp_dst[367:360] := SaturateU8(b[351:336])
tmp_dst[375:368] := SaturateU8(b[367:352])
tmp_dst[383:376] := SaturateU8(b[383:368])
tmp_dst[391:384] := SaturateU8(a[399:384])
tmp_dst[399:392] := SaturateU8(a[415:400])
tmp_dst[407:400] := SaturateU8(a[431:416])
tmp_dst[415:408] := SaturateU8(a[447:432])
tmp_dst[423:416] := SaturateU8(a[463:448])
tmp_dst[431:424] := SaturateU8(a[479:464])
tmp_dst[439:432] := SaturateU8(a[495:480])
tmp_dst[447:440] := SaturateU8(a[511:496])
tmp_dst[455:448] := SaturateU8(b[399:384])
tmp_dst[463:456] := SaturateU8(b[415:400])
tmp_dst[471:464] := SaturateU8(b[431:416])
tmp_dst[479:472] := SaturateU8(b[447:432])
tmp_dst[487:480] := SaturateU8(b[463:448])
tmp_dst[495:488] := SaturateU8(b[479:464])
tmp_dst[503:496] := SaturateU8(b[495:480])
tmp_dst[511:504] := SaturateU8(b[511:496])
FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := tmp_dst[i+7:i]
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPACKUSWB - _mm512_packus_epi16

| VPACKUSWB_ZMMu8_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 16-bit integers from "a" and "b" to packed 8-bit integers using unsigned saturation, and
store the results in "dst".

[algorithm]

dst[7:0] := SaturateU8(a[15:0])
dst[15:8] := SaturateU8(a[31:16])
dst[23:16] := SaturateU8(a[47:32])
dst[31:24] := SaturateU8(a[63:48])
dst[39:32] := SaturateU8(a[79:64])
dst[47:40] := SaturateU8(a[95:80])
dst[55:48] := SaturateU8(a[111:96])
dst[63:56] := SaturateU8(a[127:112])
dst[71:64] := SaturateU8(b[15:0])
dst[79:72] := SaturateU8(b[31:16])
dst[87:80] := SaturateU8(b[47:32])
dst[95:88] := SaturateU8(b[63:48])
dst[103:96] := SaturateU8(b[79:64])
dst[111:104] := SaturateU8(b[95:80])
dst[119:112] := SaturateU8(b[111:96])
dst[127:120] := SaturateU8(b[127:112])
dst[135:128] := SaturateU8(a[143:128])
dst[143:136] := SaturateU8(a[159:144])
dst[151:144] := SaturateU8(a[175:160])
dst[159:152] := SaturateU8(a[191:176])
dst[167:160] := SaturateU8(a[207:192])
dst[175:168] := SaturateU8(a[223:208])
dst[183:176] := SaturateU8(a[239:224])
dst[191:184] := SaturateU8(a[255:240])
dst[199:192] := SaturateU8(b[143:128])
dst[207:200] := SaturateU8(b[159:144])
dst[215:208] := SaturateU8(b[175:160])
dst[223:216] := SaturateU8(b[191:176])
dst[231:224] := SaturateU8(b[207:192])
dst[239:232] := SaturateU8(b[223:208])
dst[247:240] := SaturateU8(b[239:224])
dst[255:248] := SaturateU8(b[255:240])
dst[263:256] := SaturateU8(a[271:256])
dst[271:264] := SaturateU8(a[287:272])
dst[279:272] := SaturateU8(a[303:288])
dst[287:280] := SaturateU8(a[319:304])
dst[295:288] := SaturateU8(a[335:320])
dst[303:296] := SaturateU8(a[351:336])
dst[311:304] := SaturateU8(a[367:352])
dst[319:312] := SaturateU8(a[383:368])
dst[327:320] := SaturateU8(b[271:256])
dst[335:328] := SaturateU8(b[287:272])
dst[343:336] := SaturateU8(b[303:288])
dst[351:344] := SaturateU8(b[319:304])
dst[359:352] := SaturateU8(b[335:320])
dst[367:360] := SaturateU8(b[351:336])
dst[375:368] := SaturateU8(b[367:352])
dst[383:376] := SaturateU8(b[383:368])
dst[391:384] := SaturateU8(a[399:384])
dst[399:392] := SaturateU8(a[415:400])
dst[407:400] := SaturateU8(a[431:416])
dst[415:408] := SaturateU8(a[447:432])
dst[423:416] := SaturateU8(a[463:448])
dst[431:424] := SaturateU8(a[479:464])
dst[439:432] := SaturateU8(a[495:480])
dst[447:440] := SaturateU8(a[511:496])
dst[455:448] := SaturateU8(b[399:384])
dst[463:456] := SaturateU8(b[415:400])
dst[471:464] := SaturateU8(b[431:416])
dst[479:472] := SaturateU8(b[447:432])
dst[487:480] := SaturateU8(b[463:448])
dst[495:488] := SaturateU8(b[479:464])
dst[503:496] := SaturateU8(b[495:480])
dst[511:504] := SaturateU8(b[511:496])
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPACKUSWB - _mm_mask_packus_epi16

| VPACKUSWB_XMMu8_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 16-bit integers from "a" and "b" to packed 8-bit integers using unsigned saturation, and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

tmp_dst[7:0] := SaturateU8(a[15:0])
tmp_dst[15:8] := SaturateU8(a[31:16])
tmp_dst[23:16] := SaturateU8(a[47:32])
tmp_dst[31:24] := SaturateU8(a[63:48])
tmp_dst[39:32] := SaturateU8(a[79:64])
tmp_dst[47:40] := SaturateU8(a[95:80])
tmp_dst[55:48] := SaturateU8(a[111:96])
tmp_dst[63:56] := SaturateU8(a[127:112])
tmp_dst[71:64] := SaturateU8(b[15:0])
tmp_dst[79:72] := SaturateU8(b[31:16])
tmp_dst[87:80] := SaturateU8(b[47:32])
tmp_dst[95:88] := SaturateU8(b[63:48])
tmp_dst[103:96] := SaturateU8(b[79:64])
tmp_dst[111:104] := SaturateU8(b[95:80])
tmp_dst[119:112] := SaturateU8(b[111:96])
tmp_dst[127:120] := SaturateU8(b[127:112])
FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := tmp_dst[i+7:i]
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPACKUSWB - _mm_maskz_packus_epi16

| VPACKUSWB_XMMu8_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 16-bit integers from "a" and "b" to packed 8-bit integers using unsigned saturation, and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

tmp_dst[7:0] := SaturateU8(a[15:0])
tmp_dst[15:8] := SaturateU8(a[31:16])
tmp_dst[23:16] := SaturateU8(a[47:32])
tmp_dst[31:24] := SaturateU8(a[63:48])
tmp_dst[39:32] := SaturateU8(a[79:64])
tmp_dst[47:40] := SaturateU8(a[95:80])
tmp_dst[55:48] := SaturateU8(a[111:96])
tmp_dst[63:56] := SaturateU8(a[127:112])
tmp_dst[71:64] := SaturateU8(b[15:0])
tmp_dst[79:72] := SaturateU8(b[31:16])
tmp_dst[87:80] := SaturateU8(b[47:32])
tmp_dst[95:88] := SaturateU8(b[63:48])
tmp_dst[103:96] := SaturateU8(b[79:64])
tmp_dst[111:104] := SaturateU8(b[95:80])
tmp_dst[119:112] := SaturateU8(b[111:96])
tmp_dst[127:120] := SaturateU8(b[127:112])
FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := tmp_dst[i+7:i]
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDB - _mm256_mask_add_epi8

| VPADDB_YMMu8_MASKmskw_YMMu8_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed 8-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := a[i+7:i] + b[i+7:i]
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDB - _mm256_maskz_add_epi8

| VPADDB_YMMu8_MASKmskw_YMMu8_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed 8-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := a[i+7:i] + b[i+7:i]
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDB - _mm512_add_epi8

| VPADDB_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed 8-bit integers in "a" and "b", and store the results in "dst".

[algorithm]

FOR j := 0 to 63
    i := j*8
    dst[i+7:i] := a[i+7:i] + b[i+7:i]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDB - _mm512_mask_add_epi8

| VPADDB_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed 8-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := a[i+7:i] + b[i+7:i]
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDB - _mm512_maskz_add_epi8

| VPADDB_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed 8-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := a[i+7:i] + b[i+7:i]
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDB - _mm_mask_add_epi8

| VPADDB_XMMu8_MASKmskw_XMMu8_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed 8-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := a[i+7:i] + b[i+7:i]
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDB - _mm_maskz_add_epi8

| VPADDB_XMMu8_MASKmskw_XMMu8_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed 8-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := a[i+7:i] + b[i+7:i]
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDSB - _mm256_mask_adds_epi8

| VPADDSB_YMMi8_MASKmskw_YMMi8_YMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed signed 8-bit integers in "a" and "b" using saturation, and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := Saturate8( a[i+7:i] + b[i+7:i] )
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDSB - _mm256_maskz_adds_epi8

| VPADDSB_YMMi8_MASKmskw_YMMi8_YMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed signed 8-bit integers in "a" and "b" using saturation, and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := Saturate8( a[i+7:i] + b[i+7:i] )
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDSB - _mm512_adds_epi8

| VPADDSB_ZMMi8_MASKmskw_ZMMi8_ZMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed signed 8-bit integers in "a" and "b" using saturation, and store the results in "dst".

[algorithm]

FOR j := 0 to 63
    i := j*8
    dst[i+7:i] := Saturate8( a[i+7:i] + b[i+7:i] )
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDSB - _mm512_mask_adds_epi8

| VPADDSB_ZMMi8_MASKmskw_ZMMi8_ZMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed signed 8-bit integers in "a" and "b" using saturation, and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := Saturate8( a[i+7:i] + b[i+7:i] )
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDSB - _mm512_maskz_adds_epi8

| VPADDSB_ZMMi8_MASKmskw_ZMMi8_ZMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed signed 8-bit integers in "a" and "b" using saturation, and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := Saturate8( a[i+7:i] + b[i+7:i] )
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDSB - _mm_mask_adds_epi8

| VPADDSB_XMMi8_MASKmskw_XMMi8_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed signed 8-bit integers in "a" and "b" using saturation, and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := Saturate8( a[i+7:i] + b[i+7:i] )
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDSB - _mm_maskz_adds_epi8

| VPADDSB_XMMi8_MASKmskw_XMMi8_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed signed 8-bit integers in "a" and "b" using saturation, and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := Saturate8( a[i+7:i] + b[i+7:i] )
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDSW - _mm256_mask_adds_epi16

| VPADDSW_YMMi16_MASKmskw_YMMi16_YMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed signed 16-bit integers in "a" and "b" using saturation, and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := Saturate16( a[i+15:i] + b[i+15:i] )
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDSW - _mm256_maskz_adds_epi16

| VPADDSW_YMMi16_MASKmskw_YMMi16_YMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed signed 16-bit integers in "a" and "b" using saturation, and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := Saturate16( a[i+15:i] + b[i+15:i] )
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDSW - _mm512_adds_epi16

| VPADDSW_ZMMi16_MASKmskw_ZMMi16_ZMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed signed 16-bit integers in "a" and "b" using saturation, and store the results in "dst".

[algorithm]

FOR j := 0 to 31
    i := j*16
    dst[i+15:i] := Saturate16( a[i+15:i] + b[i+15:i] )
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDSW - _mm512_mask_adds_epi16

| VPADDSW_ZMMi16_MASKmskw_ZMMi16_ZMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed signed 16-bit integers in "a" and "b" using saturation, and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := Saturate16( a[i+15:i] + b[i+15:i] )
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDSW - _mm512_maskz_adds_epi16

| VPADDSW_ZMMi16_MASKmskw_ZMMi16_ZMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed signed 16-bit integers in "a" and "b" using saturation, and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := Saturate16( a[i+15:i] + b[i+15:i] )
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDSW - _mm_mask_adds_epi16

| VPADDSW_XMMi16_MASKmskw_XMMi16_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed signed 16-bit integers in "a" and "b" using saturation, and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := Saturate16( a[i+15:i] + b[i+15:i] )
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDSW - _mm_maskz_adds_epi16

| VPADDSW_XMMi16_MASKmskw_XMMi16_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed signed 16-bit integers in "a" and "b" using saturation, and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := Saturate16( a[i+15:i] + b[i+15:i] )
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDUSB - _mm256_mask_adds_epu8

| VPADDUSB_YMMu8_MASKmskw_YMMu8_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed unsigned 8-bit integers in "a" and "b" using saturation, and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := SaturateU8( a[i+7:i] + b[i+7:i] )
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDUSB - _mm256_maskz_adds_epu8

| VPADDUSB_YMMu8_MASKmskw_YMMu8_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed unsigned 8-bit integers in "a" and "b" using saturation, and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := SaturateU8( a[i+7:i] + b[i+7:i] )
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDUSB - _mm512_adds_epu8

| VPADDUSB_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed unsigned 8-bit integers in "a" and "b" using saturation, and store the results in "dst".

[algorithm]

FOR j := 0 to 63
    i := j*8
    dst[i+7:i] := SaturateU8( a[i+7:i] + b[i+7:i] )
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDUSB - _mm512_mask_adds_epu8

| VPADDUSB_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed unsigned 8-bit integers in "a" and "b" using saturation, and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := SaturateU8( a[i+7:i] + b[i+7:i] )
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDUSB - _mm512_maskz_adds_epu8

| VPADDUSB_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed unsigned 8-bit integers in "a" and "b" using saturation, and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := SaturateU8( a[i+7:i] + b[i+7:i] )
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDUSB - _mm_mask_adds_epu8

| VPADDUSB_XMMu8_MASKmskw_XMMu8_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed unsigned 8-bit integers in "a" and "b" using saturation, and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := SaturateU8( a[i+7:i] + b[i+7:i] )
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDUSB - _mm_maskz_adds_epu8

| VPADDUSB_XMMu8_MASKmskw_XMMu8_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed unsigned 8-bit integers in "a" and "b" using saturation, and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := SaturateU8( a[i+7:i] + b[i+7:i] )
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDUSW - _mm256_mask_adds_epu16

| VPADDUSW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed unsigned 16-bit integers in "a" and "b" using saturation, and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := SaturateU16( a[i+15:i] + b[i+15:i] )
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDUSW - _mm256_maskz_adds_epu16

| VPADDUSW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed unsigned 16-bit integers in "a" and "b" using saturation, and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := SaturateU16( a[i+15:i] + b[i+15:i] )
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDUSW - _mm512_adds_epu16

| VPADDUSW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed unsigned 16-bit integers in "a" and "b" using saturation, and store the results in "dst".

[algorithm]

FOR j := 0 to 31
    i := j*16
    dst[i+15:i] := SaturateU16( a[i+15:i] + b[i+15:i] )
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDUSW - _mm512_mask_adds_epu16

| VPADDUSW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed unsigned 16-bit integers in "a" and "b" using saturation, and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := SaturateU16( a[i+15:i] + b[i+15:i] )
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDUSW - _mm512_maskz_adds_epu16

| VPADDUSW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed unsigned 16-bit integers in "a" and "b" using saturation, and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := SaturateU16( a[i+15:i] + b[i+15:i] )
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDUSW - _mm_mask_adds_epu16

| VPADDUSW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed unsigned 16-bit integers in "a" and "b" using saturation, and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := SaturateU16( a[i+15:i] + b[i+15:i] )
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDUSW - _mm_maskz_adds_epu16

| VPADDUSW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed unsigned 16-bit integers in "a" and "b" using saturation, and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := SaturateU16( a[i+15:i] + b[i+15:i] )
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDW - _mm256_mask_add_epi16

| VPADDW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed 16-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := a[i+15:i] + b[i+15:i]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDW - _mm256_maskz_add_epi16

| VPADDW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed 16-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := a[i+15:i] + b[i+15:i]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDW - _mm512_add_epi16

| VPADDW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed 16-bit integers in "a" and "b", and store the results in "dst".

[algorithm]

FOR j := 0 to 31
    i := j*16
    dst[i+15:i] := a[i+15:i] + b[i+15:i]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDW - _mm512_mask_add_epi16

| VPADDW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed 16-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := a[i+15:i] + b[i+15:i]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDW - _mm512_maskz_add_epi16

| VPADDW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed 16-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := a[i+15:i] + b[i+15:i]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDW - _mm_mask_add_epi16

| VPADDW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed 16-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := a[i+15:i] + b[i+15:i]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDW - _mm_maskz_add_epi16

| VPADDW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed 16-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := a[i+15:i] + b[i+15:i]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPALIGNR - _mm256_mask_alignr_epi8

| VPALIGNR_YMMu8_MASKmskw_YMMu8_YMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate pairs of 16-byte blocks in "a" and "b" into a 32-byte temporary result, shift the result right by
"imm8" bytes, and store the low 16 bytes in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*128
    tmp[255:0] := ((a[i+127:i] &lt;&lt; 128)[255:0] OR b[i+127:i]) &gt;&gt; (imm8*8)
    tmp_dst[i+127:i] := tmp[127:0]
ENDFOR
FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := tmp_dst[i+7:i]
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPALIGNR - _mm256_maskz_alignr_epi8

| VPALIGNR_YMMu8_MASKmskw_YMMu8_YMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate pairs of 16-byte blocks in "a" and "b" into a 32-byte temporary result, shift the result right by
"imm8" bytes, and store the low 16 bytes in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*128
    tmp[255:0] := ((a[i+127:i] &lt;&lt; 128)[255:0] OR b[i+127:i]) &gt;&gt; (imm8*8)
    tmp_dst[i+127:i] := tmp[127:0]
ENDFOR
FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := tmp_dst[i+7:i]
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPALIGNR - _mm512_alignr_epi8

| VPALIGNR_ZMMu8_MASKmskw_ZMMu8_ZMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate pairs of 16-byte blocks in "a" and "b" into a 32-byte temporary result, shift the result right by
"imm8" bytes, and store the low 16 bytes in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*128
    tmp[255:0] := ((a[i+127:i] &lt;&lt; 128)[255:0] OR b[i+127:i]) &gt;&gt; (imm8*8)
    dst[i+127:i] := tmp[127:0]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPALIGNR - _mm512_mask_alignr_epi8

| VPALIGNR_ZMMu8_MASKmskw_ZMMu8_ZMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate pairs of 16-byte blocks in "a" and "b" into a 32-byte temporary result, shift the result right by
"imm8" bytes, and store the low 16 bytes in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*128
    tmp[255:0] := ((a[i+127:i] &lt;&lt; 128)[255:0] OR b[i+127:i]) &gt;&gt; (imm8*8)
    tmp_dst[i+127:i] := tmp[127:0]
ENDFOR
FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := tmp_dst[i+7:i]
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPALIGNR - _mm512_maskz_alignr_epi8

| VPALIGNR_ZMMu8_MASKmskw_ZMMu8_ZMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate pairs of 16-byte blocks in "a" and "b" into a 32-byte temporary result, shift the result right by
"imm8" bytes, and store the low 16 bytes in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*128
    tmp[255:0] := ((a[i+127:i] &lt;&lt; 128)[255:0] OR b[i+127:i]) &gt;&gt; (imm8*8)
    tmp_dst[i+127:i] := tmp[127:0]
ENDFOR
FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := tmp_dst[i+7:i]
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPALIGNR - _mm_mask_alignr_epi8

| VPALIGNR_XMMu8_MASKmskw_XMMu8_XMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate pairs of 16-byte blocks in "a" and "b" into a 32-byte temporary result, shift the result right by
"imm8" bytes, and store the low 16 bytes in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

tmp_dst[255:0] := ((a[127:0] &lt;&lt; 128)[255:0] OR b[127:0]) &gt;&gt; (imm8*8)
FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := tmp_dst[i+7:i]
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPALIGNR - _mm_maskz_alignr_epi8

| VPALIGNR_XMMu8_MASKmskw_XMMu8_XMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate pairs of 16-byte blocks in "a" and "b" into a 32-byte temporary result, shift the result right by
"imm8" bytes, and store the low 16 bytes in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

tmp_dst[255:0] := ((a[127:0] &lt;&lt; 128)[255:0] OR b[127:0]) &gt;&gt; (imm8*8)
FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := tmp_dst[i+7:i]
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPAVGB - _mm256_mask_avg_epu8

| VPAVGB_YMMu8_MASKmskw_YMMu8_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Average packed unsigned 8-bit integers in "a" and "b", and store the results in "dst" using writemask "k"
(elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := (a[i+7:i] + b[i+7:i] + 1) &gt;&gt; 1
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPAVGB - _mm256_maskz_avg_epu8

| VPAVGB_YMMu8_MASKmskw_YMMu8_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Average packed unsigned 8-bit integers in "a" and "b", and store the results in "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := (a[i+7:i] + b[i+7:i] + 1) &gt;&gt; 1
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPAVGB - _mm512_avg_epu8

| VPAVGB_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Average packed unsigned 8-bit integers in "a" and "b", and store the results in "dst".

[algorithm]

FOR j := 0 to 63
    i := j*8
    dst[i+7:i] := (a[i+7:i] + b[i+7:i] + 1) &gt;&gt; 1
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPAVGB - _mm512_mask_avg_epu8

| VPAVGB_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Average packed unsigned 8-bit integers in "a" and "b", and store the results in "dst" using writemask "k"
(elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := (a[i+7:i] + b[i+7:i] + 1) &gt;&gt; 1
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPAVGB - _mm512_maskz_avg_epu8

| VPAVGB_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Average packed unsigned 8-bit integers in "a" and "b", and store the results in "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := (a[i+7:i] + b[i+7:i] + 1) &gt;&gt; 1
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPAVGB - _mm_mask_avg_epu8

| VPAVGB_XMMu8_MASKmskw_XMMu8_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Average packed unsigned 8-bit integers in "a" and "b", and store the results in "dst" using writemask "k"
(elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := (a[i+7:i] + b[i+7:i] + 1) &gt;&gt; 1
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPAVGB - _mm_maskz_avg_epu8

| VPAVGB_XMMu8_MASKmskw_XMMu8_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Average packed unsigned 8-bit integers in "a" and "b", and store the results in "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := (a[i+7:i] + b[i+7:i] + 1) &gt;&gt; 1
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPAVGW - _mm256_mask_avg_epu16

| VPAVGW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Average packed unsigned 16-bit integers in "a" and "b", and store the results in "dst" using writemask "k"
(elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := (a[i+15:i] + b[i+15:i] + 1) &gt;&gt; 1
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPAVGW - _mm256_maskz_avg_epu16

| VPAVGW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Average packed unsigned 16-bit integers in "a" and "b", and store the results in "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := (a[i+15:i] + b[i+15:i] + 1) &gt;&gt; 1
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPAVGW - _mm512_avg_epu16

| VPAVGW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Average packed unsigned 16-bit integers in "a" and "b", and store the results in "dst".

[algorithm]

FOR j := 0 to 31
    i := j*16
    dst[i+15:i] := (a[i+15:i] + b[i+15:i] + 1) &gt;&gt; 1
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPAVGW - _mm512_mask_avg_epu16

| VPAVGW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Average packed unsigned 16-bit integers in "a" and "b", and store the results in "dst" using writemask "k"
(elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := (a[i+15:i] + b[i+15:i] + 1) &gt;&gt; 1
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPAVGW - _mm512_maskz_avg_epu16

| VPAVGW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Average packed unsigned 16-bit integers in "a" and "b", and store the results in "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := (a[i+15:i] + b[i+15:i] + 1) &gt;&gt; 1
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPAVGW - _mm_mask_avg_epu16

| VPAVGW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Average packed unsigned 16-bit integers in "a" and "b", and store the results in "dst" using writemask "k"
(elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := (a[i+15:i] + b[i+15:i] + 1) &gt;&gt; 1
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPAVGW - _mm_maskz_avg_epu16

| VPAVGW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Average packed unsigned 16-bit integers in "a" and "b", and store the results in "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := (a[i+15:i] + b[i+15:i] + 1) &gt;&gt; 1
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPBLENDMB - _mm256_mask_blend_epi8

| VPBLENDMB_YMMu8_MASKmskw_YMMu8_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Blend packed 8-bit integers from "a" and "b" using control mask "k", and store the results in "dst".

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := b[i+7:i]
    ELSE
        dst[i+7:i] := a[i+7:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPBLENDMB - _mm512_mask_blend_epi8

| VPBLENDMB_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Blend packed 8-bit integers from "a" and "b" using control mask "k", and store the results in "dst".

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := b[i+7:i]
    ELSE
        dst[i+7:i] := a[i+7:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPBLENDMB - _mm_mask_blend_epi8

| VPBLENDMB_XMMu8_MASKmskw_XMMu8_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Blend packed 8-bit integers from "a" and "b" using control mask "k", and store the results in "dst".

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := b[i+7:i]
    ELSE
        dst[i+7:i] := a[i+7:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPBLENDMW - _mm256_mask_blend_epi16

| VPBLENDMW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Blend packed 16-bit integers from "a" and "b" using control mask "k", and store the results in "dst".

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := b[i+15:i]
    ELSE
        dst[i+15:i] := a[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPBLENDMW - _mm512_mask_blend_epi16

| VPBLENDMW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Blend packed 16-bit integers from "a" and "b" using control mask "k", and store the results in "dst".

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := b[i+15:i]
    ELSE
        dst[i+15:i] := a[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPBLENDMW - _mm_mask_blend_epi16

| VPBLENDMW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Blend packed 16-bit integers from "a" and "b" using control mask "k", and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := b[i+15:i]
    ELSE
        dst[i+15:i] := a[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTB - _mm256_mask_broadcastb_epi8

| VPBROADCASTB_YMMu8_MASKmskw_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low packed 8-bit integer from "a" to all elements of "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := a[7:0]
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTB - _mm256_mask_set1_epi8

| VPBROADCASTB_YMMu8_MASKmskw_GPR32u8_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast 8-bit integer "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := a[7:0]
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTB - _mm256_maskz_broadcastb_epi8

| VPBROADCASTB_YMMu8_MASKmskw_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low packed 8-bit integer from "a" to all elements of "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := a[7:0]
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTB - _mm256_maskz_set1_epi8

| VPBROADCASTB_YMMu8_MASKmskw_GPR32u8_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast 8-bit integer "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := a[7:0]
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTB - _mm512_broadcastb_epi8

| VPBROADCASTB_ZMMu8_MASKmskw_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low packed 8-bit integer from "a" to all elements of "dst".

[algorithm]

FOR j := 0 to 63
    i := j*8
    dst[i+7:i] := a[7:0]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTB - _mm512_mask_broadcastb_epi8

| VPBROADCASTB_ZMMu8_MASKmskw_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low packed 8-bit integer from "a" to all elements of "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := a[7:0]
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTB - _mm512_mask_set1_epi8

| VPBROADCASTB_ZMMu8_MASKmskw_GPR32u8_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast 8-bit integer "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := a[7:0]
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTB - _mm512_maskz_broadcastb_epi8

| VPBROADCASTB_ZMMu8_MASKmskw_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low packed 8-bit integer from "a" to all elements of "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := a[7:0]
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTB - _mm512_maskz_set1_epi8

| VPBROADCASTB_ZMMu8_MASKmskw_GPR32u8_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast 8-bit integer "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := a[7:0]
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTB - _mm_mask_broadcastb_epi8

| VPBROADCASTB_XMMu8_MASKmskw_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low packed 8-bit integer from "a" to all elements of "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := a[7:0]
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTB - _mm_mask_set1_epi8

| VPBROADCASTB_XMMu8_MASKmskw_GPR32u8_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast 8-bit integer "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := a[7:0]
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTB - _mm_maskz_broadcastb_epi8

| VPBROADCASTB_XMMu8_MASKmskw_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low packed 8-bit integer from "a" to all elements of "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := a[7:0]
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTB - _mm_maskz_set1_epi8

| VPBROADCASTB_XMMu8_MASKmskw_GPR32u8_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast 8-bit integer "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := a[7:0]
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTW - _mm256_mask_broadcastw_epi16

| VPBROADCASTW_YMMu16_MASKmskw_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low packed 16-bit integer from "a" to all elements of "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := a[15:0]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTW - _mm256_mask_set1_epi16

| VPBROADCASTW_YMMu16_MASKmskw_GPR32u16_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low packed 16-bit integer from "a" to all elements of "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := a[15:0]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTW - _mm256_maskz_broadcastw_epi16

| VPBROADCASTW_YMMu16_MASKmskw_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low packed 16-bit integer from "a" to all elements of "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := a[15:0]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTW - _mm256_maskz_set1_epi16

| VPBROADCASTW_YMMu16_MASKmskw_GPR32u16_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast 16-bit integer "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := a[15:0]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTW - _mm512_broadcastw_epi16

| VPBROADCASTW_ZMMu16_MASKmskw_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low packed 16-bit integer from "a" to all elements of "dst".

[algorithm]

FOR j := 0 to 31
    i := j*16
    dst[i+15:i] := a[15:0]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTW - _mm512_mask_broadcastw_epi16

| VPBROADCASTW_ZMMu16_MASKmskw_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low packed 16-bit integer from "a" to all elements of "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := a[15:0]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTW - _mm512_mask_set1_epi16

| VPBROADCASTW_ZMMu16_MASKmskw_GPR32u16_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast 16-bit integer "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := a[15:0]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTW - _mm512_maskz_broadcastw_epi16

| VPBROADCASTW_ZMMu16_MASKmskw_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low packed 16-bit integer from "a" to all elements of "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := a[15:0]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTW - _mm512_maskz_set1_epi16

| VPBROADCASTW_ZMMu16_MASKmskw_GPR32u16_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low packed 16-bit integer from "a" to all elements of "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := a[15:0]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTW - _mm_mask_broadcastw_epi16

| VPBROADCASTW_XMMu16_MASKmskw_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low packed 16-bit integer from "a" to all elements of "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := a[15:0]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTW - _mm_mask_set1_epi16

| VPBROADCASTW_XMMu16_MASKmskw_GPR32u16_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low packed 16-bit integer from "a" to all elements of "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := a[15:0]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTW - _mm_maskz_broadcastw_epi16

| VPBROADCASTW_XMMu16_MASKmskw_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low packed 16-bit integer from "a" to all elements of "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := a[15:0]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTW - _mm_maskz_set1_epi16

| VPBROADCASTW_XMMu16_MASKmskw_GPR32u16_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low packed 16-bit integer from "a" to all elements of "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := a[15:0]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm256_cmp_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_YMMi8_YMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" based on the comparison operand specified by "imm8", and
store the results in mask vector "k".

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 31
    i := j*8
    k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm256_cmpeq_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_YMMi8_YMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" for equality, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 31
    i := j*8
    k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm256_cmpge_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_YMMi8_YMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask
vector "k".

[algorithm]

FOR j := 0 to 31
    i := j*8
    k[j] := ( a[i+7:i] &gt;= b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm256_cmpgt_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_YMMi8_YMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 31
    i := j*8
    k[j] := ( a[i+7:i] &gt; b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm256_cmple_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_YMMi8_YMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k".

[algorithm]

FOR j := 0 to 31
    i := j*8
    k[j] := ( a[i+7:i] &lt;= b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm256_cmplt_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_YMMi8_YMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 31
    i := j*8
    k[j] := ( a[i+7:i] &lt; b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm256_cmpneq_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_YMMi8_YMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 31
    i := j*8
    k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm256_mask_cmp_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_YMMi8_YMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" based on the comparison operand specified by "imm8", and
store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask
bit is not set).

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 31
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm256_mask_cmpeq_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_YMMi8_YMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" for equality, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm256_mask_cmpge_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_YMMi8_YMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] &gt;= b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm256_mask_cmpgt_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_YMMi8_YMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] &gt; b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm256_mask_cmple_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_YMMi8_YMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] &lt;= b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm256_mask_cmplt_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_YMMi8_YMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" for less-than, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] &lt; b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm256_mask_cmpneq_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_YMMi8_YMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm512_cmp_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_ZMMi8_ZMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" based on the comparison operand specified by "imm8", and
store the results in mask vector "k".

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 63
    i := j*8
    k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm512_cmpeq_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_ZMMi8_ZMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" for equality, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 63
    i := j*8
    k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm512_cmpge_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_ZMMi8_ZMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask
vector "k".

[algorithm]

FOR j := 0 to 63
    i := j*8
    k[j] := ( a[i+7:i] &gt;= b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm512_cmpgt_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_ZMMi8_ZMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 63
    i := j*8
    k[j] := ( a[i+7:i] &gt; b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm512_cmple_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_ZMMi8_ZMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k".

[algorithm]

FOR j := 0 to 63
    i := j*8
    k[j] := ( a[i+7:i] &lt;= b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm512_cmplt_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_ZMMi8_ZMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 63
    i := j*8
    k[j] := ( a[i+7:i] &lt; b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm512_cmpneq_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_ZMMi8_ZMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 63
    i := j*8
    k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm512_mask_cmp_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_ZMMi8_ZMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" based on the comparison operand specified by "imm8", and
store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask
bit is not set).

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 63
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm512_mask_cmpeq_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_ZMMi8_ZMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" for equality, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm512_mask_cmpge_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_ZMMi8_ZMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] &gt;= b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm512_mask_cmpgt_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_ZMMi8_ZMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] &gt; b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm512_mask_cmple_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_ZMMi8_ZMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] &lt;= b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm512_mask_cmplt_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_ZMMi8_ZMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" for less-than, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] &lt; b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm512_mask_cmpneq_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_ZMMi8_ZMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm_cmp_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_XMMi8_XMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" based on the comparison operand specified by "imm8", and
store the results in mask vector "k".

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 15
    i := j*8
    k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm_cmpeq_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_XMMi8_XMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" for equality, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 15
    i := j*8
    k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm_cmpge_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_XMMi8_XMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask
vector "k".

[algorithm]

FOR j := 0 to 15
    i := j*8
    k[j] := ( a[i+7:i] &gt;= b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm_cmpgt_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_XMMi8_XMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 15
    i := j*8
    k[j] := ( a[i+7:i] &gt; b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm_cmple_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_XMMi8_XMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k".

[algorithm]

FOR j := 0 to 15
    i := j*8
    k[j] := ( a[i+7:i] &lt;= b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm_cmplt_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_XMMi8_XMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 15
    i := j*8
    k[j] := ( a[i+7:i] &lt; b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm_cmpneq_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_XMMi8_XMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 15
    i := j*8
    k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm_mask_cmp_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_XMMi8_XMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" based on the comparison operand specified by "imm8", and
store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask
bit is not set).

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 15
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm_mask_cmpeq_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_XMMi8_XMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" for equality, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm_mask_cmpge_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_XMMi8_XMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] &gt;= b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm_mask_cmpgt_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_XMMi8_XMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] &gt; b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm_mask_cmple_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_XMMi8_XMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] &lt;= b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm_mask_cmplt_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_XMMi8_XMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" for less-than, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] &lt; b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPB - _mm_mask_cmpneq_epi8_mask

| VPCMPB_MASKmskw_MASKmskw_XMMi8_XMMi8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm256_cmp_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_YMMu8_YMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" based on the comparison operand specified by "imm8", and
store the results in mask vector "k".

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 31
    i := j*8
    k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm256_cmpeq_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_YMMu8_YMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" for equality, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 31
    i := j*8
    k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm256_cmpge_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_YMMu8_YMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask
vector "k".

[algorithm]

FOR j := 0 to 31
    i := j*8
    k[j] := ( a[i+7:i] &gt;= b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm256_cmpgt_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_YMMu8_YMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 31
    i := j*8
    k[j] := ( a[i+7:i] &gt; b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm256_cmple_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_YMMu8_YMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k".

[algorithm]

FOR j := 0 to 31
    i := j*8
    k[j] := ( a[i+7:i] &lt;= b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm256_cmplt_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_YMMu8_YMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 31
    i := j*8
    k[j] := ( a[i+7:i] &lt; b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm256_cmpneq_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_YMMu8_YMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 31
    i := j*8
    k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm256_mask_cmp_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_YMMu8_YMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" based on the comparison operand specified by "imm8", and
store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask
bit is not set).

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 31
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm256_mask_cmpeq_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_YMMu8_YMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" for equality, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm256_mask_cmpge_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_YMMu8_YMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] &gt;= b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm256_mask_cmpgt_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_YMMu8_YMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] &gt; b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm256_mask_cmple_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_YMMu8_YMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] &lt;= b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm256_mask_cmplt_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_YMMu8_YMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" for less-than, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] &lt; b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm256_mask_cmpneq_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_YMMu8_YMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm512_cmp_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_ZMMu8_ZMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" based on the comparison operand specified by "imm8", and
store the results in mask vector "k".

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 63
    i := j*8
    k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm512_cmpeq_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_ZMMu8_ZMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" for equality, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 63
    i := j*8
    k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm512_cmpge_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_ZMMu8_ZMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask
vector "k".

[algorithm]

FOR j := 0 to 63
    i := j*8
    k[j] := ( a[i+7:i] &gt;= b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm512_cmpgt_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_ZMMu8_ZMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 63
    i := j*8
    k[j] := ( a[i+7:i] &gt; b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm512_cmple_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_ZMMu8_ZMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k".

[algorithm]

FOR j := 0 to 63
    i := j*8
    k[j] := ( a[i+7:i] &lt;= b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm512_cmplt_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_ZMMu8_ZMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 63
    i := j*8
    k[j] := ( a[i+7:i] &lt; b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm512_cmpneq_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_ZMMu8_ZMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 63
    i := j*8
    k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm512_mask_cmp_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_ZMMu8_ZMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" based on the comparison operand specified by "imm8", and
store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask
bit is not set).

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 63
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm512_mask_cmpeq_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_ZMMu8_ZMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" for equality, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm512_mask_cmpge_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_ZMMu8_ZMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] &gt;= b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm512_mask_cmpgt_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_ZMMu8_ZMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] &gt; b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm512_mask_cmple_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_ZMMu8_ZMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] &lt;= b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm512_mask_cmplt_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_ZMMu8_ZMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" for less-than, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] &lt; b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm512_mask_cmpneq_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_ZMMu8_ZMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm_cmp_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_XMMu8_XMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" based on the comparison operand specified by "imm8", and
store the results in mask vector "k".

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 15
    i := j*8
    k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm_cmpeq_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_XMMu8_XMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" for equality, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 15
    i := j*8
    k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm_cmpge_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_XMMu8_XMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask
vector "k".

[algorithm]

FOR j := 0 to 15
    i := j*8
    k[j] := ( a[i+7:i] &gt;= b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm_cmpgt_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_XMMu8_XMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 15
    i := j*8
    k[j] := ( a[i+7:i] &gt; b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm_cmple_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_XMMu8_XMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k".

[algorithm]

FOR j := 0 to 15
    i := j*8
    k[j] := ( a[i+7:i] &lt;= b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm_cmplt_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_XMMu8_XMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 15
    i := j*8
    k[j] := ( a[i+7:i] &lt; b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm_cmpneq_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_XMMu8_XMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 15
    i := j*8
    k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm_mask_cmp_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_XMMu8_XMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" based on the comparison operand specified by "imm8", and
store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask
bit is not set).

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 15
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm_mask_cmpeq_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_XMMu8_XMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" for equality, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm_mask_cmpge_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_XMMu8_XMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] &gt;= b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm_mask_cmpgt_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_XMMu8_XMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] &gt; b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm_mask_cmple_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_XMMu8_XMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] &lt;= b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm_mask_cmplt_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_XMMu8_XMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" for less-than, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] &lt; b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUB - _mm_mask_cmpneq_epu8_mask

| VPCMPUB_MASKmskw_MASKmskw_XMMu8_XMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k1[j]
        k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm256_cmp_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_YMMu16_YMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" based on the comparison operand specified by "imm8",
and store the results in mask vector "k".

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 15
    i := j*16
    k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm256_cmpeq_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_YMMu16_YMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" for equality, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 15
    i := j*16
    k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm256_cmpge_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_YMMu16_YMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" for greater-than-or-equal, and store the results in
mask vector "k".

[algorithm]

FOR j := 0 to 15
    i := j*16
    k[j] := ( a[i+15:i] &gt;= b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm256_cmpgt_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_YMMu16_YMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 15
    i := j*16
    k[j] := ( a[i+15:i] &gt; b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm256_cmple_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_YMMu16_YMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k".

[algorithm]

FOR j := 0 to 15
    i := j*16
    k[j] := ( a[i+15:i] &lt;= b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm256_cmplt_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_YMMu16_YMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" for less-than, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 15
    i := j*16
    k[j] := ( a[i+15:i] &lt; b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm256_cmpneq_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_YMMu16_YMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" for not-equal, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 15
    i := j*16
    k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm256_mask_cmp_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_YMMu16_YMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" based on the comparison operand specified by "imm8",
and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding
mask bit is not set).

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 15
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm256_mask_cmpeq_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_YMMu16_YMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" for equality, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm256_mask_cmpge_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_YMMu16_YMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" for greater-than-or-equal, and store the results in
mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] &gt;= b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm256_mask_cmpgt_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_YMMu16_YMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] &gt; b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm256_mask_cmple_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_YMMu16_YMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] &lt;= b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm256_mask_cmplt_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_YMMu16_YMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" for less-than, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] &lt; b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm256_mask_cmpneq_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_YMMu16_YMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm512_cmp_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_ZMMu16_ZMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" based on the comparison operand specified by "imm8",
and store the results in mask vector "k".

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 31
    i := j*16
    k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm512_cmpeq_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_ZMMu16_ZMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" for equality, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 31
    i := j*16
    k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm512_cmpge_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_ZMMu16_ZMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" for greater-than-or-equal, and store the results in
mask vector "k".

[algorithm]

FOR j := 0 to 31
    i := j*16
    k[j] := ( a[i+15:i] &gt;= b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm512_cmpgt_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_ZMMu16_ZMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 31
    i := j*16
    k[j] := ( a[i+15:i] &gt; b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm512_cmple_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_ZMMu16_ZMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k".

[algorithm]

FOR j := 0 to 31
    i := j*16
    k[j] := ( a[i+15:i] &lt;= b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm512_cmplt_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_ZMMu16_ZMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" for less-than, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 31
    i := j*16
    k[j] := ( a[i+15:i] &lt; b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm512_cmpneq_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_ZMMu16_ZMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" for not-equal, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 31
    i := j*16
    k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm512_mask_cmp_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_ZMMu16_ZMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" based on the comparison operand specified by "imm8",
and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding
mask bit is not set).

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 31
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm512_mask_cmpeq_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_ZMMu16_ZMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" for equality, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm512_mask_cmpge_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_ZMMu16_ZMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" for greater-than-or-equal, and store the results in
mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] &gt;= b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm512_mask_cmpgt_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_ZMMu16_ZMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] &gt; b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm512_mask_cmple_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_ZMMu16_ZMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] &lt;= b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm512_mask_cmplt_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_ZMMu16_ZMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" for less-than, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] &lt; b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm512_mask_cmpneq_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_ZMMu16_ZMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm_cmp_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_XMMu16_XMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" based on the comparison operand specified by "imm8",
and store the results in mask vector "k".

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 7
    i := j*16
    k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm_cmpeq_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_XMMu16_XMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" for equality, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 7
    i := j*16
    k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm_cmpge_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_XMMu16_XMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" for greater-than-or-equal, and store the results in
mask vector "k".

[algorithm]

FOR j := 0 to 7
    i := j*16
    k[j] := ( a[i+15:i] &gt;= b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm_cmpgt_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_XMMu16_XMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 7
    i := j*16
    k[j] := ( a[i+15:i] &gt; b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm_cmple_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_XMMu16_XMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k".

[algorithm]

FOR j := 0 to 7
    i := j*16
    k[j] := ( a[i+15:i] &lt;= b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm_cmplt_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_XMMu16_XMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" for less-than, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 7
    i := j*16
    k[j] := ( a[i+15:i] &lt; b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm_cmpneq_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_XMMu16_XMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" for not-equal, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 7
    i := j*16
    k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm_mask_cmp_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_XMMu16_XMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" based on the comparison operand specified by "imm8",
and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding
mask bit is not set).

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 7
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm_mask_cmpeq_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_XMMu16_XMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" for equality, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm_mask_cmpge_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_XMMu16_XMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" for greater-than-or-equal, and store the results in
mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] &gt;= b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm_mask_cmpgt_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_XMMu16_XMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] &gt; b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm_mask_cmple_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_XMMu16_XMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] &lt;= b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm_mask_cmplt_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_XMMu16_XMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" for less-than, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] &lt; b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUW - _mm_mask_cmpneq_epu16_mask

| VPCMPUW_MASKmskw_MASKmskw_XMMu16_XMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm256_cmp_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_YMMi16_YMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" based on the comparison operand specified by "imm8", and
store the results in mask vector "k".

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 15
    i := j*16
    k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm256_cmpeq_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_YMMi16_YMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" for equality, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 15
    i := j*16
    k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm256_cmpge_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_YMMi16_YMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask
vector "k".

[algorithm]

FOR j := 0 to 15
    i := j*16
    k[j] := ( a[i+15:i] &gt;= b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm256_cmpgt_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_YMMi16_YMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 15
    i := j*16
    k[j] := ( a[i+15:i] &gt; b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm256_cmple_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_YMMi16_YMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k".

[algorithm]

FOR j := 0 to 15
    i := j*16
    k[j] := ( a[i+15:i] &lt;= b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm256_cmplt_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_YMMi16_YMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 15
    i := j*16
    k[j] := ( a[i+15:i] &lt; b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm256_cmpneq_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_YMMi16_YMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 15
    i := j*16
    k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm256_mask_cmp_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_YMMi16_YMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" based on the comparison operand specified by "imm8", and
store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask
bit is not set).

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 15
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm256_mask_cmpeq_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_YMMi16_YMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" for equality, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm256_mask_cmpge_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_YMMi16_YMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] &gt;= b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm256_mask_cmpgt_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_YMMi16_YMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] &gt; b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm256_mask_cmple_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_YMMi16_YMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] &lt;= b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm256_mask_cmplt_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_YMMi16_YMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" for less-than, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] &lt; b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm256_mask_cmpneq_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_YMMi16_YMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm512_cmp_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_ZMMi16_ZMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" based on the comparison operand specified by "imm8", and
store the results in mask vector "k".

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 31
    i := j*16
    k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm512_cmpeq_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_ZMMi16_ZMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" for equality, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 31
    i := j*16
    k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm512_cmpge_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_ZMMi16_ZMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask
vector "k".

[algorithm]

FOR j := 0 to 31
    i := j*16
    k[j] := ( a[i+15:i] &gt;= b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm512_cmpgt_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_ZMMi16_ZMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 31
    i := j*16
    k[j] := ( a[i+15:i] &gt; b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm512_cmple_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_ZMMi16_ZMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k".

[algorithm]

FOR j := 0 to 31
    i := j*16
    k[j] := ( a[i+15:i] &lt;= b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm512_cmplt_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_ZMMi16_ZMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 31
    i := j*16
    k[j] := ( a[i+15:i] &lt; b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm512_cmpneq_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_ZMMi16_ZMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 31
    i := j*16
    k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm512_mask_cmp_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_ZMMi16_ZMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" based on the comparison operand specified by "imm8", and
store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask
bit is not set).

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 31
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm512_mask_cmpeq_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_ZMMi16_ZMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" for equality, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm512_mask_cmpge_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_ZMMi16_ZMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] &gt;= b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm512_mask_cmpgt_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_ZMMi16_ZMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] &gt; b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm512_mask_cmple_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_ZMMi16_ZMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] &lt;= b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm512_mask_cmplt_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_ZMMi16_ZMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" for less-than, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] &lt; b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm512_mask_cmpneq_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_ZMMi16_ZMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm_cmp_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_XMMi16_XMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" based on the comparison operand specified by "imm8", and
store the results in mask vector "k".

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 7
    i := j*16
    k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm_cmpeq_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_XMMi16_XMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" for equality, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 7
    i := j*16
    k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm_cmpge_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_XMMi16_XMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask
vector "k".

[algorithm]

FOR j := 0 to 7
    i := j*16
    k[j] := ( a[i+15:i] &gt;= b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm_cmpgt_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_XMMi16_XMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 7
    i := j*16
    k[j] := ( a[i+15:i] &gt; b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm_cmple_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_XMMi16_XMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k".

[algorithm]

FOR j := 0 to 7
    i := j*16
    k[j] := ( a[i+15:i] &lt;= b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm_cmplt_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_XMMi16_XMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 7
    i := j*16
    k[j] := ( a[i+15:i] &lt; b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm_cmpneq_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_XMMi16_XMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 7
    i := j*16
    k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm_mask_cmp_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_XMMi16_XMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" based on the comparison operand specified by "imm8", and
store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask
bit is not set).

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 7
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm_mask_cmpeq_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_XMMi16_XMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" for equality, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm_mask_cmpge_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_XMMi16_XMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] &gt;= b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm_mask_cmpgt_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_XMMi16_XMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] &gt; b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm_mask_cmple_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_XMMi16_XMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] &lt;= b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm_mask_cmplt_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_XMMi16_XMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" for less-than, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] &lt; b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPW - _mm_mask_cmpneq_epi16_mask

| VPCMPW_MASKmskw_MASKmskw_XMMi16_XMMi16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k1[j]
        k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2W - _mm256_mask2_permutex2var_epi16

| VPERMI2W_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 16-bit integers in "a" and "b" across lanes using the corresponding selector and index in "idx", and
store the results in "dst" using writemask "k" (elements are copied from "idx" when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        off := 16*idx[i+3:i]
        dst[i+15:i] := idx[i+4] ? b[off+15:off] : a[off+15:off]
    ELSE
        dst[i+15:i] := idx[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMT2W - _mm256_mask_permutex2var_epi16

| VPERMT2W_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 16-bit integers in "a" and "b" across lanes using the corresponding selector and index in "idx", and
store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        off := 16*idx[i+3:i]
        dst[i+15:i] := idx[i+4] ? b[off+15:off] : a[off+15:off]
    ELSE
        dst[i+15:i] := a[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2W - _mm256_maskz_permutex2var_epi16

| VPERMI2W_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512 | VPERMT2W_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 16-bit integers in "a" and "b" across lanes using the corresponding selector and index in "idx", and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        off := 16*idx[i+3:i]
        dst[i+15:i] := idx[i+4] ? b[off+15:off] : a[off+15:off]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2W - _mm256_permutex2var_epi16

| VPERMI2W_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512 | VPERMT2W_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 16-bit integers in "a" and "b" across lanes using the corresponding selector and index in "idx", and
store the results in "dst".

[algorithm]

FOR j := 0 to 15
    i := j*16
    off := 16*idx[i+3:i]
    dst[i+15:i] := idx[i+4] ? b[off+15:off] : a[off+15:off]
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2W - _mm512_mask2_permutex2var_epi16

| VPERMI2W_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 16-bit integers in "a" and "b" across lanes using the corresponding selector and index in "idx", and
store the results in "dst" using writemask "k" (elements are copied from "idx" when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        off := 16*idx[i+4:i]
        dst[i+15:i] := idx[i+5] ? b[off+15:off] : a[off+15:off]
    ELSE
        dst[i+15:i] := idx[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMT2W - _mm512_mask_permutex2var_epi16

| VPERMT2W_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 16-bit integers in "a" and "b" across lanes using the corresponding selector and index in "idx", and
store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        off := 16*idx[i+4:i]
        dst[i+15:i] := idx[i+5] ? b[off+15:off] : a[off+15:off]
    ELSE
        dst[i+15:i] := a[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2W - _mm512_maskz_permutex2var_epi16

| VPERMI2W_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512 | VPERMT2W_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 16-bit integers in "a" and "b" across lanes using the corresponding selector and index in "idx", and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        off := 16*idx[i+4:i]
        dst[i+15:i] := idx[i+5] ? b[off+15:off] : a[off+15:off]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2W - _mm512_permutex2var_epi16

| VPERMI2W_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512 | VPERMT2W_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 16-bit integers in "a" and "b" across lanes using the corresponding selector and index in "idx", and
store the results in "dst".

[algorithm]

FOR j := 0 to 31
    i := j*16
    off := 16*idx[i+4:i]
    dst[i+15:i] := idx[i+5] ? b[off+15:off] : a[off+15:off]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2W - _mm_mask2_permutex2var_epi16

| VPERMI2W_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 16-bit integers in "a" and "b" using the corresponding selector and index in "idx", and store the
results in "dst" using writemask "k" (elements are copied from "idx" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        off := 16*idx[i+2:i]
        dst[i+15:i] := idx[i+3] ? b[off+15:off] : a[off+15:off]
    ELSE
        dst[i+15:i] := idx[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMT2W - _mm_mask_permutex2var_epi16

| VPERMT2W_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 16-bit integers in "a" and "b" using the corresponding selector and index in "idx", and store the
results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        off := 16*idx[i+2:i]
        dst[i+15:i] := idx[i+3] ? b[off+15:off] : a[off+15:off]
    ELSE
        dst[i+15:i] := a[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2W - _mm_maskz_permutex2var_epi16

| VPERMI2W_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512 | VPERMT2W_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 16-bit integers in "a" and "b" using the corresponding selector and index in "idx", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        off := 16*idx[i+2:i]
        dst[i+15:i] := idx[i+3] ? b[off+15:off] : a[off+15:off]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2W - _mm_permutex2var_epi16

| VPERMI2W_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512 | VPERMT2W_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 16-bit integers in "a" and "b" using the corresponding selector and index in "idx", and store the
results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*16
    off := 16*idx[i+2:i]
    dst[i+15:i] := idx[i+3] ? b[off+15:off] : a[off+15:off]
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMW - _mm256_mask_permutexvar_epi16

| VPERMW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 16-bit integers in "a" across lanes using the corresponding index in "idx", and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    id := idx[i+3:i]*16
    IF k[j]
        dst[i+15:i] := a[id+15:id]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMW - _mm256_maskz_permutexvar_epi16

| VPERMW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 16-bit integers in "a" across lanes using the corresponding index in "idx", and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    id := idx[i+3:i]*16
    IF k[j]
        dst[i+15:i] := a[id+15:id]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMW - _mm256_permutexvar_epi16

| VPERMW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 16-bit integers in "a" across lanes using the corresponding index in "idx", and store the results in
"dst".

[algorithm]

FOR j := 0 to 15
    i := j*16
    id := idx[i+3:i]*16
    dst[i+15:i] := a[id+15:id]
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMW - _mm512_mask_permutexvar_epi16

| VPERMW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 16-bit integers in "a" across lanes using the corresponding index in "idx", and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    id := idx[i+4:i]*16
    IF k[j]
        dst[i+15:i] := a[id+15:id]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMW - _mm512_maskz_permutexvar_epi16

| VPERMW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 16-bit integers in "a" across lanes using the corresponding index in "idx", and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    id := idx[i+4:i]*16
    IF k[j]
        dst[i+15:i] := a[id+15:id]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMW - _mm512_permutexvar_epi16

| VPERMW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 16-bit integers in "a" across lanes using the corresponding index in "idx", and store the results in
"dst".

[algorithm]

FOR j := 0 to 31
    i := j*16
    id := idx[i+4:i]*16
    dst[i+15:i] := a[id+15:id]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMW - _mm_mask_permutexvar_epi16

| VPERMW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 16-bit integers in "a" using the corresponding index in "idx", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    id := idx[i+2:i]*16
    IF k[j]
        dst[i+15:i] := a[id+15:id]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMW - _mm_maskz_permutexvar_epi16

| VPERMW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 16-bit integers in "a" using the corresponding index in "idx", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    id := idx[i+2:i]*16
    IF k[j]
        dst[i+15:i] := a[id+15:id]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMW - _mm_permutexvar_epi16

| VPERMW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 16-bit integers in "a" using the corresponding index in "idx", and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*16
    id := idx[i+2:i]*16
    dst[i+15:i] := a[id+15:id]
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMADDUBSW - _mm256_mask_maddubs_epi16

| VPMADDUBSW_YMMi16_MASKmskw_YMMi16_YMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed unsigned 8-bit integers in "a" by packed signed 8-bit integers in "b", producing intermediate
signed 16-bit integers. Horizontally add adjacent pairs of intermediate signed 16-bit integers, and pack the
saturated results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := Saturate16( a[i+15:i+8]*b[i+15:i+8] + a[i+7:i]*b[i+7:i] )
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMADDUBSW - _mm256_maskz_maddubs_epi16

| VPMADDUBSW_YMMi16_MASKmskw_YMMi16_YMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed unsigned 8-bit integers in "a" by packed signed 8-bit integers in "b", producing intermediate
signed 16-bit integers. Horizontally add adjacent pairs of intermediate signed 16-bit integers, and pack the
saturated results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := Saturate16( a[i+15:i+8]*b[i+15:i+8] + a[i+7:i]*b[i+7:i] )
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMADDUBSW - _mm512_maddubs_epi16

| VPMADDUBSW_ZMMi16_MASKmskw_ZMMi16_ZMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Vertically multiply each unsigned 8-bit integer from "a" with the corresponding signed 8-bit integer from "b",
producing intermediate signed 16-bit integers. Horizontally add adjacent pairs of intermediate signed 16-bit
integers, and pack the saturated results in "dst".

[algorithm]

FOR j := 0 to 31
    i := j*16
    dst[i+15:i] := Saturate16( a[i+15:i+8]*b[i+15:i+8] + a[i+7:i]*b[i+7:i] )
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMADDUBSW - _mm512_mask_maddubs_epi16

| VPMADDUBSW_ZMMi16_MASKmskw_ZMMi16_ZMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed unsigned 8-bit integers in "a" by packed signed 8-bit integers in "b", producing intermediate
signed 16-bit integers. Horizontally add adjacent pairs of intermediate signed 16-bit integers, and pack the
saturated results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := Saturate16( a[i+15:i+8]*b[i+15:i+8] + a[i+7:i]*b[i+7:i] )
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMADDUBSW - _mm512_maskz_maddubs_epi16

| VPMADDUBSW_ZMMi16_MASKmskw_ZMMi16_ZMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed unsigned 8-bit integers in "a" by packed signed 8-bit integers in "b", producing intermediate
signed 16-bit integers. Horizontally add adjacent pairs of intermediate signed 16-bit integers, and pack the
saturated results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := Saturate16( a[i+15:i+8]*b[i+15:i+8] + a[i+7:i]*b[i+7:i] )
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMADDUBSW - _mm_mask_maddubs_epi16

| VPMADDUBSW_XMMi16_MASKmskw_XMMi16_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed unsigned 8-bit integers in "a" by packed signed 8-bit integers in "b", producing intermediate
signed 16-bit integers. Horizontally add adjacent pairs of intermediate signed 16-bit integers, and pack the
saturated results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := Saturate16( a[i+15:i+8]*b[i+15:i+8] + a[i+7:i]*b[i+7:i] )
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMADDUBSW - _mm_maskz_maddubs_epi16

| VPMADDUBSW_XMMi16_MASKmskw_XMMi16_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed unsigned 8-bit integers in "a" by packed signed 8-bit integers in "b", producing intermediate
signed 16-bit integers. Horizontally add adjacent pairs of intermediate signed 16-bit integers, and pack the
saturated results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := Saturate16( a[i+15:i+8]*b[i+15:i+8] + a[i+7:i]*b[i+7:i] )
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMADDWD - _mm256_mask_madd_epi16

| VPMADDWD_YMMi32_MASKmskw_YMMi16_YMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed signed 16-bit integers in "a" and "b", producing intermediate signed 32-bit integers.
Horizontally add adjacent pairs of intermediate 32-bit integers, and pack the results in "dst" using writemask
"k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := SignExtend32(a[i+31:i+16]*b[i+31:i+16]) + SignExtend32(a[i+15:i]*b[i+15:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMADDWD - _mm256_maskz_madd_epi16

| VPMADDWD_YMMi32_MASKmskw_YMMi16_YMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed signed 16-bit integers in "a" and "b", producing intermediate signed 32-bit integers.
Horizontally add adjacent pairs of intermediate 32-bit integers, and pack the results in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := SignExtend32(a[i+31:i+16]*b[i+31:i+16]) + SignExtend32(a[i+15:i]*b[i+15:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMADDWD - _mm512_madd_epi16

| VPMADDWD_ZMMi32_MASKmskw_ZMMi16_ZMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed signed 16-bit integers in "a" and "b", producing intermediate signed 32-bit integers.
Horizontally add adjacent pairs of intermediate 32-bit integers, and pack the results in "dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := SignExtend32(a[i+31:i+16]*b[i+31:i+16]) + SignExtend32(a[i+15:i]*b[i+15:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMADDWD - _mm512_mask_madd_epi16

| VPMADDWD_ZMMi32_MASKmskw_ZMMi16_ZMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed signed 16-bit integers in "a" and "b", producing intermediate signed 32-bit integers.
Horizontally add adjacent pairs of intermediate 32-bit integers, and pack the results in "dst" using writemask
"k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := SignExtend32(a[i+31:i+16]*b[i+31:i+16]) + SignExtend32(a[i+15:i]*b[i+15:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMADDWD - _mm512_maskz_madd_epi16

| VPMADDWD_ZMMi32_MASKmskw_ZMMi16_ZMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed signed 16-bit integers in "a" and "b", producing intermediate signed 32-bit integers.
Horizontally add adjacent pairs of intermediate 32-bit integers, and pack the results in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := SignExtend32(a[i+31:i+16]*b[i+31:i+16]) + SignExtend32(a[i+15:i]*b[i+15:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMADDWD - _mm_mask_madd_epi16

| VPMADDWD_XMMi32_MASKmskw_XMMi16_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed signed 16-bit integers in "a" and "b", producing intermediate signed 32-bit integers.
Horizontally add adjacent pairs of intermediate 32-bit integers, and pack the results in "dst" using writemask
"k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := SignExtend32(a[i+31:i+16]*b[i+31:i+16]) + SignExtend32(a[i+15:i]*b[i+15:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMADDWD - _mm_maskz_madd_epi16

| VPMADDWD_XMMi32_MASKmskw_XMMi16_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed signed 16-bit integers in "a" and "b", producing intermediate signed 32-bit integers.
Horizontally add adjacent pairs of intermediate 32-bit integers, and pack the results in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := SignExtend32(a[i+31:i+16]*b[i+31:i+16]) + SignExtend32(a[i+15:i]*b[i+15:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXSB - _mm256_mask_max_epi8

| VPMAXSB_YMMi8_MASKmskw_YMMi8_YMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b", and store packed maximum values in "dst" using writemask
"k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := MAX(a[i+7:i], b[i+7:i])
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXSB - _mm256_maskz_max_epi8

| VPMAXSB_YMMi8_MASKmskw_YMMi8_YMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := MAX(a[i+7:i], b[i+7:i])
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXSB - _mm512_mask_max_epi8

| VPMAXSB_ZMMi8_MASKmskw_ZMMi8_ZMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b", and store packed maximum values in "dst" using writemask
"k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := MAX(a[i+7:i], b[i+7:i])
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXSB - _mm512_maskz_max_epi8

| VPMAXSB_ZMMi8_MASKmskw_ZMMi8_ZMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := MAX(a[i+7:i], b[i+7:i])
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXSB - _mm512_max_epi8

| VPMAXSB_ZMMi8_MASKmskw_ZMMi8_ZMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b", and store packed maximum values in "dst".

[algorithm]

FOR j := 0 to 63
    i := j*8
    dst[i+7:i] := MAX(a[i+7:i], b[i+7:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXSB - _mm_mask_max_epi8

| VPMAXSB_XMMi8_MASKmskw_XMMi8_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b", and store packed maximum values in "dst" using writemask
"k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := MAX(a[i+7:i], b[i+7:i])
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXSB - _mm_maskz_max_epi8

| VPMAXSB_XMMi8_MASKmskw_XMMi8_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := MAX(a[i+7:i], b[i+7:i])
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXSW - _mm256_mask_max_epi16

| VPMAXSW_YMMi16_MASKmskw_YMMi16_YMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b", and store packed maximum values in "dst" using writemask
"k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := MAX(a[i+15:i], b[i+15:i])
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXSW - _mm256_maskz_max_epi16

| VPMAXSW_YMMi16_MASKmskw_YMMi16_YMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := MAX(a[i+15:i], b[i+15:i])
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXSW - _mm512_mask_max_epi16

| VPMAXSW_ZMMi16_MASKmskw_ZMMi16_ZMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b", and store packed maximum values in "dst" using writemask
"k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := MAX(a[i+15:i], b[i+15:i])
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXSW - _mm512_maskz_max_epi16

| VPMAXSW_ZMMi16_MASKmskw_ZMMi16_ZMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := MAX(a[i+15:i], b[i+15:i])
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXSW - _mm512_max_epi16

| VPMAXSW_ZMMi16_MASKmskw_ZMMi16_ZMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b", and store packed maximum values in "dst".

[algorithm]

FOR j := 0 to 31
    i := j*16
    dst[i+15:i] := MAX(a[i+15:i], b[i+15:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXSW - _mm_mask_max_epi16

| VPMAXSW_XMMi16_MASKmskw_XMMi16_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b", and store packed maximum values in "dst" using writemask
"k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := MAX(a[i+15:i], b[i+15:i])
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXSW - _mm_maskz_max_epi16

| VPMAXSW_XMMi16_MASKmskw_XMMi16_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := MAX(a[i+15:i], b[i+15:i])
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXUB - _mm256_mask_max_epu8

| VPMAXUB_YMMu8_MASKmskw_YMMu8_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b", and store packed maximum values in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := MAX(a[i+7:i], b[i+7:i])
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXUB - _mm256_maskz_max_epu8

| VPMAXUB_YMMu8_MASKmskw_YMMu8_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := MAX(a[i+7:i], b[i+7:i])
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXUB - _mm512_mask_max_epu8

| VPMAXUB_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b", and store packed maximum values in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := MAX(a[i+7:i], b[i+7:i])
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXUB - _mm512_maskz_max_epu8

| VPMAXUB_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := MAX(a[i+7:i], b[i+7:i])
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXUB - _mm512_max_epu8

| VPMAXUB_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b", and store packed maximum values in "dst".

[algorithm]

FOR j := 0 to 63
    i := j*8
    dst[i+7:i] := MAX(a[i+7:i], b[i+7:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXUB - _mm_mask_max_epu8

| VPMAXUB_XMMu8_MASKmskw_XMMu8_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b", and store packed maximum values in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := MAX(a[i+7:i], b[i+7:i])
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXUB - _mm_maskz_max_epu8

| VPMAXUB_XMMu8_MASKmskw_XMMu8_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := MAX(a[i+7:i], b[i+7:i])
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXUW - _mm256_mask_max_epu16

| VPMAXUW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b", and store packed maximum values in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := MAX(a[i+15:i], b[i+15:i])
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXUW - _mm256_maskz_max_epu16

| VPMAXUW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b", and store packed maximum values in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := MAX(a[i+15:i], b[i+15:i])
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXUW - _mm512_mask_max_epu16

| VPMAXUW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b", and store packed maximum values in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := MAX(a[i+15:i], b[i+15:i])
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXUW - _mm512_maskz_max_epu16

| VPMAXUW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b", and store packed maximum values in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := MAX(a[i+15:i], b[i+15:i])
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXUW - _mm512_max_epu16

| VPMAXUW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b", and store packed maximum values in "dst".

[algorithm]

FOR j := 0 to 31
    i := j*16
    dst[i+15:i] := MAX(a[i+15:i], b[i+15:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXUW - _mm_mask_max_epu16

| VPMAXUW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b", and store packed maximum values in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := MAX(a[i+15:i], b[i+15:i])
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXUW - _mm_maskz_max_epu16

| VPMAXUW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b", and store packed maximum values in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := MAX(a[i+15:i], b[i+15:i])
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINSB - _mm256_mask_min_epi8

| VPMINSB_YMMi8_MASKmskw_YMMi8_YMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b", and store packed minimum values in "dst" using writemask
"k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := MIN(a[i+7:i], b[i+7:i])
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINSB - _mm256_maskz_min_epi8

| VPMINSB_YMMi8_MASKmskw_YMMi8_YMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := MIN(a[i+7:i], b[i+7:i])
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINSB - _mm512_mask_min_epi8

| VPMINSB_ZMMi8_MASKmskw_ZMMi8_ZMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b", and store packed minimum values in "dst" using writemask
"k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := MIN(a[i+7:i], b[i+7:i])
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINSB - _mm512_maskz_min_epi8

| VPMINSB_ZMMi8_MASKmskw_ZMMi8_ZMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := MIN(a[i+7:i], b[i+7:i])
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINSB - _mm512_min_epi8

| VPMINSB_ZMMi8_MASKmskw_ZMMi8_ZMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b", and store packed minimum values in "dst".

[algorithm]

FOR j := 0 to 63
    i := j*8
    dst[i+7:i] := MIN(a[i+7:i], b[i+7:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINSB - _mm_mask_min_epi8

| VPMINSB_XMMi8_MASKmskw_XMMi8_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b", and store packed minimum values in "dst" using writemask
"k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := MIN(a[i+7:i], b[i+7:i])
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINSB - _mm_maskz_min_epi8

| VPMINSB_XMMi8_MASKmskw_XMMi8_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 8-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := MIN(a[i+7:i], b[i+7:i])
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINSW - _mm256_mask_min_epi16

| VPMINSW_YMMi16_MASKmskw_YMMi16_YMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b", and store packed minimum values in "dst" using writemask
"k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := MIN(a[i+15:i], b[i+15:i])
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINSW - _mm256_maskz_min_epi16

| VPMINSW_YMMi16_MASKmskw_YMMi16_YMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := MIN(a[i+15:i], b[i+15:i])
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINSW - _mm512_mask_min_epi16

| VPMINSW_ZMMi16_MASKmskw_ZMMi16_ZMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b", and store packed minimum values in "dst" using writemask
"k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := MIN(a[i+15:i], b[i+15:i])
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINSW - _mm512_maskz_min_epi16

| VPMINSW_ZMMi16_MASKmskw_ZMMi16_ZMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := MIN(a[i+15:i], b[i+15:i])
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINSW - _mm512_min_epi16

| VPMINSW_ZMMi16_MASKmskw_ZMMi16_ZMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b", and store packed minimum values in "dst".

[algorithm]

FOR j := 0 to 31
    i := j*16
    dst[i+15:i] := MIN(a[i+15:i], b[i+15:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINSW - _mm_mask_min_epi16

| VPMINSW_XMMi16_MASKmskw_XMMi16_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b", and store packed minimum values in "dst" using writemask
"k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := MIN(a[i+15:i], b[i+15:i])
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINSW - _mm_maskz_min_epi16

| VPMINSW_XMMi16_MASKmskw_XMMi16_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 16-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := MIN(a[i+15:i], b[i+15:i])
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINUB - _mm256_mask_min_epu8

| VPMINUB_YMMu8_MASKmskw_YMMu8_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b", and store packed minimum values in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := MIN(a[i+7:i], b[i+7:i])
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINUB - _mm256_maskz_min_epu8

| VPMINUB_YMMu8_MASKmskw_YMMu8_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := MIN(a[i+7:i], b[i+7:i])
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINUB - _mm512_mask_min_epu8

| VPMINUB_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b", and store packed minimum values in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := MIN(a[i+7:i], b[i+7:i])
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINUB - _mm512_maskz_min_epu8

| VPMINUB_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := MIN(a[i+7:i], b[i+7:i])
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINUB - _mm512_min_epu8

| VPMINUB_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b", and store packed minimum values in "dst".

[algorithm]

FOR j := 0 to 63
    i := j*8
    dst[i+7:i] := MIN(a[i+7:i], b[i+7:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINUB - _mm_mask_min_epu8

| VPMINUB_XMMu8_MASKmskw_XMMu8_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b", and store packed minimum values in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := MIN(a[i+7:i], b[i+7:i])
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINUB - _mm_maskz_min_epu8

| VPMINUB_XMMu8_MASKmskw_XMMu8_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 8-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := MIN(a[i+7:i], b[i+7:i])
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINUW - _mm256_mask_min_epu16

| VPMINUW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b", and store packed minimum values in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := MIN(a[i+15:i], b[i+15:i])
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINUW - _mm256_maskz_min_epu16

| VPMINUW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b", and store packed minimum values in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := MIN(a[i+15:i], b[i+15:i])
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINUW - _mm512_mask_min_epu16

| VPMINUW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b", and store packed minimum values in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := MIN(a[i+15:i], b[i+15:i])
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINUW - _mm512_maskz_min_epu16

| VPMINUW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b", and store packed minimum values in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := MIN(a[i+15:i], b[i+15:i])
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINUW - _mm512_min_epu16

| VPMINUW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b", and store packed minimum values in "dst".

[algorithm]

FOR j := 0 to 31
    i := j*16
    dst[i+15:i] := MIN(a[i+15:i], b[i+15:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINUW - _mm_mask_min_epu16

| VPMINUW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b", and store packed minimum values in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := MIN(a[i+15:i], b[i+15:i])
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINUW - _mm_maskz_min_epu16

| VPMINUW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 16-bit integers in "a" and "b", and store packed minimum values in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := MIN(a[i+15:i], b[i+15:i])
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVB2M - _mm256_movepi8_mask

| VPMOVB2M_MASKmskw_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Set each bit of mask register "k" based on the most significant bit of the corresponding packed 8-bit integer
in "a".

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF a[i+7]
        k[j] := 1
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVB2M - _mm512_movepi8_mask

| VPMOVB2M_MASKmskw_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Set each bit of mask register "k" based on the most significant bit of the corresponding packed 8-bit integer
in "a".

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF a[i+7]
        k[j] := 1
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVB2M - _mm_movepi8_mask

| VPMOVB2M_MASKmskw_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Set each bit of mask register "k" based on the most significant bit of the corresponding packed 8-bit integer
in "a".

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF a[i+7]
        k[j] := 1
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVM2B - _mm256_movm_epi8

| VPMOVM2B_YMMu8_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Set each packed 8-bit integer in "dst" to all ones or all zeros based on the value of the corresponding bit in
"k".

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := 0xFF
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVM2B - _mm512_movm_epi8

| VPMOVM2B_ZMMu8_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Set each packed 8-bit integer in "dst" to all ones or all zeros based on the value of the corresponding bit in
"k".

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := 0xFF
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVM2B - _mm_movm_epi8

| VPMOVM2B_XMMu8_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Set each packed 8-bit integer in "dst" to all ones or all zeros based on the value of the corresponding bit in
"k".

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := 0xFF
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVM2W - _mm256_movm_epi16

| VPMOVM2W_YMMu16_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Set each packed 16-bit integer in "dst" to all ones or all zeros based on the value of the corresponding bit
in "k".

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := 0xFFFF
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVM2W - _mm512_movm_epi16

| VPMOVM2W_ZMMu16_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Set each packed 16-bit integer in "dst" to all ones or all zeros based on the value of the corresponding bit
in "k".

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := 0xFFFF
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVM2W - _mm_movm_epi16

| VPMOVM2W_XMMu16_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Set each packed 16-bit integer in "dst" to all ones or all zeros based on the value of the corresponding bit
in "k".

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := 0xFFFF
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSWB - _mm256_cvtsepi16_epi8

| VPMOVSWB_XMMi8_MASKmskw_YMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 16-bit integers in "a" to packed 8-bit integers with signed saturation, and store the
results in "dst".

[algorithm]

FOR j := 0 to 15
    i := 16*j
    l := 8*j
    dst[l+7:l] := Saturate8(a[i+15:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSWB - _mm256_mask_cvtsepi16_epi8

| VPMOVSWB_XMMi8_MASKmskw_YMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 16-bit integers in "a" to packed 8-bit integers with signed saturation, and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 15
    i := 16*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := Saturate8(a[i+15:i])
    ELSE
        dst[l+7:l] := src[l+7:l]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSWB - _mm256_mask_cvtsepi16_storeu_epi8

| VPMOVSWB_MEMi8_MASKmskw_YMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 16-bit integers in "a" to packed 8-bit integers with signed saturation, and store the
active results (those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 15
    i := 16*j
    l := 8*j
    IF k[j]
        MEM[base_addr+l+7:base_addr+l] := Saturate8(a[i+15:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVSWB - _mm256_maskz_cvtsepi16_epi8

| VPMOVSWB_XMMi8_MASKmskw_YMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 16-bit integers in "a" to packed 8-bit integers with signed saturation, and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := 16*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := Saturate8(a[i+15:i])
    ELSE
        dst[l+7:l] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSWB - _mm512_cvtsepi16_epi8

| VPMOVSWB_YMMi8_MASKmskw_ZMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 16-bit integers in "a" to packed 8-bit integers with signed saturation, and store the
results in "dst".

[algorithm]

FOR j := 0 to 31
    i := 16*j
    l := 8*j
    dst[l+7:l] := Saturate8(a[i+15:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSWB - _mm512_mask_cvtsepi16_epi8

| VPMOVSWB_YMMi8_MASKmskw_ZMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 16-bit integers in "a" to packed 8-bit integers with signed saturation, and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 31
    i := 16*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := Saturate8(a[i+15:i])
    ELSE
        dst[l+7:l] := src[l+7:l]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSWB - _mm512_mask_cvtsepi16_storeu_epi8

| VPMOVSWB_MEMi8_MASKmskw_ZMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 16-bit integers in "a" to packed 8-bit integers with signed saturation, and store the
active results (those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 31
    i := 16*j
    l := 8*j
    IF k[j]
        MEM[base_addr+l+7:base_addr+l] := Saturate8(a[i+15:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVSWB - _mm512_maskz_cvtsepi16_epi8

| VPMOVSWB_YMMi8_MASKmskw_ZMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 16-bit integers in "a" to packed 8-bit integers with signed saturation, and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := 16*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := Saturate8(a[i+15:i])
    ELSE
        dst[l+7:l] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSWB - _mm_cvtsepi16_epi8

| VPMOVSWB_XMMi8_MASKmskw_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 16-bit integers in "a" to packed 8-bit integers with signed saturation, and store the
results in "dst".

[algorithm]

FOR j := 0 to 7
    i := 16*j
    l := 8*j
    dst[l+7:l] := Saturate8(a[i+15:i])
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSWB - _mm_mask_cvtsepi16_epi8

| VPMOVSWB_XMMi8_MASKmskw_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 16-bit integers in "a" to packed 8-bit integers with signed saturation, and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := 16*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := Saturate8(a[i+15:i])
    ELSE
        dst[l+7:l] := src[l+7:l]
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSWB - _mm_mask_cvtsepi16_storeu_epi8

| VPMOVSWB_MEMi8_MASKmskw_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 16-bit integers in "a" to packed 8-bit integers with signed saturation, and store the
active results (those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 7
    i := 16*j
    l := 8*j
    IF k[j]
        MEM[base_addr+l+7:base_addr+l] := Saturate8(a[i+15:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVSWB - _mm_maskz_cvtsepi16_epi8

| VPMOVSWB_XMMi8_MASKmskw_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 16-bit integers in "a" to packed 8-bit integers with signed saturation, and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 16*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := Saturate8(a[i+15:i])
    ELSE
        dst[l+7:l] := 0
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXBW - _mm256_mask_cvtepi8_epi16

| VPMOVSXBW_YMMi16_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 8-bit integers in "a" to packed 16-bit integers, and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    l := j*16
    IF k[j]
        dst[l+15:l] := SignExtend16(a[i+7:i])
    ELSE
        dst[l+15:l] := src[l+15:l]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXBW - _mm256_maskz_cvtepi8_epi16

| VPMOVSXBW_YMMi16_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 8-bit integers in "a" to packed 16-bit integers, and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    l := j*16
    IF k[j]
        dst[l+15:l] := SignExtend16(a[i+7:i])
    ELSE
        dst[l+15:l] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXBW - _mm512_cvtepi8_epi16

| VPMOVSXBW_ZMMi16_MASKmskw_YMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 8-bit integers in "a" to packed 16-bit integers, and store the results in "dst".

[algorithm]

FOR j := 0 to 31
    i := j*8
    l := j*16
    dst[l+15:l] := SignExtend16(a[i+7:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXBW - _mm512_mask_cvtepi8_epi16

| VPMOVSXBW_ZMMi16_MASKmskw_YMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 8-bit integers in "a" to packed 16-bit integers, and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    l := j*16
    IF k[j]
        dst[l+15:l] := SignExtend16(a[i+7:i])
    ELSE
        dst[l+15:l] := src[l+15:l]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXBW - _mm512_maskz_cvtepi8_epi16

| VPMOVSXBW_ZMMi16_MASKmskw_YMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 8-bit integers in "a" to packed 16-bit integers, and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    l := j*16
    IF k[j]
        dst[l+15:l] := SignExtend16(a[i+7:i])
    ELSE
        dst[l+15:l] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXBW - _mm_mask_cvtepi8_epi16

| VPMOVSXBW_XMMi16_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 8-bit integers in "a" to packed 16-bit integers, and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*8
    l := j*16
    IF k[j]
        dst[l+15:l] := SignExtend16(a[i+7:i])
    ELSE
        dst[l+15:l] := src[l+15:l]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXBW - _mm_maskz_cvtepi8_epi16

| VPMOVSXBW_XMMi16_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 8-bit integers in "a" to packed 16-bit integers, and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*8
    l := j*16
    IF k[j]
        dst[l+15:l] := SignExtend16(a[i+7:i])
    ELSE
        dst[l+15:l] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSWB - _mm256_cvtusepi16_epi8

| VPMOVUSWB_XMMu8_MASKmskw_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 16-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and
store the results in "dst".

[algorithm]

FOR j := 0 to 15
    i := 16*j
    l := 8*j
    dst[l+7:l] := SaturateU8(a[i+15:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSWB - _mm256_mask_cvtusepi16_epi8

| VPMOVUSWB_XMMu8_MASKmskw_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 16-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 15
    i := 16*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := SaturateU8(a[i+15:i])
    ELSE
        dst[l+7:l] := src[l+7:l]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSWB - _mm256_mask_cvtusepi16_storeu_epi8

| VPMOVUSWB_MEMu8_MASKmskw_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 16-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and
store the active results (those with their respective bit set in writemask "k") to unaligned memory at
"base_addr".

[algorithm]

FOR j := 0 to 15
    i := 16*j
    l := 8*j
    IF k[j]
        MEM[base_addr+l+7:base_addr+l] := SaturateU8(a[i+15:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVUSWB - _mm256_maskz_cvtusepi16_epi8

| VPMOVUSWB_XMMu8_MASKmskw_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 16-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 15
    i := 16*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := SaturateU8(a[i+15:i])
    ELSE
        dst[l+7:l] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSWB - _mm512_cvtusepi16_epi8

| VPMOVUSWB_YMMu8_MASKmskw_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 16-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and
store the results in "dst".

[algorithm]

FOR j := 0 to 31
    i := 16*j
    l := 8*j
    dst[l+7:l] := SaturateU8(a[i+15:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSWB - _mm512_mask_cvtusepi16_epi8

| VPMOVUSWB_YMMu8_MASKmskw_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 16-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 31
    i := 16*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := SaturateU8(a[i+15:i])
    ELSE
        dst[l+7:l] := src[l+7:l]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSWB - _mm512_mask_cvtusepi16_storeu_epi8

| VPMOVUSWB_MEMu8_MASKmskw_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 16-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and
store the active results (those with their respective bit set in writemask "k") to unaligned memory at
"base_addr".

[algorithm]

FOR j := 0 to 31
    i := 16*j
    l := 8*j
    IF k[j]
        MEM[base_addr+l+7:base_addr+l] := SaturateU8(a[i+15:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVUSWB - _mm512_maskz_cvtusepi16_epi8

| VPMOVUSWB_YMMu8_MASKmskw_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 16-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 31
    i := 16*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := SaturateU8(a[i+15:i])
    ELSE
        dst[l+7:l] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSWB - _mm_cvtusepi16_epi8

| VPMOVUSWB_XMMu8_MASKmskw_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 16-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and
store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := 16*j
    l := 8*j
    dst[l+7:l] := SaturateU8(a[i+15:i])
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSWB - _mm_mask_cvtusepi16_epi8

| VPMOVUSWB_XMMu8_MASKmskw_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 16-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 7
    i := 16*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := SaturateU8(a[i+15:i])
    ELSE
        dst[l+7:l] := src[l+7:l]
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSWB - _mm_mask_cvtusepi16_storeu_epi8

| VPMOVUSWB_MEMu8_MASKmskw_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 16-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and
store the active results (those with their respective bit set in writemask "k") to unaligned memory at
"base_addr".

[algorithm]

FOR j := 0 to 7
    i := 16*j
    l := 8*j
    IF k[j]
        MEM[base_addr+l+7:base_addr+l] := SaturateU8(a[i+15:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVUSWB - _mm_maskz_cvtusepi16_epi8

| VPMOVUSWB_XMMu8_MASKmskw_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 16-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := 16*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := SaturateU8(a[i+15:i])
    ELSE
        dst[l+7:l] := 0
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVW2M - _mm256_movepi16_mask

| VPMOVW2M_MASKmskw_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Set each bit of mask register "k" based on the most significant bit of the corresponding packed 16-bit integer
in "a".

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF a[i+15]
        k[j] := 1
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVW2M - _mm512_movepi16_mask

| VPMOVW2M_MASKmskw_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Set each bit of mask register "k" based on the most significant bit of the corresponding packed 16-bit integer
in "a".

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF a[i+15]
        k[j] := 1
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVW2M - _mm_movepi16_mask

| VPMOVW2M_MASKmskw_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Set each bit of mask register "k" based on the most significant bit of the corresponding packed 16-bit integer
in "a".

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF a[i+15]
        k[j] := 1
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVWB - _mm256_cvtepi16_epi8

| VPMOVWB_XMMu8_MASKmskw_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 16-bit integers in "a" to packed 8-bit integers with truncation, and store the results in
"dst".

[algorithm]

FOR j := 0 to 15
    i := 16*j
    l := 8*j
    dst[l+7:l] := Truncate8(a[i+15:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVWB - _mm256_mask_cvtepi16_epi8

| VPMOVWB_XMMu8_MASKmskw_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 16-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := 16*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := Truncate8(a[i+15:i])
    ELSE
        dst[l+7:l] := src[l+7:l]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVWB - _mm256_mask_cvtepi16_storeu_epi8

| VPMOVWB_MEMu8_MASKmskw_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 16-bit integers in "a" to packed 8-bit integers with truncation, and store the active results
(those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 15
    i := 16*j
    l := 8*j
    IF k[j]
        MEM[base_addr+l+7:base_addr+l] := Truncate8(a[i+15:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVWB - _mm256_maskz_cvtepi16_epi8

| VPMOVWB_XMMu8_MASKmskw_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 16-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := 16*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := Truncate8(a[i+15:i])
    ELSE
        dst[l+7:l] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVWB - _mm512_cvtepi16_epi8

| VPMOVWB_YMMu8_MASKmskw_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 16-bit integers in "a" to packed 8-bit integers with truncation, and store the results in
"dst".

[algorithm]

FOR j := 0 to 31
    i := 16*j
    l := 8*j
    dst[l+7:l] := Truncate8(a[i+15:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVWB - _mm512_mask_cvtepi16_epi8

| VPMOVWB_YMMu8_MASKmskw_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 16-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := 16*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := Truncate8(a[i+15:i])
    ELSE
        dst[l+7:l] := src[l+7:l]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVWB - _mm512_mask_cvtepi16_storeu_epi8

| VPMOVWB_MEMu8_MASKmskw_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 16-bit integers in "a" to packed 8-bit integers with truncation, and store the active results
(those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 31
    i := 16*j
    l := 8*j
    IF k[j]
        MEM[base_addr+l+7:base_addr+l] := Truncate8(a[i+15:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVWB - _mm512_maskz_cvtepi16_epi8

| VPMOVWB_YMMu8_MASKmskw_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 16-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := 16*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := Truncate8(a[i+15:i])
    ELSE
        dst[l+7:l] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVWB - _mm_cvtepi16_epi8

| VPMOVWB_XMMu8_MASKmskw_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 16-bit integers in "a" to packed 8-bit integers with truncation, and store the results in
"dst".

[algorithm]

FOR j := 0 to 7
    i := 16*j
    l := 8*j
    dst[l+7:l] := Truncate8(a[i+15:i])
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVWB - _mm_mask_cvtepi16_epi8

| VPMOVWB_XMMu8_MASKmskw_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 16-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 16*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := Truncate8(a[i+15:i])
    ELSE
        dst[l+7:l] := src[l+7:l]
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVWB - _mm_mask_cvtepi16_storeu_epi8

| VPMOVWB_MEMu8_MASKmskw_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 16-bit integers in "a" to packed 8-bit integers with truncation, and store the active results
(those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 7
    i := 16*j
    l := 8*j
    IF k[j]
        MEM[base_addr+l+7:base_addr+l] := Truncate8(a[i+15:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVWB - _mm_maskz_cvtepi16_epi8

| VPMOVWB_XMMu8_MASKmskw_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 16-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 16*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := Truncate8(a[i+15:i])
    ELSE
        dst[l+7:l] := 0
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXBW - _mm256_mask_cvtepu8_epi16

| VPMOVZXBW_YMMi16_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 8-bit integers in "a" to packed 16-bit integers, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    l := j*16
    IF k[j]
        dst[l+15:l] := ZeroExtend16(a[i+7:i])
    ELSE
        dst[l+15:l] := src[l+15:l]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXBW - _mm256_maskz_cvtepu8_epi16

| VPMOVZXBW_YMMi16_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 8-bit integers in "a" to packed 16-bit integers, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    l := j*16
    IF k[j]
        dst[l+15:l] := ZeroExtend16(a[i+7:i])
    ELSE
        dst[l+15:l] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXBW - _mm512_cvtepu8_epi16

| VPMOVZXBW_ZMMi16_MASKmskw_YMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 8-bit integers in "a" to packed 16-bit integers, and store the results in "dst".

[algorithm]

FOR j := 0 to 31
    i := j*8
    l := j*16
    dst[l+15:l] := ZeroExtend16(a[i+7:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXBW - _mm512_mask_cvtepu8_epi16

| VPMOVZXBW_ZMMi16_MASKmskw_YMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 8-bit integers in "a" to packed 16-bit integers, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    l := j*16
    IF k[j]
        dst[l+15:l] := ZeroExtend16(a[i+7:i])
    ELSE
        dst[l+15:l] := src[l+15:l]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXBW - _mm512_maskz_cvtepu8_epi16

| VPMOVZXBW_ZMMi16_MASKmskw_YMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 8-bit integers in "a" to packed 16-bit integers, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    l := j*16
    IF k[j]
        dst[l+15:l] := ZeroExtend16(a[i+7:i])
    ELSE
        dst[l+15:l] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXBW - _mm_mask_cvtepu8_epi16

| VPMOVZXBW_XMMi16_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 8-bit integers in "a" to packed 16-bit integers, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*8
    l := j*16
    IF k[j]
        dst[l+15:l] := ZeroExtend16(a[i+7:i])
    ELSE
        dst[l+15:l] := src[l+15:l]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXBW - _mm_maskz_cvtepu8_epi16

| VPMOVZXBW_XMMi16_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 8-bit integers in "a" to packed 16-bit integers, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*8
    l := j*16
    IF k[j]
        dst[l+15:l] := ZeroExtend16(a[i+7:i])
    ELSE
        dst[l+15:l] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULHRSW - _mm256_mask_mulhrs_epi16

| VPMULHRSW_YMMi16_MASKmskw_YMMi16_YMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed signed 16-bit integers in "a" and "b", producing intermediate signed 32-bit integers. Truncate
each intermediate integer to the 18 most significant bits, round by adding 1, and store bits [16:1] to "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        tmp[31:0] := ((SignExtend32(a[i+15:i]) * SignExtend32(b[i+15:i])) &gt;&gt; 14) + 1
        dst[i+15:i] := tmp[16:1]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULHRSW - _mm256_maskz_mulhrs_epi16

| VPMULHRSW_YMMi16_MASKmskw_YMMi16_YMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed signed 16-bit integers in "a" and "b", producing intermediate signed 32-bit integers. Truncate
each intermediate integer to the 18 most significant bits, round by adding 1, and store bits [16:1] to "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        tmp[31:0] := ((SignExtend32(a[i+15:i]) * SignExtend32(b[i+15:i])) &gt;&gt; 14) + 1
        dst[i+15:i] := tmp[16:1]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULHRSW - _mm512_mask_mulhrs_epi16

| VPMULHRSW_ZMMi16_MASKmskw_ZMMi16_ZMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed signed 16-bit integers in "a" and "b", producing intermediate signed 32-bit integers. Truncate
each intermediate integer to the 18 most significant bits, round by adding 1, and store bits [16:1] to "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        tmp[31:0] := ((SignExtend32(a[i+15:i]) * SignExtend32(b[i+15:i])) &gt;&gt; 14) + 1
        dst[i+15:i] := tmp[16:1]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULHRSW - _mm512_maskz_mulhrs_epi16

| VPMULHRSW_ZMMi16_MASKmskw_ZMMi16_ZMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed signed 16-bit integers in "a" and "b", producing intermediate signed 32-bit integers. Truncate
each intermediate integer to the 18 most significant bits, round by adding 1, and store bits [16:1] to "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        tmp[31:0] := ((SignExtend32(a[i+15:i]) * SignExtend32(b[i+15:i])) &gt;&gt; 14) + 1
        dst[i+15:i] := tmp[16:1]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULHRSW - _mm512_mulhrs_epi16

| VPMULHRSW_ZMMi16_MASKmskw_ZMMi16_ZMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed signed 16-bit integers in "a" and "b", producing intermediate signed 32-bit integers. Truncate
each intermediate integer to the 18 most significant bits, round by adding 1, and store bits [16:1] to "dst".

[algorithm]

FOR j := 0 to 31
    i := j*16
    tmp[31:0] := ((SignExtend32(a[i+15:i]) * SignExtend32(b[i+15:i])) &gt;&gt; 14) + 1
    dst[i+15:i] := tmp[16:1]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULHRSW - _mm_mask_mulhrs_epi16

| VPMULHRSW_XMMi16_MASKmskw_XMMi16_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed signed 16-bit integers in "a" and "b", producing intermediate signed 32-bit integers. Truncate
each intermediate integer to the 18 most significant bits, round by adding 1, and store bits [16:1] to "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        tmp[31:0] := ((SignExtend32(a[i+15:i]) * SignExtend32(b[i+15:i])) &gt;&gt; 14) + 1
        dst[i+15:i] := tmp[16:1]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULHRSW - _mm_maskz_mulhrs_epi16

| VPMULHRSW_XMMi16_MASKmskw_XMMi16_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed signed 16-bit integers in "a" and "b", producing intermediate signed 32-bit integers. Truncate
each intermediate integer to the 18 most significant bits, round by adding 1, and store bits [16:1] to "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        tmp[31:0] := ((SignExtend32(a[i+15:i]) * SignExtend32(b[i+15:i])) &gt;&gt; 14) + 1
        dst[i+15:i] := tmp[16:1]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULHUW - _mm256_mask_mulhi_epu16

| VPMULHUW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the packed unsigned 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store
the high 16 bits of the intermediate integers in "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        tmp[31:0] := a[i+15:i] * b[i+15:i]
        dst[i+15:i] := tmp[31:16]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULHUW - _mm256_maskz_mulhi_epu16

| VPMULHUW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the packed unsigned 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store
the high 16 bits of the intermediate integers in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        tmp[31:0] := a[i+15:i] * b[i+15:i]
        dst[i+15:i] := tmp[31:16]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULHUW - _mm512_mask_mulhi_epu16

| VPMULHUW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the packed unsigned 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store
the high 16 bits of the intermediate integers in "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        tmp[31:0] := a[i+15:i] * b[i+15:i]
        dst[i+15:i] := tmp[31:16]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULHUW - _mm512_maskz_mulhi_epu16

| VPMULHUW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the packed unsigned 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store
the high 16 bits of the intermediate integers in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        tmp[31:0] := a[i+15:i] * b[i+15:i]
        dst[i+15:i] := tmp[31:16]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULHUW - _mm512_mulhi_epu16

| VPMULHUW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the packed unsigned 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store
the high 16 bits of the intermediate integers in "dst".

[algorithm]

FOR j := 0 to 31
    i := j*16
    tmp[31:0] := a[i+15:i] * b[i+15:i]
    dst[i+15:i] := tmp[31:16]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULHUW - _mm_mask_mulhi_epu16

| VPMULHUW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the packed unsigned 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store
the high 16 bits of the intermediate integers in "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        tmp[31:0] := a[i+15:i] * b[i+15:i]
        dst[i+15:i] := tmp[31:16]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULHUW - _mm_maskz_mulhi_epu16

| VPMULHUW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the packed unsigned 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store
the high 16 bits of the intermediate integers in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        tmp[31:0] := a[i+15:i] * b[i+15:i]
        dst[i+15:i] := tmp[31:16]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULHW - _mm256_mask_mulhi_epi16

| VPMULHW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the packed signed 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store
the high 16 bits of the intermediate integers in "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        tmp[31:0] := SignExtend32(a[i+15:i]) * SignExtend32(b[i+15:i])
        dst[i+15:i] := tmp[31:16]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULHW - _mm256_maskz_mulhi_epi16

| VPMULHW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the packed signed 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store
the high 16 bits of the intermediate integers in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        tmp[31:0] := SignExtend32(a[i+15:i]) * SignExtend32(b[i+15:i])
        dst[i+15:i] := tmp[31:16]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULHW - _mm512_mask_mulhi_epi16

| VPMULHW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the packed signed 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store
the high 16 bits of the intermediate integers in "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        tmp[31:0] := SignExtend32(a[i+15:i]) * SignExtend32(b[i+15:i])
        dst[i+15:i] := tmp[31:16]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULHW - _mm512_maskz_mulhi_epi16

| VPMULHW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the packed signed 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store
the high 16 bits of the intermediate integers in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        tmp[31:0] := SignExtend32(a[i+15:i]) * SignExtend32(b[i+15:i])
        dst[i+15:i] := tmp[31:16]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULHW - _mm512_mulhi_epi16

| VPMULHW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the packed signed 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store
the high 16 bits of the intermediate integers in "dst".

[algorithm]

FOR j := 0 to 31
    i := j*16
    tmp[31:0] := SignExtend32(a[i+15:i]) * SignExtend32(b[i+15:i])
    dst[i+15:i] := tmp[31:16]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULHW - _mm_mask_mulhi_epi16

| VPMULHW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the packed signed 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store
the high 16 bits of the intermediate integers in "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        tmp[31:0] := SignExtend32(a[i+15:i]) * SignExtend32(b[i+15:i])
        dst[i+15:i] := tmp[31:16]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULHW - _mm_maskz_mulhi_epi16

| VPMULHW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the packed signed 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store
the high 16 bits of the intermediate integers in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        tmp[31:0] := SignExtend32(a[i+15:i]) * SignExtend32(b[i+15:i])
        dst[i+15:i] := tmp[31:16]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULLW - _mm256_mask_mullo_epi16

| VPMULLW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the packed 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store the low
16 bits of the intermediate integers in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        tmp[31:0] := SignExtend32(a[i+15:i]) * SignExtend32(b[i+15:i])
        dst[i+15:i] := tmp[15:0]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULLW - _mm256_maskz_mullo_epi16

| VPMULLW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the packed 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store the low
16 bits of the intermediate integers in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        tmp[31:0] := SignExtend32(a[i+15:i]) * SignExtend32(b[i+15:i])
        dst[i+15:i] := tmp[15:0]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULLW - _mm512_mask_mullo_epi16

| VPMULLW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the packed 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store the low
16 bits of the intermediate integers in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        tmp[31:0] := SignExtend32(a[i+15:i]) * SignExtend32(b[i+15:i])
        dst[i+15:i] := tmp[15:0]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULLW - _mm512_maskz_mullo_epi16

| VPMULLW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the packed 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store the low
16 bits of the intermediate integers in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        tmp[31:0] := SignExtend32(a[i+15:i]) * SignExtend32(b[i+15:i])
        dst[i+15:i] := tmp[15:0]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULLW - _mm512_mullo_epi16

| VPMULLW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the packed 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store the low
16 bits of the intermediate integers in "dst".

[algorithm]

FOR j := 0 to 31
    i := j*16
    tmp[31:0] := SignExtend32(a[i+15:i]) * SignExtend32(b[i+15:i])
    dst[i+15:i] := tmp[15:0]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULLW - _mm_mask_mullo_epi16

| VPMULLW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the packed 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store the low
16 bits of the intermediate integers in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        tmp[31:0] := SignExtend32(a[i+15:i]) * SignExtend32(b[i+15:i])
        dst[i+15:i] := tmp[15:0]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULLW - _mm_maskz_mullo_epi16

| VPMULLW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the packed 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store the low
16 bits of the intermediate integers in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        tmp[31:0] := SignExtend32(a[i+15:i]) * SignExtend32(b[i+15:i])
        dst[i+15:i] := tmp[15:0]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSADBW - _mm512_sad_epu8

| VPSADBW_ZMMu16_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the absolute differences of packed unsigned 8-bit integers in "a" and "b", then horizontally sum each
consecutive 8 differences to produce eight unsigned 16-bit integers, and pack these unsigned 16-bit integers in
the low 16 bits of 64-bit elements in "dst".

[algorithm]

FOR j := 0 to 63
    i := j*8
    tmp[i+7:i] := ABS(a[i+7:i] - b[i+7:i])
ENDFOR
FOR j := 0 to 7
    i := j*64
    dst[i+15:i] := tmp[i+7:i] + tmp[i+15:i+8] + tmp[i+23:i+16] + tmp[i+31:i+24] + \
                   tmp[i+39:i+32] + tmp[i+47:i+40] + tmp[i+55:i+48] + tmp[i+63:i+56]
    dst[i+63:i+16] := 0
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHUFB - _mm256_mask_shuffle_epi8

| VPSHUFB_YMMu8_MASKmskw_YMMu8_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle packed 8-bit integers in "a" according to shuffle control mask in the corresponding 8-bit element of
"b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding
mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        IF b[i+7] == 1
            dst[i+7:i] := 0
        ELSE
            index[4:0] := b[i+3:i] + (j &amp; 0x10)
            dst[i+7:i] := a[index*8+7:index*8]
        FI
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHUFB - _mm256_maskz_shuffle_epi8

| VPSHUFB_YMMu8_MASKmskw_YMMu8_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle packed 8-bit integers in "a" according to shuffle control mask in the corresponding 8-bit element of
"b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        IF b[i+7] == 1
            dst[i+7:i] := 0
        ELSE
            index[4:0] := b[i+3:i] + (j &amp; 0x10)
            dst[i+7:i] := a[index*8+7:index*8]
        FI
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHUFB - _mm512_mask_shuffle_epi8

| VPSHUFB_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 8-bit integers in "a" within 128-bit lanes using the control in the corresponding 8-bit element of
"b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding
mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        IF b[i+7] == 1
            dst[i+7:i] := 0
        ELSE
            index[5:0] := b[i+3:i] + (j &amp; 0x30)
            dst[i+7:i] := a[index*8+7:index*8]
        FI
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHUFB - _mm512_maskz_shuffle_epi8

| VPSHUFB_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle packed 8-bit integers in "a" according to shuffle control mask in the corresponding 8-bit element of
"b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        IF b[i+7] == 1
            dst[i+7:i] := 0
        ELSE
            index[5:0] := b[i+3:i] + (j &amp; 0x30)
            dst[i+7:i] := a[index*8+7:index*8]
        FI
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHUFB - _mm512_shuffle_epi8

| VPSHUFB_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle packed 8-bit integers in "a" according to shuffle control mask in the corresponding 8-bit element of
"b", and store the results in "dst".

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF b[i+7] == 1
        dst[i+7:i] := 0
    ELSE
        index[5:0] := b[i+3:i] + (j &amp; 0x30)
        dst[i+7:i] := a[index*8+7:index*8]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHUFB - _mm_mask_shuffle_epi8

| VPSHUFB_XMMu8_MASKmskw_XMMu8_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle packed 8-bit integers in "a" according to shuffle control mask in the corresponding 8-bit element of
"b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding
mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        IF b[i+7] == 1
            dst[i+7:i] := 0
        ELSE
            index[3:0] := b[i+3:i]
            dst[i+7:i] := a[index*8+7:index*8]
        FI
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHUFB - _mm_maskz_shuffle_epi8

| VPSHUFB_XMMu8_MASKmskw_XMMu8_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle packed 8-bit integers in "a" according to shuffle control mask in the corresponding 8-bit element of
"b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        IF b[i+7] == 1
            dst[i+7:i] := 0
        ELSE
            index[3:0] := b[i+3:i]
            dst[i+7:i] := a[index*8+7:index*8]
        FI
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHUFHW - _mm256_mask_shufflehi_epi16

| VPSHUFHW_YMMu16_MASKmskw_YMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 16-bit integers in the high 64 bits of 128-bit lanes of "a" using the control in "imm8". Store the
results in the high 64 bits of 128-bit lanes of "dst", with the low 64 bits of 128-bit lanes being copied from
from "a" to "dst", using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

tmp_dst[63:0] := a[63:0]
tmp_dst[79:64] := (a &gt;&gt; (imm8[1:0] * 16))[79:64]
tmp_dst[95:80] := (a &gt;&gt; (imm8[3:2] * 16))[79:64]
tmp_dst[111:96] := (a &gt;&gt; (imm8[5:4] * 16))[79:64]
tmp_dst[127:112] := (a &gt;&gt; (imm8[7:6] * 16))[79:64]
tmp_dst[191:128] := a[191:128]
tmp_dst[207:192] := (a &gt;&gt; (imm8[1:0] * 16))[207:192]
tmp_dst[223:208] := (a &gt;&gt; (imm8[3:2] * 16))[207:192]
tmp_dst[239:224] := (a &gt;&gt; (imm8[5:4] * 16))[207:192]
tmp_dst[255:240] := (a &gt;&gt; (imm8[7:6] * 16))[207:192]
FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHUFHW - _mm256_maskz_shufflehi_epi16

| VPSHUFHW_YMMu16_MASKmskw_YMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 16-bit integers in the high 64 bits of 128-bit lanes of "a" using the control in "imm8". Store the
results in the high 64 bits of 128-bit lanes of "dst", with the low 64 bits of 128-bit lanes being copied from
from "a" to "dst", using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

tmp_dst[63:0] := a[63:0]
tmp_dst[79:64] := (a &gt;&gt; (imm8[1:0] * 16))[79:64]
tmp_dst[95:80] := (a &gt;&gt; (imm8[3:2] * 16))[79:64]
tmp_dst[111:96] := (a &gt;&gt; (imm8[5:4] * 16))[79:64]
tmp_dst[127:112] := (a &gt;&gt; (imm8[7:6] * 16))[79:64]
tmp_dst[191:128] := a[191:128]
tmp_dst[207:192] := (a &gt;&gt; (imm8[1:0] * 16))[207:192]
tmp_dst[223:208] := (a &gt;&gt; (imm8[3:2] * 16))[207:192]
tmp_dst[239:224] := (a &gt;&gt; (imm8[5:4] * 16))[207:192]
tmp_dst[255:240] := (a &gt;&gt; (imm8[7:6] * 16))[207:192]
FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHUFHW - _mm512_mask_shufflehi_epi16

| VPSHUFHW_ZMMu16_MASKmskw_ZMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 16-bit integers in the high 64 bits of 128-bit lanes of "a" using the control in "imm8". Store the
results in the high 64 bits of 128-bit lanes of "dst", with the low 64 bits of 128-bit lanes being copied from
from "a" to "dst", using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

tmp_dst[63:0] := a[63:0]
tmp_dst[79:64] := (a &gt;&gt; (imm8[1:0] * 16))[79:64]
tmp_dst[95:80] := (a &gt;&gt; (imm8[3:2] * 16))[79:64]
tmp_dst[111:96] := (a &gt;&gt; (imm8[5:4] * 16))[79:64]
tmp_dst[127:112] := (a &gt;&gt; (imm8[7:6] * 16))[79:64]
tmp_dst[191:128] := a[191:128]
tmp_dst[207:192] := (a &gt;&gt; (imm8[1:0] * 16))[207:192]
tmp_dst[223:208] := (a &gt;&gt; (imm8[3:2] * 16))[207:192]
tmp_dst[239:224] := (a &gt;&gt; (imm8[5:4] * 16))[207:192]
tmp_dst[255:240] := (a &gt;&gt; (imm8[7:6] * 16))[207:192]
tmp_dst[319:256] := a[319:256]
tmp_dst[335:320] := (a &gt;&gt; (imm8[1:0] * 16))[335:320]
tmp_dst[351:336] := (a &gt;&gt; (imm8[3:2] * 16))[335:320]
tmp_dst[367:352] := (a &gt;&gt; (imm8[5:4] * 16))[335:320]
tmp_dst[383:368] := (a &gt;&gt; (imm8[7:6] * 16))[335:320]
tmp_dst[447:384] := a[447:384]
tmp_dst[463:448] := (a &gt;&gt; (imm8[1:0] * 16))[463:448]
tmp_dst[479:464] := (a &gt;&gt; (imm8[3:2] * 16))[463:448]
tmp_dst[495:480] := (a &gt;&gt; (imm8[5:4] * 16))[463:448]
tmp_dst[511:496] := (a &gt;&gt; (imm8[7:6] * 16))[463:448]
FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHUFHW - _mm512_maskz_shufflehi_epi16

| VPSHUFHW_ZMMu16_MASKmskw_ZMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 16-bit integers in the high 64 bits of 128-bit lanes of "a" using the control in "imm8". Store the
results in the high 64 bits of 128-bit lanes of "dst", with the low 64 bits of 128-bit lanes being copied from
from "a" to "dst", using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

tmp_dst[63:0] := a[63:0]
tmp_dst[79:64] := (a &gt;&gt; (imm8[1:0] * 16))[79:64]
tmp_dst[95:80] := (a &gt;&gt; (imm8[3:2] * 16))[79:64]
tmp_dst[111:96] := (a &gt;&gt; (imm8[5:4] * 16))[79:64]
tmp_dst[127:112] := (a &gt;&gt; (imm8[7:6] * 16))[79:64]
tmp_dst[191:128] := a[191:128]
tmp_dst[207:192] := (a &gt;&gt; (imm8[1:0] * 16))[207:192]
tmp_dst[223:208] := (a &gt;&gt; (imm8[3:2] * 16))[207:192]
tmp_dst[239:224] := (a &gt;&gt; (imm8[5:4] * 16))[207:192]
tmp_dst[255:240] := (a &gt;&gt; (imm8[7:6] * 16))[207:192]
tmp_dst[319:256] := a[319:256]
tmp_dst[335:320] := (a &gt;&gt; (imm8[1:0] * 16))[335:320]
tmp_dst[351:336] := (a &gt;&gt; (imm8[3:2] * 16))[335:320]
tmp_dst[367:352] := (a &gt;&gt; (imm8[5:4] * 16))[335:320]
tmp_dst[383:368] := (a &gt;&gt; (imm8[7:6] * 16))[335:320]
tmp_dst[447:384] := a[447:384]
tmp_dst[463:448] := (a &gt;&gt; (imm8[1:0] * 16))[463:448]
tmp_dst[479:464] := (a &gt;&gt; (imm8[3:2] * 16))[463:448]
tmp_dst[495:480] := (a &gt;&gt; (imm8[5:4] * 16))[463:448]
tmp_dst[511:496] := (a &gt;&gt; (imm8[7:6] * 16))[463:448]
FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHUFHW - _mm512_shufflehi_epi16

| VPSHUFHW_ZMMu16_MASKmskw_ZMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 16-bit integers in the high 64 bits of 128-bit lanes of "a" using the control in "imm8". Store the
results in the high 64 bits of 128-bit lanes of "dst", with the low 64 bits of 128-bit lanes being copied from
from "a" to "dst".

[algorithm]

dst[63:0] := a[63:0]
dst[79:64] := (a &gt;&gt; (imm8[1:0] * 16))[79:64]
dst[95:80] := (a &gt;&gt; (imm8[3:2] * 16))[79:64]
dst[111:96] := (a &gt;&gt; (imm8[5:4] * 16))[79:64]
dst[127:112] := (a &gt;&gt; (imm8[7:6] * 16))[79:64]
dst[191:128] := a[191:128]
dst[207:192] := (a &gt;&gt; (imm8[1:0] * 16))[207:192]
dst[223:208] := (a &gt;&gt; (imm8[3:2] * 16))[207:192]
dst[239:224] := (a &gt;&gt; (imm8[5:4] * 16))[207:192]
dst[255:240] := (a &gt;&gt; (imm8[7:6] * 16))[207:192]
dst[319:256] := a[319:256]
dst[335:320] := (a &gt;&gt; (imm8[1:0] * 16))[335:320]
dst[351:336] := (a &gt;&gt; (imm8[3:2] * 16))[335:320]
dst[367:352] := (a &gt;&gt; (imm8[5:4] * 16))[335:320]
dst[383:368] := (a &gt;&gt; (imm8[7:6] * 16))[335:320]
dst[447:384] := a[447:384]
dst[463:448] := (a &gt;&gt; (imm8[1:0] * 16))[463:448]
dst[479:464] := (a &gt;&gt; (imm8[3:2] * 16))[463:448]
dst[495:480] := (a &gt;&gt; (imm8[5:4] * 16))[463:448]
dst[511:496] := (a &gt;&gt; (imm8[7:6] * 16))[463:448]
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHUFHW - _mm_mask_shufflehi_epi16

| VPSHUFHW_XMMu16_MASKmskw_XMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 16-bit integers in the high 64 bits of "a" using the control in "imm8". Store the results in the high
64 bits of "dst", with the low 64 bits being copied from from "a" to "dst", using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set).

[algorithm]

tmp_dst[63:0] := a[63:0]
tmp_dst[79:64] := (a &gt;&gt; (imm8[1:0] * 16))[79:64]
tmp_dst[95:80] := (a &gt;&gt; (imm8[3:2] * 16))[79:64]
tmp_dst[111:96] := (a &gt;&gt; (imm8[5:4] * 16))[79:64]
tmp_dst[127:112] := (a &gt;&gt; (imm8[7:6] * 16))[79:64]
FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHUFHW - _mm_maskz_shufflehi_epi16

| VPSHUFHW_XMMu16_MASKmskw_XMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 16-bit integers in the high 64 bits of "a" using the control in "imm8". Store the results in the high
64 bits of "dst", with the low 64 bits being copied from from "a" to "dst", using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

tmp_dst[63:0] := a[63:0]
tmp_dst[79:64] := (a &gt;&gt; (imm8[1:0] * 16))[79:64]
tmp_dst[95:80] := (a &gt;&gt; (imm8[3:2] * 16))[79:64]
tmp_dst[111:96] := (a &gt;&gt; (imm8[5:4] * 16))[79:64]
tmp_dst[127:112] := (a &gt;&gt; (imm8[7:6] * 16))[79:64]
FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHUFLW - _mm256_mask_shufflelo_epi16

| VPSHUFLW_YMMu16_MASKmskw_YMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 16-bit integers in the low 64 bits of 128-bit lanes of "a" using the control in "imm8". Store the
results in the low 64 bits of 128-bit lanes of "dst", with the high 64 bits of 128-bit lanes being copied from
from "a" to "dst", using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

tmp_dst[15:0] := (a &gt;&gt; (imm8[1:0] * 16))[15:0]
tmp_dst[31:16] := (a &gt;&gt; (imm8[3:2] * 16))[15:0]
tmp_dst[47:32] := (a &gt;&gt; (imm8[5:4] * 16))[15:0]
tmp_dst[63:48] := (a &gt;&gt; (imm8[7:6] * 16))[15:0]
tmp_dst[127:64] := a[127:64]
tmp_dst[143:128] := (a &gt;&gt; (imm8[1:0] * 16))[143:128]
tmp_dst[159:144] := (a &gt;&gt; (imm8[3:2] * 16))[143:128]
tmp_dst[175:160] := (a &gt;&gt; (imm8[5:4] * 16))[143:128]
tmp_dst[191:176] := (a &gt;&gt; (imm8[7:6] * 16))[143:128]
tmp_dst[255:192] := a[255:192]
FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHUFLW - _mm256_maskz_shufflelo_epi16

| VPSHUFLW_YMMu16_MASKmskw_YMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 16-bit integers in the low 64 bits of 128-bit lanes of "a" using the control in "imm8". Store the
results in the low 64 bits of 128-bit lanes of "dst", with the high 64 bits of 128-bit lanes being copied from
from "a" to "dst", using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

tmp_dst[15:0] := (a &gt;&gt; (imm8[1:0] * 16))[15:0]
tmp_dst[31:16] := (a &gt;&gt; (imm8[3:2] * 16))[15:0]
tmp_dst[47:32] := (a &gt;&gt; (imm8[5:4] * 16))[15:0]
tmp_dst[63:48] := (a &gt;&gt; (imm8[7:6] * 16))[15:0]
tmp_dst[127:64] := a[127:64]
tmp_dst[143:128] := (a &gt;&gt; (imm8[1:0] * 16))[143:128]
tmp_dst[159:144] := (a &gt;&gt; (imm8[3:2] * 16))[143:128]
tmp_dst[175:160] := (a &gt;&gt; (imm8[5:4] * 16))[143:128]
tmp_dst[191:176] := (a &gt;&gt; (imm8[7:6] * 16))[143:128]
tmp_dst[255:192] := a[255:192]
FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHUFLW - _mm512_mask_shufflelo_epi16

| VPSHUFLW_ZMMu16_MASKmskw_ZMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 16-bit integers in the low 64 bits of 128-bit lanes of "a" using the control in "imm8". Store the
results in the low 64 bits of 128-bit lanes of "dst", with the high 64 bits of 128-bit lanes being copied from
from "a" to "dst", using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

tmp_dst[15:0] := (a &gt;&gt; (imm8[1:0] * 16))[15:0]
tmp_dst[31:16] := (a &gt;&gt; (imm8[3:2] * 16))[15:0]
tmp_dst[47:32] := (a &gt;&gt; (imm8[5:4] * 16))[15:0]
tmp_dst[63:48] := (a &gt;&gt; (imm8[7:6] * 16))[15:0]
tmp_dst[127:64] := a[127:64]
tmp_dst[143:128] := (a &gt;&gt; (imm8[1:0] * 16))[143:128]
tmp_dst[159:144] := (a &gt;&gt; (imm8[3:2] * 16))[143:128]
tmp_dst[175:160] := (a &gt;&gt; (imm8[5:4] * 16))[143:128]
tmp_dst[191:176] := (a &gt;&gt; (imm8[7:6] * 16))[143:128]
tmp_dst[255:192] := a[255:192]
tmp_dst[271:256] := (a &gt;&gt; (imm8[1:0] * 16))[271:256]
tmp_dst[287:272] := (a &gt;&gt; (imm8[3:2] * 16))[271:256]
tmp_dst[303:288] := (a &gt;&gt; (imm8[5:4] * 16))[271:256]
tmp_dst[319:304] := (a &gt;&gt; (imm8[7:6] * 16))[271:256]
tmp_dst[383:320] := a[383:320]
tmp_dst[399:384] := (a &gt;&gt; (imm8[1:0] * 16))[399:384]
tmp_dst[415:400] := (a &gt;&gt; (imm8[3:2] * 16))[399:384]
tmp_dst[431:416] := (a &gt;&gt; (imm8[5:4] * 16))[399:384]
tmp_dst[447:432] := (a &gt;&gt; (imm8[7:6] * 16))[399:384]
tmp_dst[511:448] := a[511:448]
FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHUFLW - _mm512_maskz_shufflelo_epi16

| VPSHUFLW_ZMMu16_MASKmskw_ZMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 16-bit integers in the low 64 bits of 128-bit lanes of "a" using the control in "imm8". Store the
results in the low 64 bits of 128-bit lanes of "dst", with the high 64 bits of 128-bit lanes being copied from
from "a" to "dst", using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

tmp_dst[15:0] := (a &gt;&gt; (imm8[1:0] * 16))[15:0]
tmp_dst[31:16] := (a &gt;&gt; (imm8[3:2] * 16))[15:0]
tmp_dst[47:32] := (a &gt;&gt; (imm8[5:4] * 16))[15:0]
tmp_dst[63:48] := (a &gt;&gt; (imm8[7:6] * 16))[15:0]
tmp_dst[127:64] := a[127:64]
tmp_dst[143:128] := (a &gt;&gt; (imm8[1:0] * 16))[143:128]
tmp_dst[159:144] := (a &gt;&gt; (imm8[3:2] * 16))[143:128]
tmp_dst[175:160] := (a &gt;&gt; (imm8[5:4] * 16))[143:128]
tmp_dst[191:176] := (a &gt;&gt; (imm8[7:6] * 16))[143:128]
tmp_dst[255:192] := a[255:192]
tmp_dst[271:256] := (a &gt;&gt; (imm8[1:0] * 16))[271:256]
tmp_dst[287:272] := (a &gt;&gt; (imm8[3:2] * 16))[271:256]
tmp_dst[303:288] := (a &gt;&gt; (imm8[5:4] * 16))[271:256]
tmp_dst[319:304] := (a &gt;&gt; (imm8[7:6] * 16))[271:256]
tmp_dst[383:320] := a[383:320]
tmp_dst[399:384] := (a &gt;&gt; (imm8[1:0] * 16))[399:384]
tmp_dst[415:400] := (a &gt;&gt; (imm8[3:2] * 16))[399:384]
tmp_dst[431:416] := (a &gt;&gt; (imm8[5:4] * 16))[399:384]
tmp_dst[447:432] := (a &gt;&gt; (imm8[7:6] * 16))[399:384]
tmp_dst[511:448] := a[511:448]
FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHUFLW - _mm512_shufflelo_epi16

| VPSHUFLW_ZMMu16_MASKmskw_ZMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 16-bit integers in the low 64 bits of 128-bit lanes of "a" using the control in "imm8". Store the
results in the low 64 bits of 128-bit lanes of "dst", with the high 64 bits of 128-bit lanes being copied from
from "a" to "dst".

[algorithm]

dst[15:0] := (a &gt;&gt; (imm8[1:0] * 16))[15:0]
dst[31:16] := (a &gt;&gt; (imm8[3:2] * 16))[15:0]
dst[47:32] := (a &gt;&gt; (imm8[5:4] * 16))[15:0]
dst[63:48] := (a &gt;&gt; (imm8[7:6] * 16))[15:0]
dst[127:64] := a[127:64]
dst[143:128] := (a &gt;&gt; (imm8[1:0] * 16))[143:128]
dst[159:144] := (a &gt;&gt; (imm8[3:2] * 16))[143:128]
dst[175:160] := (a &gt;&gt; (imm8[5:4] * 16))[143:128]
dst[191:176] := (a &gt;&gt; (imm8[7:6] * 16))[143:128]
dst[255:192] := a[255:192]
dst[271:256] := (a &gt;&gt; (imm8[1:0] * 16))[271:256]
dst[287:272] := (a &gt;&gt; (imm8[3:2] * 16))[271:256]
dst[303:288] := (a &gt;&gt; (imm8[5:4] * 16))[271:256]
dst[319:304] := (a &gt;&gt; (imm8[7:6] * 16))[271:256]
dst[383:320] := a[383:320]
dst[399:384] := (a &gt;&gt; (imm8[1:0] * 16))[399:384]
dst[415:400] := (a &gt;&gt; (imm8[3:2] * 16))[399:384]
dst[431:416] := (a &gt;&gt; (imm8[5:4] * 16))[399:384]
dst[447:432] := (a &gt;&gt; (imm8[7:6] * 16))[399:384]
dst[511:448] := a[511:448]
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHUFLW - _mm_mask_shufflelo_epi16

| VPSHUFLW_XMMu16_MASKmskw_XMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 16-bit integers in the low 64 bits of "a" using the control in "imm8". Store the results in the low 64
bits of "dst", with the high 64 bits being copied from from "a" to "dst", using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set).

[algorithm]

tmp_dst[15:0] := (a &gt;&gt; (imm8[1:0] * 16))[15:0]
tmp_dst[31:16] := (a &gt;&gt; (imm8[3:2] * 16))[15:0]
tmp_dst[47:32] := (a &gt;&gt; (imm8[5:4] * 16))[15:0]
tmp_dst[63:48] := (a &gt;&gt; (imm8[7:6] * 16))[15:0]
tmp_dst[127:64] := a[127:64]
FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHUFLW - _mm_maskz_shufflelo_epi16

| VPSHUFLW_XMMu16_MASKmskw_XMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 16-bit integers in the low 64 bits of "a" using the control in "imm8". Store the results in the low 64
bits of "dst", with the high 64 bits being copied from from "a" to "dst", using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

tmp_dst[15:0] := (a &gt;&gt; (imm8[1:0] * 16))[15:0]
tmp_dst[31:16] := (a &gt;&gt; (imm8[3:2] * 16))[15:0]
tmp_dst[47:32] := (a &gt;&gt; (imm8[5:4] * 16))[15:0]
tmp_dst[63:48] := (a &gt;&gt; (imm8[7:6] * 16))[15:0]
tmp_dst[127:64] := a[127:64]
FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLDQ - _mm512_bslli_epi128

| VPSLLDQ_ZMMu8_ZMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift 128-bit lanes in "a" left by "imm8" bytes while shifting in zeros, and store the results in "dst".

[algorithm]

tmp := imm8[7:0]
IF tmp &gt; 15
    tmp := 16
FI
dst[127:0] := a[127:0] &lt;&lt; (tmp*8)
dst[255:128] := a[255:128] &lt;&lt; (tmp*8)
dst[383:256] := a[383:256] &lt;&lt; (tmp*8)
dst[511:384] := a[511:384] &lt;&lt; (tmp*8)
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLVW - _mm256_mask_sllv_epi16

| VPSLLVW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" left by the amount specified by the corresponding element in "count" while
shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        IF count[i+15:i] &lt; 16
            dst[i+15:i] := ZeroExtend16(a[i+15:i] &lt;&lt; count[i+15:i])
        ELSE
            dst[i+15:i] := 0
        FI
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLVW - _mm256_maskz_sllv_epi16

| VPSLLVW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" left by the amount specified by the corresponding element in "count" while
shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        IF count[i+15:i] &lt; 16
            dst[i+15:i] := ZeroExtend16(a[i+15:i] &lt;&lt; count[i+15:i])
        ELSE
            dst[i+15:i] := 0
        FI
    ELSE
        dst[i+15:i] := 0
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLVW - _mm256_sllv_epi16

| VPSLLVW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" left by the amount specified by the corresponding element in "count" while
shifting in zeros, and store the results in "dst".

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF count[i+15:i] &lt; 16
        dst[i+15:i] := ZeroExtend16(a[i+15:i] &lt;&lt; count[i+15:i])
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLVW - _mm512_mask_sllv_epi16

| VPSLLVW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" left by the amount specified by the corresponding element in "count" while
shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        IF count[i+15:i] &lt; 16
            dst[i+15:i] := ZeroExtend16(a[i+15:i] &lt;&lt; count[i+15:i])
        ELSE
            dst[i+15:i] := 0
        FI
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLVW - _mm512_maskz_sllv_epi16

| VPSLLVW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" left by the amount specified by the corresponding element in "count" while
shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        IF count[i+15:i] &lt; 16
            dst[i+15:i] := ZeroExtend16(a[i+15:i] &lt;&lt; count[i+15:i])
        ELSE
            dst[i+15:i] := 0
        FI
    ELSE
        dst[i+15:i] := 0
    FI    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLVW - _mm512_sllv_epi16

| VPSLLVW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" left by the amount specified by the corresponding element in "count" while
shifting in zeros, and store the results in "dst".

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF count[i+15:i] &lt; 16
        dst[i+15:i] := ZeroExtend16(a[i+15:i] &lt;&lt; count[i+15:i])
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLVW - _mm_mask_sllv_epi16

| VPSLLVW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" left by the amount specified by the corresponding element in "count" while
shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        IF count[i+15:i] &lt; 16
            dst[i+15:i] := ZeroExtend16(a[i+15:i] &lt;&lt; count[i+15:i])
        ELSE
            dst[i+15:i] := 0
        FI
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLVW - _mm_maskz_sllv_epi16

| VPSLLVW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" left by the amount specified by the corresponding element in "count" while
shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        IF count[i+15:i] &lt; 16
            dst[i+15:i] := ZeroExtend16(a[i+15:i] &lt;&lt; count[i+15:i])
        ELSE
            dst[i+15:i] := 0
        FI
    ELSE
        dst[i+15:i] := 0
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLVW - _mm_sllv_epi16

| VPSLLVW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" left by the amount specified by the corresponding element in "count" while
shifting in zeros, and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF count[i+15:i] &lt; 16
        dst[i+15:i] := ZeroExtend16(a[i+15:i] &lt;&lt; count[i+15:i])
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLW - _mm256_mask_sll_epi16

| VPSLLW_YMMu16_MASKmskw_YMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        IF count[63:0] &gt; 15
            dst[i+15:i] := 0
        ELSE
            dst[i+15:i] := ZeroExtend16(a[i+15:i] &lt;&lt; count[63:0])
        FI
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLW - _mm256_mask_slli_epi16

| VPSLLW_YMMu16_MASKmskw_YMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" left by "imm8" while shifting in zeros, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        IF imm8[7:0] &gt; 15
            dst[i+15:i] := 0
        ELSE
            dst[i+15:i] := ZeroExtend16(a[i+15:i] &lt;&lt; imm8[7:0])
        FI
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLW - _mm256_maskz_sll_epi16

| VPSLLW_YMMu16_MASKmskw_YMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        IF count[63:0] &gt; 15
            dst[i+15:i] := 0
        ELSE
            dst[i+15:i] := ZeroExtend16(a[i+15:i] &lt;&lt; count[63:0])
        FI
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLW - _mm256_maskz_slli_epi16

| VPSLLW_YMMu16_MASKmskw_YMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" left by "imm8" while shifting in zeros, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        IF imm8[7:0] &gt; 15
            dst[i+15:i] := 0
        ELSE
            dst[i+15:i] := ZeroExtend16(a[i+15:i] &lt;&lt; imm8[7:0])
        FI
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLW - _mm512_mask_sll_epi16

| VPSLLW_ZMMu16_MASKmskw_ZMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        IF count[63:0] &gt; 15
            dst[i+15:i] := 0
        ELSE
            dst[i+15:i] := ZeroExtend16(a[i+15:i] &lt;&lt; count[63:0])
        FI
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLW - _mm512_mask_slli_epi16

| VPSLLW_ZMMu16_MASKmskw_ZMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" left by "imm8" while shifting in zeros, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        IF imm8[7:0] &gt; 15
            dst[i+15:i] := 0
        ELSE
            dst[i+15:i] := ZeroExtend16(a[i+15:i] &lt;&lt; imm8[7:0])
        FI
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLW - _mm512_maskz_sll_epi16

| VPSLLW_ZMMu16_MASKmskw_ZMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        IF count[63:0] &gt; 15
            dst[i+15:i] := 0
        ELSE
            dst[i+15:i] := ZeroExtend16(a[i+15:i] &lt;&lt; count[63:0])
        FI
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLW - _mm512_maskz_slli_epi16

| VPSLLW_ZMMu16_MASKmskw_ZMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" left by "imm8" while shifting in zeros, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        IF imm8[7:0] &gt; 15
            dst[i+15:i] := 0
        ELSE
            dst[i+15:i] := ZeroExtend16(a[i+15:i] &lt;&lt; imm8[7:0])
        FI
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLW - _mm512_sll_epi16

| VPSLLW_ZMMu16_MASKmskw_ZMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst".

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF count[63:0] &gt; 15
        dst[i+15:i] := 0
    ELSE
        dst[i+15:i] := ZeroExtend16(a[i+15:i] &lt;&lt; count[63:0])
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLW - _mm512_slli_epi16

| VPSLLW_ZMMu16_MASKmskw_ZMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" left by "imm8" while shifting in zeros, and store the results in "dst".

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF imm8[7:0] &gt; 15
        dst[i+15:i] := 0
    ELSE
        dst[i+15:i] := ZeroExtend16(a[i+15:i] &lt;&lt; imm8[7:0])
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLW - _mm_mask_sll_epi16

| VPSLLW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        IF count[63:0] &gt; 15
            dst[i+15:i] := 0
        ELSE
            dst[i+15:i] := ZeroExtend16(a[i+15:i] &lt;&lt; count[63:0])
        FI
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLW - _mm_mask_slli_epi16

| VPSLLW_XMMu16_MASKmskw_XMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" left by "imm8" while shifting in zeros, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        IF imm8[7:0] &gt; 15
            dst[i+15:i] := 0
        ELSE
            dst[i+15:i] := ZeroExtend16(a[i+15:i] &lt;&lt; imm8[7:0])
        FI
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLW - _mm_maskz_sll_epi16

| VPSLLW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        IF count[63:0] &gt; 15
            dst[i+15:i] := 0
        ELSE
            dst[i+15:i] := ZeroExtend16(a[i+15:i] &lt;&lt; count[63:0])
        FI
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLW - _mm_maskz_slli_epi16

| VPSLLW_XMMu16_MASKmskw_XMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" left by "imm8" while shifting in zeros, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        IF imm8[7:0] &gt; 15
            dst[i+15:i] := 0
        ELSE
            dst[i+15:i] := ZeroExtend16(a[i+15:i] &lt;&lt; imm8[7:0])
        FI
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAVW - _mm256_mask_srav_epi16

| VPSRAVW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in sign bits, and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        IF count[i+15:i] &lt; 16
            dst[i+15:i] := SignExtend16(a[i+15:i] &gt;&gt; count[i+15:i])
        ELSE
            dst[i+15:i] := (a[i+15] ? 0xFFFF : 0)
        FI
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAVW - _mm256_maskz_srav_epi16

| VPSRAVW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in sign bits, and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        IF count[i+15:i] &lt; 16
            dst[i+15:i] := SignExtend16(a[i+15:i] &gt;&gt; count[i+15:i])
        ELSE
            dst[i+15:i] := (a[i+15] ? 0xFFFF : 0)
        FI
    ELSE
        dst[i+15:i] := 0
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAVW - _mm256_srav_epi16

| VPSRAVW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in sign bits, and store the results in "dst".

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF count[i+15:i] &lt; 16
        dst[i+15:i] := SignExtend16(a[i+15:i] &gt;&gt; count[i+15:i])
    ELSE
        dst[i+15:i] := (a[i+15] ? 0xFFFF : 0)
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAVW - _mm512_mask_srav_epi16

| VPSRAVW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in sign bits, and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        IF count[i+15:i] &lt; 16
            dst[i+15:i] := SignExtend16(a[i+15:i] &gt;&gt; count[i+15:i])
        ELSE
            dst[i+15:i] := (a[i+15] ? 0xFFFF : 0)
        FI
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAVW - _mm512_maskz_srav_epi16

| VPSRAVW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in sign bits, and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        IF count[i+15:i] &lt; 16
            dst[i+15:i] := SignExtend16(a[i+15:i] &gt;&gt; count[i+15:i])
        ELSE
            dst[i+15:i] := (a[i+15] ? 0xFFFF : 0)
        FI
    ELSE
        dst[i+15:i] := 0
    FI    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAVW - _mm512_srav_epi16

| VPSRAVW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in sign bits, and store the results in "dst".

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF count[i+15:i] &lt; 16
        dst[i+15:i] := SignExtend16(a[i+15:i] &gt;&gt; count[i+15:i])
    ELSE
        dst[i+15:i] := (a[i+15] ? 0xFFFF : 0)
    FI    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAVW - _mm_mask_srav_epi16

| VPSRAVW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in sign bits, and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        IF count[i+15:i] &lt; 16
            dst[i+15:i] := SignExtend16(a[i+15:i] &gt;&gt; count[i+15:i])
        ELSE
            dst[i+15:i] := (a[i+15] ? 0xFFFF : 0)
        FI
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAVW - _mm_maskz_srav_epi16

| VPSRAVW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in sign bits, and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        IF count[i+15:i] &lt; 16
            dst[i+15:i] := SignExtend16(a[i+15:i] &gt;&gt; count[i+15:i])
        ELSE
            dst[i+15:i] := (a[i+15] ? 0xFFFF : 0)
        FI
    ELSE
        dst[i+15:i] := 0
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAVW - _mm_srav_epi16

| VPSRAVW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in sign bits, and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF count[i+15:i] &lt; 16
        dst[i+15:i] := SignExtend16(a[i+15:i] &gt;&gt; count[i+15:i])
    ELSE
        dst[i+15:i] := (a[i+15] ? 0xFFFF : 0)
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAW - _mm256_mask_sra_epi16

| VPSRAW_YMMu16_MASKmskw_YMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by "count" while shifting in sign bits, and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        IF count[63:0] &gt; 15
            dst[i+15:i] := (a[i+15] ? 0xFFFF : 0x0)
        ELSE
            dst[i+15:i] := SignExtend16(a[i+15:i] &gt;&gt; count[63:0])
        FI
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAW - _mm256_mask_srai_epi16

| VPSRAW_YMMu16_MASKmskw_YMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by "imm8" while shifting in sign bits, and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        IF imm8[7:0] &gt; 15
            dst[i+15:i] := (a[i+15] ? 0xFFFF : 0x0)
        ELSE
            dst[i+15:i] := SignExtend16(a[i+15:i] &gt;&gt; imm8[7:0])
        FI
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAW - _mm256_maskz_sra_epi16

| VPSRAW_YMMu16_MASKmskw_YMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by "count" while shifting in sign bits, and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        IF count[63:0] &gt; 15
            dst[i+15:i] := (a[i+15] ? 0xFFFF : 0x0)
        ELSE
            dst[i+15:i] := SignExtend16(a[i+15:i] &gt;&gt; count[63:0])
        FI
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAW - _mm256_maskz_srai_epi16

| VPSRAW_YMMu16_MASKmskw_YMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by "imm8" while shifting in sign bits, and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        IF imm8[7:0] &gt; 15
            dst[i+15:i] := (a[i+15] ? 0xFFFF : 0x0)
        ELSE
            dst[i+15:i] := SignExtend16(a[i+15:i] &gt;&gt; imm8[7:0])
        FI
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAW - _mm512_mask_sra_epi16

| VPSRAW_ZMMu16_MASKmskw_ZMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by "count" while shifting in sign bits, and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        IF count[63:0] &gt; 15
            dst[i+15:i] := (a[i+15] ? 0xFFFF : 0x0)
        ELSE
            dst[i+15:i] := SignExtend16(a[i+15:i] &gt;&gt; count[63:0])
        FI
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAW - _mm512_mask_srai_epi16

| VPSRAW_ZMMu16_MASKmskw_ZMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by "imm8" while shifting in sign bits, and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        IF imm8[7:0] &gt; 15
            dst[i+15:i] := (a[i+15] ? 0xFFFF : 0x0)
        ELSE
            dst[i+15:i] := SignExtend16(a[i+15:i] &gt;&gt; imm8[7:0])
        FI
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAW - _mm512_maskz_sra_epi16

| VPSRAW_ZMMu16_MASKmskw_ZMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by "count" while shifting in sign bits, and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        IF count[63:0] &gt; 15
            dst[i+15:i] := (a[i+15] ? 0xFFFF : 0x0)
        ELSE
            dst[i+15:i] := SignExtend16(a[i+15:i] &gt;&gt; count[63:0])
        FI
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAW - _mm512_maskz_srai_epi16

| VPSRAW_ZMMu16_MASKmskw_ZMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by "imm8" while shifting in sign bits, and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        IF imm8[7:0] &gt; 15
            dst[i+15:i] := (a[i+15] ? 0xFFFF : 0x0)
        ELSE
            dst[i+15:i] := SignExtend16(a[i+15:i] &gt;&gt; imm8[7:0])
        FI
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAW - _mm512_sra_epi16

| VPSRAW_ZMMu16_MASKmskw_ZMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by "count" while shifting in sign bits, and store the results in
"dst".

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF count[63:0] &gt; 15
        dst[i+15:i] := (a[i+15] ? 0xFFFF : 0x0)
    ELSE
        dst[i+15:i] := SignExtend16(a[i+15:i] &gt;&gt; count[63:0])
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAW - _mm512_srai_epi16

| VPSRAW_ZMMu16_MASKmskw_ZMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by "imm8" while shifting in sign bits, and store the results in
"dst".

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF imm8[7:0] &gt; 15
        dst[i+15:i] := (a[i+15] ? 0xFFFF : 0x0)
    ELSE
        dst[i+15:i] := SignExtend16(a[i+15:i] &gt;&gt; imm8[7:0])
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAW - _mm_mask_sra_epi16

| VPSRAW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by "count" while shifting in sign bits, and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        IF count[63:0] &gt; 15
            dst[i+15:i] := (a[i+15] ? 0xFFFF : 0x0)
        ELSE
            dst[i+15:i] := SignExtend16(a[i+15:i] &gt;&gt; count[63:0])
        FI
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAW - _mm_mask_srai_epi16

| VPSRAW_XMMu16_MASKmskw_XMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by "imm8" while shifting in sign bits, and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        IF imm8[7:0] &gt; 15
            dst[i+15:i] := (a[i+15] ? 0xFFFF : 0x0)
        ELSE
            dst[i+15:i] := SignExtend16(a[i+15:i] &gt;&gt; imm8[7:0])
        FI
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAW - _mm_maskz_sra_epi16

| VPSRAW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by "count" while shifting in sign bits, and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        IF count[63:0] &gt; 15
            dst[i+15:i] := (a[i+15] ? 0xFFFF : 0x0)
        ELSE
            dst[i+15:i] := SignExtend16(a[i+15:i] &gt;&gt; count[63:0])
        FI
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAW - _mm_maskz_srai_epi16

| VPSRAW_XMMu16_MASKmskw_XMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by "imm8" while shifting in sign bits, and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        IF imm8[7:0] &gt; 15
            dst[i+15:i] := (a[i+15] ? 0xFFFF : 0x0)
        ELSE
            dst[i+15:i] := SignExtend16(a[i+15:i] &gt;&gt; imm8[7:0])
        FI
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLDQ - _mm512_bsrli_epi128

| VPSRLDQ_ZMMu8_ZMMu8_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift 128-bit lanes in "a" right by "imm8" bytes while shifting in zeros, and store the results in "dst".

[algorithm]

tmp := imm8[7:0]
IF tmp &gt; 15
    tmp := 16
FI
dst[127:0] := a[127:0] &gt;&gt; (tmp*8)
dst[255:128] := a[255:128] &gt;&gt; (tmp*8)
dst[383:256] := a[383:256] &gt;&gt; (tmp*8)
dst[511:384] := a[511:384] &gt;&gt; (tmp*8)
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLVW - _mm256_mask_srlv_epi16

| VPSRLVW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        IF count[i+15:i] &lt; 16
            dst[i+15:i] := ZeroExtend16(a[i+15:i] &gt;&gt; count[i+15:i])
        ELSE
            dst[i+15:i] := 0
        FI
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLVW - _mm256_maskz_srlv_epi16

| VPSRLVW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        IF count[i+15:i] &lt; 16
            dst[i+15:i] := ZeroExtend16(a[i+15:i] &gt;&gt; count[i+15:i])
        ELSE
            dst[i+15:i] := 0
        FI
    ELSE
        dst[i+15:i] := 0
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLVW - _mm256_srlv_epi16

| VPSRLVW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in zeros, and store the results in "dst".

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF count[i+15:i] &lt; 16
        dst[i+15:i] := ZeroExtend16(a[i+15:i] &gt;&gt; count[i+15:i])
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLVW - _mm512_mask_srlv_epi16

| VPSRLVW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        IF count[i+15:i] &lt; 16
            dst[i+15:i] := ZeroExtend16(a[i+15:i] &gt;&gt; count[i+15:i])
        ELSE
            dst[i+15:i] := 0
        FI
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLVW - _mm512_maskz_srlv_epi16

| VPSRLVW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        IF count[i+15:i] &lt; 16
            dst[i+15:i] := ZeroExtend16(a[i+15:i] &gt;&gt; count[i+15:i])
        ELSE
            dst[i+15:i] := 0
        FI
    ELSE
        dst[i+15:i] := 0
    FI    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLVW - _mm512_srlv_epi16

| VPSRLVW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in zeros, and store the results in "dst".

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF count[i+15:i] &lt; 16
        dst[i+15:i] := ZeroExtend16(a[i+15:i] &gt;&gt; count[i+15:i])
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLVW - _mm_mask_srlv_epi16

| VPSRLVW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        IF count[i+15:i] &lt; 16
            dst[i+15:i] := ZeroExtend16(a[i+15:i] &gt;&gt; count[i+15:i])
        ELSE
            dst[i+15:i] := 0
        FI
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLVW - _mm_maskz_srlv_epi16

| VPSRLVW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        IF count[i+15:i] &lt; 16
            dst[i+15:i] := ZeroExtend16(a[i+15:i] &gt;&gt; count[i+15:i])
        ELSE
            dst[i+15:i] := 0
        FI
    ELSE
        dst[i+15:i] := 0
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLVW - _mm_srlv_epi16

| VPSRLVW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in zeros, and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF count[i+15:i] &lt; 16
        dst[i+15:i] := ZeroExtend16(a[i+15:i] &gt;&gt; count[i+15:i])
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLW - _mm256_mask_srl_epi16

| VPSRLW_YMMu16_MASKmskw_YMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        IF count[63:0] &gt; 15
            dst[i+15:i] := 0
        ELSE
            dst[i+15:i] := ZeroExtend16(a[i+15:i] &gt;&gt; count[63:0])
        FI
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLW - _mm256_mask_srli_epi16

| VPSRLW_YMMu16_MASKmskw_YMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by "imm8" while shifting in zeros, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        IF imm8[7:0] &gt; 15
            dst[i+15:i] := 0
        ELSE
            dst[i+15:i] := ZeroExtend16(a[i+15:i] &gt;&gt; imm8[7:0])
        FI
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLW - _mm256_maskz_srl_epi16

| VPSRLW_YMMu16_MASKmskw_YMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        IF count[63:0] &gt; 15
            dst[i+15:i] := 0
        ELSE
            dst[i+15:i] := ZeroExtend16(a[i+15:i] &gt;&gt; count[63:0])
        FI
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLW - _mm256_maskz_srli_epi16

| VPSRLW_YMMu16_MASKmskw_YMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by "imm8" while shifting in zeros, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        IF imm8[7:0] &gt; 15
            dst[i+15:i] := 0
        ELSE
            dst[i+15:i] := ZeroExtend16(a[i+15:i] &gt;&gt; imm8[7:0])
        FI
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLW - _mm512_mask_srl_epi16

| VPSRLW_ZMMu16_MASKmskw_ZMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        IF count[63:0] &gt; 15
            dst[i+15:i] := 0
        ELSE
            dst[i+15:i] := ZeroExtend16(a[i+15:i] &gt;&gt; count[63:0])
        FI
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLW - _mm512_mask_srli_epi16

| VPSRLW_ZMMu16_MASKmskw_ZMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by "imm8" while shifting in zeros, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        IF imm8[7:0] &gt; 15
            dst[i+15:i] := 0
        ELSE
            dst[i+15:i] := ZeroExtend16(a[i+15:i] &gt;&gt; imm8[7:0])
        FI
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLW - _mm512_maskz_srl_epi16

| VPSRLW_ZMMu16_MASKmskw_ZMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        IF count[63:0] &gt; 15
            dst[i+15:i] := 0
        ELSE
            dst[i+15:i] := ZeroExtend16(a[i+15:i] &gt;&gt; count[63:0])
        FI
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLW - _mm512_maskz_srli_epi16

| VPSRLW_ZMMu16_MASKmskw_ZMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by "imm8" while shifting in zeros, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        IF imm8[7:0] &gt; 15
            dst[i+15:i] := 0
        ELSE
            dst[i+15:i] := ZeroExtend16(a[i+15:i] &gt;&gt; imm8[7:0])
        FI
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLW - _mm512_srl_epi16

| VPSRLW_ZMMu16_MASKmskw_ZMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst".

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF count[63:0] &gt; 15
        dst[i+15:i] := 0
    ELSE
        dst[i+15:i] := ZeroExtend16(a[i+15:i] &gt;&gt; count[63:0])
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLW - _mm512_srli_epi16

| VPSRLW_ZMMu16_MASKmskw_ZMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by "imm8" while shifting in zeros, and store the results in "dst".

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF imm8[7:0] &gt; 15
        dst[i+15:i] := 0
    ELSE
        dst[i+15:i] := ZeroExtend16(a[i+15:i] &gt;&gt; imm8[7:0])
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLW - _mm_mask_srl_epi16

| VPSRLW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        IF count[63:0] &gt; 15
            dst[i+15:i] := 0
        ELSE
            dst[i+15:i] := ZeroExtend16(a[i+15:i] &gt;&gt; count[63:0])
        FI
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLW - _mm_mask_srli_epi16

| VPSRLW_XMMu16_MASKmskw_XMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by "imm8" while shifting in zeros, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        IF imm8[7:0] &gt; 15
            dst[i+15:i] := 0
        ELSE
            dst[i+15:i] := ZeroExtend16(a[i+15:i] &gt;&gt; imm8[7:0])
        FI
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLW - _mm_maskz_srl_epi16

| VPSRLW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        IF count[63:0] &gt; 15
            dst[i+15:i] := 0
        ELSE
            dst[i+15:i] := ZeroExtend16(a[i+15:i] &gt;&gt; count[63:0])
        FI
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLW - _mm_maskz_srli_epi16

| VPSRLW_XMMu16_MASKmskw_XMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 16-bit integers in "a" right by "imm8" while shifting in zeros, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        IF imm8[7:0] &gt; 15
            dst[i+15:i] := 0
        ELSE
            dst[i+15:i] := ZeroExtend16(a[i+15:i] &gt;&gt; imm8[7:0])
        FI
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBB - _mm256_mask_sub_epi8

| VPSUBB_YMMu8_MASKmskw_YMMu8_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed 8-bit integers in "b" from packed 8-bit integers in "a", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := a[i+7:i] - b[i+7:i]
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBB - _mm256_maskz_sub_epi8

| VPSUBB_YMMu8_MASKmskw_YMMu8_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed 8-bit integers in "b" from packed 8-bit integers in "a", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := a[i+7:i] - b[i+7:i]
    ELSE
        dst[i+7:i] := 0
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBB - _mm512_mask_sub_epi8

| VPSUBB_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed 8-bit integers in "b" from packed 8-bit integers in "a", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := a[i+7:i] - b[i+7:i]
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBB - _mm512_maskz_sub_epi8

| VPSUBB_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed 8-bit integers in "b" from packed 8-bit integers in "a", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := a[i+7:i] - b[i+7:i]
    ELSE
        dst[i+7:i] := 0
    FI    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBB - _mm512_sub_epi8

| VPSUBB_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed 8-bit integers in "b" from packed 8-bit integers in "a", and store the results in "dst".

[algorithm]

FOR j := 0 to 63
    i := j*8
    dst[i+7:i] := a[i+7:i] - b[i+7:i]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBB - _mm_mask_sub_epi8

| VPSUBB_XMMu8_MASKmskw_XMMu8_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed 8-bit integers in "b" from packed 8-bit integers in "a", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := a[i+7:i] - b[i+7:i]
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBB - _mm_maskz_sub_epi8

| VPSUBB_XMMu8_MASKmskw_XMMu8_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed 8-bit integers in "b" from packed 8-bit integers in "a", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := a[i+7:i] - b[i+7:i]
    ELSE
        dst[i+7:i] := 0
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBSB - _mm256_mask_subs_epi8

| VPSUBSB_YMMi8_MASKmskw_YMMi8_YMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed signed 8-bit integers in "b" from packed 8-bit integers in "a" using saturation, and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := Saturate8(a[i+7:i] - b[i+7:i])
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBSB - _mm256_maskz_subs_epi8

| VPSUBSB_YMMi8_MASKmskw_YMMi8_YMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed signed 8-bit integers in "b" from packed 8-bit integers in "a" using saturation, and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := Saturate8(a[i+7:i] - b[i+7:i])
    ELSE
        dst[i+7:i] := 0
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBSB - _mm512_mask_subs_epi8

| VPSUBSB_ZMMi8_MASKmskw_ZMMi8_ZMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed signed 8-bit integers in "b" from packed 8-bit integers in "a" using saturation, and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := Saturate8(a[i+7:i] - b[i+7:i])
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBSB - _mm512_maskz_subs_epi8

| VPSUBSB_ZMMi8_MASKmskw_ZMMi8_ZMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed signed 8-bit integers in "b" from packed 8-bit integers in "a" using saturation, and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := Saturate8(a[i+7:i] - b[i+7:i])
    ELSE
        dst[i+7:i] := 0
    FI    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBSB - _mm512_subs_epi8

| VPSUBSB_ZMMi8_MASKmskw_ZMMi8_ZMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed signed 8-bit integers in "b" from packed 8-bit integers in "a" using saturation, and store the
results in "dst".

[algorithm]

FOR j := 0 to 63
    i := j*8
    dst[i+7:i] := Saturate8(a[i+7:i] - b[i+7:i])    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBSB - _mm_mask_subs_epi8

| VPSUBSB_XMMi8_MASKmskw_XMMi8_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed signed 8-bit integers in "b" from packed 8-bit integers in "a" using saturation, and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := Saturate8(a[i+7:i] - b[i+7:i])
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBSB - _mm_maskz_subs_epi8

| VPSUBSB_XMMi8_MASKmskw_XMMi8_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed signed 8-bit integers in "b" from packed 8-bit integers in "a" using saturation, and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := Saturate8(a[i+7:i] - b[i+7:i])
    ELSE
        dst[i+7:i] := 0
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBSW - _mm256_mask_subs_epi16

| VPSUBSW_YMMi16_MASKmskw_YMMi16_YMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed signed 16-bit integers in "b" from packed 16-bit integers in "a" using saturation, and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := Saturate16(a[i+15:i] - b[i+15:i])
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBSW - _mm256_maskz_subs_epi16

| VPSUBSW_YMMi16_MASKmskw_YMMi16_YMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed signed 16-bit integers in "b" from packed 16-bit integers in "a" using saturation, and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := Saturate16(a[i+15:i] - b[i+15:i])
    ELSE
        dst[i+15:i] := 0
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBSW - _mm512_mask_subs_epi16

| VPSUBSW_ZMMi16_MASKmskw_ZMMi16_ZMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed signed 16-bit integers in "b" from packed 16-bit integers in "a" using saturation, and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := Saturate16(a[i+15:i] - b[i+15:i])
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBSW - _mm512_maskz_subs_epi16

| VPSUBSW_ZMMi16_MASKmskw_ZMMi16_ZMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed signed 16-bit integers in "b" from packed 16-bit integers in "a" using saturation, and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := Saturate16(a[i+15:i] - b[i+15:i])
    ELSE
        dst[i+15:i] := 0
    FI    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBSW - _mm512_subs_epi16

| VPSUBSW_ZMMi16_MASKmskw_ZMMi16_ZMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed signed 16-bit integers in "b" from packed 16-bit integers in "a" using saturation, and store
the results in "dst".

[algorithm]

FOR j := 0 to 31
    i := j*16
    dst[i+15:i] := Saturate16(a[i+15:i] - b[i+15:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBSW - _mm_mask_subs_epi16

| VPSUBSW_XMMi16_MASKmskw_XMMi16_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed signed 16-bit integers in "b" from packed 16-bit integers in "a" using saturation, and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := Saturate16(a[i+15:i] - b[i+15:i])
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBSW - _mm_maskz_subs_epi16

| VPSUBSW_XMMi16_MASKmskw_XMMi16_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed signed 16-bit integers in "b" from packed 16-bit integers in "a" using saturation, and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := Saturate16(a[i+15:i] - b[i+15:i])
    ELSE
        dst[i+15:i] := 0
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBUSB - _mm256_mask_subs_epu8

| VPSUBUSB_YMMu8_MASKmskw_YMMu8_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed unsigned 8-bit integers in "b" from packed unsigned 8-bit integers in "a" using saturation,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := SaturateU8(a[i+7:i] - b[i+7:i])
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBUSB - _mm256_maskz_subs_epu8

| VPSUBUSB_YMMu8_MASKmskw_YMMu8_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed unsigned 8-bit integers in "b" from packed unsigned 8-bit integers in "a" using saturation,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := SaturateU8(a[i+7:i] - b[i+7:i])
    ELSE
        dst[i+7:i] := 0
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBUSB - _mm512_mask_subs_epu8

| VPSUBUSB_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed unsigned 8-bit integers in "b" from packed unsigned 8-bit integers in "a" using saturation,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := SaturateU8(a[i+7:i] - b[i+7:i])
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBUSB - _mm512_maskz_subs_epu8

| VPSUBUSB_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed unsigned 8-bit integers in "b" from packed unsigned 8-bit integers in "a" using saturation,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := SaturateU8(a[i+7:i] - b[i+7:i])
    ELSE
        dst[i+7:i] := 0
    FI    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBUSB - _mm512_subs_epu8

| VPSUBUSB_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed unsigned 8-bit integers in "b" from packed unsigned 8-bit integers in "a" using saturation,
and store the results in "dst".

[algorithm]

FOR j := 0 to 63
    i := j*8
    dst[i+7:i] := SaturateU8(a[i+7:i] - b[i+7:i])    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBUSB - _mm_mask_subs_epu8

| VPSUBUSB_XMMu8_MASKmskw_XMMu8_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed unsigned 8-bit integers in "b" from packed unsigned 8-bit integers in "a" using saturation,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := SaturateU8(a[i+7:i] - b[i+7:i])
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBUSB - _mm_maskz_subs_epu8

| VPSUBUSB_XMMu8_MASKmskw_XMMu8_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed unsigned 8-bit integers in "b" from packed unsigned 8-bit integers in "a" using saturation,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := SaturateU8(a[i+7:i] - b[i+7:i])
    ELSE
        dst[i+7:i] := 0
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBUSW - _mm256_mask_subs_epu16

| VPSUBUSW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed unsigned 16-bit integers in "b" from packed unsigned 16-bit integers in "a" using saturation,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := SaturateU16(a[i+15:i] - b[i+15:i])
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBUSW - _mm256_maskz_subs_epu16

| VPSUBUSW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed unsigned 16-bit integers in "b" from packed unsigned 16-bit integers in "a" using saturation,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := SaturateU16(a[i+15:i] - b[i+15:i])
    ELSE
        dst[i+15:i] := 0
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBUSW - _mm512_mask_subs_epu16

| VPSUBUSW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed unsigned 16-bit integers in "b" from packed unsigned 16-bit integers in "a" using saturation,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := SaturateU16(a[i+15:i] - b[i+15:i])
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBUSW - _mm512_maskz_subs_epu16

| VPSUBUSW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed unsigned 16-bit integers in "b" from packed unsigned 16-bit integers in "a" using saturation,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := SaturateU16(a[i+15:i] - b[i+15:i])
    ELSE
        dst[i+15:i] := 0
    FI    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBUSW - _mm512_subs_epu16

| VPSUBUSW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed unsigned 16-bit integers in "b" from packed unsigned 16-bit integers in "a" using saturation,
and store the results in "dst".

[algorithm]

FOR j := 0 to 31
    i := j*16
    dst[i+15:i] := SaturateU16(a[i+15:i] - b[i+15:i])    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBUSW - _mm_mask_subs_epu16

| VPSUBUSW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed unsigned 16-bit integers in "b" from packed unsigned 16-bit integers in "a" using saturation,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := SaturateU16(a[i+15:i] - b[i+15:i])
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBUSW - _mm_maskz_subs_epu16

| VPSUBUSW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed unsigned 16-bit integers in "b" from packed unsigned 16-bit integers in "a" using saturation,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := SaturateU16(a[i+15:i] - b[i+15:i])
    ELSE
        dst[i+15:i] := 0
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBW - _mm256_mask_sub_epi16

| VPSUBW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed 16-bit integers in "b" from packed 16-bit integers in "a", and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := a[i+15:i] - b[i+15:i]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBW - _mm256_maskz_sub_epi16

| VPSUBW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed 16-bit integers in "b" from packed 16-bit integers in "a", and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := a[i+15:i] - b[i+15:i]
    ELSE
        dst[i+15:i] := 0
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBW - _mm512_mask_sub_epi16

| VPSUBW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed 16-bit integers in "b" from packed 16-bit integers in "a", and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := a[i+15:i] - b[i+15:i]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBW - _mm512_maskz_sub_epi16

| VPSUBW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed 16-bit integers in "b" from packed 16-bit integers in "a", and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := a[i+15:i] - b[i+15:i]
    ELSE
        dst[i+15:i] := 0
    FI    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBW - _mm512_sub_epi16

| VPSUBW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed 16-bit integers in "b" from packed 16-bit integers in "a", and store the results in "dst".

[algorithm]

FOR j := 0 to 31
    i := j*16
    dst[i+15:i] := a[i+15:i] - b[i+15:i]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBW - _mm_mask_sub_epi16

| VPSUBW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed 16-bit integers in "b" from packed 16-bit integers in "a", and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := a[i+15:i] - b[i+15:i]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBW - _mm_maskz_sub_epi16

| VPSUBW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed 16-bit integers in "b" from packed 16-bit integers in "a", and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := a[i+15:i] - b[i+15:i]
    ELSE
        dst[i+15:i] := 0
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTMB - _mm256_mask_test_epi8_mask

| VPTESTMB_MASKmskw_MASKmskw_YMMu8_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed 8-bit integers in "a" and "b", producing intermediate 8-bit values, and set
the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is non-zero.

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k1[j]
        k[j] := ((a[i+7:i] AND b[i+7:i]) != 0) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTMB - _mm256_test_epi8_mask

| VPTESTMB_MASKmskw_MASKmskw_YMMu8_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed 8-bit integers in "a" and "b", producing intermediate 8-bit values, and set
the corresponding bit in result mask "k" if the intermediate value is non-zero.

[algorithm]

FOR j := 0 to 31
    i := j*8
    k[j] := ((a[i+7:i] AND b[i+7:i]) != 0) ? 1 : 0
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTMB - _mm512_mask_test_epi8_mask

| VPTESTMB_MASKmskw_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed 8-bit integers in "a" and "b", producing intermediate 8-bit values, and set
the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is non-zero.

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k1[j]
        k[j] := ((a[i+7:i] AND b[i+7:i]) != 0) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTMB - _mm512_test_epi8_mask

| VPTESTMB_MASKmskw_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed 8-bit integers in "a" and "b", producing intermediate 8-bit values, and set
the corresponding bit in result mask "k" if the intermediate value is non-zero.

[algorithm]

FOR j := 0 to 63
    i := j*8
    k[j] := ((a[i+7:i] AND b[i+7:i]) != 0) ? 1 : 0
ENDFOR
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTMB - _mm_mask_test_epi8_mask

| VPTESTMB_MASKmskw_MASKmskw_XMMu8_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed 8-bit integers in "a" and "b", producing intermediate 8-bit values, and set
the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is non-zero.

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k1[j]
        k[j] := ((a[i+7:i] AND b[i+7:i]) != 0) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTMB - _mm_test_epi8_mask

| VPTESTMB_MASKmskw_MASKmskw_XMMu8_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed 8-bit integers in "a" and "b", producing intermediate 8-bit values, and set
the corresponding bit in result mask "k" if the intermediate value is non-zero.

[algorithm]

FOR j := 0 to 15
    i := j*8
    k[j] := ((a[i+7:i] AND b[i+7:i]) != 0) ? 1 : 0
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTMW - _mm256_mask_test_epi16_mask

| VPTESTMW_MASKmskw_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed 16-bit integers in "a" and "b", producing intermediate 16-bit values, and
set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is non-zero.

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k1[j]
        k[j] := ((a[i+15:i] AND b[i+15:i]) != 0) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTMW - _mm256_test_epi16_mask

| VPTESTMW_MASKmskw_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed 16-bit integers in "a" and "b", producing intermediate 16-bit values, and
set the corresponding bit in result mask "k" if the intermediate value is non-zero.

[algorithm]

FOR j := 0 to 15
    i := j*16
    k[j] := ((a[i+15:i] AND b[i+15:i]) != 0) ? 1 : 0
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTMW - _mm512_mask_test_epi16_mask

| VPTESTMW_MASKmskw_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed 16-bit integers in "a" and "b", producing intermediate 16-bit values, and
set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is non-zero.

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k1[j]
        k[j] := ((a[i+15:i] AND b[i+15:i]) != 0) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTMW - _mm512_test_epi16_mask

| VPTESTMW_MASKmskw_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed 16-bit integers in "a" and "b", producing intermediate 16-bit values, and
set the corresponding bit in result mask "k" if the intermediate value is non-zero.

[algorithm]

FOR j := 0 to 31
    i := j*16
    k[j] := ((a[i+15:i] AND b[i+15:i]) != 0) ? 1 : 0
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTMW - _mm_mask_test_epi16_mask

| VPTESTMW_MASKmskw_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed 16-bit integers in "a" and "b", producing intermediate 16-bit values, and
set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is non-zero.

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k1[j]
        k[j] := ((a[i+15:i] AND b[i+15:i]) != 0) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTMW - _mm_test_epi16_mask

| VPTESTMW_MASKmskw_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed 16-bit integers in "a" and "b", producing intermediate 16-bit values, and
set the corresponding bit in result mask "k" if the intermediate value is non-zero.

[algorithm]

FOR j := 0 to 7
    i := j*16
    k[j] := ((a[i+15:i] AND b[i+15:i]) != 0) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTNMB - _mm256_mask_testn_epi8_mask

| VPTESTNMB_MASKmskw_MASKmskw_YMMu8_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NAND of packed 8-bit integers in "a" and "b", producing intermediate 8-bit values, and set
the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is zero.

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k1[j]
        k[j] := ((a[i+7:i] AND b[i+7:i]) == 0) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTNMB - _mm256_testn_epi8_mask

| VPTESTNMB_MASKmskw_MASKmskw_YMMu8_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NAND of packed 8-bit integers in "a" and "b", producing intermediate 8-bit values, and set
the corresponding bit in result mask "k" if the intermediate value is zero.

[algorithm]

FOR j := 0 to 31
    i := j*8
    k[j] := ((a[i+7:i] AND b[i+7:i]) == 0) ? 1 : 0
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTNMB - _mm512_mask_testn_epi8_mask

| VPTESTNMB_MASKmskw_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NAND of packed 8-bit integers in "a" and "b", producing intermediate 8-bit values, and set
the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is zero.

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k1[j]
        k[j] := ((a[i+7:i] AND b[i+7:i]) == 0) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTNMB - _mm512_testn_epi8_mask

| VPTESTNMB_MASKmskw_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NAND of packed 8-bit integers in "a" and "b", producing intermediate 8-bit values, and set
the corresponding bit in result mask "k" if the intermediate value is zero.

[algorithm]

FOR j := 0 to 63
    i := j*8
    k[j] := ((a[i+7:i] AND b[i+7:i]) == 0) ? 1 : 0
ENDFOR
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTNMB - _mm_mask_testn_epi8_mask

| VPTESTNMB_MASKmskw_MASKmskw_XMMu8_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NAND of packed 8-bit integers in "a" and "b", producing intermediate 8-bit values, and set
the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is zero.

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k1[j]
        k[j] := ((a[i+7:i] AND b[i+7:i]) == 0) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTNMB - _mm_testn_epi8_mask

| VPTESTNMB_MASKmskw_MASKmskw_XMMu8_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NAND of packed 8-bit integers in "a" and "b", producing intermediate 8-bit values, and set
the corresponding bit in result mask "k" if the intermediate value is zero.

[algorithm]

FOR j := 0 to 15
    i := j*8
    k[j] := ((a[i+7:i] AND b[i+7:i]) == 0) ? 1 : 0
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTNMW - _mm256_mask_testn_epi16_mask

| VPTESTNMW_MASKmskw_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NAND of packed 16-bit integers in "a" and "b", producing intermediate 16-bit values, and
set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is zero.

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k1[j]
        k[j] := ((a[i+15:i] AND b[i+15:i]) == 0) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTNMW - _mm256_testn_epi16_mask

| VPTESTNMW_MASKmskw_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NAND of packed 16-bit integers in "a" and "b", producing intermediate 16-bit values, and
set the corresponding bit in result mask "k" if the intermediate value is zero.

[algorithm]

FOR j := 0 to 15
    i := j*16
    k[j] := ((a[i+15:i] AND b[i+15:i]) == 0) ? 1 : 0
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTNMW - _mm512_mask_testn_epi16_mask

| VPTESTNMW_MASKmskw_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NAND of packed 16-bit integers in "a" and "b", producing intermediate 16-bit values, and
set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is zero.

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k1[j]
        k[j] := ((a[i+15:i] AND b[i+15:i]) == 0) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTNMW - _mm512_testn_epi16_mask

| VPTESTNMW_MASKmskw_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NAND of packed 16-bit integers in "a" and "b", producing intermediate 16-bit values, and
set the corresponding bit in result mask "k" if the intermediate value is zero.

[algorithm]

FOR j := 0 to 31
    i := j*16
    k[j] := ((a[i+15:i] AND b[i+15:i]) == 0) ? 1 : 0
ENDFOR
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTNMW - _mm_mask_testn_epi16_mask

| VPTESTNMW_MASKmskw_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NAND of packed 16-bit integers in "a" and "b", producing intermediate 16-bit values, and
set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is zero.

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k1[j]
        k[j] := ((a[i+15:i] AND b[i+15:i]) == 0) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTNMW - _mm_testn_epi16_mask

| VPTESTNMW_MASKmskw_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NAND of packed 16-bit integers in "a" and "b", producing intermediate 16-bit values, and
set the corresponding bit in result mask "k" if the intermediate value is zero.

[algorithm]

FOR j := 0 to 7
    i := j*16
    k[j] := ((a[i+15:i] AND b[i+15:i]) == 0) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKHBW - _mm256_mask_unpackhi_epi8

| VPUNPCKHBW_YMMu8_MASKmskw_YMMu8_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 8-bit integers from the high half of each 128-bit lane in "a" and "b", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

DEFINE INTERLEAVE_HIGH_BYTES(src1[127:0], src2[127:0]) {
    dst[7:0] := src1[71:64] 
    dst[15:8] := src2[71:64] 
    dst[23:16] := src1[79:72] 
    dst[31:24] := src2[79:72] 
    dst[39:32] := src1[87:80] 
    dst[47:40] := src2[87:80] 
    dst[55:48] := src1[95:88] 
    dst[63:56] := src2[95:88] 
    dst[71:64] := src1[103:96] 
    dst[79:72] := src2[103:96] 
    dst[87:80] := src1[111:104] 
    dst[95:88] := src2[111:104] 
    dst[103:96] := src1[119:112] 
    dst[111:104] := src2[119:112] 
    dst[119:112] := src1[127:120] 
    dst[127:120] := src2[127:120] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_HIGH_BYTES(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_BYTES(a[255:128], b[255:128])
FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := tmp_dst[i+7:i]
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKHBW - _mm256_maskz_unpackhi_epi8

| VPUNPCKHBW_YMMu8_MASKmskw_YMMu8_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 8-bit integers from the high half of each 128-bit lane in "a" and "b", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_HIGH_BYTES(src1[127:0], src2[127:0]) {
    dst[7:0] := src1[71:64] 
    dst[15:8] := src2[71:64] 
    dst[23:16] := src1[79:72] 
    dst[31:24] := src2[79:72] 
    dst[39:32] := src1[87:80] 
    dst[47:40] := src2[87:80] 
    dst[55:48] := src1[95:88] 
    dst[63:56] := src2[95:88] 
    dst[71:64] := src1[103:96] 
    dst[79:72] := src2[103:96] 
    dst[87:80] := src1[111:104] 
    dst[95:88] := src2[111:104] 
    dst[103:96] := src1[119:112] 
    dst[111:104] := src2[119:112] 
    dst[119:112] := src1[127:120] 
    dst[127:120] := src2[127:120] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_HIGH_BYTES(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_BYTES(a[255:128], b[255:128])
FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := tmp_dst[i+7:i]
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKHBW - _mm512_mask_unpackhi_epi8

| VPUNPCKHBW_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 8-bit integers from the high half of each 128-bit lane in "a" and "b", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

DEFINE INTERLEAVE_HIGH_BYTES(src1[127:0], src2[127:0]) {
    dst[7:0] := src1[71:64] 
    dst[15:8] := src2[71:64] 
    dst[23:16] := src1[79:72] 
    dst[31:24] := src2[79:72] 
    dst[39:32] := src1[87:80] 
    dst[47:40] := src2[87:80] 
    dst[55:48] := src1[95:88] 
    dst[63:56] := src2[95:88] 
    dst[71:64] := src1[103:96] 
    dst[79:72] := src2[103:96] 
    dst[87:80] := src1[111:104] 
    dst[95:88] := src2[111:104] 
    dst[103:96] := src1[119:112] 
    dst[111:104] := src2[119:112] 
    dst[119:112] := src1[127:120] 
    dst[127:120] := src2[127:120] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_HIGH_BYTES(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_BYTES(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_HIGH_BYTES(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_HIGH_BYTES(a[511:384], b[511:384])
FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := tmp_dst[i+7:i]
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKHBW - _mm512_maskz_unpackhi_epi8

| VPUNPCKHBW_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 8-bit integers from the high half of each 128-bit lane in "a" and "b", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_HIGH_BYTES(src1[127:0], src2[127:0]) {
    dst[7:0] := src1[71:64] 
    dst[15:8] := src2[71:64] 
    dst[23:16] := src1[79:72] 
    dst[31:24] := src2[79:72] 
    dst[39:32] := src1[87:80] 
    dst[47:40] := src2[87:80] 
    dst[55:48] := src1[95:88] 
    dst[63:56] := src2[95:88] 
    dst[71:64] := src1[103:96] 
    dst[79:72] := src2[103:96] 
    dst[87:80] := src1[111:104] 
    dst[95:88] := src2[111:104] 
    dst[103:96] := src1[119:112] 
    dst[111:104] := src2[119:112] 
    dst[119:112] := src1[127:120] 
    dst[127:120] := src2[127:120] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_HIGH_BYTES(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_BYTES(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_HIGH_BYTES(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_HIGH_BYTES(a[511:384], b[511:384])
FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := tmp_dst[i+7:i]
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKHBW - _mm512_unpackhi_epi8

| VPUNPCKHBW_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 8-bit integers from the high half of each 128-bit lane in "a" and "b", and store the
results in "dst".

[algorithm]

DEFINE INTERLEAVE_HIGH_BYTES(src1[127:0], src2[127:0]) {
    dst[7:0] := src1[71:64] 
    dst[15:8] := src2[71:64] 
    dst[23:16] := src1[79:72] 
    dst[31:24] := src2[79:72] 
    dst[39:32] := src1[87:80] 
    dst[47:40] := src2[87:80] 
    dst[55:48] := src1[95:88] 
    dst[63:56] := src2[95:88] 
    dst[71:64] := src1[103:96] 
    dst[79:72] := src2[103:96] 
    dst[87:80] := src1[111:104] 
    dst[95:88] := src2[111:104] 
    dst[103:96] := src1[119:112] 
    dst[111:104] := src2[119:112] 
    dst[119:112] := src1[127:120] 
    dst[127:120] := src2[127:120] 
    RETURN dst[127:0]
}
dst[127:0] := INTERLEAVE_HIGH_BYTES(a[127:0], b[127:0])
dst[255:128] := INTERLEAVE_HIGH_BYTES(a[255:128], b[255:128])
dst[383:256] := INTERLEAVE_HIGH_BYTES(a[383:256], b[383:256])
dst[511:384] := INTERLEAVE_HIGH_BYTES(a[511:384], b[511:384])
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKHBW - _mm_mask_unpackhi_epi8

| VPUNPCKHBW_XMMu8_MASKmskw_XMMu8_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 8-bit integers from the high half of "a" and "b", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_HIGH_BYTES(src1[127:0], src2[127:0]) {
    dst[7:0] := src1[71:64] 
    dst[15:8] := src2[71:64] 
    dst[23:16] := src1[79:72] 
    dst[31:24] := src2[79:72] 
    dst[39:32] := src1[87:80] 
    dst[47:40] := src2[87:80] 
    dst[55:48] := src1[95:88] 
    dst[63:56] := src2[95:88] 
    dst[71:64] := src1[103:96] 
    dst[79:72] := src2[103:96] 
    dst[87:80] := src1[111:104] 
    dst[95:88] := src2[111:104] 
    dst[103:96] := src1[119:112] 
    dst[111:104] := src2[119:112] 
    dst[119:112] := src1[127:120] 
    dst[127:120] := src2[127:120] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_HIGH_BYTES(a[127:0], b[127:0])
FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := tmp_dst[i+7:i]
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKHBW - _mm_maskz_unpackhi_epi8

| VPUNPCKHBW_XMMu8_MASKmskw_XMMu8_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 8-bit integers from the high half of "a" and "b", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_HIGH_BYTES(src1[127:0], src2[127:0]) {
    dst[7:0] := src1[71:64] 
    dst[15:8] := src2[71:64] 
    dst[23:16] := src1[79:72] 
    dst[31:24] := src2[79:72] 
    dst[39:32] := src1[87:80] 
    dst[47:40] := src2[87:80] 
    dst[55:48] := src1[95:88] 
    dst[63:56] := src2[95:88] 
    dst[71:64] := src1[103:96] 
    dst[79:72] := src2[103:96] 
    dst[87:80] := src1[111:104] 
    dst[95:88] := src2[111:104] 
    dst[103:96] := src1[119:112] 
    dst[111:104] := src2[119:112] 
    dst[119:112] := src1[127:120] 
    dst[127:120] := src2[127:120] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_HIGH_BYTES(a[127:0], b[127:0])
FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := tmp_dst[i+7:i]
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKHWD - _mm256_mask_unpackhi_epi16

| VPUNPCKHWD_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 16-bit integers from the high half of each 128-bit lane in "a" and "b", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

DEFINE INTERLEAVE_HIGH_WORDS(src1[127:0], src2[127:0]) {
    dst[15:0] := src1[79:64]
    dst[31:16] := src2[79:64] 
    dst[47:32] := src1[95:80] 
    dst[63:48] := src2[95:80] 
    dst[79:64] := src1[111:96] 
    dst[95:80] := src2[111:96] 
    dst[111:96] := src1[127:112] 
    dst[127:112] := src2[127:112] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_HIGH_WORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_WORDS(a[255:128], b[255:128])
FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKHWD - _mm256_maskz_unpackhi_epi16

| VPUNPCKHWD_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 16-bit integers from the high half of each 128-bit lane in "a" and "b", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_HIGH_WORDS(src1[127:0], src2[127:0]) {
    dst[15:0] := src1[79:64]
    dst[31:16] := src2[79:64] 
    dst[47:32] := src1[95:80] 
    dst[63:48] := src2[95:80] 
    dst[79:64] := src1[111:96] 
    dst[95:80] := src2[111:96] 
    dst[111:96] := src1[127:112] 
    dst[127:112] := src2[127:112] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_HIGH_WORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_WORDS(a[255:128], b[255:128])
FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKHWD - _mm512_mask_unpackhi_epi16

| VPUNPCKHWD_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 16-bit integers from the high half of each 128-bit lane in "a" and "b", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

DEFINE INTERLEAVE_HIGH_WORDS(src1[127:0], src2[127:0]) {
    dst[15:0] := src1[79:64]
    dst[31:16] := src2[79:64] 
    dst[47:32] := src1[95:80] 
    dst[63:48] := src2[95:80] 
    dst[79:64] := src1[111:96] 
    dst[95:80] := src2[111:96] 
    dst[111:96] := src1[127:112] 
    dst[127:112] := src2[127:112] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_HIGH_WORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_WORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_HIGH_WORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_HIGH_WORDS(a[511:384], b[511:384])
FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKHWD - _mm512_maskz_unpackhi_epi16

| VPUNPCKHWD_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 16-bit integers from the high half of each 128-bit lane in "a" and "b", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_HIGH_WORDS(src1[127:0], src2[127:0]) {
    dst[15:0] := src1[79:64]
    dst[31:16] := src2[79:64] 
    dst[47:32] := src1[95:80] 
    dst[63:48] := src2[95:80] 
    dst[79:64] := src1[111:96] 
    dst[95:80] := src2[111:96] 
    dst[111:96] := src1[127:112] 
    dst[127:112] := src2[127:112] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_HIGH_WORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_WORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_HIGH_WORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_HIGH_WORDS(a[511:384], b[511:384])
FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKHWD - _mm512_unpackhi_epi16

| VPUNPCKHWD_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 16-bit integers from the high half of each 128-bit lane in "a" and "b", and store the
results in "dst".

[algorithm]

DEFINE INTERLEAVE_HIGH_WORDS(src1[127:0], src2[127:0]) {
    dst[15:0] := src1[79:64]
    dst[31:16] := src2[79:64] 
    dst[47:32] := src1[95:80] 
    dst[63:48] := src2[95:80] 
    dst[79:64] := src1[111:96] 
    dst[95:80] := src2[111:96] 
    dst[111:96] := src1[127:112] 
    dst[127:112] := src2[127:112] 
    RETURN dst[127:0]
}
dst[127:0] := INTERLEAVE_HIGH_WORDS(a[127:0], b[127:0])
dst[255:128] := INTERLEAVE_HIGH_WORDS(a[255:128], b[255:128])
dst[383:256] := INTERLEAVE_HIGH_WORDS(a[383:256], b[383:256])
dst[511:384] := INTERLEAVE_HIGH_WORDS(a[511:384], b[511:384])
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKHWD - _mm_mask_unpackhi_epi16

| VPUNPCKHWD_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 16-bit integers from the high half of "a" and "b", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_HIGH_WORDS(src1[127:0], src2[127:0]) {
    dst[15:0] := src1[79:64]
    dst[31:16] := src2[79:64] 
    dst[47:32] := src1[95:80] 
    dst[63:48] := src2[95:80] 
    dst[79:64] := src1[111:96] 
    dst[95:80] := src2[111:96] 
    dst[111:96] := src1[127:112] 
    dst[127:112] := src2[127:112] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_HIGH_WORDS(a[127:0], b[127:0])
FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKHWD - _mm_maskz_unpackhi_epi16

| VPUNPCKHWD_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 16-bit integers from the high half of "a" and "b", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_HIGH_WORDS(src1[127:0], src2[127:0]) {
    dst[15:0] := src1[79:64]
    dst[31:16] := src2[79:64] 
    dst[47:32] := src1[95:80] 
    dst[63:48] := src2[95:80] 
    dst[79:64] := src1[111:96] 
    dst[95:80] := src2[111:96] 
    dst[111:96] := src1[127:112] 
    dst[127:112] := src2[127:112] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_HIGH_WORDS(a[127:0], b[127:0])
FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKLBW - _mm256_mask_unpacklo_epi8

| VPUNPCKLBW_YMMu8_MASKmskw_YMMu8_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 8-bit integers from the low half of each 128-bit lane in "a" and "b", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

DEFINE INTERLEAVE_BYTES(src1[127:0], src2[127:0]) {
    dst[7:0] := src1[7:0] 
    dst[15:8] := src2[7:0] 
    dst[23:16] := src1[15:8] 
    dst[31:24] := src2[15:8] 
    dst[39:32] := src1[23:16] 
    dst[47:40] := src2[23:16] 
    dst[55:48] := src1[31:24] 
    dst[63:56] := src2[31:24] 
    dst[71:64] := src1[39:32]
    dst[79:72] := src2[39:32] 
    dst[87:80] := src1[47:40] 
    dst[95:88] := src2[47:40] 
    dst[103:96] := src1[55:48] 
    dst[111:104] := src2[55:48] 
    dst[119:112] := src1[63:56] 
    dst[127:120] := src2[63:56] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_BYTES(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_BYTES(a[255:128], b[255:128])
FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := tmp_dst[i+7:i]
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKLBW - _mm256_maskz_unpacklo_epi8

| VPUNPCKLBW_YMMu8_MASKmskw_YMMu8_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 8-bit integers from the low half of each 128-bit lane in "a" and "b", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_BYTES(src1[127:0], src2[127:0]) {
    dst[7:0] := src1[7:0] 
    dst[15:8] := src2[7:0] 
    dst[23:16] := src1[15:8] 
    dst[31:24] := src2[15:8] 
    dst[39:32] := src1[23:16] 
    dst[47:40] := src2[23:16] 
    dst[55:48] := src1[31:24] 
    dst[63:56] := src2[31:24] 
    dst[71:64] := src1[39:32]
    dst[79:72] := src2[39:32] 
    dst[87:80] := src1[47:40] 
    dst[95:88] := src2[47:40] 
    dst[103:96] := src1[55:48] 
    dst[111:104] := src2[55:48] 
    dst[119:112] := src1[63:56] 
    dst[127:120] := src2[63:56] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_BYTES(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_BYTES(a[255:128], b[255:128])
FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := tmp_dst[i+7:i]
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKLBW - _mm512_mask_unpacklo_epi8

| VPUNPCKLBW_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 8-bit integers from the low half of each 128-bit lane in "a" and "b", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

DEFINE INTERLEAVE_BYTES(src1[127:0], src2[127:0]) {
    dst[7:0] := src1[7:0] 
    dst[15:8] := src2[7:0] 
    dst[23:16] := src1[15:8] 
    dst[31:24] := src2[15:8] 
    dst[39:32] := src1[23:16] 
    dst[47:40] := src2[23:16] 
    dst[55:48] := src1[31:24] 
    dst[63:56] := src2[31:24] 
    dst[71:64] := src1[39:32]
    dst[79:72] := src2[39:32] 
    dst[87:80] := src1[47:40] 
    dst[95:88] := src2[47:40] 
    dst[103:96] := src1[55:48] 
    dst[111:104] := src2[55:48] 
    dst[119:112] := src1[63:56] 
    dst[127:120] := src2[63:56] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_BYTES(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_BYTES(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_BYTES(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_BYTES(a[511:384], b[511:384])
FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := tmp_dst[i+7:i]
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKLBW - _mm512_maskz_unpacklo_epi8

| VPUNPCKLBW_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 8-bit integers from the low half of each 128-bit lane in "a" and "b", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_BYTES(src1[127:0], src2[127:0]) {
    dst[7:0] := src1[7:0] 
    dst[15:8] := src2[7:0] 
    dst[23:16] := src1[15:8] 
    dst[31:24] := src2[15:8] 
    dst[39:32] := src1[23:16] 
    dst[47:40] := src2[23:16] 
    dst[55:48] := src1[31:24] 
    dst[63:56] := src2[31:24] 
    dst[71:64] := src1[39:32]
    dst[79:72] := src2[39:32] 
    dst[87:80] := src1[47:40] 
    dst[95:88] := src2[47:40] 
    dst[103:96] := src1[55:48] 
    dst[111:104] := src2[55:48] 
    dst[119:112] := src1[63:56] 
    dst[127:120] := src2[63:56] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_BYTES(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_BYTES(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_BYTES(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_BYTES(a[511:384], b[511:384])
FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := tmp_dst[i+7:i]
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKLBW - _mm512_unpacklo_epi8

| VPUNPCKLBW_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 8-bit integers from the low half of each 128-bit lane in "a" and "b", and store the
results in "dst".

[algorithm]

DEFINE INTERLEAVE_BYTES(src1[127:0], src2[127:0]) {
    dst[7:0] := src1[7:0] 
    dst[15:8] := src2[7:0] 
    dst[23:16] := src1[15:8] 
    dst[31:24] := src2[15:8] 
    dst[39:32] := src1[23:16] 
    dst[47:40] := src2[23:16] 
    dst[55:48] := src1[31:24] 
    dst[63:56] := src2[31:24] 
    dst[71:64] := src1[39:32]
    dst[79:72] := src2[39:32] 
    dst[87:80] := src1[47:40] 
    dst[95:88] := src2[47:40] 
    dst[103:96] := src1[55:48] 
    dst[111:104] := src2[55:48] 
    dst[119:112] := src1[63:56] 
    dst[127:120] := src2[63:56] 
    RETURN dst[127:0]
}
dst[127:0] := INTERLEAVE_BYTES(a[127:0], b[127:0])
dst[255:128] := INTERLEAVE_BYTES(a[255:128], b[255:128])
dst[383:256] := INTERLEAVE_BYTES(a[383:256], b[383:256])
dst[511:384] := INTERLEAVE_BYTES(a[511:384], b[511:384])
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKLBW - _mm_mask_unpacklo_epi8

| VPUNPCKLBW_XMMu8_MASKmskw_XMMu8_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 8-bit integers from the low half of "a" and "b", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_BYTES(src1[127:0], src2[127:0]) {
    dst[7:0] := src1[7:0] 
    dst[15:8] := src2[7:0] 
    dst[23:16] := src1[15:8] 
    dst[31:24] := src2[15:8] 
    dst[39:32] := src1[23:16] 
    dst[47:40] := src2[23:16] 
    dst[55:48] := src1[31:24] 
    dst[63:56] := src2[31:24] 
    dst[71:64] := src1[39:32]
    dst[79:72] := src2[39:32] 
    dst[87:80] := src1[47:40] 
    dst[95:88] := src2[47:40] 
    dst[103:96] := src1[55:48] 
    dst[111:104] := src2[55:48] 
    dst[119:112] := src1[63:56] 
    dst[127:120] := src2[63:56] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_BYTES(a[127:0], b[127:0])
FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := tmp_dst[i+7:i]
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKLBW - _mm_maskz_unpacklo_epi8

| VPUNPCKLBW_XMMu8_MASKmskw_XMMu8_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 8-bit integers from the low half of "a" and "b", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_BYTES(src1[127:0], src2[127:0]) {
    dst[7:0] := src1[7:0] 
    dst[15:8] := src2[7:0] 
    dst[23:16] := src1[15:8] 
    dst[31:24] := src2[15:8] 
    dst[39:32] := src1[23:16] 
    dst[47:40] := src2[23:16] 
    dst[55:48] := src1[31:24] 
    dst[63:56] := src2[31:24] 
    dst[71:64] := src1[39:32]
    dst[79:72] := src2[39:32] 
    dst[87:80] := src1[47:40] 
    dst[95:88] := src2[47:40] 
    dst[103:96] := src1[55:48] 
    dst[111:104] := src2[55:48] 
    dst[119:112] := src1[63:56] 
    dst[127:120] := src2[63:56] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_BYTES(a[127:0], b[127:0])
FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := tmp_dst[i+7:i]
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKLWD - _mm256_mask_unpacklo_epi16

| VPUNPCKLWD_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 16-bit integers from the low half of each 128-bit lane in "a" and "b", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

DEFINE INTERLEAVE_WORDS(src1[127:0], src2[127:0]) {
    dst[15:0] := src1[15:0] 
    dst[31:16] := src2[15:0] 
    dst[47:32] := src1[31:16] 
    dst[63:48] := src2[31:16] 
    dst[79:64] := src1[47:32] 
    dst[95:80] := src2[47:32] 
    dst[111:96] := src1[63:48] 
    dst[127:112] := src2[63:48] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_WORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_WORDS(a[255:128], b[255:128])
FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKLWD - _mm256_maskz_unpacklo_epi16

| VPUNPCKLWD_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 16-bit integers from the low half of each 128-bit lane in "a" and "b", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_WORDS(src1[127:0], src2[127:0]) {
    dst[15:0] := src1[15:0] 
    dst[31:16] := src2[15:0] 
    dst[47:32] := src1[31:16] 
    dst[63:48] := src2[31:16] 
    dst[79:64] := src1[47:32] 
    dst[95:80] := src2[47:32] 
    dst[111:96] := src1[63:48] 
    dst[127:112] := src2[63:48] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_WORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_WORDS(a[255:128], b[255:128])
FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKLWD - _mm512_mask_unpacklo_epi16

| VPUNPCKLWD_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 16-bit integers from the low half of each 128-bit lane in "a" and "b", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

DEFINE INTERLEAVE_WORDS(src1[127:0], src2[127:0]) {
    dst[15:0] := src1[15:0] 
    dst[31:16] := src2[15:0] 
    dst[47:32] := src1[31:16] 
    dst[63:48] := src2[31:16] 
    dst[79:64] := src1[47:32] 
    dst[95:80] := src2[47:32] 
    dst[111:96] := src1[63:48] 
    dst[127:112] := src2[63:48] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_WORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_WORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_WORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_WORDS(a[511:384], b[511:384])
FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKLWD - _mm512_maskz_unpacklo_epi16

| VPUNPCKLWD_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 16-bit integers from the low half of each 128-bit lane in "a" and "b", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_WORDS(src1[127:0], src2[127:0]) {
    dst[15:0] := src1[15:0] 
    dst[31:16] := src2[15:0] 
    dst[47:32] := src1[31:16] 
    dst[63:48] := src2[31:16] 
    dst[79:64] := src1[47:32] 
    dst[95:80] := src2[47:32] 
    dst[111:96] := src1[63:48] 
    dst[127:112] := src2[63:48] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_WORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_WORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_WORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_WORDS(a[511:384], b[511:384])
FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKLWD - _mm512_unpacklo_epi16

| VPUNPCKLWD_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 16-bit integers from the low half of each 128-bit lane in "a" and "b", and store the
results in "dst".

[algorithm]

DEFINE INTERLEAVE_WORDS(src1[127:0], src2[127:0]) {
    dst[15:0] := src1[15:0] 
    dst[31:16] := src2[15:0] 
    dst[47:32] := src1[31:16] 
    dst[63:48] := src2[31:16] 
    dst[79:64] := src1[47:32] 
    dst[95:80] := src2[47:32] 
    dst[111:96] := src1[63:48] 
    dst[127:112] := src2[63:48] 
    RETURN dst[127:0]
}
dst[127:0] := INTERLEAVE_WORDS(a[127:0], b[127:0])
dst[255:128] := INTERLEAVE_WORDS(a[255:128], b[255:128])
dst[383:256] := INTERLEAVE_WORDS(a[383:256], b[383:256])
dst[511:384] := INTERLEAVE_WORDS(a[511:384], b[511:384])
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKLWD - _mm_mask_unpacklo_epi16

| VPUNPCKLWD_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 16-bit integers from the low half of "a" and "b", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_WORDS(src1[127:0], src2[127:0]) {
    dst[15:0] := src1[15:0] 
    dst[31:16] := src2[15:0] 
    dst[47:32] := src1[31:16] 
    dst[63:48] := src2[31:16] 
    dst[79:64] := src1[47:32] 
    dst[95:80] := src2[47:32] 
    dst[111:96] := src1[63:48] 
    dst[127:112] := src2[63:48] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_WORDS(a[127:0], b[127:0])
FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKLWD - _mm_maskz_unpacklo_epi16

| VPUNPCKLWD_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 16-bit integers from the low half of "a" and "b", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_WORDS(src1[127:0], src2[127:0]) {
    dst[15:0] := src1[15:0] 
    dst[31:16] := src2[15:0] 
    dst[47:32] := src1[31:16] 
    dst[63:48] := src2[31:16] 
    dst[79:64] := src1[47:32] 
    dst[95:80] := src2[47:32] 
    dst[111:96] := src1[63:48] 
    dst[127:112] := src2[63:48] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_WORDS(a[127:0], b[127:0])
FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := tmp_dst[i+15:i]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU16 - _mm512_storeu_epi16

| VMOVDQU16_MEMu16_MASKmskw_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Store 512-bits (composed of 32 packed 16-bit integers) from "a" into memory.
		"mem_addr" does not need to be
aligned on any particular boundary.

[algorithm]

MEM[mem_addr+511:mem_addr] := a[511:0]

--------------------------------------------------------------------------------------------------------------

## VMOVDQU8 - _mm512_storeu_epi8

| VMOVDQU8_MEMu8_MASKmskw_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Store 512-bits (composed of 64 packed 8-bit integers) from "a" into memory.
		"mem_addr" does not need to be
aligned on any particular boundary.

[algorithm]

MEM[mem_addr+511:mem_addr] := a[511:0]

--------------------------------------------------------------------------------------------------------------

## VMOVDQU16 - _mm256_storeu_epi16

| VMOVDQU16_MEMu16_MASKmskw_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Store 256-bits (composed of 16 packed 16-bit integers) from "a" into memory.
		"mem_addr" does not need to be
aligned on any particular boundary.

[algorithm]

MEM[mem_addr+255:mem_addr] := a[255:0]

--------------------------------------------------------------------------------------------------------------

## VMOVDQU8 - _mm256_storeu_epi8

| VMOVDQU8_MEMu8_MASKmskw_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Store 256-bits (composed of 32 packed 8-bit integers) from "a" into memory.
		"mem_addr" does not need to be
aligned on any particular boundary.

[algorithm]

MEM[mem_addr+255:mem_addr] := a[255:0]

--------------------------------------------------------------------------------------------------------------

## VMOVDQU16 - _mm_storeu_epi16

| VMOVDQU16_MEMu16_MASKmskw_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Store 128-bits (composed of 8 packed 16-bit integers) from "a" into memory.
		"mem_addr" does not need to be
aligned on any particular boundary.

[algorithm]

MEM[mem_addr+127:mem_addr] := a[127:0]

--------------------------------------------------------------------------------------------------------------

## VMOVDQU8 - _mm_storeu_epi8

| VMOVDQU8_MEMu8_MASKmskw_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Store 128-bits (composed of 16 packed 8-bit integers) from "a" into memory.
		"mem_addr" does not need to be
aligned on any particular boundary.

[algorithm]

MEM[mem_addr+127:mem_addr] := a[127:0]

--------------------------------------------------------------------------------------------------------------

## VMOVDQU16 - _mm512_loadu_epi16

| VMOVDQU16_ZMMu16_MASKmskw_MEMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Load 512-bits (composed of 32 packed 16-bit integers) from memory into "dst".
		"mem_addr" does not need to be
aligned on any particular boundary.

[algorithm]

dst[511:0] := MEM[mem_addr+511:mem_addr]
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU8 - _mm512_loadu_epi8

| VMOVDQU8_ZMMu8_MASKmskw_MEMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Load 512-bits (composed of 64 packed 8-bit integers) from memory into "dst".
		"mem_addr" does not need to be
aligned on any particular boundary.

[algorithm]

dst[511:0] := MEM[mem_addr+511:mem_addr]
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU16 - _mm256_loadu_epi16

| VMOVDQU16_YMMu16_MASKmskw_MEMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Load 256-bits (composed of 16 packed 16-bit integers) from memory into "dst".
		"mem_addr" does not need to be
aligned on any particular boundary.

[algorithm]

dst[255:0] := MEM[mem_addr+255:mem_addr]
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU8 - _mm256_loadu_epi8

| VMOVDQU8_YMMu8_MASKmskw_MEMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Load 256-bits (composed of 32 packed 8-bit integers) from memory into "dst".
		"mem_addr" does not need to be
aligned on any particular boundary.

[algorithm]

dst[255:0] := MEM[mem_addr+255:mem_addr]
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU16 - _mm_loadu_epi16

| VMOVDQU16_XMMu16_MASKmskw_MEMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Load 128-bits (composed of 8 packed 16-bit integers) from memory into "dst".
		"mem_addr" does not need to be
aligned on any particular boundary.

[algorithm]

dst[127:0] := MEM[mem_addr+127:mem_addr]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU8 - _mm_loadu_epi8

| VMOVDQU8_XMMu8_MASKmskw_MEMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Load 128-bits (composed of 16 packed 8-bit integers) from memory into "dst".
		"mem_addr" does not need to be
aligned on any particular boundary.

[algorithm]

dst[127:0] := MEM[mem_addr+127:mem_addr]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## KADDD - _kadd_mask32

| KADDD_MASKmskw_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Add 32-bit masks in "a" and "b", and store the result in "k".

[algorithm]

k[31:0] := a[31:0] + b[31:0]
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## KADDQ - _kadd_mask64

| KADDQ_MASKmskw_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Add 64-bit masks in "a" and "b", and store the result in "k".

[algorithm]

k[63:0] := a[63:0] + b[63:0]
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## KANDD - _kand_mask32

| KANDD_MASKmskw_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of 32-bit masks "a" and "b", and store the result in "k".

[algorithm]

k[31:0] := a[31:0] AND b[31:0]
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## KANDQ - _kand_mask64

| KANDQ_MASKmskw_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of 64-bit masks "a" and "b", and store the result in "k".

[algorithm]

k[63:0] := a[63:0] AND b[63:0]
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## KANDND - _kandn_mask32

| KANDND_MASKmskw_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of 32-bit masks "a" and then AND with "b", and store the result in "k".

[algorithm]

k[31:0] := (NOT a[31:0]) AND b[31:0]
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## KANDNQ - _kandn_mask64

| KANDNQ_MASKmskw_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of 64-bit masks "a" and then AND with "b", and store the result in "k".

[algorithm]

k[63:0] := (NOT a[63:0]) AND b[63:0]
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## KNOTD - _knot_mask32

| KNOTD_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of 32-bit mask "a", and store the result in "k".

[algorithm]

k[31:0] := NOT a[31:0]
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## KNOTQ - _knot_mask64

| KNOTQ_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of 64-bit mask "a", and store the result in "k".

[algorithm]

k[63:0] := NOT a[63:0]
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## KORD - _kor_mask32

| KORD_MASKmskw_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of 32-bit masks "a" and "b", and store the result in "k".

[algorithm]

k[31:0] := a[31:0] OR b[31:0]
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## KORQ - _kor_mask64

| KORQ_MASKmskw_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of 64-bit masks "a" and "b", and store the result in "k".

[algorithm]

k[63:0] := a[63:0] OR b[63:0]
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## KXNORD - _kxnor_mask32

| KXNORD_MASKmskw_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XNOR of 32-bit masks "a" and "b", and store the result in "k".

[algorithm]

k[31:0] := NOT (a[31:0] XOR b[31:0])
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## KXNORQ - _kxnor_mask64

| KXNORQ_MASKmskw_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XNOR of 64-bit masks "a" and "b", and store the result in "k".

[algorithm]

k[63:0] := NOT (a[63:0] XOR b[63:0])
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## KXORD - _kxor_mask32

| KXORD_MASKmskw_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XOR of 32-bit masks "a" and "b", and store the result in "k".

[algorithm]

k[31:0] := a[31:0] XOR b[31:0]
k[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## KXORQ - _kxor_mask64

| KXORQ_MASKmskw_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XOR of 64-bit masks "a" and "b", and store the result in "k".

[algorithm]

k[63:0] := a[63:0] XOR b[63:0]
k[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## KSHIFTLD - _kshiftli_mask32

| KSHIFTLD_MASKmskw_MASKmskw_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift the bits of 32-bit mask "a" left by "count" while shifting in zeros, and store the least significant 32
bits of the result in "k".

[algorithm]

k[MAX:0] := 0
IF count[7:0] &lt;= 31
    k[31:0] := a[31:0] &lt;&lt; count[7:0]
FI

--------------------------------------------------------------------------------------------------------------

## KSHIFTLQ - _kshiftli_mask64

| KSHIFTLQ_MASKmskw_MASKmskw_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift the bits of 64-bit mask "a" left by "count" while shifting in zeros, and store the least significant 64
bits of the result in "k".

[algorithm]

k[MAX:0] := 0
IF count[7:0] &lt;= 63
    k[63:0] := a[63:0] &lt;&lt; count[7:0]
FI

--------------------------------------------------------------------------------------------------------------

## KSHIFTRD - _kshiftri_mask32

| KSHIFTRD_MASKmskw_MASKmskw_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift the bits of 32-bit mask "a" right by "count" while shifting in zeros, and store the least significant 32
bits of the result in "k".

[algorithm]

k[MAX:0] := 0
IF count[7:0] &lt;= 31
    k[31:0] := a[31:0] &gt;&gt; count[7:0]
FI

--------------------------------------------------------------------------------------------------------------

## KSHIFTRQ - _kshiftri_mask64

| KSHIFTRQ_MASKmskw_MASKmskw_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift the bits of 64-bit mask "a" right by "count" while shifting in zeros, and store the least significant 64
bits of the result in "k".

[algorithm]

k[MAX:0] := 0
IF count[7:0] &lt;= 63
    k[63:0] := a[63:0] &gt;&gt; count[7:0]
FI

--------------------------------------------------------------------------------------------------------------

## KMOVD - _load_mask32

| KMOVD_MASKmskw_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Load 32-bit mask from memory into "k".

[algorithm]

k[31:0] := MEM[mem_addr+31:mem_addr]

--------------------------------------------------------------------------------------------------------------

## KMOVQ - _load_mask64

| KMOVQ_MASKmskw_MEMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Load 64-bit mask from memory into "k".

[algorithm]

k[63:0] := MEM[mem_addr+63:mem_addr]

--------------------------------------------------------------------------------------------------------------

## KMOVD - _store_mask32

| KMOVD_MEMu32_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Store 32-bit mask from "a" into memory.

[algorithm]

MEM[mem_addr+31:mem_addr] := a[31:0]

--------------------------------------------------------------------------------------------------------------

## KMOVQ - _store_mask64

| KMOVQ_MEMu64_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Store 64-bit mask from "a" into memory.

[algorithm]

MEM[mem_addr+63:mem_addr] := a[63:0]

--------------------------------------------------------------------------------------------------------------

## KORTESTD - _kortest_mask32_u8

| KORTESTD_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of 32-bit masks "a" and "b". If the result is all zeros, store 1 in "dst", otherwise
store 0 in "dst". If the result is all ones, store 1 in "all_ones", otherwise store 0 in "all_ones".

[algorithm]

tmp[31:0] := a[31:0] OR b[31:0]
IF tmp[31:0] == 0x0
    dst := 1
ELSE
    dst := 0
FI
IF tmp[31:0] == 0xFFFFFFFF
    MEM[all_ones+7:all_ones] := 1
ELSE
    MEM[all_ones+7:all_ones] := 0
FI

--------------------------------------------------------------------------------------------------------------

## KORTESTD - _kortestz_mask32_u8

| KORTESTD_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of 32-bit masks "a" and "b". If the result is all zeroes, store 1 in "dst", otherwise
store 0 in "dst".

[algorithm]

tmp[31:0] := a[31:0] OR b[31:0]
IF tmp[31:0] == 0x0
    dst := 1
ELSE
    dst := 0
FI

--------------------------------------------------------------------------------------------------------------

## KORTESTD - _kortestc_mask32_u8

| KORTESTD_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of 32-bit masks "a" and "b". If the result is all ones, store 1 in "dst", otherwise
store 0 in "dst".

[algorithm]

tmp[31:0] := a[31:0] OR b[31:0]
IF tmp[31:0] == 0xFFFFFFFF
    dst := 1
ELSE
    dst := 0
FI

--------------------------------------------------------------------------------------------------------------

## KORTESTQ - _kortest_mask64_u8

| KORTESTQ_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of 64-bit masks "a" and "b". If the result is all zeros, store 1 in "dst", otherwise
store 0 in "dst". If the result is all ones, store 1 in "all_ones", otherwise store 0 in "all_ones".

[algorithm]

tmp[63:0] := a[63:0] OR b[63:0]
IF tmp[63:0] == 0x0
    dst := 1
ELSE
    dst := 0
FI
IF tmp[7:0] == 0xFFFFFFFFFFFFFFFF
    MEM[all_ones+7:all_ones] := 1
ELSE
    MEM[all_ones+7:all_ones] := 0
FI

--------------------------------------------------------------------------------------------------------------

## KORTESTQ - _kortestz_mask64_u8

| KORTESTQ_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of 64-bit masks "a" and "b". If the result is all zeroes, store 1 in "dst", otherwise
store 0 in "dst".

[algorithm]

tmp[63:0] := a[63:0] OR b[63:0]
IF tmp[63:0] == 0x0
    dst := 1
ELSE
    dst := 0
FI

--------------------------------------------------------------------------------------------------------------

## KORTESTQ - _kortestc_mask64_u8

| KORTESTQ_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of 64-bit masks "a" and "b". If the result is all ones, store 1 in "dst", otherwise
store 0 in "dst".

[algorithm]

tmp[63:0] := a[63:0] OR b[63:0]
IF tmp[63:0] == 0xFFFFFFFFFFFFFFFF
    dst := 1
ELSE
    dst := 0
FI

--------------------------------------------------------------------------------------------------------------

## KTESTD - _ktest_mask32_u8

| KTESTD_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of 32-bit masks "a" and "b", and if the result is all zeros, store 1 in "dst",
otherwise store 0 in "dst". Compute the bitwise NOT of "a" and then AND with "b", if the result is all zeros,
store 1 in "and_not", otherwise store 0 in "and_not".

[algorithm]

tmp1[31:0] := a[31:0] AND b[31:0]
IF tmp1[31:0] == 0x0
    dst := 1
ELSE
    dst := 0
FI
tmp2[31:0] := (NOT a[31:0]) AND b[31:0]
IF tmp2[31:0] == 0x0
    MEM[and_not+7:and_not] := 1
ELSE
    MEM[and_not+7:and_not] := 0
FI

--------------------------------------------------------------------------------------------------------------

## KTESTD - _ktestz_mask32_u8

| KTESTD_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of 32-bit masks "a" and "b", and if the result is all zeros, store 1 in "dst",
otherwise store 0 in "dst".

[algorithm]

tmp[31:0] := a[31:0] AND b[31:0]
IF tmp[31:0] == 0x0
    dst := 1
ELSE
    dst := 0
FI

--------------------------------------------------------------------------------------------------------------

## KTESTD - _ktestc_mask32_u8

| KTESTD_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of 32-bit mask "a" and then AND with "b", if the result is all zeroes, store 1 in
"dst", otherwise store 0 in "dst".

[algorithm]

tmp[31:0] := (NOT a[31:0]) AND b[31:0]
IF tmp[31:0] == 0x0
    dst := 1
ELSE
    dst := 0
FI

--------------------------------------------------------------------------------------------------------------

## KTESTQ - _ktest_mask64_u8

| KTESTQ_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of 64-bit masks "a" and "b", and if the result is all zeros, store 1 in "dst",
otherwise store 0 in "dst". Compute the bitwise NOT of "a" and then AND with "b", if the result is all zeros,
store 1 in "and_not", otherwise store 0 in "and_not".

[algorithm]

tmp1[63:0] := a[63:0] AND b[63:0]
IF tmp1[63:0] == 0x0
    dst := 1
ELSE
    dst := 0
FI
tmp2[63:0] := (NOT a[63:0]) AND b[63:0]
IF tmp2[63:0] == 0x0
    MEM[and_not+7:and_not] := 1
ELSE
    MEM[and_not+7:and_not] := 0
FI

--------------------------------------------------------------------------------------------------------------

## KTESTQ - _ktestz_mask64_u8

| KTESTQ_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of 64-bit masks "a" and "b", and if the result is all zeros, store 1 in "dst",
otherwise store 0 in "dst".

[algorithm]

tmp[63:0] := a[63:0] AND b[63:0]
IF tmp[63:0] == 0x0
    dst := 1
ELSE
    dst := 0
FI

--------------------------------------------------------------------------------------------------------------

## KTESTQ - _ktestc_mask64_u8

| KTESTQ_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of 64-bit mask "a" and then AND with "b", if the result is all zeroes, store 1 in
"dst", otherwise store 0 in "dst".

[algorithm]

tmp[63:0] := (NOT a[63:0]) AND b[63:0]
IF tmp[63:0] == 0x0
    dst := 1
ELSE
    dst := 0
FI

--------------------------------------------------------------------------------------------------------------

## KMOVD - _cvtmask32_u32

| KMOVD_GPR32u32_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Convert 32-bit mask "a" into an integer value, and store the result in "dst".

[algorithm]

dst := ZeroExtend32(a[31:0])

--------------------------------------------------------------------------------------------------------------

## KMOVQ - _cvtmask64_u64

| KMOVQ_GPR64u64_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Convert 64-bit mask "a" into an integer value, and store the result in "dst".

[algorithm]

dst := ZeroExtend64(a[63:0])

--------------------------------------------------------------------------------------------------------------

## KMOVD - _cvtu32_mask32

| KMOVD_MASKmskw_GPR32u32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert integer value "a" into an 32-bit mask, and store the result in "k".

[algorithm]

k := ZeroExtend32(a[31:0])

--------------------------------------------------------------------------------------------------------------

## KMOVQ - _cvtu64_mask64

| KMOVQ_MASKmskw_GPR64u64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert integer value "a" into an 64-bit mask, and store the result in "k".

[algorithm]

k := ZeroExtend64(a[63:0])

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTMB2Q - _mm256_broadcastmb_epi64

| VPBROADCASTMB2Q_YMMu64_MASKu64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low 8-bits from input mask "k" to all 64-bit elements of "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    dst[i+63:i] := ZeroExtend64(k[7:0])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTMB2Q - _mm_broadcastmb_epi64

| VPBROADCASTMB2Q_XMMu64_MASKu64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low 8-bits from input mask "k" to all 64-bit elements of "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    dst[i+63:i] := ZeroExtend64(k[7:0])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTMW2D - _mm256_broadcastmw_epi32

| VPBROADCASTMW2D_YMMu32_MASKu32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low 16-bits from input mask "k" to all 32-bit elements of "dst".

[algorithm]

FOR j := 0 to 7
    i := j*32
    dst[i+31:i] := ZeroExtend32(k[15:0])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTMW2D - _mm_broadcastmw_epi32

| VPBROADCASTMW2D_XMMu32_MASKu32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low 16-bits from input mask "k" to all 32-bit elements of "dst".

[algorithm]

FOR j := 0 to 3
    i := j*32
    dst[i+31:i] := ZeroExtend32(k[15:0])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPCONFLICTD - _mm256_conflict_epi32

| VPCONFLICTD_YMMu32_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Test each 32-bit element of "a" for equality with all other elements in "a" closer to the least significant
bit. Each element's comparison forms a zero extended bit vector in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*32
    FOR k := 0 to j-1
        m := k*32
        dst[i+k] := (a[i+31:i] == a[m+31:m]) ? 1 : 0
    ENDFOR
    dst[i+31:i+j] := 0
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPCONFLICTD - _mm256_mask_conflict_epi32

| VPCONFLICTD_YMMu32_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Test each 32-bit element of "a" for equality with all other elements in "a" closer to the least significant
bit using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). Each
element's comparison forms a zero extended bit vector in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        FOR l := 0 to j-1
            m := l*32
            dst[i+l] := (a[i+31:i] == a[m+31:m]) ? 1 : 0
        ENDFOR
        dst[i+31:i+j] := 0
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPCONFLICTD - _mm256_maskz_conflict_epi32

| VPCONFLICTD_YMMu32_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Test each 32-bit element of "a" for equality with all other elements in "a" closer to the least significant
bit using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). Each element's
comparison forms a zero extended bit vector in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        FOR l := 0 to j-1
            m := l*32
            dst[i+l] := (a[i+31:i] == a[m+31:m]) ? 1 : 0
        ENDFOR
        dst[i+31:i+j] := 0
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPCONFLICTD - _mm_conflict_epi32

| VPCONFLICTD_XMMu32_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Test each 32-bit element of "a" for equality with all other elements in "a" closer to the least significant
bit. Each element's comparison forms a zero extended bit vector in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*32
    FOR k := 0 to j-1
        m := k*32
        dst[i+k] := (a[i+31:i] == a[m+31:m]) ? 1 : 0
    ENDFOR
    dst[i+31:i+j] := 0
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPCONFLICTD - _mm_mask_conflict_epi32

| VPCONFLICTD_XMMu32_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Test each 32-bit element of "a" for equality with all other elements in "a" closer to the least significant
bit using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). Each
element's comparison forms a zero extended bit vector in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        FOR l := 0 to j-1
            m := l*32
            dst[i+l] := (a[i+31:i] == a[m+31:m]) ? 1 : 0
        ENDFOR
        dst[i+31:i+j] := 0
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPCONFLICTD - _mm_maskz_conflict_epi32

| VPCONFLICTD_XMMu32_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Test each 32-bit element of "a" for equality with all other elements in "a" closer to the least significant
bit using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). Each element's
comparison forms a zero extended bit vector in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        FOR l := 0 to j-1
            m := l*32
            dst[i+l] := (a[i+31:i] == a[m+31:m]) ? 1 : 0
        ENDFOR
        dst[i+31:i+j] := 0
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPCONFLICTQ - _mm256_conflict_epi64

| VPCONFLICTQ_YMMu64_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Test each 64-bit element of "a" for equality with all other elements in "a" closer to the least significant
bit. Each element's comparison forms a zero extended bit vector in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    FOR k := 0 to j-1
        m := k*64
        dst[i+k] := (a[i+63:i] == a[m+63:m]) ? 1 : 0
    ENDFOR
    dst[i+63:i+j] := 0
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPCONFLICTQ - _mm256_mask_conflict_epi64

| VPCONFLICTQ_YMMu64_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Test each 64-bit element of "a" for equality with all other elements in "a" closer to the least significant
bit using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). Each
element's comparison forms a zero extended bit vector in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        FOR l := 0 to j-1
            m := l*64
            dst[i+l] := (a[i+63:i] == a[m+63:m]) ? 1 : 0
        ENDFOR
        dst[i+63:i+j] := 0
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPCONFLICTQ - _mm256_maskz_conflict_epi64

| VPCONFLICTQ_YMMu64_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Test each 64-bit element of "a" for equality with all other elements in "a" closer to the least significant
bit using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). Each element's
comparison forms a zero extended bit vector in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        FOR l := 0 to j-1
            m := l*64
            dst[i+l] := (a[i+63:i] == a[m+63:m]) ? 1 : 0
        ENDFOR
        dst[i+63:i+j] := 0
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPCONFLICTQ - _mm_conflict_epi64

| VPCONFLICTQ_XMMu64_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Test each 64-bit element of "a" for equality with all other elements in "a" closer to the least significant
bit. Each element's comparison forms a zero extended bit vector in "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    FOR k := 0 to j-1
        m := k*64
        dst[i+k] := (a[i+63:i] == a[m+63:m]) ? 1 : 0
    ENDFOR
    dst[i+63:i+j] := 0
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPCONFLICTQ - _mm_mask_conflict_epi64

| VPCONFLICTQ_XMMu64_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Test each 64-bit element of "a" for equality with all other elements in "a" closer to the least significant
bit using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). Each
element's comparison forms a zero extended bit vector in "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        FOR l := 0 to j-1
            m := l*64
            dst[i+l] := (a[i+63:i] == a[m+63:m]) ? 1 : 0
        ENDFOR
        dst[i+63:i+j] := 0
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPCONFLICTQ - _mm_maskz_conflict_epi64

| VPCONFLICTQ_XMMu64_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Test each 64-bit element of "a" for equality with all other elements in "a" closer to the least significant
bit using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). Each element's
comparison forms a zero extended bit vector in "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        FOR l := 0 to j-1
            m := l*64
            dst[i+l] := (a[i+63:i] == a[m+63:m]) ? 1 : 0
        ENDFOR
        dst[i+63:i+j] := 0
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPLZCNTD - _mm256_lzcnt_epi32

| VPLZCNTD_YMMu32_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Counts the number of leading zero bits in each packed 32-bit integer in "a", and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*32
    tmp := 31
    dst[i+31:i] := 0
    DO WHILE (tmp &gt;= 0 AND a[i+tmp] == 0)
        tmp := tmp - 1
        dst[i+31:i] := dst[i+31:i] + 1
    OD
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPLZCNTD - _mm256_mask_lzcnt_epi32

| VPLZCNTD_YMMu32_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Counts the number of leading zero bits in each packed 32-bit integer in "a", and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        tmp := 31
        dst[i+31:i] := 0
        DO WHILE (tmp &gt;= 0 AND a[i+tmp] == 0)
            tmp := tmp - 1
            dst[i+31:i] := dst[i+31:i] + 1
        OD
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPLZCNTD - _mm256_maskz_lzcnt_epi32

| VPLZCNTD_YMMu32_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Counts the number of leading zero bits in each packed 32-bit integer in "a", and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        tmp := 31
        dst[i+31:i] := 0
        DO WHILE (tmp &gt;= 0 AND a[i+tmp] == 0)
            tmp := tmp - 1
            dst[i+31:i] := dst[i+31:i] + 1
        OD
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPLZCNTD - _mm_lzcnt_epi32

| VPLZCNTD_XMMu32_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Counts the number of leading zero bits in each packed 32-bit integer in "a", and store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*32
    tmp := 31
    dst[i+31:i] := 0
    DO WHILE (tmp &gt;= 0 AND a[i+tmp] == 0)
        tmp := tmp - 1
        dst[i+31:i] := dst[i+31:i] + 1
    OD
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPLZCNTD - _mm_mask_lzcnt_epi32

| VPLZCNTD_XMMu32_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Counts the number of leading zero bits in each packed 32-bit integer in "a", and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        tmp := 31
        dst[i+31:i] := 0
        DO WHILE (tmp &gt;= 0 AND a[i+tmp] == 0)
            tmp := tmp - 1
            dst[i+31:i] := dst[i+31:i] + 1
        OD
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPLZCNTD - _mm_maskz_lzcnt_epi32

| VPLZCNTD_XMMu32_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Counts the number of leading zero bits in each packed 32-bit integer in "a", and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        tmp := 31
        dst[i+31:i] := 0
        DO WHILE (tmp &gt;= 0 AND a[i+tmp] == 0)
            tmp := tmp - 1
            dst[i+31:i] := dst[i+31:i] + 1
        OD
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPLZCNTQ - _mm256_lzcnt_epi64

| VPLZCNTQ_YMMu64_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Counts the number of leading zero bits in each packed 64-bit integer in "a", and store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    tmp := 63
    dst[i+63:i] := 0
    DO WHILE (tmp &gt;= 0 AND a[i+tmp] == 0)
        tmp := tmp - 1
        dst[i+63:i] := dst[i+63:i] + 1
    OD
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPLZCNTQ - _mm256_mask_lzcnt_epi64

| VPLZCNTQ_YMMu64_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Counts the number of leading zero bits in each packed 64-bit integer in "a", and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        tmp := 63
        dst[i+63:i] := 0
        DO WHILE (tmp &gt;= 0 AND a[i+tmp] == 0)
            tmp := tmp - 1
            dst[i+63:i] := dst[i+63:i] + 1
        OD
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPLZCNTQ - _mm256_maskz_lzcnt_epi64

| VPLZCNTQ_YMMu64_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Counts the number of leading zero bits in each packed 64-bit integer in "a", and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        tmp := 63
        dst[i+63:i] := 0
        DO WHILE (tmp &gt;= 0 AND a[i+tmp] == 0)
            tmp := tmp - 1
            dst[i+63:i] := dst[i+63:i] + 1
        OD
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPLZCNTQ - _mm_lzcnt_epi64

| VPLZCNTQ_XMMu64_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Counts the number of leading zero bits in each packed 64-bit integer in "a", and store the results in "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    tmp := 63
    dst[i+63:i] := 0
    DO WHILE (tmp &gt;= 0 AND a[i+tmp] == 0)
        tmp := tmp - 1
        dst[i+63:i] := dst[i+63:i] + 1
    OD
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPLZCNTQ - _mm_mask_lzcnt_epi64

| VPLZCNTQ_XMMu64_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Counts the number of leading zero bits in each packed 64-bit integer in "a", and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        tmp := 63
        dst[i+63:i] := 0
        DO WHILE (tmp &gt;= 0 AND a[i+tmp] == 0)
            tmp := tmp - 1
            dst[i+63:i] := dst[i+63:i] + 1
        OD
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPLZCNTQ - _mm_maskz_lzcnt_epi64

| VPLZCNTQ_XMMu64_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Counts the number of leading zero bits in each packed 64-bit integer in "a", and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        tmp := 63
        dst[i+63:i] := 0
        DO WHILE (tmp &gt;= 0 AND a[i+tmp] == 0)
            tmp := tmp - 1
            dst[i+63:i] := dst[i+63:i] + 1
        OD
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTMB2Q - _mm512_broadcastmb_epi64

| VPBROADCASTMB2Q_ZMMu64_MASKu64_AVX512CD

--------------------------------------------------------------------------------------------------------------
Broadcast the low 8-bits from input mask "k" to all 64-bit elements of "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := ZeroExtend64(k[7:0])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTMW2D - _mm512_broadcastmw_epi32

| VPBROADCASTMW2D_ZMMu32_MASKu32_AVX512CD

--------------------------------------------------------------------------------------------------------------
Broadcast the low 16-bits from input mask "k" to all 32-bit elements of "dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := ZeroExtend32(k[15:0])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPCONFLICTD - _mm512_conflict_epi32

| VPCONFLICTD_ZMMu32_MASKmskw_ZMMu32_AVX512CD

--------------------------------------------------------------------------------------------------------------
Test each 32-bit element of "a" for equality with all other elements in "a" closer to the least significant
bit. Each element's comparison forms a zero extended bit vector in "dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    FOR k := 0 to j-1
        m := k*32
        dst[i+k] := (a[i+31:i] == a[m+31:m]) ? 1 : 0
    ENDFOR
    dst[i+31:i+j] := 0
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPCONFLICTD - _mm512_mask_conflict_epi32

| VPCONFLICTD_ZMMu32_MASKmskw_ZMMu32_AVX512CD

--------------------------------------------------------------------------------------------------------------
Test each 32-bit element of "a" for equality with all other elements in "a" closer to the least significant
bit using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). Each
element's comparison forms a zero extended bit vector in "dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        FOR l := 0 to j-1
            m := l*32
            dst[i+l] := (a[i+31:i] == a[m+31:m]) ? 1 : 0
        ENDFOR
        dst[i+31:i+j] := 0
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPCONFLICTD - _mm512_maskz_conflict_epi32

| VPCONFLICTD_ZMMu32_MASKmskw_ZMMu32_AVX512CD

--------------------------------------------------------------------------------------------------------------
Test each 32-bit element of "a" for equality with all other elements in "a" closer to the least significant
bit using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). Each element's
comparison forms a zero extended bit vector in "dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        FOR l := 0 to j-1
            m := l*32
            dst[i+l] := (a[i+31:i] == a[m+31:m]) ? 1 : 0
        ENDFOR
        dst[i+31:i+j] := 0
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPCONFLICTQ - _mm512_conflict_epi64

| VPCONFLICTQ_ZMMu64_MASKmskw_ZMMu64_AVX512CD

--------------------------------------------------------------------------------------------------------------
Test each 64-bit element of "a" for equality with all other elements in "a" closer to the least significant
bit. Each element's comparison forms a zero extended bit vector in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    FOR k := 0 to j-1
        m := k*64
        dst[i+k] := (a[i+63:i] == a[m+63:m]) ? 1 : 0
    ENDFOR
    dst[i+63:i+j] := 0
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPCONFLICTQ - _mm512_mask_conflict_epi64

| VPCONFLICTQ_ZMMu64_MASKmskw_ZMMu64_AVX512CD

--------------------------------------------------------------------------------------------------------------
Test each 64-bit element of "a" for equality with all other elements in "a" closer to the least significant
bit using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). Each
element's comparison forms a zero extended bit vector in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        FOR l := 0 to j-1
            m := l*64
            dst[i+l] := (a[i+63:i] == a[m+63:m]) ? 1 : 0
        ENDFOR
        dst[i+63:i+j] := 0
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPCONFLICTQ - _mm512_maskz_conflict_epi64

| VPCONFLICTQ_ZMMu64_MASKmskw_ZMMu64_AVX512CD

--------------------------------------------------------------------------------------------------------------
Test each 64-bit element of "a" for equality with all other elements in "a" closer to the least significant
bit using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). Each element's
comparison forms a zero extended bit vector in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        FOR l := 0 to j-1
            m := l*64
            dst[i+l] := (a[i+63:i] == a[m+63:m]) ? 1 : 0
        ENDFOR
        dst[i+63:i+j] := 0
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPLZCNTD - _mm512_lzcnt_epi32

| VPLZCNTD_ZMMu32_MASKmskw_ZMMu32_AVX512CD

--------------------------------------------------------------------------------------------------------------
Counts the number of leading zero bits in each packed 32-bit integer in "a", and store the results in "dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    tmp := 31
    dst[i+31:i] := 0
    DO WHILE (tmp &gt;= 0 AND a[i+tmp] == 0)
        tmp := tmp - 1
        dst[i+31:i] := dst[i+31:i] + 1
    OD
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPLZCNTD - _mm512_mask_lzcnt_epi32

| VPLZCNTD_ZMMu32_MASKmskw_ZMMu32_AVX512CD

--------------------------------------------------------------------------------------------------------------
Counts the number of leading zero bits in each packed 32-bit integer in "a", and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        tmp := 31
        dst[i+31:i] := 0
        DO WHILE (tmp &gt;= 0 AND a[i+tmp] == 0)
            tmp := tmp - 1
            dst[i+31:i] := dst[i+31:i] + 1
        OD
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPLZCNTD - _mm512_maskz_lzcnt_epi32

| VPLZCNTD_ZMMu32_MASKmskw_ZMMu32_AVX512CD

--------------------------------------------------------------------------------------------------------------
Counts the number of leading zero bits in each packed 32-bit integer in "a", and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        tmp := 31
        dst[i+31:i] := 0
        DO WHILE (tmp &gt;= 0 AND a[i+tmp] == 0)
            tmp := tmp - 1
            dst[i+31:i] := dst[i+31:i] + 1
        OD
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPLZCNTQ - _mm512_lzcnt_epi64

| VPLZCNTQ_ZMMu64_MASKmskw_ZMMu64_AVX512CD

--------------------------------------------------------------------------------------------------------------
Counts the number of leading zero bits in each packed 64-bit integer in "a", and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    tmp := 63
    dst[i+63:i] := 0
    DO WHILE (tmp &gt;= 0 AND a[i+tmp] == 0)
        tmp := tmp - 1
        dst[i+63:i] := dst[i+63:i] + 1
    OD
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPLZCNTQ - _mm512_mask_lzcnt_epi64

| VPLZCNTQ_ZMMu64_MASKmskw_ZMMu64_AVX512CD

--------------------------------------------------------------------------------------------------------------
Counts the number of leading zero bits in each packed 64-bit integer in "a", and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        tmp := 63
        dst[i+63:i] := 0
        DO WHILE (tmp &gt;= 0 AND a[i+tmp] == 0)
            tmp := tmp - 1
            dst[i+63:i] := dst[i+63:i] + 1
        OD
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPLZCNTQ - _mm512_maskz_lzcnt_epi64

| VPLZCNTQ_ZMMu64_MASKmskw_ZMMu64_AVX512CD

--------------------------------------------------------------------------------------------------------------
Counts the number of leading zero bits in each packed 64-bit integer in "a", and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        tmp := 63
        dst[i+63:i] := 0
        DO WHILE (tmp &gt;= 0 AND a[i+tmp] == 0)
            tmp := tmp - 1
            dst[i+63:i] := dst[i+63:i] + 1
        OD
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VANDNPD - _mm256_mask_andnot_pd

| VANDNPD_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of packed double-precision (64-bit) floating-point elements in "a" and then AND with
"b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding
mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VANDNPD - _mm256_maskz_andnot_pd

| VANDNPD_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of packed double-precision (64-bit) floating-point elements in "a" and then AND with
"b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VANDNPD - _mm512_andnot_pd

| VANDNPD_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of packed double-precision (64-bit) floating-point elements in "a" and then AND with
"b", and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VANDNPD - _mm512_mask_andnot_pd

| VANDNPD_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of packed double-precision (64-bit) floating-point elements in "a" and then AND with
"b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding
mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VANDNPD - _mm512_maskz_andnot_pd

| VANDNPD_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of packed double-precision (64-bit) floating-point elements in "a" and then AND with
"b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VANDNPD - _mm_mask_andnot_pd

| VANDNPD_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of packed double-precision (64-bit) floating-point elements in "a" and then AND with
"b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding
mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VANDNPD - _mm_maskz_andnot_pd

| VANDNPD_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of packed double-precision (64-bit) floating-point elements in "a" and then AND with
"b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VANDNPS - _mm256_mask_andnot_ps

| VANDNPS_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of packed single-precision (32-bit) floating-point elements in "a" and then AND with
"b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding
mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VANDNPS - _mm256_maskz_andnot_ps

| VANDNPS_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of packed single-precision (32-bit) floating-point elements in "a" and then AND with
"b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VANDNPS - _mm512_andnot_ps

| VANDNPS_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of packed single-precision (32-bit) floating-point elements in "a" and then AND with
"b", and store the results in "dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VANDNPS - _mm512_mask_andnot_ps

| VANDNPS_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of packed single-precision (32-bit) floating-point elements in "a" and then AND with
"b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding
mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VANDNPS - _mm512_maskz_andnot_ps

| VANDNPS_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of packed single-precision (32-bit) floating-point elements in "a" and then AND with
"b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VANDNPS - _mm_mask_andnot_ps

| VANDNPS_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of packed single-precision (32-bit) floating-point elements in "a" and then AND with
"b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding
mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VANDNPS - _mm_maskz_andnot_ps

| VANDNPS_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of packed single-precision (32-bit) floating-point elements in "a" and then AND with
"b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VANDPD - _mm256_mask_and_pd

| VANDPD_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed double-precision (64-bit) floating-point elements in "a" and "b", and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := (a[i+63:i] AND b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VANDPD - _mm256_maskz_and_pd

| VANDPD_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed double-precision (64-bit) floating-point elements in "a" and "b", and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := (a[i+63:i] AND b[i+63:i])
    ELSE
        dst[i+63:i] := 0 
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VANDPD - _mm512_and_pd

| VANDPD_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed double-precision (64-bit) floating-point elements in "a" and "b", and store
the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := (a[i+63:i] AND b[i+63:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VANDPD - _mm512_mask_and_pd

| VANDPD_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed double-precision (64-bit) floating-point elements in "a" and "b", and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := (a[i+63:i] AND b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VANDPD - _mm512_maskz_and_pd

| VANDPD_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed double-precision (64-bit) floating-point elements in "a" and "b", and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := (a[i+63:i] AND b[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VANDPD - _mm_mask_and_pd

| VANDPD_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed double-precision (64-bit) floating-point elements in "a" and "b", and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := (a[i+63:i] AND b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VANDPD - _mm_maskz_and_pd

| VANDPD_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed double-precision (64-bit) floating-point elements in "a" and "b", and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := (a[i+63:i] AND b[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VANDPS - _mm256_mask_and_ps

| VANDPS_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed single-precision (32-bit) floating-point elements in "a" and "b", and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := (a[i+31:i] AND b[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VANDPS - _mm256_maskz_and_ps

| VANDPS_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed single-precision (32-bit) floating-point elements in "a" and "b", and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := (a[i+31:i] AND b[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VANDPS - _mm512_and_ps

| VANDPS_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed single-precision (32-bit) floating-point elements in "a" and "b", and store
the results in "dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := (a[i+31:i] AND b[i+31:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VANDPS - _mm512_mask_and_ps

| VANDPS_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed single-precision (32-bit) floating-point elements in "a" and "b", and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := (a[i+31:i] AND b[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VANDPS - _mm512_maskz_and_ps

| VANDPS_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed single-precision (32-bit) floating-point elements in "a" and "b", and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := (a[i+31:i] AND b[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VANDPS - _mm_mask_and_ps

| VANDPS_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed single-precision (32-bit) floating-point elements in "a" and "b", and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := (a[i+31:i] AND b[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VANDPS - _mm_maskz_and_ps

| VANDPS_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed single-precision (32-bit) floating-point elements in "a" and "b", and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := (a[i+31:i] AND b[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTF32X2 - _mm256_broadcast_f32x2

| VBROADCASTF32X2_YMMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the lower 2 packed single-precision (32-bit) floating-point elements from "a" to all elements of
"dst".

[algorithm]

FOR j := 0 to 7
    i := j*32
    n := (j % 2)*32
    dst[i+31:i] := a[n+31:n]
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTF32X2 - _mm256_mask_broadcast_f32x2

| VBROADCASTF32X2_YMMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the lower 2 packed single-precision (32-bit) floating-point elements from "a" to all elements of
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    n := (j % 2)*32
    IF k[j]
        dst[i+31:i] := a[n+31:n]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTF32X2 - _mm256_maskz_broadcast_f32x2

| VBROADCASTF32X2_YMMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the lower 2 packed single-precision (32-bit) floating-point elements from "a" to all elements of
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    n := (j % 2)*32
    IF k[j]
        dst[i+31:i] := a[n+31:n]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTF32X2 - _mm512_broadcast_f32x2

| VBROADCASTF32X2_ZMMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the lower 2 packed single-precision (32-bit) floating-point elements from "a" to all elements of
"dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    n := (j % 2)*32
    dst[i+31:i] := a[n+31:n]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTF32X2 - _mm512_mask_broadcast_f32x2

| VBROADCASTF32X2_ZMMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the lower 2 packed single-precision (32-bit) floating-point elements from "a" to all elements of
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    n := (j % 2)*32
    IF k[j]
        dst[i+31:i] := a[n+31:n]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTF32X2 - _mm512_maskz_broadcast_f32x2

| VBROADCASTF32X2_ZMMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the lower 2 packed single-precision (32-bit) floating-point elements from "a" to all elements of
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    n := (j % 2)*32
    IF k[j]
        dst[i+31:i] := a[n+31:n]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTF32X8 - _mm512_broadcast_f32x8

| VBROADCASTF32X8_ZMMf32_MASKmskw_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the 8 packed single-precision (32-bit) floating-point elements from "a" to all elements of "dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    n := (j % 8)*32
    dst[i+31:i] := a[n+31:n]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTF32X8 - _mm512_mask_broadcast_f32x8

| VBROADCASTF32X8_ZMMf32_MASKmskw_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the 8 packed single-precision (32-bit) floating-point elements from "a" to all elements of "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    n := (j % 8)*32
    IF k[j]
        dst[i+31:i] := a[n+31:n]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTF32X8 - _mm512_maskz_broadcast_f32x8

| VBROADCASTF32X8_ZMMf32_MASKmskw_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the 8 packed single-precision (32-bit) floating-point elements from "a" to all elements of "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    n := (j % 8)*32
    IF k[j]
        dst[i+31:i] := a[n+31:n]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTF64X2 - _mm256_broadcast_f64x2

| VBROADCASTF64X2_YMMf64_MASKmskw_MEMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the 2 packed double-precision (64-bit) floating-point elements from "a" to all elements of "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    n := (j % 2)*64
    dst[i+63:i] := a[n+63:n]
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTF64X2 - _mm256_mask_broadcast_f64x2

| VBROADCASTF64X2_YMMf64_MASKmskw_MEMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the 2 packed double-precision (64-bit) floating-point elements from "a" to all elements of "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    n := (j % 2)*64
    IF k[j]
        dst[i+63:i] := a[n+63:n]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTF64X2 - _mm256_maskz_broadcast_f64x2

| VBROADCASTF64X2_YMMf64_MASKmskw_MEMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the 2 packed double-precision (64-bit) floating-point elements from "a" to all elements of "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    n := (j % 2)*64
    IF k[j]
        dst[i+63:i] := a[n+63:n]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTF64X2 - _mm512_broadcast_f64x2

| VBROADCASTF64X2_ZMMf64_MASKmskw_MEMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the 2 packed double-precision (64-bit) floating-point elements from "a" to all elements of "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    n := (j % 2)*64
    dst[i+63:i] := a[n+63:n]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTF64X2 - _mm512_mask_broadcast_f64x2

| VBROADCASTF64X2_ZMMf64_MASKmskw_MEMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the 2 packed double-precision (64-bit) floating-point elements from "a" to all elements of "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    n := (j % 2)*64
    IF k[j]
        dst[i+63:i] := a[n+63:n]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTF64X2 - _mm512_maskz_broadcast_f64x2

| VBROADCASTF64X2_ZMMf64_MASKmskw_MEMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the 2 packed double-precision (64-bit) floating-point elements from "a" to all elements of "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    n := (j % 2)*64
    IF k[j]
        dst[i+63:i] := a[n+63:n]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTI32X2 - _mm256_broadcast_i32x2

| VBROADCASTI32X2_YMMu32_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the lower 2 packed 32-bit integers from "a" to all elements of "dst.

[algorithm]

FOR j := 0 to 7
    i := j*32
    n := (j % 2)*32
    dst[i+31:i] := a[n+31:n]
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTI32X2 - _mm256_mask_broadcast_i32x2

| VBROADCASTI32X2_YMMu32_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the lower 2 packed 32-bit integers from "a" to all elements of "dst" using writemask "k" (elements
are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    n := (j % 2)*32
    IF k[j]
        dst[i+31:i] := a[n+31:n]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTI32X2 - _mm256_maskz_broadcast_i32x2

| VBROADCASTI32X2_YMMu32_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the lower 2 packed 32-bit integers from "a" to all elements of "dst" using zeromask "k" (elements
are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    n := (j % 2)*32
    IF k[j]
        dst[i+31:i] := a[n+31:n]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTI32X2 - _mm512_broadcast_i32x2

| VBROADCASTI32X2_ZMMu32_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the lower 2 packed 32-bit integers from "a" to all elements of "dst.

[algorithm]

FOR j := 0 to 15
    i := j*32
    n := (j % 2)*32
    dst[i+31:i] := a[n+31:n]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTI32X2 - _mm512_mask_broadcast_i32x2

| VBROADCASTI32X2_ZMMu32_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the lower 2 packed 32-bit integers from "a" to all elements of "dst" using writemask "k" (elements
are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    n := (j % 2)*32
    IF k[j]
        dst[i+31:i] := a[n+31:n]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTI32X2 - _mm512_maskz_broadcast_i32x2

| VBROADCASTI32X2_ZMMu32_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the lower 2 packed 32-bit integers from "a" to all elements of "dst" using zeromask "k" (elements
are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    n := (j % 2)*32
    IF k[j]
        dst[i+31:i] := a[n+31:n]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTI32X2 - _mm_broadcast_i32x2

| VBROADCASTI32X2_XMMu32_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the lower 2 packed 32-bit integers from "a" to all elements of "dst.

[algorithm]

FOR j := 0 to 3
    i := j*32
    n := (j % 2)*32
    dst[i+31:i] := a[n+31:n]
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTI32X2 - _mm_mask_broadcast_i32x2

| VBROADCASTI32X2_XMMu32_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the lower 2 packed 32-bit integers from "a" to all elements of "dst" using writemask "k" (elements
are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    n := (j % 2)*32
    IF k[j]
        dst[i+31:i] := a[n+31:n]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTI32X2 - _mm_maskz_broadcast_i32x2

| VBROADCASTI32X2_XMMu32_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the lower 2 packed 32-bit integers from "a" to all elements of "dst" using zeromask "k" (elements
are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    n := (j % 2)*32
    IF k[j]
        dst[i+31:i] := a[n+31:n]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTI32X8 - _mm512_broadcast_i32x8

| VBROADCASTI32X8_ZMMu32_MASKmskw_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the 8 packed 32-bit integers from "a" to all elements of "dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    n := (j % 8)*32
    dst[i+31:i] := a[n+31:n]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTI32X8 - _mm512_mask_broadcast_i32x8

| VBROADCASTI32X8_ZMMu32_MASKmskw_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the 8 packed 32-bit integers from "a" to all elements of "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    n := (j % 8)*32
    IF k[j]
        dst[i+31:i] := a[n+31:n]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTI32X8 - _mm512_maskz_broadcast_i32x8

| VBROADCASTI32X8_ZMMu32_MASKmskw_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the 8 packed 32-bit integers from "a" to all elements of "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    n := (j % 8)*32
    IF k[j]
        dst[i+31:i] := a[n+31:n]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTI64X2 - _mm256_broadcast_i64x2

| VBROADCASTI64X2_YMMu64_MASKmskw_MEMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the 2 packed 64-bit integers from "a" to all elements of "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    n := (j % 2)*64
    dst[i+63:i] := a[n+63:n]
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTI64X2 - _mm256_mask_broadcast_i64x2

| VBROADCASTI64X2_YMMu64_MASKmskw_MEMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the 2 packed 64-bit integers from "a" to all elements of "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    n := (j % 2)*64
    IF k[j]
        dst[i+63:i] := a[n+63:n]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTI64X2 - _mm256_maskz_broadcast_i64x2

| VBROADCASTI64X2_YMMu64_MASKmskw_MEMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the 2 packed 64-bit integers from "a" to all elements of "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    n := (j % 2)*64
    IF k[j]
        dst[i+63:i] := a[n+63:n]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTI64X2 - _mm512_broadcast_i64x2

| VBROADCASTI64X2_ZMMu64_MASKmskw_MEMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the 2 packed 64-bit integers from "a" to all elements of "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    n := (j % 2)*64
    dst[i+63:i] := a[n+63:n]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTI64X2 - _mm512_mask_broadcast_i64x2

| VBROADCASTI64X2_ZMMu64_MASKmskw_MEMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the 2 packed 64-bit integers from "a" to all elements of "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    n := (j % 2)*64
    IF k[j]
        dst[i+63:i] := a[n+63:n]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTI64X2 - _mm512_maskz_broadcast_i64x2

| VBROADCASTI64X2_ZMMu64_MASKmskw_MEMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the 2 packed 64-bit integers from "a" to all elements of "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    n := (j % 2)*64
    IF k[j]
        dst[i+63:i] := a[n+63:n]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2QQ - _mm256_cvtpd_epi64

| VCVTPD2QQ_YMMi64_MASKmskw_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers, and store
the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2QQ - _mm256_mask_cvtpd_epi64

| VCVTPD2QQ_YMMi64_MASKmskw_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers, and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2QQ - _mm256_maskz_cvtpd_epi64

| VCVTPD2QQ_YMMi64_MASKmskw_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers, and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2QQ - _mm512_cvt_roundpd_epi64

| VCVTPD2QQ_ZMMi64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers, and store
the results in "dst". 
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2QQ - _mm512_cvtpd_epi64

| VCVTPD2QQ_ZMMi64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers, and store
the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2QQ - _mm512_mask_cvt_roundpd_epi64

| VCVTPD2QQ_ZMMi64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers, and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set). 
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2QQ - _mm512_mask_cvtpd_epi64

| VCVTPD2QQ_ZMMi64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers, and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2QQ - _mm512_maskz_cvt_roundpd_epi64

| VCVTPD2QQ_ZMMi64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers, and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2QQ - _mm512_maskz_cvtpd_epi64

| VCVTPD2QQ_ZMMi64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers, and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2QQ - _mm_cvtpd_epi64

| VCVTPD2QQ_XMMi64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers, and store
the results in "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2QQ - _mm_mask_cvtpd_epi64

| VCVTPD2QQ_XMMi64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers, and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2QQ - _mm_maskz_cvtpd_epi64

| VCVTPD2QQ_XMMi64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers, and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2UQQ - _mm256_cvtpd_epu64

| VCVTPD2UQQ_YMMu64_MASKmskw_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers,
and store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    dst[i+63:i] := Convert_FP64_To_UInt64(a[i+63:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2UQQ - _mm256_mask_cvtpd_epu64

| VCVTPD2UQQ_YMMu64_MASKmskw_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_FP64_To_UInt64(a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2UQQ - _mm256_maskz_cvtpd_epu64

| VCVTPD2UQQ_YMMu64_MASKmskw_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_FP64_To_UInt64(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2UQQ - _mm512_cvt_roundpd_epu64

| VCVTPD2UQQ_ZMMu64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers,
and store the results in "dst". 
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := Convert_FP64_To_UInt64(a[i+63:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2UQQ - _mm512_cvtpd_epu64

| VCVTPD2UQQ_ZMMu64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers,
and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := Convert_FP64_To_UInt64(a[i+63:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2UQQ - _mm512_mask_cvt_roundpd_epu64

| VCVTPD2UQQ_ZMMu64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set). 
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_FP64_To_UInt64(a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2UQQ - _mm512_mask_cvtpd_epu64

| VCVTPD2UQQ_ZMMu64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_FP64_To_UInt64(a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2UQQ - _mm512_maskz_cvt_roundpd_epu64

| VCVTPD2UQQ_ZMMu64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set). 
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_FP64_To_UInt64(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2UQQ - _mm512_maskz_cvtpd_epu64

| VCVTPD2UQQ_ZMMu64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_FP64_To_UInt64(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2UQQ - _mm_cvtpd_epu64

| VCVTPD2UQQ_XMMu64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers,
and store the results in "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    dst[i+63:i] := Convert_FP64_To_UInt64(a[i+63:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2UQQ - _mm_mask_cvtpd_epu64

| VCVTPD2UQQ_XMMu64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_FP64_To_UInt64(a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2UQQ - _mm_maskz_cvtpd_epu64

| VCVTPD2UQQ_XMMu64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_FP64_To_UInt64(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2QQ - _mm256_cvtps_epi64

| VCVTPS2QQ_YMMi64_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers, and store
the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    l := j*32
    dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2QQ - _mm256_mask_cvtps_epi64

| VCVTPS2QQ_YMMi64_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers, and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2QQ - _mm256_maskz_cvtps_epi64

| VCVTPS2QQ_YMMi64_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers, and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2QQ - _mm512_cvt_roundps_epi64

| VCVTPS2QQ_ZMMi64_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers, and store
the results in "dst". 
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2QQ - _mm512_cvtps_epi64

| VCVTPS2QQ_ZMMi64_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers, and store
the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2QQ - _mm512_mask_cvt_roundps_epi64

| VCVTPS2QQ_ZMMi64_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers, and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set). 
	 [round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2QQ - _mm512_mask_cvtps_epi64

| VCVTPS2QQ_ZMMi64_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers, and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2QQ - _mm512_maskz_cvt_roundps_epi64

| VCVTPS2QQ_ZMMi64_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers, and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2QQ - _mm512_maskz_cvtps_epi64

| VCVTPS2QQ_ZMMi64_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers, and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2QQ - _mm_cvtps_epi64

| VCVTPS2QQ_XMMi64_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers, and store
the results in "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    l := j*32
    dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2QQ - _mm_mask_cvtps_epi64

| VCVTPS2QQ_XMMi64_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers, and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2QQ - _mm_maskz_cvtps_epi64

| VCVTPS2QQ_XMMi64_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers, and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2UQQ - _mm256_cvtps_epu64

| VCVTPS2UQQ_YMMu64_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers,
and store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    l := j*32
    dst[i+63:i] := Convert_FP32_To_UInt64(a[l+31:l])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2UQQ - _mm256_mask_cvtps_epu64

| VCVTPS2UQQ_YMMu64_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_FP32_To_UInt64(a[l+31:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2UQQ - _mm256_maskz_cvtps_epu64

| VCVTPS2UQQ_YMMu64_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_FP32_To_UInt64(a[l+31:l])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2UQQ - _mm512_cvt_roundps_epu64

| VCVTPS2UQQ_ZMMu64_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers,
and store the results in "dst". 
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    dst[i+63:i] := Convert_FP32_To_UInt64(a[l+31:l])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2UQQ - _mm512_cvtps_epu64

| VCVTPS2UQQ_ZMMu64_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers,
and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    dst[i+63:i] := Convert_FP32_To_UInt64(a[l+31:l])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2UQQ - _mm512_mask_cvt_roundps_epu64

| VCVTPS2UQQ_ZMMu64_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set). 
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_FP32_To_UInt64(a[l+31:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2UQQ - _mm512_mask_cvtps_epu64

| VCVTPS2UQQ_ZMMu64_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_FP32_To_UInt64(a[l+31:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2UQQ - _mm512_maskz_cvt_roundps_epu64

| VCVTPS2UQQ_ZMMu64_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_FP32_To_UInt64(a[l+31:l])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2UQQ - _mm512_maskz_cvtps_epu64

| VCVTPS2UQQ_ZMMu64_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_FP32_To_UInt64(a[l+31:l])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2UQQ - _mm_cvtps_epu64

| VCVTPS2UQQ_XMMu64_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers,
and store the results in "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    l := j*32
    dst[i+63:i] := Convert_FP32_To_UInt64(a[l+31:l])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2UQQ - _mm_mask_cvtps_epu64

| VCVTPS2UQQ_XMMu64_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_FP32_To_UInt64(a[l+31:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2UQQ - _mm_maskz_cvtps_epu64

| VCVTPS2UQQ_XMMu64_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_FP32_To_UInt64(a[l+31:l])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTQQ2PD - _mm256_cvtepi64_pd

| VCVTQQ2PD_YMMi64_MASKmskw_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and
store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTQQ2PD - _mm256_mask_cvtepi64_pd

| VCVTQQ2PD_YMMi64_MASKmskw_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTQQ2PD - _mm256_maskz_cvtepi64_pd

| VCVTQQ2PD_YMMi64_MASKmskw_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTQQ2PD - _mm512_cvt_roundepi64_pd

| VCVTQQ2PD_ZMMi64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and
store the results in "dst".
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTQQ2PD - _mm512_cvtepi64_pd

| VCVTQQ2PD_ZMMi64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and
store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTQQ2PD - _mm512_mask_cvt_roundepi64_pd

| VCVTQQ2PD_ZMMi64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTQQ2PD - _mm512_mask_cvtepi64_pd

| VCVTQQ2PD_ZMMi64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTQQ2PD - _mm512_maskz_cvt_roundepi64_pd

| VCVTQQ2PD_ZMMi64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTQQ2PD - _mm512_maskz_cvtepi64_pd

| VCVTQQ2PD_ZMMi64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTQQ2PD - _mm_cvtepi64_pd

| VCVTQQ2PD_XMMi64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and
store the results in "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTQQ2PD - _mm_mask_cvtepi64_pd

| VCVTQQ2PD_XMMi64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTQQ2PD - _mm_maskz_cvtepi64_pd

| VCVTQQ2PD_XMMi64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTQQ2PS - _mm256_cvtepi64_ps

| VCVTQQ2PS_XMMf32_MASKmskw_YMMu64_AVX512_VL256

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and
store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    l := j*32
    dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTQQ2PS - _mm256_mask_cvtepi64_ps

| VCVTQQ2PS_XMMf32_MASKmskw_YMMu64_AVX512_VL256

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    l := j*32
    IF k[j]
        dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
    ELSE
        dst[l+31:l] := src[l+31:l]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTQQ2PS - _mm256_maskz_cvtepi64_ps

| VCVTQQ2PS_XMMf32_MASKmskw_YMMu64_AVX512_VL256

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    l := j*32
    IF k[j]
        dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
    ELSE
        dst[l+31:l] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTQQ2PS - _mm512_cvt_roundepi64_ps

| VCVTQQ2PS_YMMf32_MASKmskw_ZMMu64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and
store the results in "dst".
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTQQ2PS - _mm512_cvtepi64_ps

| VCVTQQ2PS_YMMf32_MASKmskw_ZMMu64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and
store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTQQ2PS - _mm512_mask_cvt_roundepi64_ps

| VCVTQQ2PS_YMMf32_MASKmskw_ZMMu64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    IF k[j]
        dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
    ELSE
        dst[l+31:l] := src[l+31:l]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTQQ2PS - _mm512_mask_cvtepi64_ps

| VCVTQQ2PS_YMMf32_MASKmskw_ZMMu64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    IF k[j]
        dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
    ELSE
        dst[l+31:l] := src[l+31:l]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTQQ2PS - _mm512_maskz_cvt_roundepi64_ps

| VCVTQQ2PS_YMMf32_MASKmskw_ZMMu64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    IF k[j]
        dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
    ELSE
        dst[l+31:l] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTQQ2PS - _mm512_maskz_cvtepi64_ps

| VCVTQQ2PS_YMMf32_MASKmskw_ZMMu64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    IF k[j]
        dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
    ELSE
        dst[l+31:l] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTQQ2PS - _mm_cvtepi64_ps

| VCVTQQ2PS_XMMf32_MASKmskw_XMMu64_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and
store the results in "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    l := j*32
    dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTQQ2PS - _mm_mask_cvtepi64_ps

| VCVTQQ2PS_XMMf32_MASKmskw_XMMu64_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    l := j*32
    IF k[j]
        dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
    ELSE
        dst[l+31:l] := src[l+31:l]
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTQQ2PS - _mm_maskz_cvtepi64_ps

| VCVTQQ2PS_XMMf32_MASKmskw_XMMu64_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    l := j*32
    IF k[j]
        dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
    ELSE
        dst[l+31:l] := 0
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2QQ - _mm256_cvttpd_epi64

| VCVTTPD2QQ_YMMi64_MASKmskw_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers with
truncation, and store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2QQ - _mm256_mask_cvttpd_epi64

| VCVTTPD2QQ_YMMi64_MASKmskw_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers with
truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2QQ - _mm256_maskz_cvttpd_epi64

| VCVTTPD2QQ_YMMi64_MASKmskw_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers with
truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2QQ - _mm512_cvtt_roundpd_epi64

| VCVTTPD2QQ_ZMMi64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers with
truncation, and store the results in "dst". [sae_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2QQ - _mm512_cvttpd_epi64

| VCVTTPD2QQ_ZMMi64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers with
truncation, and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2QQ - _mm512_mask_cvtt_roundpd_epi64

| VCVTTPD2QQ_ZMMi64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers with
truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set). [sae_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2QQ - _mm512_mask_cvttpd_epi64

| VCVTTPD2QQ_ZMMi64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers with
truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2QQ - _mm512_maskz_cvtt_roundpd_epi64

| VCVTTPD2QQ_ZMMi64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers with
truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set). [sae_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2QQ - _mm512_maskz_cvttpd_epi64

| VCVTTPD2QQ_ZMMi64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers with
truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2QQ - _mm_cvttpd_epi64

| VCVTTPD2QQ_XMMi64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers with
truncation, and store the results in "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2QQ - _mm_mask_cvttpd_epi64

| VCVTTPD2QQ_XMMi64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers with
truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2QQ - _mm_maskz_cvttpd_epi64

| VCVTTPD2QQ_XMMi64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers with
truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2UQQ - _mm256_cvttpd_epu64

| VCVTTPD2UQQ_YMMu64_MASKmskw_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers
with truncation, and store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    dst[i+63:i] := Convert_FP64_To_UInt64_Truncate(a[i+63:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2UQQ - _mm256_mask_cvttpd_epu64

| VCVTTPD2UQQ_YMMu64_MASKmskw_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers
with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_FP64_To_UInt64_Truncate(a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2UQQ - _mm256_maskz_cvttpd_epu64

| VCVTTPD2UQQ_YMMu64_MASKmskw_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers
with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_FP64_To_UInt64_Truncate(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2UQQ - _mm512_cvtt_roundpd_epu64

| VCVTTPD2UQQ_ZMMu64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers
with truncation, and store the results in "dst". [sae_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := Convert_FP64_To_UInt64_Truncate(a[i+63:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2UQQ - _mm512_cvttpd_epu64

| VCVTTPD2UQQ_ZMMu64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers
with truncation, and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := Convert_FP64_To_UInt64_Truncate(a[i+63:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2UQQ - _mm512_mask_cvtt_roundpd_epu64

| VCVTTPD2UQQ_ZMMu64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers
with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set). [sae_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_FP64_To_UInt64_Truncate(a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2UQQ - _mm512_mask_cvttpd_epu64

| VCVTTPD2UQQ_ZMMu64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers
with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_FP64_To_UInt64_Truncate(a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2UQQ - _mm512_maskz_cvtt_roundpd_epu64

| VCVTTPD2UQQ_ZMMu64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers
with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set). [sae_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_FP64_To_UInt64_Truncate(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2UQQ - _mm512_maskz_cvttpd_epu64

| VCVTTPD2UQQ_ZMMu64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers
with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_FP64_To_UInt64_Truncate(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2UQQ - _mm_cvttpd_epu64

| VCVTTPD2UQQ_XMMu64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers
with truncation, and store the results in "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    dst[i+63:i] := Convert_FP64_To_UInt64_Truncate(a[i+63:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2UQQ - _mm_mask_cvttpd_epu64

| VCVTTPD2UQQ_XMMu64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers
with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_FP64_To_UInt64_Truncate(a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2UQQ - _mm_maskz_cvttpd_epu64

| VCVTTPD2UQQ_XMMu64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers
with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_FP64_To_UInt64_Truncate(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2QQ - _mm256_cvttps_epi64

| VCVTTPS2QQ_YMMi64_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers with
truncation, and store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    l := j*32
    dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2QQ - _mm256_mask_cvttps_epi64

| VCVTTPS2QQ_YMMi64_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers with
truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2QQ - _mm256_maskz_cvttps_epi64

| VCVTTPS2QQ_YMMi64_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers with
truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2QQ - _mm512_cvtt_roundps_epi64

| VCVTTPS2QQ_ZMMi64_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers with
truncation, and store the results in "dst". [sae_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2QQ - _mm512_cvttps_epi64

| VCVTTPS2QQ_ZMMi64_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers with
truncation, and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2QQ - _mm512_mask_cvtt_roundps_epi64

| VCVTTPS2QQ_ZMMi64_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers with
truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set). [sae_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2QQ - _mm512_mask_cvttps_epi64

| VCVTTPS2QQ_ZMMi64_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers with
truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2QQ - _mm512_maskz_cvtt_roundps_epi64

| VCVTTPS2QQ_ZMMi64_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers with
truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set). [sae_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2QQ - _mm512_maskz_cvttps_epi64

| VCVTTPS2QQ_ZMMi64_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers with
truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2QQ - _mm_cvttps_epi64

| VCVTTPS2QQ_XMMi64_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers with
truncation, and store the results in "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    l := j*32
    dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2QQ - _mm_mask_cvttps_epi64

| VCVTTPS2QQ_XMMi64_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers with
truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2QQ - _mm_maskz_cvttps_epi64

| VCVTTPS2QQ_XMMi64_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers with
truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2UQQ - _mm256_cvttps_epu64

| VCVTTPS2UQQ_YMMu64_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers
with truncation, and store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    l := j*32
    dst[i+63:i] := Convert_FP32_To_UInt64_Truncate(a[l+31:l])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2UQQ - _mm256_mask_cvttps_epu64

| VCVTTPS2UQQ_YMMu64_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers
with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_FP32_To_UInt64_Truncate(a[l+31:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2UQQ - _mm256_maskz_cvttps_epu64

| VCVTTPS2UQQ_YMMu64_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers
with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_FP32_To_UInt64_Truncate(a[l+31:l])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2UQQ - _mm512_cvtt_roundps_epu64

| VCVTTPS2UQQ_ZMMu64_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers
with truncation, and store the results in "dst". [sae_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    dst[i+63:i] := Convert_FP32_To_UInt64_Truncate(a[l+31:l])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2UQQ - _mm512_cvttps_epu64

| VCVTTPS2UQQ_ZMMu64_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers
with truncation, and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    dst[i+63:i] := Convert_FP32_To_UInt64_Truncate(a[l+31:l])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2UQQ - _mm512_mask_cvtt_roundps_epu64

| VCVTTPS2UQQ_ZMMu64_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers
with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set). [sae_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_FP32_To_UInt64_Truncate(a[l+31:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2UQQ - _mm512_mask_cvttps_epu64

| VCVTTPS2UQQ_ZMMu64_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers
with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_FP32_To_UInt64_Truncate(a[l+31:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2UQQ - _mm512_maskz_cvtt_roundps_epu64

| VCVTTPS2UQQ_ZMMu64_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers
with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set). [sae_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_FP32_To_UInt64_Truncate(a[l+31:l])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2UQQ - _mm512_maskz_cvttps_epu64

| VCVTTPS2UQQ_ZMMu64_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers
with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_FP32_To_UInt64_Truncate(a[l+31:l])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2UQQ - _mm_cvttps_epu64

| VCVTTPS2UQQ_XMMu64_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers
with truncation, and store the results in "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    l := j*32
    dst[i+63:i] := Convert_FP32_To_UInt64_Truncate(a[l+31:l])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2UQQ - _mm_mask_cvttps_epu64

| VCVTTPS2UQQ_XMMu64_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers
with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_FP32_To_UInt64_Truncate(a[l+31:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2UQQ - _mm_maskz_cvttps_epu64

| VCVTTPS2UQQ_XMMu64_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers
with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_FP32_To_UInt64_Truncate(a[l+31:l])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUQQ2PD - _mm256_cvtepu64_pd

| VCVTUQQ2PD_YMMf64_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements,
and store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUQQ2PD - _mm256_mask_cvtepu64_pd

| VCVTUQQ2PD_YMMf64_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUQQ2PD - _mm256_maskz_cvtepu64_pd

| VCVTUQQ2PD_YMMf64_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUQQ2PD - _mm512_cvt_roundepu64_pd

| VCVTUQQ2PD_ZMMf64_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements,
and store the results in "dst".
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUQQ2PD - _mm512_cvtepu64_pd

| VCVTUQQ2PD_ZMMf64_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements,
and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUQQ2PD - _mm512_mask_cvt_roundepu64_pd

| VCVTUQQ2PD_ZMMf64_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set). 
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUQQ2PD - _mm512_mask_cvtepu64_pd

| VCVTUQQ2PD_ZMMf64_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUQQ2PD - _mm512_maskz_cvt_roundepu64_pd

| VCVTUQQ2PD_ZMMf64_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUQQ2PD - _mm512_maskz_cvtepu64_pd

| VCVTUQQ2PD_ZMMf64_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUQQ2PD - _mm_cvtepu64_pd

| VCVTUQQ2PD_XMMf64_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements,
and store the results in "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUQQ2PD - _mm_mask_cvtepu64_pd

| VCVTUQQ2PD_XMMf64_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUQQ2PD - _mm_maskz_cvtepu64_pd

| VCVTUQQ2PD_XMMf64_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUQQ2PS - _mm256_cvtepu64_ps

| VCVTUQQ2PS_XMMf32_MASKmskw_YMMu64_AVX512_VL256

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements,
and store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    l := j*32
    dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUQQ2PS - _mm256_mask_cvtepu64_ps

| VCVTUQQ2PS_XMMf32_MASKmskw_YMMu64_AVX512_VL256

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    l := j*32
    IF k[j]
        dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
    ELSE
        dst[l+31:l] := src[l+31:l]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUQQ2PS - _mm256_maskz_cvtepu64_ps

| VCVTUQQ2PS_XMMf32_MASKmskw_YMMu64_AVX512_VL256

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    l := j*32
    IF k[j]
        dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
    ELSE
        dst[l+31:l] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUQQ2PS - _mm512_cvt_roundepu64_ps

| VCVTUQQ2PS_YMMf32_MASKmskw_ZMMu64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements,
and store the results in "dst".
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUQQ2PS - _mm512_cvtepu64_ps

| VCVTUQQ2PS_YMMf32_MASKmskw_ZMMu64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements,
and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUQQ2PS - _mm512_mask_cvt_roundepu64_ps

| VCVTUQQ2PS_YMMf32_MASKmskw_ZMMu64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set). 
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    IF k[j]
        dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
    ELSE
        dst[l+31:l] := src[l+31:l]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUQQ2PS - _mm512_mask_cvtepu64_ps

| VCVTUQQ2PS_YMMf32_MASKmskw_ZMMu64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    IF k[j]
        dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
    ELSE
        dst[l+31:l] := src[l+31:l]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUQQ2PS - _mm512_maskz_cvt_roundepu64_ps

| VCVTUQQ2PS_YMMf32_MASKmskw_ZMMu64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    IF k[j]
        dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
    ELSE
        dst[l+31:l] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUQQ2PS - _mm512_maskz_cvtepu64_ps

| VCVTUQQ2PS_YMMf32_MASKmskw_ZMMu64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    IF k[j]
        dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
    ELSE
        dst[l+31:l] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUQQ2PS - _mm_cvtepu64_ps

| VCVTUQQ2PS_XMMf32_MASKmskw_XMMu64_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements,
and store the results in "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    l := j*32
    dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUQQ2PS - _mm_mask_cvtepu64_ps

| VCVTUQQ2PS_XMMf32_MASKmskw_XMMu64_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    l := j*32
    IF k[j]
        dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
    ELSE
        dst[l+31:l] := src[l+31:l]
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUQQ2PS - _mm_maskz_cvtepu64_ps

| VCVTUQQ2PS_XMMf32_MASKmskw_XMMu64_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    l := j*32
    IF k[j]
        dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
    ELSE
        dst[l+31:l] := 0
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VEXTRACTF32X8 - _mm512_extractf32x8_ps

| VEXTRACTF32X8_YMMf32_MASKmskw_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract 256 bits (composed of 8 packed single-precision (32-bit) floating-point elements) from "a", selected
with "imm8", and store the result in "dst".

[algorithm]

CASE imm8[0] OF
0: dst[255:0] := a[255:0]
1: dst[255:0] := a[511:256]
ESAC
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VEXTRACTF32X8 - _mm512_mask_extractf32x8_ps

| VEXTRACTF32X8_YMMf32_MASKmskw_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract 256 bits (composed of 8 packed single-precision (32-bit) floating-point elements) from "a", selected
with "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

CASE imm8[0] OF
0: tmp[255:0] := a[255:0]
1: tmp[255:0] := a[511:256]
ESAC
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VEXTRACTF32X8 - _mm512_maskz_extractf32x8_ps

| VEXTRACTF32X8_YMMf32_MASKmskw_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract 256 bits (composed of 8 packed single-precision (32-bit) floating-point elements) from "a", selected
with "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set).

[algorithm]

CASE imm8[0] OF
0: tmp[255:0] := a[255:0]
1: tmp[255:0] := a[511:256]
ESAC
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VEXTRACTF64X2 - _mm256_extractf64x2_pd

| VEXTRACTF64X2_XMMf64_MASKmskw_YMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract 128 bits (composed of 2 packed double-precision (64-bit) floating-point elements) from "a", selected
with "imm8", and store the result in "dst".

[algorithm]

CASE imm8[0] OF
0: dst[127:0] := a[127:0]
1: dst[127:0] := a[255:128]
ESAC
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VEXTRACTF64X2 - _mm256_mask_extractf64x2_pd

| VEXTRACTF64X2_XMMf64_MASKmskw_YMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract 128 bits (composed of 2 packed double-precision (64-bit) floating-point elements) from "a", selected
with "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

CASE imm8[0] OF
0: tmp[127:0] := a[127:0]
1: tmp[127:0] := a[255:128]
ESAC
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VEXTRACTF64X2 - _mm256_maskz_extractf64x2_pd

| VEXTRACTF64X2_XMMf64_MASKmskw_YMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract 128 bits (composed of 2 packed double-precision (64-bit) floating-point elements) from "a", selected
with "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set).

[algorithm]

CASE imm8[0] OF
0: tmp[127:0] := a[127:0]
1: tmp[127:0] := a[255:128]
ESAC
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VEXTRACTF64X2 - _mm512_extractf64x2_pd

| VEXTRACTF64X2_XMMf64_MASKmskw_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract 128 bits (composed of 2 packed double-precision (64-bit) floating-point elements) from "a", selected
with "imm8", and store the result in "dst".

[algorithm]

CASE imm8[1:0] OF
0: dst[127:0] := a[127:0]
1: dst[127:0] := a[255:128]
2: dst[127:0] := a[383:256]
3: dst[127:0] := a[511:384]
ESAC
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VEXTRACTF64X2 - _mm512_mask_extractf64x2_pd

| VEXTRACTF64X2_XMMf64_MASKmskw_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract 128 bits (composed of 2 packed double-precision (64-bit) floating-point elements) from "a", selected
with "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

CASE imm8[1:0] OF
0: tmp[127:0] := a[127:0]
1: tmp[127:0] := a[255:128]
2: tmp[127:0] := a[383:256]
3: tmp[127:0] := a[511:384]
ESAC
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VEXTRACTF64X2 - _mm512_maskz_extractf64x2_pd

| VEXTRACTF64X2_XMMf64_MASKmskw_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract 128 bits (composed of 2 packed double-precision (64-bit) floating-point elements) from "a", selected
with "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set).

[algorithm]

CASE imm8[1:0] OF
0: tmp[127:0] := a[127:0]
1: tmp[127:0] := a[255:128]
2: tmp[127:0] := a[383:256]
3: tmp[127:0] := a[511:384]
ESAC
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VEXTRACTI32X8 - _mm512_extracti32x8_epi32

| VEXTRACTI32X8_YMMu32_MASKmskw_ZMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract 256 bits (composed of 8 packed 32-bit integers) from "a", selected with "imm8", and store the result
in "dst".

[algorithm]

CASE imm8[0] OF
0: dst[255:0] := a[255:0]
1: dst[255:0] := a[511:256]
ESAC
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VEXTRACTI32X8 - _mm512_mask_extracti32x8_epi32

| VEXTRACTI32X8_YMMu32_MASKmskw_ZMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract 256 bits (composed of 8 packed 32-bit integers) from "a", selected with "imm8", and store the results
in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

CASE imm8[0] OF
0: tmp[255:0] := a[255:0]
1: tmp[255:0] := a[511:256]
ESAC
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VEXTRACTI32X8 - _mm512_maskz_extracti32x8_epi32

| VEXTRACTI32X8_YMMu32_MASKmskw_ZMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract 256 bits (composed of 8 packed 32-bit integers) from "a", selected with "imm8", and store the results
in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

CASE imm8[0] OF
0: tmp[255:0] := a[255:0]
1: tmp[255:0] := a[511:256]
ESAC
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VEXTRACTI64X2 - _mm256_extracti64x2_epi64

| VEXTRACTI64X2_XMMu64_MASKmskw_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract 128 bits (composed of 2 packed 64-bit integers) from "a", selected with "imm8", and store the result
in "dst".

[algorithm]

CASE imm8[0] OF
0: dst[127:0] := a[127:0]
1: dst[127:0] := a[255:128]
ESAC
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VEXTRACTI64X2 - _mm256_mask_extracti64x2_epi64

| VEXTRACTI64X2_XMMu64_MASKmskw_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract 128 bits (composed of 2 packed 64-bit integers) from "a", selected with "imm8", and store the results
in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

CASE imm8[0] OF
0: tmp[127:0] := a[127:0]
1: tmp[127:0] := a[255:128]
ESAC
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VEXTRACTI64X2 - _mm256_maskz_extracti64x2_epi64

| VEXTRACTI64X2_XMMu64_MASKmskw_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract 128 bits (composed of 2 packed 64-bit integers) from "a", selected with "imm8", and store the results
in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

CASE imm8[0] OF
0: tmp[127:0] := a[127:0]
1: tmp[127:0] := a[255:128]
ESAC
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VEXTRACTI64X2 - _mm512_extracti64x2_epi64

| VEXTRACTI64X2_XMMu64_MASKmskw_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract 128 bits (composed of 2 packed 64-bit integers) from "a", selected with "imm8", and store the result
in "dst".

[algorithm]

CASE imm8[1:0] OF
0: dst[127:0] := a[127:0]
1: dst[127:0] := a[255:128]
2: dst[127:0] := a[383:256]
3: dst[127:0] := a[511:384]
ESAC
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VEXTRACTI64X2 - _mm512_mask_extracti64x2_epi64

| VEXTRACTI64X2_XMMu64_MASKmskw_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract 128 bits (composed of 2 packed 64-bit integers) from "a", selected with "imm8", and store the results
in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

CASE imm8[1:0] OF
0: tmp[127:0] := a[127:0]
1: tmp[127:0] := a[255:128]
2: tmp[127:0] := a[383:256]
3: tmp[127:0] := a[511:384]
ESAC
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VEXTRACTI64X2 - _mm512_maskz_extracti64x2_epi64

| VEXTRACTI64X2_XMMu64_MASKmskw_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract 128 bits (composed of 2 packed 64-bit integers) from "a", selected with "imm8", and store the results
in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

CASE imm8[1:0] OF
0: tmp[127:0] := a[127:0]
1: tmp[127:0] := a[255:128]
2: tmp[127:0] := a[383:256]
3: tmp[127:0] := a[511:384]
ESAC
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFPCLASSPD - _mm256_fpclass_pd_mask

| VFPCLASSPD_MASKmskw_MASKmskw_YMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Test packed double-precision (64-bit) floating-point elements in "a" for special categories specified by
"imm8", and store the results in mask vector "k".
	[fpclass_note]

[algorithm]

FOR j := 0 to 3
    i := j*64
    k[j] := CheckFPClass_FP64(a[i+63:i], imm8[7:0])
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VFPCLASSPD - _mm256_mask_fpclass_pd_mask

| VFPCLASSPD_MASKmskw_MASKmskw_YMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Test packed double-precision (64-bit) floating-point elements in "a" for special categories specified by
"imm8", and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the
corresponding mask bit is not set).
	[fpclass_note]

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k1[j]
        k[j] := CheckFPClass_FP64(a[i+63:i], imm8[7:0])
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VFPCLASSPD - _mm512_fpclass_pd_mask

| VFPCLASSPD_MASKmskw_MASKmskw_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Test packed double-precision (64-bit) floating-point elements in "a" for special categories specified by
"imm8", and store the results in mask vector "k".
	[fpclass_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    k[j] := CheckFPClass_FP64(a[i+63:i], imm8[7:0])
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VFPCLASSPD - _mm512_mask_fpclass_pd_mask

| VFPCLASSPD_MASKmskw_MASKmskw_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Test packed double-precision (64-bit) floating-point elements in "a" for special categories specified by
"imm8", and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the
corresponding mask bit is not set).
	[fpclass_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k1[j]
        k[j] := CheckFPClass_FP64(a[i+63:i], imm8[7:0])
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VFPCLASSPD - _mm_fpclass_pd_mask

| VFPCLASSPD_MASKmskw_MASKmskw_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Test packed double-precision (64-bit) floating-point elements in "a" for special categories specified by
"imm8", and store the results in mask vector "k".
	[fpclass_note]

[algorithm]

FOR j := 0 to 1
    i := j*64
    k[j] := CheckFPClass_FP64(a[i+63:i], imm8[7:0])
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VFPCLASSPD - _mm_mask_fpclass_pd_mask

| VFPCLASSPD_MASKmskw_MASKmskw_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Test packed double-precision (64-bit) floating-point elements in "a" for special categories specified by
"imm8", and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the
corresponding mask bit is not set).
	[fpclass_note]

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k1[j]
        k[j] := CheckFPClass_FP64(a[i+63:i], imm8[7:0])
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VFPCLASSPS - _mm256_fpclass_ps_mask

| VFPCLASSPS_MASKmskw_MASKmskw_YMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Test packed single-precision (32-bit) floating-point elements in "a" for special categories specified by
"imm8", and store the results in mask vector "k".
	[fpclass_note]

[algorithm]

FOR j := 0 to 7
    i := j*32
    k[j] := CheckFPClass_FP32(a[i+31:i], imm8[7:0])
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VFPCLASSPS - _mm256_mask_fpclass_ps_mask

| VFPCLASSPS_MASKmskw_MASKmskw_YMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Test packed single-precision (32-bit) floating-point elements in "a" for special categories specified by
"imm8", and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the
corresponding mask bit is not set).
	[fpclass_note]

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k1[j]
        k[j] := CheckFPClass_FP32(a[i+31:i], imm8[7:0])
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VFPCLASSPS - _mm512_fpclass_ps_mask

| VFPCLASSPS_MASKmskw_MASKmskw_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Test packed single-precision (32-bit) floating-point elements in "a" for special categories specified by
"imm8", and store the results in mask vector "k".
	[fpclass_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    k[j] := CheckFPClass_FP32(a[i+31:i], imm8[7:0])
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VFPCLASSPS - _mm512_mask_fpclass_ps_mask

| VFPCLASSPS_MASKmskw_MASKmskw_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Test packed single-precision (32-bit) floating-point elements in "a" for special categories specified by
"imm8", and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the
corresponding mask bit is not set).
	[fpclass_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k1[j]
        k[j] := CheckFPClass_FP32(a[i+31:i], imm8[7:0])
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VFPCLASSPS - _mm_fpclass_ps_mask

| VFPCLASSPS_MASKmskw_MASKmskw_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Test packed single-precision (32-bit) floating-point elements in "a" for special categories specified by
"imm8", and store the results in mask vector "k".
	[fpclass_note]

[algorithm]

FOR j := 0 to 3
    i := j*32
    k[j] := CheckFPClass_FP32(a[i+31:i], imm8[7:0])
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VFPCLASSPS - _mm_mask_fpclass_ps_mask

| VFPCLASSPS_MASKmskw_MASKmskw_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Test packed single-precision (32-bit) floating-point elements in "a" for special categories specified by
"imm8", and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the
corresponding mask bit is not set).
	[fpclass_note]

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k1[j]
        k[j] := CheckFPClass_FP32(a[i+31:i], imm8[7:0])
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VFPCLASSSD - _mm_fpclass_sd_mask

| VFPCLASSSD_MASKmskw_MASKmskw_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Test the lower double-precision (64-bit) floating-point element in "a" for special categories specified by
"imm8", and store the result in mask vector "k".
	[fpclass_note]

[algorithm]

k[0] := CheckFPClass_FP64(a[63:0], imm8[7:0])
k[MAX:1] := 0

--------------------------------------------------------------------------------------------------------------

## VFPCLASSSD - _mm_mask_fpclass_sd_mask

| VFPCLASSSD_MASKmskw_MASKmskw_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Test the lower double-precision (64-bit) floating-point element in "a" for special categories specified by
"imm8", and store the result in mask vector "k" using zeromask "k1" (the element is zeroed out when mask bit 0
is not set).
	[fpclass_note]

[algorithm]

IF k1[0]
    k[0] := CheckFPClass_FP64(a[63:0], imm8[7:0])
ELSE
    k[0] := 0
FI
k[MAX:1] := 0

--------------------------------------------------------------------------------------------------------------

## VFPCLASSSS - _mm_fpclass_ss_mask

| VFPCLASSSS_MASKmskw_MASKmskw_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Test the lower single-precision (32-bit) floating-point element in "a" for special categories specified by
"imm8", and store the result in mask vector "k.
	[fpclass_note]

[algorithm]

k[0] := CheckFPClass_FP32(a[31:0], imm8[7:0])
k[MAX:1] := 0

--------------------------------------------------------------------------------------------------------------

## VFPCLASSSS - _mm_mask_fpclass_ss_mask

| VFPCLASSSS_MASKmskw_MASKmskw_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Test the lower single-precision (32-bit) floating-point element in "a" for special categories specified by
"imm8", and store the result in mask vector "k" using zeromask "k1" (the element is zeroed out when mask bit 0
is not set).
	[fpclass_note]

[algorithm]

IF k1[0]
    k[0] := CheckFPClass_FP32(a[31:0], imm8[7:0])
ELSE
    k[0] := 0
FI
k[MAX:1] := 0

--------------------------------------------------------------------------------------------------------------

## VINSERTF32X8 - _mm512_insertf32x8

| VINSERTF32X8_ZMMf32_MASKmskw_ZMMf32_YMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Copy "a" to "dst", then insert 256 bits (composed of 8 packed single-precision (32-bit) floating-point
elements) from "b" into "dst" at the location specified by "imm8".

[algorithm]

dst[511:0] := a[511:0]
CASE (imm8[0]) OF
0: dst[255:0] := b[255:0]
1: dst[511:256] := b[255:0]
ESAC
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VINSERTF32X8 - _mm512_mask_insertf32x8

| VINSERTF32X8_ZMMf32_MASKmskw_ZMMf32_YMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Copy "a" to "tmp", then insert 256 bits (composed of 8 packed single-precision (32-bit) floating-point
elements) from "b" into "tmp" at the location specified by "imm8". Store "tmp" to "dst" using writemask "k"
(elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

tmp[511:0] := a[511:0]
CASE (imm8[0]) OF
0: tmp[255:0] := b[255:0]
1: tmp[511:256] := b[255:0]
ESAC
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VINSERTF32X8 - _mm512_maskz_insertf32x8

| VINSERTF32X8_ZMMf32_MASKmskw_ZMMf32_YMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Copy "a" to "tmp", then insert 256 bits (composed of 8 packed single-precision (32-bit) floating-point
elements) from "b" into "tmp" at the location specified by "imm8". Store "tmp" to "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

tmp[511:0] := a[511:0]
CASE (imm8[0]) OF
0: tmp[255:0] := b[255:0]
1: tmp[511:256] := b[255:0]
ESAC
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VINSERTF64X2 - _mm256_insertf64x2

| VINSERTF64X2_YMMf64_MASKmskw_YMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Copy "a" to "dst", then insert 128 bits (composed of 2 packed double-precision (64-bit) floating-point
elements) from "b" into "dst" at the location specified by "imm8".

[algorithm]

dst[255:0] := a[255:0]
CASE imm8[0] OF
0: dst[127:0] := b[127:0]
1: dst[255:128] := b[127:0]
ESAC
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VINSERTF64X2 - _mm256_mask_insertf64x2

| VINSERTF64X2_YMMf64_MASKmskw_YMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Copy "a" to "tmp", then insert 128 bits (composed of 2 packed double-precision (64-bit) floating-point
elements) from "b" into "tmp" at the location specified by "imm8". Store "tmp" to "dst" using writemask "k"
(elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

tmp[255:0] := a[255:0]
CASE (imm8[0]) OF
0: tmp[127:0] := b[127:0]
1: tmp[255:128] := b[127:0]
ESAC
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VINSERTF64X2 - _mm256_maskz_insertf64x2

| VINSERTF64X2_YMMf64_MASKmskw_YMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Copy "a" to "tmp", then insert 128 bits (composed of 2 packed double-precision (64-bit) floating-point
elements) from "b" into "tmp" at the location specified by "imm8". Store "tmp" to "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

tmp[255:0] := a[255:0]
CASE (imm8[0]) OF
0: tmp[127:0] := b[127:0]
1: tmp[255:128] := b[127:0]
ESAC
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VINSERTF64X2 - _mm512_insertf64x2

| VINSERTF64X2_ZMMf64_MASKmskw_ZMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Copy "a" to "dst", then insert 128 bits (composed of 2 packed double-precision (64-bit) floating-point
elements) from "b" into "dst" at the location specified by "imm8".

[algorithm]

dst[511:0] := a[511:0]
CASE imm8[1:0] OF
0: dst[127:0] := b[127:0]
1: dst[255:128] := b[127:0]
2: dst[383:256] := b[127:0]
3: dst[511:384] := b[127:0]
ESAC
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VINSERTF64X2 - _mm512_mask_insertf64x2

| VINSERTF64X2_ZMMf64_MASKmskw_ZMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Copy "a" to "tmp", then insert 128 bits (composed of 2 packed double-precision (64-bit) floating-point
elements) from "b" into "tmp" at the location specified by "imm8". Store "tmp" to "dst" using writemask "k"
(elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

tmp[511:0] := a[511:0]
CASE (imm8[1:0]) OF
0: tmp[127:0] := b[127:0]
1: tmp[255:128] := b[127:0]
2: tmp[383:256] := b[127:0]
3: tmp[511:384] := b[127:0]
ESAC
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VINSERTF64X2 - _mm512_maskz_insertf64x2

| VINSERTF64X2_ZMMf64_MASKmskw_ZMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Copy "a" to "tmp", then insert 128 bits (composed of 2 packed double-precision (64-bit) floating-point
elements) from "b" into "tmp" at the location specified by "imm8". Store "tmp" to "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

tmp[511:0] := a[511:0]
CASE (imm8[1:0]) OF
0: tmp[127:0] := b[127:0]
1: tmp[255:128] := b[127:0]
2: tmp[383:256] := b[127:0]
3: tmp[511:384] := b[127:0]
ESAC
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VINSERTI32X8 - _mm512_inserti32x8

| VINSERTI32X8_ZMMu32_MASKmskw_ZMMu32_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Copy "a" to "dst", then insert 256 bits (composed of 8 packed 32-bit integers) from "b" into "dst" at the
location specified by "imm8".

[algorithm]

dst[511:0] := a[511:0]
CASE imm8[0] OF
0: dst[255:0] := b[255:0]
1: dst[511:256] := b[255:0]
ESAC
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VINSERTI32X8 - _mm512_mask_inserti32x8

| VINSERTI32X8_ZMMu32_MASKmskw_ZMMu32_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Copy "a" to "tmp", then insert 256 bits (composed of 8 packed 32-bit integers) from "b" into "tmp" at the
location specified by "imm8". Store "tmp" to "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

tmp[511:0] := a[511:0]
CASE (imm8[0]) OF
0: tmp[255:0] := b[255:0]
1: tmp[511:256] := b[255:0]
ESAC
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VINSERTI32X8 - _mm512_maskz_inserti32x8

| VINSERTI32X8_ZMMu32_MASKmskw_ZMMu32_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Copy "a" to "tmp", then insert 256 bits (composed of 8 packed 32-bit integers) from "b" into "tmp" at the
location specified by "imm8". Store "tmp" to "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

tmp[511:0] := a[511:0]
CASE (imm8[0]) OF
0: tmp[255:0] := b[255:0]
1: tmp[511:256] := b[255:0]
ESAC
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VINSERTI64X2 - _mm256_inserti64x2

| VINSERTI64X2_YMMu64_MASKmskw_YMMu64_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Copy "a" to "dst", then insert 128 bits (composed of 2 packed 64-bit integers) from "b" into "dst" at the
location specified by "imm8".

[algorithm]

dst[255:0] := a[255:0]
CASE imm8[0] OF
0: dst[127:0] := b[127:0]
1: dst[255:128] := b[127:0]
ESAC
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VINSERTI64X2 - _mm256_mask_inserti64x2

| VINSERTI64X2_YMMu64_MASKmskw_YMMu64_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Copy "a" to "tmp", then insert 128 bits (composed of 2 packed 64-bit integers) from "b" into "tmp" at the
location specified by "imm8". Store "tmp" to "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

tmp[255:0] := a[255:0]
CASE (imm8[0]) OF
0: tmp[127:0] := b[127:0]
1: tmp[255:128] := b[127:0]
ESAC
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VINSERTI64X2 - _mm256_maskz_inserti64x2

| VINSERTI64X2_YMMu64_MASKmskw_YMMu64_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Copy "a" to "tmp", then insert 128 bits (composed of 2 packed 64-bit integers) from "b" into "tmp" at the
location specified by "imm8". Store "tmp" to "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

tmp[255:0] := a[255:0]
CASE (imm8[0]) OF
0: tmp[127:0] := b[127:0]
1: tmp[255:128] := b[127:0]
ESAC
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VINSERTI64X2 - _mm512_inserti64x2

| VINSERTI64X2_ZMMu64_MASKmskw_ZMMu64_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Copy "a" to "dst", then insert 128 bits (composed of 2 packed 64-bit integers) from "b" into "dst" at the
location specified by "imm8".

[algorithm]

dst[511:0] := a[511:0]
CASE imm8[1:0] OF
0: dst[127:0] := b[127:0]
1: dst[255:128] := b[127:0]
2: dst[383:256] := b[127:0]
3: dst[511:384] := b[127:0]
ESAC
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VINSERTI64X2 - _mm512_mask_inserti64x2

| VINSERTI64X2_ZMMu64_MASKmskw_ZMMu64_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Copy "a" to "tmp", then insert 128 bits (composed of 2 packed 64-bit integers) from "b" into "tmp" at the
location specified by "imm8". Store "tmp" to "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

tmp[511:0] := a[511:0]
CASE (imm8[1:0]) OF
0: tmp[127:0] := b[127:0]
1: tmp[255:128] := b[127:0]
2: tmp[383:256] := b[127:0]
3: tmp[511:384] := b[127:0]
ESAC
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VINSERTI64X2 - _mm512_maskz_inserti64x2

| VINSERTI64X2_ZMMu64_MASKmskw_ZMMu64_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Copy "a" to "tmp", then insert 128 bits (composed of 2 packed 64-bit integers) from "b" into "tmp" at the
location specified by "imm8". Store "tmp" to "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

tmp[511:0] := a[511:0]
CASE (imm8[1:0]) OF
0: tmp[127:0] := b[127:0]
1: tmp[255:128] := b[127:0]
2: tmp[383:256] := b[127:0]
3: tmp[511:384] := b[127:0]
ESAC
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VORPD - _mm256_mask_or_pd

| VORPD_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of packed double-precision (64-bit) floating-point elements in "a" and "b", and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] OR b[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VORPD - _mm256_maskz_or_pd

| VORPD_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of packed double-precision (64-bit) floating-point elements in "a" and "b", and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] OR b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VORPD - _mm512_mask_or_pd

| VORPD_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of packed double-precision (64-bit) floating-point elements in "a" and "b", and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] OR b[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VORPD - _mm512_maskz_or_pd

| VORPD_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of packed double-precision (64-bit) floating-point elements in "a" and "b", and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] OR b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VORPD - _mm512_or_pd

| VORPD_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of packed double-precision (64-bit) floating-point elements in "a" and "b", and store
the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := a[i+63:i] OR b[i+63:i]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VORPD - _mm_mask_or_pd

| VORPD_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of packed double-precision (64-bit) floating-point elements in "a" and "b", and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] OR b[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VORPD - _mm_maskz_or_pd

| VORPD_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of packed double-precision (64-bit) floating-point elements in "a" and "b", and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] OR b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VORPS - _mm256_mask_or_ps

| VORPS_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of packed single-precision (32-bit) floating-point elements in "a" and "b", and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] OR b[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VORPS - _mm256_maskz_or_ps

| VORPS_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of packed single-precision (32-bit) floating-point elements in "a" and "b", and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] OR b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VORPS - _mm512_mask_or_ps

| VORPS_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of packed single-precision (32-bit) floating-point elements in "a" and "b", and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] OR b[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VORPS - _mm512_maskz_or_ps

| VORPS_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of packed single-precision (32-bit) floating-point elements in "a" and "b", and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] OR b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VORPS - _mm512_or_ps

| VORPS_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of packed single-precision (32-bit) floating-point elements in "a" and "b", and store
the results in "dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := a[i+31:i] OR b[i+31:i]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VORPS - _mm_mask_or_ps

| VORPS_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of packed single-precision (32-bit) floating-point elements in "a" and "b", and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] OR b[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VORPS - _mm_maskz_or_ps

| VORPS_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of packed single-precision (32-bit) floating-point elements in "a" and "b", and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] OR b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVD2M - _mm256_movepi32_mask

| VPMOVD2M_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Set each bit of mask register "k" based on the most significant bit of the corresponding packed 32-bit integer
in "a".

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF a[i+31]
        k[j] := 1
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVD2M - _mm512_movepi32_mask

| VPMOVD2M_MASKmskw_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Set each bit of mask register "k" based on the most significant bit of the corresponding packed 32-bit integer
in "a".

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF a[i+31]
        k[j] := 1
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVD2M - _mm_movepi32_mask

| VPMOVD2M_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Set each bit of mask register "k" based on the most significant bit of the corresponding packed 32-bit integer
in "a".

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF a[i+31]
        k[j] := 1
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVM2D - _mm256_movm_epi32

| VPMOVM2D_YMMu32_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Set each packed 32-bit integer in "dst" to all ones or all zeros based on the value of the corresponding bit
in "k".

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := 0xFFFFFFFF
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVM2D - _mm512_movm_epi32

| VPMOVM2D_ZMMu32_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Set each packed 32-bit integer in "dst" to all ones or all zeros based on the value of the corresponding bit
in "k".

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := 0xFFFFFFFF
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVM2D - _mm_movm_epi32

| VPMOVM2D_XMMu32_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Set each packed 32-bit integer in "dst" to all ones or all zeros based on the value of the corresponding bit
in "k".

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := 0xFFFFFFFF
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVM2Q - _mm256_movm_epi64

| VPMOVM2Q_YMMu64_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Set each packed 64-bit integer in "dst" to all ones or all zeros based on the value of the corresponding bit
in "k".

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := 0xFFFFFFFFFFFFFFFF
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVM2Q - _mm512_movm_epi64

| VPMOVM2Q_ZMMu64_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Set each packed 64-bit integer in "dst" to all ones or all zeros based on the value of the corresponding bit
in "k".

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := 0xFFFFFFFFFFFFFFFF
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVM2Q - _mm_movm_epi64

| VPMOVM2Q_XMMu64_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Set each packed 64-bit integer in "dst" to all ones or all zeros based on the value of the corresponding bit
in "k".

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := 0xFFFFFFFFFFFFFFFF
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVQ2M - _mm256_movepi64_mask

| VPMOVQ2M_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Set each bit of mask register "k" based on the most significant bit of the corresponding packed 64-bit integer
in "a".

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF a[i+63]
        k[j] := 1
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVQ2M - _mm512_movepi64_mask

| VPMOVQ2M_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Set each bit of mask register "k" based on the most significant bit of the corresponding packed 64-bit integer
in "a".

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF a[i+63]
        k[j] := 1
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVQ2M - _mm_movepi64_mask

| VPMOVQ2M_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Set each bit of mask register "k" based on the most significant bit of the corresponding packed 64-bit integer
in "a".

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF a[i+63]
        k[j] := 1
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULLQ - _mm256_mask_mullo_epi64

| VPMULLQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the packed 64-bit integers in "a" and "b", producing intermediate 128-bit integers, and store the low
64 bits of the intermediate integers in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        tmp[127:0] := a[i+63:i] * b[i+63:i]
        dst[i+63:i] := tmp[63:0]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULLQ - _mm256_maskz_mullo_epi64

| VPMULLQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the packed 64-bit integers in "a" and "b", producing intermediate 128-bit integers, and store the low
64 bits of the intermediate integers in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        tmp[127:0] := a[i+63:i] * b[i+63:i]
        dst[i+63:i] := tmp[63:0]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULLQ - _mm256_mullo_epi64

| VPMULLQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the packed 64-bit integers in "a" and "b", producing intermediate 128-bit integers, and store the low
64 bits of the intermediate integers in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    tmp[127:0] := a[i+63:i] * b[i+63:i]
    dst[i+63:i] := tmp[63:0]
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULLQ - _mm512_mask_mullo_epi64

| VPMULLQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the packed 64-bit integers in "a" and "b", producing intermediate 128-bit integers, and store the low
64 bits of the intermediate integers in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        tmp[127:0] := a[i+63:i] * b[i+63:i]
        dst[i+63:i] := tmp[63:0]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULLQ - _mm512_maskz_mullo_epi64

| VPMULLQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the packed 64-bit integers in "a" and "b", producing intermediate 128-bit integers, and store the low
64 bits of the intermediate integers in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        tmp[127:0] := a[i+63:i] * b[i+63:i]
        dst[i+63:i] := tmp[63:0]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULLQ - _mm512_mullo_epi64

| VPMULLQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the packed 64-bit integers in "a" and "b", producing intermediate 128-bit integers, and store the low
64 bits of the intermediate integers in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    tmp[127:0] := a[i+63:i] * b[i+63:i]
    dst[i+63:i] := tmp[63:0]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULLQ - _mm_mask_mullo_epi64

| VPMULLQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the packed 64-bit integers in "a" and "b", producing intermediate 128-bit integers, and store the low
64 bits of the intermediate integers in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        tmp[127:0] := a[i+63:i] * b[i+63:i]
        dst[i+63:i] := tmp[63:0]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULLQ - _mm_maskz_mullo_epi64

| VPMULLQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the packed 64-bit integers in "a" and "b", producing intermediate 128-bit integers, and store the low
64 bits of the intermediate integers in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        tmp[127:0] := a[i+63:i] * b[i+63:i]
        dst[i+63:i] := tmp[63:0]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULLQ - _mm_mullo_epi64

| VPMULLQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the packed 64-bit integers in "a" and "b", producing intermediate 128-bit integers, and store the low
64 bits of the intermediate integers in "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    tmp[127:0] := a[i+63:i] * b[i+63:i]
    dst[i+63:i] := tmp[63:0]
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRANGEPD - _mm256_mask_range_pd

| VRANGEPD_YMMf64_MASKmskw_YMMf64_YMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed
double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
	imm8[1:0] specifies
the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign
control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.

[algorithm]

DEFINE RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0]) {
    CASE opCtl[1:0] OF
    0: tmp[63:0] := (src1[63:0] &lt;= src2[63:0]) ? src1[63:0] : src2[63:0]
    1: tmp[63:0] := (src1[63:0] &lt;= src2[63:0]) ? src2[63:0] : src1[63:0]
    2: tmp[63:0] := (ABS(src1[63:0]) &lt;= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
    3: tmp[63:0] := (ABS(src1[63:0]) &lt;= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
    ESAC
    
    CASE signSelCtl[1:0] OF
    0: dst[63:0] := (src1[63] &lt;&lt; 63) OR (tmp[62:0])
    1: dst[63:0] := tmp[63:0]
    2: dst[63:0] := (0 &lt;&lt; 63) OR (tmp[62:0])
    3: dst[63:0] := (1 &lt;&lt; 63) OR (tmp[62:0])
    ESAC
    
    RETURN dst
}
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VRANGEPD - _mm256_maskz_range_pd

| VRANGEPD_YMMf64_MASKmskw_YMMf64_YMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed
double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation
control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 =
sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.

[algorithm]

DEFINE RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0]) {
    CASE opCtl[1:0] OF
    0: tmp[63:0] := (src1[63:0] &lt;= src2[63:0]) ? src1[63:0] : src2[63:0]
    1: tmp[63:0] := (src1[63:0] &lt;= src2[63:0]) ? src2[63:0] : src1[63:0]
    2: tmp[63:0] := (ABS(src1[63:0]) &lt;= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
    3: tmp[63:0] := (ABS(src1[63:0]) &lt;= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
    ESAC
    
    CASE signSelCtl[1:0] OF
    0: dst[63:0] := (src1[63] &lt;&lt; 63) OR (tmp[62:0])
    1: dst[63:0] := tmp[63:0]
    2: dst[63:0] := (0 &lt;&lt; 63) OR (tmp[62:0])
    3: dst[63:0] := (1 &lt;&lt; 63) OR (tmp[62:0])
    ESAC
    
    RETURN dst
}
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VRANGEPD - _mm256_range_pd

| VRANGEPD_YMMf64_MASKmskw_YMMf64_YMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed
double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst".
	imm8[1:0]
specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies
the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.

[algorithm]

DEFINE RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0]) {
    CASE opCtl[1:0] OF
    0: tmp[63:0] := (src1[63:0] &lt;= src2[63:0]) ? src1[63:0] : src2[63:0]
    1: tmp[63:0] := (src1[63:0] &lt;= src2[63:0]) ? src2[63:0] : src1[63:0]
    2: tmp[63:0] := (ABS(src1[63:0]) &lt;= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
    3: tmp[63:0] := (ABS(src1[63:0]) &lt;= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
    ESAC
    
    CASE signSelCtl[1:0] OF
    0: dst[63:0] := (src1[63] &lt;&lt; 63) OR (tmp[62:0])
    1: dst[63:0] := tmp[63:0]
    2: dst[63:0] := (0 &lt;&lt; 63) OR (tmp[62:0])
    3: dst[63:0] := (1 &lt;&lt; 63) OR (tmp[62:0])
    ESAC
    
    RETURN dst
}
FOR j := 0 to 3
    i := j*64
    dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VRANGEPD - _mm512_mask_range_pd

| VRANGEPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed
double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
	imm8[1:0] specifies
the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign
control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.

[algorithm]

DEFINE RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0]) {
    CASE opCtl[1:0] OF
    0: tmp[63:0] := (src1[63:0] &lt;= src2[63:0]) ? src1[63:0] : src2[63:0]
    1: tmp[63:0] := (src1[63:0] &lt;= src2[63:0]) ? src2[63:0] : src1[63:0]
    2: tmp[63:0] := (ABS(src1[63:0]) &lt;= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
    3: tmp[63:0] := (ABS(src1[63:0]) &lt;= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
    ESAC
    
    CASE signSelCtl[1:0] OF
    0: dst[63:0] := (src1[63] &lt;&lt; 63) OR (tmp[62:0])
    1: dst[63:0] := tmp[63:0]
    2: dst[63:0] := (0 &lt;&lt; 63) OR (tmp[62:0])
    3: dst[63:0] := (1 &lt;&lt; 63) OR (tmp[62:0])
    ESAC
    
    RETURN dst
}
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRANGEPD - _mm512_mask_range_round_pd

| VRANGEPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed
double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
	imm8[1:0] specifies
the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign
control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit. [sae_note]

[algorithm]

DEFINE RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0]) {
    CASE opCtl[1:0] OF
    0: tmp[63:0] := (src1[63:0] &lt;= src2[63:0]) ? src1[63:0] : src2[63:0]
    1: tmp[63:0] := (src1[63:0] &lt;= src2[63:0]) ? src2[63:0] : src1[63:0]
    2: tmp[63:0] := (ABS(src1[63:0]) &lt;= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
    3: tmp[63:0] := (ABS(src1[63:0]) &lt;= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
    ESAC
    
    CASE signSelCtl[1:0] OF
    0: dst[63:0] := (src1[63] &lt;&lt; 63) OR (tmp[62:0])
    1: dst[63:0] := tmp[63:0]
    2: dst[63:0] := (0 &lt;&lt; 63) OR (tmp[62:0])
    3: dst[63:0] := (1 &lt;&lt; 63) OR (tmp[62:0])
    ESAC
    
    RETURN dst
}
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRANGEPD - _mm512_maskz_range_pd

| VRANGEPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed
double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation
control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 =
sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.

[algorithm]

DEFINE RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0]) {
    CASE opCtl[1:0] OF
    0: tmp[63:0] := (src1[63:0] &lt;= src2[63:0]) ? src1[63:0] : src2[63:0]
    1: tmp[63:0] := (src1[63:0] &lt;= src2[63:0]) ? src2[63:0] : src1[63:0]
    2: tmp[63:0] := (ABS(src1[63:0]) &lt;= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
    3: tmp[63:0] := (ABS(src1[63:0]) &lt;= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
    ESAC
    
    CASE signSelCtl[1:0] OF
    0: dst[63:0] := (src1[63] &lt;&lt; 63) OR (tmp[62:0])
    1: dst[63:0] := tmp[63:0]
    2: dst[63:0] := (0 &lt;&lt; 63) OR (tmp[62:0])
    3: dst[63:0] := (1 &lt;&lt; 63) OR (tmp[62:0])
    ESAC
    
    RETURN dst
}
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRANGEPD - _mm512_maskz_range_round_pd

| VRANGEPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed
double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation
control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 =
sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit. [sae_note]

[algorithm]

DEFINE RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0]) {
    CASE opCtl[1:0] OF
    0: tmp[63:0] := (src1[63:0] &lt;= src2[63:0]) ? src1[63:0] : src2[63:0]
    1: tmp[63:0] := (src1[63:0] &lt;= src2[63:0]) ? src2[63:0] : src1[63:0]
    2: tmp[63:0] := (ABS(src1[63:0]) &lt;= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
    3: tmp[63:0] := (ABS(src1[63:0]) &lt;= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
    ESAC
    
    CASE signSelCtl[1:0] OF
    0: dst[63:0] := (src1[63] &lt;&lt; 63) OR (tmp[62:0])
    1: dst[63:0] := tmp[63:0]
    2: dst[63:0] := (0 &lt;&lt; 63) OR (tmp[62:0])
    3: dst[63:0] := (1 &lt;&lt; 63) OR (tmp[62:0])
    ESAC
    
    RETURN dst
}
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRANGEPD - _mm512_range_pd

| VRANGEPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed
double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst".
	imm8[1:0]
specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies
the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.

[algorithm]

DEFINE RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0]) {
    CASE opCtl[1:0] OF
    0: tmp[63:0] := (src1[63:0] &lt;= src2[63:0]) ? src1[63:0] : src2[63:0]
    1: tmp[63:0] := (src1[63:0] &lt;= src2[63:0]) ? src2[63:0] : src1[63:0]
    2: tmp[63:0] := (ABS(src1[63:0]) &lt;= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
    3: tmp[63:0] := (ABS(src1[63:0]) &lt;= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
    ESAC
    
    CASE signSelCtl[1:0] OF
    0: dst[63:0] := (src1[63] &lt;&lt; 63) OR (tmp[62:0])
    1: dst[63:0] := tmp[63:0]
    2: dst[63:0] := (0 &lt;&lt; 63) OR (tmp[62:0])
    3: dst[63:0] := (1 &lt;&lt; 63) OR (tmp[62:0])
    ESAC
    
    RETURN dst
}
FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRANGEPD - _mm512_range_round_pd

| VRANGEPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed
double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst".
	imm8[1:0]
specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies
the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
[sae_note]

[algorithm]

DEFINE RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0]) {
    CASE opCtl[1:0] OF
    0: tmp[63:0] := (src1[63:0] &lt;= src2[63:0]) ? src1[63:0] : src2[63:0]
    1: tmp[63:0] := (src1[63:0] &lt;= src2[63:0]) ? src2[63:0] : src1[63:0]
    2: tmp[63:0] := (ABS(src1[63:0]) &lt;= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
    3: tmp[63:0] := (ABS(src1[63:0]) &lt;= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
    ESAC
    
    CASE signSelCtl[1:0] OF
    0: dst[63:0] := (src1[63] &lt;&lt; 63) OR (tmp[62:0])
    1: dst[63:0] := tmp[63:0]
    2: dst[63:0] := (0 &lt;&lt; 63) OR (tmp[62:0])
    3: dst[63:0] := (1 &lt;&lt; 63) OR (tmp[62:0])
    ESAC
    
    RETURN dst
}
FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRANGEPD - _mm_mask_range_pd

| VRANGEPD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed
double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
	imm8[1:0] specifies
the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign
control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.

[algorithm]

DEFINE RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0]) {
    CASE opCtl[1:0] OF
    0: tmp[63:0] := (src1[63:0] &lt;= src2[63:0]) ? src1[63:0] : src2[63:0]
    1: tmp[63:0] := (src1[63:0] &lt;= src2[63:0]) ? src2[63:0] : src1[63:0]
    2: tmp[63:0] := (ABS(src1[63:0]) &lt;= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
    3: tmp[63:0] := (ABS(src1[63:0]) &lt;= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
    ESAC
    
    CASE signSelCtl[1:0] OF
    0: dst[63:0] := (src1[63] &lt;&lt; 63) OR (tmp[62:0])
    1: dst[63:0] := tmp[63:0]
    2: dst[63:0] := (0 &lt;&lt; 63) OR (tmp[62:0])
    3: dst[63:0] := (1 &lt;&lt; 63) OR (tmp[62:0])
    ESAC
    
    RETURN dst
}
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRANGEPD - _mm_maskz_range_pd

| VRANGEPD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed
double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation
control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 =
sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.

[algorithm]

DEFINE RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0]) {
    CASE opCtl[1:0] OF
    0: tmp[63:0] := (src1[63:0] &lt;= src2[63:0]) ? src1[63:0] : src2[63:0]
    1: tmp[63:0] := (src1[63:0] &lt;= src2[63:0]) ? src2[63:0] : src1[63:0]
    2: tmp[63:0] := (ABS(src1[63:0]) &lt;= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
    3: tmp[63:0] := (ABS(src1[63:0]) &lt;= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
    ESAC
    
    CASE signSelCtl[1:0] OF
    0: dst[63:0] := (src1[63] &lt;&lt; 63) OR (tmp[62:0])
    1: dst[63:0] := tmp[63:0]
    2: dst[63:0] := (0 &lt;&lt; 63) OR (tmp[62:0])
    3: dst[63:0] := (1 &lt;&lt; 63) OR (tmp[62:0])
    ESAC
    
    RETURN dst
}
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRANGEPD - _mm_range_pd

| VRANGEPD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed
double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst".
	imm8[1:0]
specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies
the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.

[algorithm]

DEFINE RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0]) {
    CASE opCtl[1:0] OF
    0: tmp[63:0] := (src1[63:0] &lt;= src2[63:0]) ? src1[63:0] : src2[63:0]
    1: tmp[63:0] := (src1[63:0] &lt;= src2[63:0]) ? src2[63:0] : src1[63:0]
    2: tmp[63:0] := (ABS(src1[63:0]) &lt;= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
    3: tmp[63:0] := (ABS(src1[63:0]) &lt;= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
    ESAC
    
    CASE signSelCtl[1:0] OF
    0: dst[63:0] := (src1[63] &lt;&lt; 63) OR (tmp[62:0])
    1: dst[63:0] := tmp[63:0]
    2: dst[63:0] := (0 &lt;&lt; 63) OR (tmp[62:0])
    3: dst[63:0] := (1 &lt;&lt; 63) OR (tmp[62:0])
    ESAC
    
    RETURN dst
}
FOR j := 0 to 1
    i := j*64
    dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRANGEPS - _mm256_mask_range_ps

| VRANGEPS_YMMf32_MASKmskw_YMMf32_YMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed
single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
	imm8[1:0] specifies
the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign
control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.

[algorithm]

DEFINE RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0]) {
    CASE opCtl[1:0] OF
    0: tmp[31:0] := (src1[31:0] &lt;= src2[31:0]) ? src1[31:0] : src2[31:0]
    1: tmp[31:0] := (src1[31:0] &lt;= src2[31:0]) ? src2[31:0] : src1[31:0]
    2: tmp[31:0] := (ABS(src1[31:0]) &lt;= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
    3: tmp[31:0] := (ABS(src1[31:0]) &lt;= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
    ESAC
    
    CASE signSelCtl[1:0] OF
    0: dst[31:0] := (src1[31] &lt;&lt; 31) OR (tmp[30:0])
    1: dst[31:0] := tmp[63:0]
    2: dst[31:0] := (0 &lt;&lt; 31) OR (tmp[30:0])
    3: dst[31:0] := (1 &lt;&lt; 31) OR (tmp[30:0])
    ESAC
    
    RETURN dst
}
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VRANGEPS - _mm256_maskz_range_ps

| VRANGEPS_YMMf32_MASKmskw_YMMf32_YMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed
single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation
control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 =
sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.

[algorithm]

DEFINE RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0]) {
    CASE opCtl[1:0] OF
    0: tmp[31:0] := (src1[31:0] &lt;= src2[31:0]) ? src1[31:0] : src2[31:0]
    1: tmp[31:0] := (src1[31:0] &lt;= src2[31:0]) ? src2[31:0] : src1[31:0]
    2: tmp[31:0] := (ABS(src1[31:0]) &lt;= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
    3: tmp[31:0] := (ABS(src1[31:0]) &lt;= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
    ESAC
    
    CASE signSelCtl[1:0] OF
    0: dst[31:0] := (src1[31] &lt;&lt; 31) OR (tmp[30:0])
    1: dst[31:0] := tmp[63:0]
    2: dst[31:0] := (0 &lt;&lt; 31) OR (tmp[30:0])
    3: dst[31:0] := (1 &lt;&lt; 31) OR (tmp[30:0])
    ESAC
    
    RETURN dst
}
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VRANGEPS - _mm256_range_ps

| VRANGEPS_YMMf32_MASKmskw_YMMf32_YMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed
single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst".
	imm8[1:0]
specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies
the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.

[algorithm]

DEFINE RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0]) {
    CASE opCtl[1:0] OF
    0: tmp[31:0] := (src1[31:0] &lt;= src2[31:0]) ? src1[31:0] : src2[31:0]
    1: tmp[31:0] := (src1[31:0] &lt;= src2[31:0]) ? src2[31:0] : src1[31:0]
    2: tmp[31:0] := (ABS(src1[31:0]) &lt;= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
    3: tmp[31:0] := (ABS(src1[31:0]) &lt;= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
    ESAC
    
    CASE signSelCtl[1:0] OF
    0: dst[31:0] := (src1[31] &lt;&lt; 31) OR (tmp[30:0])
    1: dst[31:0] := tmp[63:0]
    2: dst[31:0] := (0 &lt;&lt; 31) OR (tmp[30:0])
    3: dst[31:0] := (1 &lt;&lt; 31) OR (tmp[30:0])
    ESAC
    
    RETURN dst
}
FOR j := 0 to 7
    i := j*32
    dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VRANGEPS - _mm512_mask_range_ps

| VRANGEPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed
single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
	imm8[1:0] specifies
the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign
control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.

[algorithm]

DEFINE RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0]) {
    CASE opCtl[1:0] OF
    0: tmp[31:0] := (src1[31:0] &lt;= src2[31:0]) ? src1[31:0] : src2[31:0]
    1: tmp[31:0] := (src1[31:0] &lt;= src2[31:0]) ? src2[31:0] : src1[31:0]
    2: tmp[31:0] := (ABS(src1[31:0]) &lt;= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
    3: tmp[31:0] := (ABS(src1[31:0]) &lt;= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
    ESAC
    
    CASE signSelCtl[1:0] OF
    0: dst[31:0] := (src1[31] &lt;&lt; 31) OR (tmp[30:0])
    1: dst[31:0] := tmp[63:0]
    2: dst[31:0] := (0 &lt;&lt; 31) OR (tmp[30:0])
    3: dst[31:0] := (1 &lt;&lt; 31) OR (tmp[30:0])
    ESAC
    
    RETURN dst
}
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRANGEPS - _mm512_mask_range_round_ps

| VRANGEPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed
single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
	imm8[1:0] specifies
the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign
control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit. [sae_note]

[algorithm]

DEFINE RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0]) {
    CASE opCtl[1:0] OF
    0: tmp[31:0] := (src1[31:0] &lt;= src2[31:0]) ? src1[31:0] : src2[31:0]
    1: tmp[31:0] := (src1[31:0] &lt;= src2[31:0]) ? src2[31:0] : src1[31:0]
    2: tmp[31:0] := (ABS(src1[31:0]) &lt;= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
    3: tmp[31:0] := (ABS(src1[31:0]) &lt;= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
    ESAC
    
    CASE signSelCtl[1:0] OF
    0: dst[31:0] := (src1[31] &lt;&lt; 31) OR (tmp[30:0])
    1: dst[31:0] := tmp[63:0]
    2: dst[31:0] := (0 &lt;&lt; 31) OR (tmp[30:0])
    3: dst[31:0] := (1 &lt;&lt; 31) OR (tmp[30:0])
    ESAC
    
    RETURN dst
}
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRANGEPS - _mm512_maskz_range_ps

| VRANGEPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed
single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation
control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 =
sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.

[algorithm]

DEFINE RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0]) {
    CASE opCtl[1:0] OF
    0: tmp[31:0] := (src1[31:0] &lt;= src2[31:0]) ? src1[31:0] : src2[31:0]
    1: tmp[31:0] := (src1[31:0] &lt;= src2[31:0]) ? src2[31:0] : src1[31:0]
    2: tmp[31:0] := (ABS(src1[31:0]) &lt;= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
    3: tmp[31:0] := (ABS(src1[31:0]) &lt;= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
    ESAC
    
    CASE signSelCtl[1:0] OF
    0: dst[31:0] := (src1[31] &lt;&lt; 31) OR (tmp[30:0])
    1: dst[31:0] := tmp[63:0]
    2: dst[31:0] := (0 &lt;&lt; 31) OR (tmp[30:0])
    3: dst[31:0] := (1 &lt;&lt; 31) OR (tmp[30:0])
    ESAC
    
    RETURN dst
}
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRANGEPS - _mm512_maskz_range_round_ps

| VRANGEPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed
single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation
control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 =
sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit. [sae_note]

[algorithm]

DEFINE RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0]) {
    CASE opCtl[1:0] OF
    0: tmp[31:0] := (src1[31:0] &lt;= src2[31:0]) ? src1[31:0] : src2[31:0]
    1: tmp[31:0] := (src1[31:0] &lt;= src2[31:0]) ? src2[31:0] : src1[31:0]
    2: tmp[31:0] := (ABS(src1[31:0]) &lt;= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
    3: tmp[31:0] := (ABS(src1[31:0]) &lt;= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
    ESAC
    
    CASE signSelCtl[1:0] OF
    0: dst[31:0] := (src1[31] &lt;&lt; 31) OR (tmp[30:0])
    1: dst[31:0] := tmp[63:0]
    2: dst[31:0] := (0 &lt;&lt; 31) OR (tmp[30:0])
    3: dst[31:0] := (1 &lt;&lt; 31) OR (tmp[30:0])
    ESAC
    
    RETURN dst
}
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRANGEPS - _mm512_range_ps

| VRANGEPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed
single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst".
	imm8[1:0]
specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies
the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.

[algorithm]

DEFINE RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0]) {
    CASE opCtl[1:0] OF
    0: tmp[31:0] := (src1[31:0] &lt;= src2[31:0]) ? src1[31:0] : src2[31:0]
    1: tmp[31:0] := (src1[31:0] &lt;= src2[31:0]) ? src2[31:0] : src1[31:0]
    2: tmp[31:0] := (ABS(src1[31:0]) &lt;= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
    3: tmp[31:0] := (ABS(src1[31:0]) &lt;= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
    ESAC
    
    CASE signSelCtl[1:0] OF
    0: dst[31:0] := (src1[31] &lt;&lt; 31) OR (tmp[30:0])
    1: dst[31:0] := tmp[63:0]
    2: dst[31:0] := (0 &lt;&lt; 31) OR (tmp[30:0])
    3: dst[31:0] := (1 &lt;&lt; 31) OR (tmp[30:0])
    ESAC
    
    RETURN dst
}
FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRANGEPS - _mm512_range_round_ps

| VRANGEPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed
single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst".
	imm8[1:0]
specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies
the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
[sae_note]

[algorithm]

DEFINE RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0]) {
    CASE opCtl[1:0] OF
    0: tmp[31:0] := (src1[31:0] &lt;= src2[31:0]) ? src1[31:0] : src2[31:0]
    1: tmp[31:0] := (src1[31:0] &lt;= src2[31:0]) ? src2[31:0] : src1[31:0]
    2: tmp[31:0] := (ABS(src1[31:0]) &lt;= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
    3: tmp[31:0] := (ABS(src1[31:0]) &lt;= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
    ESAC
    
    CASE signSelCtl[1:0] OF
    0: dst[31:0] := (src1[31] &lt;&lt; 31) OR (tmp[30:0])
    1: dst[31:0] := tmp[63:0]
    2: dst[31:0] := (0 &lt;&lt; 31) OR (tmp[30:0])
    3: dst[31:0] := (1 &lt;&lt; 31) OR (tmp[30:0])
    ESAC
    
    RETURN dst
}
FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRANGEPS - _mm_mask_range_ps

| VRANGEPS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed
single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
	imm8[1:0] specifies
the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign
control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.

[algorithm]

DEFINE RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0]) {
    CASE opCtl[1:0] OF
    0: tmp[31:0] := (src1[31:0] &lt;= src2[31:0]) ? src1[31:0] : src2[31:0]
    1: tmp[31:0] := (src1[31:0] &lt;= src2[31:0]) ? src2[31:0] : src1[31:0]
    2: tmp[31:0] := (ABS(src1[31:0]) &lt;= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
    3: tmp[31:0] := (ABS(src1[31:0]) &lt;= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
    ESAC
    
    CASE signSelCtl[1:0] OF
    0: dst[31:0] := (src1[31] &lt;&lt; 31) OR (tmp[30:0])
    1: dst[31:0] := tmp[63:0]
    2: dst[31:0] := (0 &lt;&lt; 31) OR (tmp[30:0])
    3: dst[31:0] := (1 &lt;&lt; 31) OR (tmp[30:0])
    ESAC
    
    RETURN dst
}
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRANGEPS - _mm_maskz_range_ps

| VRANGEPS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed
single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation
control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 =
sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.

[algorithm]

DEFINE RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0]) {
    CASE opCtl[1:0] OF
    0: tmp[31:0] := (src1[31:0] &lt;= src2[31:0]) ? src1[31:0] : src2[31:0]
    1: tmp[31:0] := (src1[31:0] &lt;= src2[31:0]) ? src2[31:0] : src1[31:0]
    2: tmp[31:0] := (ABS(src1[31:0]) &lt;= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
    3: tmp[31:0] := (ABS(src1[31:0]) &lt;= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
    ESAC
    
    CASE signSelCtl[1:0] OF
    0: dst[31:0] := (src1[31] &lt;&lt; 31) OR (tmp[30:0])
    1: dst[31:0] := tmp[63:0]
    2: dst[31:0] := (0 &lt;&lt; 31) OR (tmp[30:0])
    3: dst[31:0] := (1 &lt;&lt; 31) OR (tmp[30:0])
    ESAC
    
    RETURN dst
}
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRANGEPS - _mm_range_ps

| VRANGEPS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed
single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst".
	imm8[1:0]
specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies
the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.

[algorithm]

DEFINE RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0]) {
    CASE opCtl[1:0] OF
    0: tmp[31:0] := (src1[31:0] &lt;= src2[31:0]) ? src1[31:0] : src2[31:0]
    1: tmp[31:0] := (src1[31:0] &lt;= src2[31:0]) ? src2[31:0] : src1[31:0]
    2: tmp[31:0] := (ABS(src1[31:0]) &lt;= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
    3: tmp[31:0] := (ABS(src1[31:0]) &lt;= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
    ESAC
    
    CASE signSelCtl[1:0] OF
    0: dst[31:0] := (src1[31] &lt;&lt; 31) OR (tmp[30:0])
    1: dst[31:0] := tmp[63:0]
    2: dst[31:0] := (0 &lt;&lt; 31) OR (tmp[30:0])
    3: dst[31:0] := (1 &lt;&lt; 31) OR (tmp[30:0])
    ESAC
    
    RETURN dst
}
FOR j := 0 to 3
    i := j*32
    dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRANGESD - _mm_mask_range_round_sd

| VRANGESD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for the lower
double-precision (64-bit) floating-point element in "a" and "b", store the result in the lower element of "dst"
using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper element
from "a" to the upper element of "dst".
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
compare result, 10 = clear sign bit, 11 = set sign bit. [sae_note]

[algorithm]

DEFINE RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0]) {
    CASE opCtl[1:0] OF
    0: tmp[63:0] := (src1[63:0] &lt;= src2[63:0]) ? src1[63:0] : src2[63:0]
    1: tmp[63:0] := (src1[63:0] &lt;= src2[63:0]) ? src2[63:0] : src1[63:0]
    2: tmp[63:0] := (ABS(src1[63:0]) &lt;= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
    3: tmp[63:0] := (ABS(src1[63:0]) &lt;= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
    ESAC
    
    CASE signSelCtl[1:0] OF
    0: dst[63:0] := (src1[63] &lt;&lt; 63) OR (tmp[62:0])
    1: dst[63:0] := tmp[63:0]
    2: dst[63:0] := (0 &lt;&lt; 63) OR (tmp[62:0])
    3: dst[63:0] := (1 &lt;&lt; 63) OR (tmp[62:0])
    ESAC
    
    RETURN dst
}
IF k[0]
    dst[63:0] := RANGE(a[63:0], b[63:0], imm8[1:0], imm8[3:2])
ELSE
    dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRANGESD - _mm_mask_range_sd

| VRANGESD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for the lower
double-precision (64-bit) floating-point element in "a" and "b", store the result in the lower element of "dst"
using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper element
from "a" to the upper element of "dst".
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
compare result, 10 = clear sign bit, 11 = set sign bit.

[algorithm]

DEFINE RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0]) {
    CASE opCtl[1:0] OF
    0: tmp[63:0] := (src1[63:0] &lt;= src2[63:0]) ? src1[63:0] : src2[63:0]
    1: tmp[63:0] := (src1[63:0] &lt;= src2[63:0]) ? src2[63:0] : src1[63:0]
    2: tmp[63:0] := (ABS(src1[63:0]) &lt;= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
    3: tmp[63:0] := (ABS(src1[63:0]) &lt;= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
    ESAC
    
    CASE signSelCtl[1:0] OF
    0: dst[63:0] := (src1[63] &lt;&lt; 63) OR (tmp[62:0])
    1: dst[63:0] := tmp[63:0]
    2: dst[63:0] := (0 &lt;&lt; 63) OR (tmp[62:0])
    3: dst[63:0] := (1 &lt;&lt; 63) OR (tmp[62:0])
    ESAC
    
    RETURN dst
}
IF k[0]
    dst[63:0] := RANGE(a[63:0], b[63:0], imm8[1:0], imm8[3:2])
ELSE
    dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRANGESD - _mm_maskz_range_round_sd

| VRANGESD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for the lower
double-precision (64-bit) floating-point element in "a" and "b", store the result in the lower element of "dst"
using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a"
to the upper element of "dst".
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute
max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result,
10 = clear sign bit, 11 = set sign bit. [sae_note]

[algorithm]

DEFINE RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0]) {
    CASE opCtl[1:0] OF
    0: tmp[63:0] := (src1[63:0] &lt;= src2[63:0]) ? src1[63:0] : src2[63:0]
    1: tmp[63:0] := (src1[63:0] &lt;= src2[63:0]) ? src2[63:0] : src1[63:0]
    2: tmp[63:0] := (ABS(src1[63:0]) &lt;= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
    3: tmp[63:0] := (ABS(src1[63:0]) &lt;= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
    ESAC
    
    CASE signSelCtl[1:0] OF
    0: dst[63:0] := (src1[63] &lt;&lt; 63) OR (tmp[62:0])
    1: dst[63:0] := tmp[63:0]
    2: dst[63:0] := (0 &lt;&lt; 63) OR (tmp[62:0])
    3: dst[63:0] := (1 &lt;&lt; 63) OR (tmp[62:0])
    ESAC
    
    RETURN dst
}
IF k[0]
    dst[63:0] := RANGE(a[63:0], b[63:0], imm8[1:0], imm8[3:2])
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRANGESD - _mm_maskz_range_sd

| VRANGESD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for the lower
double-precision (64-bit) floating-point element in "a" and "b", store the result in the lower element of "dst"
using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a"
to the upper element of "dst".
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute
max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result,
10 = clear sign bit, 11 = set sign bit.

[algorithm]

DEFINE RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0]) {
    CASE opCtl[1:0] OF
    0: tmp[63:0] := (src1[63:0] &lt;= src2[63:0]) ? src1[63:0] : src2[63:0]
    1: tmp[63:0] := (src1[63:0] &lt;= src2[63:0]) ? src2[63:0] : src1[63:0]
    2: tmp[63:0] := (ABS(src1[63:0]) &lt;= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
    3: tmp[63:0] := (ABS(src1[63:0]) &lt;= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
    ESAC
    
    CASE signSelCtl[1:0] OF
    0: dst[63:0] := (src1[63] &lt;&lt; 63) OR (tmp[62:0])
    1: dst[63:0] := tmp[63:0]
    2: dst[63:0] := (0 &lt;&lt; 63) OR (tmp[62:0])
    3: dst[63:0] := (1 &lt;&lt; 63) OR (tmp[62:0])
    ESAC
    
    RETURN dst
}
IF k[0]
    dst[63:0] := RANGE(a[63:0], b[63:0], imm8[1:0], imm8[3:2])
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRANGESD - _mm_range_round_sd

| VRANGESD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for the lower
double-precision (64-bit) floating-point element in "a" and "b", store the result in the lower element of
"dst", and copy the upper element from "a" to the upper element of "dst".
	imm8[1:0] specifies the operation
control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 =
sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit. [sae_note]

[algorithm]

DEFINE RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0]) {
    CASE opCtl[1:0] OF
    0: tmp[63:0] := (src1[63:0] &lt;= src2[63:0]) ? src1[63:0] : src2[63:0]
    1: tmp[63:0] := (src1[63:0] &lt;= src2[63:0]) ? src2[63:0] : src1[63:0]
    2: tmp[63:0] := (ABS(src1[63:0]) &lt;= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
    3: tmp[63:0] := (ABS(src1[63:0]) &lt;= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
    ESAC
    
    CASE signSelCtl[1:0] OF
    0: dst[63:0] := (src1[63] &lt;&lt; 63) OR (tmp[62:0])
    1: dst[63:0] := tmp[63:0]
    2: dst[63:0] := (0 &lt;&lt; 63) OR (tmp[62:0])
    3: dst[63:0] := (1 &lt;&lt; 63) OR (tmp[62:0])
    ESAC
    
    RETURN dst
}
dst[63:0] := RANGE(a[63:0], b[63:0], imm8[1:0], imm8[3:2])
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRANGESS - _mm_mask_range_round_ss

| VRANGESS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for the lower
single-precision (32-bit) floating-point element in "a" and "b", store the result in the lower element of "dst"
using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper 3 packed
elements from "a" to the upper elements of "dst".
	imm8[1:0] specifies the operation control: 00 = min, 01 =
max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign
from compare result, 10 = clear sign bit, 11 = set sign bit. [sae_note]

[algorithm]

DEFINE RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0]) {
    CASE opCtl[1:0] OF
    0: tmp[31:0] := (src1[31:0] &lt;= src2[31:0]) ? src1[31:0] : src2[31:0]
    1: tmp[31:0] := (src1[31:0] &lt;= src2[31:0]) ? src2[31:0] : src1[31:0]
    2: tmp[31:0] := (ABS(src1[31:0]) &lt;= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
    3: tmp[31:0] := (ABS(src1[31:0]) &lt;= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
    ESAC
    
    CASE signSelCtl[1:0] OF
    0: dst[31:0] := (src1[31] &lt;&lt; 31) OR (tmp[30:0])
    1: dst[31:0] := tmp[31:0]
    2: dst[31:0] := (0 &lt;&lt; 31) OR (tmp[30:0])
    3: dst[31:0] := (1 &lt;&lt; 31) OR (tmp[30:0])
    ESAC
    
    RETURN dst
}
IF k[0]
    dst[31:0] := RANGE(a[31:0], b[31:0], imm8[1:0], imm8[3:2])
ELSE
    dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRANGESS - _mm_mask_range_ss

| VRANGESS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for the lower
single-precision (32-bit) floating-point element in "a" and "b", store the result in the lower element of "dst"
using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper 3 packed
elements from "a" to the upper elements of "dst".
	imm8[1:0] specifies the operation control: 00 = min, 01 =
max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign
from compare result, 10 = clear sign bit, 11 = set sign bit.

[algorithm]

DEFINE RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0]) {
    CASE opCtl[1:0] OF
    0: tmp[31:0] := (src1[31:0] &lt;= src2[31:0]) ? src1[31:0] : src2[31:0]
    1: tmp[31:0] := (src1[31:0] &lt;= src2[31:0]) ? src2[31:0] : src1[31:0]
    2: tmp[31:0] := (ABS(src1[31:0]) &lt;= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
    3: tmp[31:0] := (ABS(src1[31:0]) &lt;= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
    ESAC
    
    CASE signSelCtl[1:0] OF
    0: dst[31:0] := (src1[31] &lt;&lt; 31) OR (tmp[30:0])
    1: dst[31:0] := tmp[31:0]
    2: dst[31:0] := (0 &lt;&lt; 31) OR (tmp[30:0])
    3: dst[31:0] := (1 &lt;&lt; 31) OR (tmp[30:0])
    ESAC
    
    RETURN dst
}
IF k[0]
    dst[31:0] := RANGE(a[31:0], b[31:0], imm8[1:0], imm8[3:2])
ELSE
    dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRANGESS - _mm_maskz_range_round_ss

| VRANGESS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for the lower
single-precision (32-bit) floating-point element in "a" and "b", store the result in the lower element of "dst"
using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements
from "a" to the upper elements of "dst".
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
compare result, 10 = clear sign bit, 11 = set sign bit. [sae_note]

[algorithm]

DEFINE RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0]) {
    CASE opCtl[1:0] OF
    0: tmp[31:0] := (src1[31:0] &lt;= src2[31:0]) ? src1[31:0] : src2[31:0]
    1: tmp[31:0] := (src1[31:0] &lt;= src2[31:0]) ? src2[31:0] : src1[31:0]
    2: tmp[31:0] := (ABS(src1[31:0]) &lt;= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
    3: tmp[31:0] := (ABS(src1[31:0]) &lt;= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
    ESAC
    
    CASE signSelCtl[1:0] OF
    0: dst[31:0] := (src1[31] &lt;&lt; 31) OR (tmp[30:0])
    1: dst[31:0] := tmp[31:0]
    2: dst[31:0] := (0 &lt;&lt; 31) OR (tmp[30:0])
    3: dst[31:0] := (1 &lt;&lt; 31) OR (tmp[30:0])
    ESAC
    
    RETURN dst
}
IF k[0]
    dst[31:0] := RANGE(a[31:0], b[31:0], imm8[1:0], imm8[3:2])
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRANGESS - _mm_maskz_range_ss

| VRANGESS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for the lower
single-precision (32-bit) floating-point element in "a" and "b", store the result in the lower element of "dst"
using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements
from "a" to the upper elements of "dst".
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
compare result, 10 = clear sign bit, 11 = set sign bit.

[algorithm]

DEFINE RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0]) {
    CASE opCtl[1:0] OF
    0: tmp[31:0] := (src1[31:0] &lt;= src2[31:0]) ? src1[31:0] : src2[31:0]
    1: tmp[31:0] := (src1[31:0] &lt;= src2[31:0]) ? src2[31:0] : src1[31:0]
    2: tmp[31:0] := (ABS(src1[31:0]) &lt;= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
    3: tmp[31:0] := (ABS(src1[31:0]) &lt;= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
    ESAC
    
    CASE signSelCtl[1:0] OF
    0: dst[31:0] := (src1[31] &lt;&lt; 31) OR (tmp[30:0])
    1: dst[31:0] := tmp[31:0]
    2: dst[31:0] := (0 &lt;&lt; 31) OR (tmp[30:0])
    3: dst[31:0] := (1 &lt;&lt; 31) OR (tmp[30:0])
    ESAC
    
    RETURN dst
}
IF k[0]
    dst[31:0] := RANGE(a[31:0], b[31:0], imm8[1:0], imm8[3:2])
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRANGESS - _mm_range_round_ss

| VRANGESS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for the lower
single-precision (32-bit) floating-point element in "a" and "b", store the result in the lower element of
"dst", and copy the upper 3 packed elements from "a" to the upper elements of "dst".
	imm8[1:0] specifies the
operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign
control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit. [sae_note]

[algorithm]

DEFINE RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0]) {
    CASE opCtl[1:0] OF
    0: tmp[31:0] := (src1[31:0] &lt;= src2[31:0]) ? src1[31:0] : src2[31:0]
    1: tmp[31:0] := (src1[31:0] &lt;= src2[31:0]) ? src2[31:0] : src1[31:0]
    2: tmp[31:0] := (ABS(src1[31:0]) &lt;= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
    3: tmp[31:0] := (ABS(src1[31:0]) &lt;= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
    ESAC
    
    CASE signSelCtl[1:0] OF
    0: dst[31:0] := (src1[31] &lt;&lt; 31) OR (tmp[30:0])
    1: dst[31:0] := tmp[31:0]
    2: dst[31:0] := (0 &lt;&lt; 31) OR (tmp[30:0])
    3: dst[31:0] := (1 &lt;&lt; 31) OR (tmp[30:0])
    ESAC
    
    RETURN dst
}
dst[31:0] := RANGE(a[31:0], b[31:0], imm8[1:0], imm8[3:2])
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VREDUCEPD - _mm256_mask_reduce_pd

| VREDUCEPD_YMMf64_MASKmskw_YMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract the reduced argument of packed double-precision (64-bit) floating-point elements in "a" by the number
of bits specified by "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set). [round_imm_note]

[algorithm]

DEFINE ReduceArgumentPD(src1[63:0], imm8[7:0]) {
    m[63:0] := FP64(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[63:0] := POW(2.0, -m) * ROUND(POW(2.0, m) * src1[63:0], imm8[3:0])
    tmp[63:0] := src1[63:0] - tmp[63:0]
    IF IsInf(tmp[63:0])
        tmp[63:0] := FP64(0.0)
    FI
    RETURN tmp[63:0]
}
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := ReduceArgumentPD(a[i+63:i], imm8[7:0])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VREDUCEPD - _mm256_maskz_reduce_pd

| VREDUCEPD_YMMf64_MASKmskw_YMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract the reduced argument of packed double-precision (64-bit) floating-point elements in "a" by the number
of bits specified by "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set). [round_imm_note]

[algorithm]

DEFINE ReduceArgumentPD(src1[63:0], imm8[7:0]) {
    m[63:0] := FP64(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[63:0] := POW(2.0, -m) * ROUND(POW(2.0, m) * src1[63:0], imm8[3:0])
    tmp[63:0] := src1[63:0] - tmp[63:0]
    IF IsInf(tmp[63:0])
        tmp[63:0] := FP64(0.0)
    FI
    RETURN tmp[63:0]
}
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := ReduceArgumentPD(a[i+63:i], imm8[7:0])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VREDUCEPD - _mm256_reduce_pd

| VREDUCEPD_YMMf64_MASKmskw_YMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract the reduced argument of packed double-precision (64-bit) floating-point elements in "a" by the number
of bits specified by "imm8", and store the results in "dst". [round_imm_note]

[algorithm]

DEFINE ReduceArgumentPD(src1[63:0], imm8[7:0]) {
    m[63:0] := FP64(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[63:0] := POW(2.0, -m) * ROUND(POW(2.0, m) * src1[63:0], imm8[3:0])
    tmp[63:0] := src1[63:0] - tmp[63:0]
    IF IsInf(tmp[63:0])
        tmp[63:0] := FP64(0.0)
    FI
    RETURN tmp[63:0]
}
FOR j := 0 to 3
    i := j*64
    dst[i+63:i] := ReduceArgumentPD(a[i+63:i], imm8[7:0])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VREDUCEPD - _mm512_mask_reduce_pd

| VREDUCEPD_ZMMf64_MASKmskw_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract the reduced argument of packed double-precision (64-bit) floating-point elements in "a" by the number
of bits specified by "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set). [round_imm_note]

[algorithm]

DEFINE ReduceArgumentPD(src1[63:0], imm8[7:0]) {
    m[63:0] := FP64(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[63:0] := POW(2.0, -m) * ROUND(POW(2.0, m) * src1[63:0], imm8[3:0])
    tmp[63:0] := src1[63:0] - tmp[63:0]
    IF IsInf(tmp[63:0])
        tmp[63:0] := FP64(0.0)
    FI
    RETURN tmp[63:0]
}
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := ReduceArgumentPD(a[i+63:i], imm8[7:0])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VREDUCEPD - _mm512_mask_reduce_round_pd

| VREDUCEPD_ZMMf64_MASKmskw_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract the reduced argument of packed double-precision (64-bit) floating-point elements in "a" by the number
of bits specified by "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set). [round_imm_note][sae_note]

[algorithm]

DEFINE ReduceArgumentPD(src1[63:0], imm8[7:0]) {
    m[63:0] := FP64(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[63:0] := POW(2.0, -m) * ROUND(POW(2.0, m) * src1[63:0], imm8[3:0])
    tmp[63:0] := src1[63:0] - tmp[63:0]
    IF IsInf(tmp[63:0])
        tmp[63:0] := FP64(0.0)
    FI
    RETURN tmp[63:0]
}
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := ReduceArgumentPD(a[i+63:i], imm8[7:0])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VREDUCEPD - _mm512_maskz_reduce_pd

| VREDUCEPD_ZMMf64_MASKmskw_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract the reduced argument of packed double-precision (64-bit) floating-point elements in "a" by the number
of bits specified by "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set). [round_imm_note]

[algorithm]

DEFINE ReduceArgumentPD(src1[63:0], imm8[7:0]) {
    m[63:0] := FP64(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[63:0] := POW(2.0, -m) * ROUND(POW(2.0, m) * src1[63:0], imm8[3:0])
    tmp[63:0] := src1[63:0] - tmp[63:0]
    IF IsInf(tmp[63:0])
        tmp[63:0] := FP64(0.0)
    FI
    RETURN tmp[63:0]
}
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := ReduceArgumentPD(a[i+63:i], imm8[7:0])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VREDUCEPD - _mm512_maskz_reduce_round_pd

| VREDUCEPD_ZMMf64_MASKmskw_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract the reduced argument of packed double-precision (64-bit) floating-point elements in "a" by the number
of bits specified by "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set). [round_imm_note][sae_note]

[algorithm]

DEFINE ReduceArgumentPD(src1[63:0], imm8[7:0]) {
    m[63:0] := FP64(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[63:0] := POW(2.0, -m) * ROUND(POW(2.0, m) * src1[63:0], imm8[3:0])
    tmp[63:0] := src1[63:0] - tmp[63:0]
    IF IsInf(tmp[63:0])
        tmp[63:0] := FP64(0.0)
    FI
    RETURN tmp[63:0]
}
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := ReduceArgumentPD(a[i+63:i], imm8[7:0])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VREDUCEPD - _mm512_reduce_pd

| VREDUCEPD_ZMMf64_MASKmskw_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract the reduced argument of packed double-precision (64-bit) floating-point elements in "a" by the number
of bits specified by "imm8", and store the results in "dst". [round_imm_note]

[algorithm]

DEFINE ReduceArgumentPD(src1[63:0], imm8[7:0]) {
    m[63:0] := FP64(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[63:0] := POW(2.0, -m) * ROUND(POW(2.0, m) * src1[63:0], imm8[3:0])
    tmp[63:0] := src1[63:0] - tmp[63:0]
    IF IsInf(tmp[63:0])
        tmp[63:0] := FP64(0.0)
    FI
    RETURN tmp[63:0]
}
FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := ReduceArgumentPD(a[i+63:i], imm8[7:0])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VREDUCEPD - _mm512_reduce_round_pd

| VREDUCEPD_ZMMf64_MASKmskw_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract the reduced argument of packed double-precision (64-bit) floating-point elements in "a" by the number
of bits specified by "imm8", and store the results in "dst". [round_imm_note][sae_note]

[algorithm]

DEFINE ReduceArgumentPD(src1[63:0], imm8[7:0]) {
    m[63:0] := FP64(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[63:0] := POW(2.0, -m) * ROUND(POW(2.0, m) * src1[63:0], imm8[3:0])
    tmp[63:0] := src1[63:0] - tmp[63:0]
    IF IsInf(tmp[63:0])
        tmp[63:0] := FP64(0.0)
    FI
    RETURN tmp[63:0]
}
FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := ReduceArgumentPD(a[i+63:i], imm8[7:0])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VREDUCEPD - _mm_mask_reduce_pd

| VREDUCEPD_XMMf64_MASKmskw_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract the reduced argument of packed double-precision (64-bit) floating-point elements in "a" by the number
of bits specified by "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set). [round_imm_note]

[algorithm]

DEFINE ReduceArgumentPD(src1[63:0], imm8[7:0]) {
    m[63:0] := FP64(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[63:0] := POW(2.0, -m) * ROUND(POW(2.0, m) * src1[63:0], imm8[3:0])
    tmp[63:0] := src1[63:0] - tmp[63:0]
    IF IsInf(tmp[63:0])
        tmp[63:0] := FP64(0.0)
    FI
    RETURN tmp[63:0]
}
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := ReduceArgumentPD(a[i+63:i], imm8[7:0])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VREDUCEPD - _mm_maskz_reduce_pd

| VREDUCEPD_XMMf64_MASKmskw_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract the reduced argument of packed double-precision (64-bit) floating-point elements in "a" by the number
of bits specified by "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set). [round_imm_note]

[algorithm]

DEFINE ReduceArgumentPD(src1[63:0], imm8[7:0]) {
    m[63:0] := FP64(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[63:0] := POW(2.0, -m) * ROUND(POW(2.0, m) * src1[63:0], imm8[3:0])
    tmp[63:0] := src1[63:0] - tmp[63:0]
    IF IsInf(tmp[63:0])
        tmp[63:0] := FP64(0.0)
    FI
    RETURN tmp[63:0]
}
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := ReduceArgumentPD(a[i+63:i], imm8[7:0])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VREDUCEPD - _mm_reduce_pd

| VREDUCEPD_XMMf64_MASKmskw_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract the reduced argument of packed double-precision (64-bit) floating-point elements in "a" by the number
of bits specified by "imm8", and store the results in "dst". [round_imm_note]

[algorithm]

DEFINE ReduceArgumentPD(src1[63:0], imm8[7:0]) {
    m[63:0] := FP64(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[63:0] := POW(2.0, -m) * ROUND(POW(2.0, m) * src1[63:0], imm8[3:0])
    tmp[63:0] := src1[63:0] - tmp[63:0]
    IF IsInf(tmp[63:0])
        tmp[63:0] := FP64(0.0)
    FI
    RETURN tmp[63:0]
}
FOR j := 0 to 1
    i := j*64
    dst[i+63:i] := ReduceArgumentPD(a[i+63:i], imm8[7:0])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VREDUCEPS - _mm256_mask_reduce_ps

| VREDUCEPS_YMMf32_MASKmskw_YMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract the reduced argument of packed single-precision (32-bit) floating-point elements in "a" by the number
of bits specified by "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set). [round_imm_note]

[algorithm]

DEFINE ReduceArgumentPS(src1[31:0], imm8[7:0]) {
    m[31:0] := FP32(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[31:0] := POW(FP32(2.0), -m) * ROUND(POW(FP32(2.0), m) * src1[31:0], imm8[3:0])
    tmp[31:0] := src1[31:0] - tmp[31:0]
    IF IsInf(tmp[31:0])
        tmp[31:0] := FP32(0.0)
    FI
    RETURN tmp[31:0]
}
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := ReduceArgumentPS(a[i+31:i], imm8[7:0])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VREDUCEPS - _mm256_maskz_reduce_ps

| VREDUCEPS_YMMf32_MASKmskw_YMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract the reduced argument of packed single-precision (32-bit) floating-point elements in "a" by the number
of bits specified by "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set). [round_imm_note]

[algorithm]

DEFINE ReduceArgumentPS(src1[31:0], imm8[7:0]) {
    m[31:0] := FP32(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[31:0] := POW(FP32(2.0), -m) * ROUND(POW(FP32(2.0), m) * src1[31:0], imm8[3:0])
    tmp[31:0] := src1[31:0] - tmp[31:0]
    IF IsInf(tmp[31:0])
        tmp[31:0] := FP32(0.0)
    FI
    RETURN tmp[31:0]
}
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := ReduceArgumentPS(a[i+31:i], imm8[7:0])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VREDUCEPS - _mm256_reduce_ps

| VREDUCEPS_YMMf32_MASKmskw_YMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract the reduced argument of packed single-precision (32-bit) floating-point elements in "a" by the number
of bits specified by "imm8", and store the results in "dst". [round_imm_note]

[algorithm]

DEFINE ReduceArgumentPS(src1[31:0], imm8[7:0]) {
    m[31:0] := FP32(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[31:0] := POW(FP32(2.0), -m) * ROUND(POW(FP32(2.0), m) * src1[31:0], imm8[3:0])
    tmp[31:0] := src1[31:0] - tmp[31:0]
    RETURN tmp[31:0]
    IF IsInf(tmp[31:0])
        tmp[31:0] := FP32(0.0)
    FI
}
FOR j := 0 to 7
    i := j*32
    dst[i+31:i] := ReduceArgumentPS(a[i+31:i], imm8[7:0])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VREDUCEPS - _mm512_mask_reduce_ps

| VREDUCEPS_ZMMf32_MASKmskw_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract the reduced argument of packed single-precision (32-bit) floating-point elements in "a" by the number
of bits specified by "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set). [round_imm_note]

[algorithm]

DEFINE ReduceArgumentPS(src1[31:0], imm8[7:0]) {
    m[31:0] := FP32(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[31:0] := POW(FP32(2.0), -m) * ROUND(POW(FP32(2.0), m) * src1[31:0], imm8[3:0])
    tmp[31:0] := src1[31:0] - tmp[31:0]
    IF IsInf(tmp[31:0])
        tmp[31:0] := FP32(0.0)
    FI
    RETURN tmp[31:0]
}
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := ReduceArgumentPS(a[i+31:i], imm8[7:0])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VREDUCEPS - _mm512_mask_reduce_round_ps

| VREDUCEPS_ZMMf32_MASKmskw_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract the reduced argument of packed single-precision (32-bit) floating-point elements in "a" by the number
of bits specified by "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set). [round_imm_note][sae_note]

[algorithm]

DEFINE ReduceArgumentPS(src1[31:0], imm8[7:0]) {
    m[31:0] := FP32(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[31:0] := POW(FP32(2.0), -m) * ROUND(POW(FP32(2.0), m) * src1[31:0], imm8[3:0])
    tmp[31:0] := src1[31:0] - tmp[31:0]
    IF IsInf(tmp[31:0])
        tmp[31:0] := FP32(0.0)
    FI
    RETURN tmp[31:0]
}
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := ReduceArgumentPS(a[i+31:i], imm8[7:0])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VREDUCEPS - _mm512_maskz_reduce_ps

| VREDUCEPS_ZMMf32_MASKmskw_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract the reduced argument of packed single-precision (32-bit) floating-point elements in "a" by the number
of bits specified by "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set). [round_imm_note]

[algorithm]

DEFINE ReduceArgumentPS(src1[31:0], imm8[7:0]) {
    m[31:0] := FP32(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[31:0] := POW(FP32(2.0), -m) * ROUND(POW(FP32(2.0), m) * src1[31:0], imm8[3:0])
    tmp[31:0] := src1[31:0] - tmp[31:0]
    IF IsInf(tmp[31:0])
        tmp[31:0] := FP32(0.0)
    FI
    RETURN tmp[31:0]
}
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := ReduceArgumentPS(a[i+31:i], imm8[7:0])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VREDUCEPS - _mm512_maskz_reduce_round_ps

| VREDUCEPS_ZMMf32_MASKmskw_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract the reduced argument of packed single-precision (32-bit) floating-point elements in "a" by the number
of bits specified by "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set). [round_imm_note][sae_note]

[algorithm]

DEFINE ReduceArgumentPS(src1[31:0], imm8[7:0]) {
    m[31:0] := FP32(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[31:0] := POW(FP32(2.0), -m) * ROUND(POW(FP32(2.0), m) * src1[31:0], imm8[3:0])
    tmp[31:0] := src1[31:0] - tmp[31:0]
    IF IsInf(tmp[31:0])
        tmp[31:0] := FP32(0.0)
    FI
    RETURN tmp[31:0]
}
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := ReduceArgumentPS(a[i+31:i], imm8[7:0])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VREDUCEPS - _mm512_reduce_ps

| VREDUCEPS_ZMMf32_MASKmskw_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract the reduced argument of packed single-precision (32-bit) floating-point elements in "a" by the number
of bits specified by "imm8", and store the results in "dst". [round_imm_note]

[algorithm]

DEFINE ReduceArgumentPS(src1[31:0], imm8[7:0]) {
    m[31:0] := FP32(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[31:0] := POW(FP32(2.0), -m) * ROUND(POW(FP32(2.0), m) * src1[31:0], imm8[3:0])
    tmp[31:0] := src1[31:0] - tmp[31:0]
    IF IsInf(tmp[31:0])
        tmp[31:0] := FP32(0.0)
    FI
    RETURN tmp[31:0]
}
FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := ReduceArgumentPS(a[i+31:i], imm8[7:0])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VREDUCEPS - _mm512_reduce_round_ps

| VREDUCEPS_ZMMf32_MASKmskw_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract the reduced argument of packed single-precision (32-bit) floating-point elements in "a" by the number
of bits specified by "imm8", and store the results in "dst". [round_imm_note][sae_note]

[algorithm]

DEFINE ReduceArgumentPS(src1[31:0], imm8[7:0]) {
    m[31:0] := FP32(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[31:0] := POW(FP32(2.0), -m) * ROUND(POW(FP32(2.0), m) * src1[31:0], imm8[3:0])
    tmp[31:0] := src1[31:0] - tmp[31:0]
    IF IsInf(tmp[31:0])
        tmp[31:0] := FP32(0.0)
    FI
    RETURN tmp[31:0]
}
FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := ReduceArgumentPS(a[i+31:i], imm8[7:0])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VREDUCEPS - _mm_mask_reduce_ps

| VREDUCEPS_XMMf32_MASKmskw_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract the reduced argument of packed single-precision (32-bit) floating-point elements in "a" by the number
of bits specified by "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set). [round_imm_note]

[algorithm]

DEFINE ReduceArgumentPS(src1[31:0], imm8[7:0]) {
    m[31:0] := FP32(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[31:0] := POW(FP32(2.0), -m) * ROUND(POW(FP32(2.0), m) * src1[31:0], imm8[3:0])
    tmp[31:0] := src1[31:0] - tmp[31:0]
    IF IsInf(tmp[31:0])
        tmp[31:0] := FP32(0.0)
    FI
    RETURN tmp[31:0]
}
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := ReduceArgumentPS(a[i+31:i], imm8[7:0])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VREDUCEPS - _mm_maskz_reduce_ps

| VREDUCEPS_XMMf32_MASKmskw_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract the reduced argument of packed single-precision (32-bit) floating-point elements in "a" by the number
of bits specified by "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set). [round_imm_note]

[algorithm]

DEFINE ReduceArgumentPS(src1[31:0], imm8[7:0]) {
    m[31:0] := FP32(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[31:0] := POW(FP32(2.0), -m) * ROUND(POW(FP32(2.0), m) * src1[31:0], imm8[3:0])
    tmp[31:0] := src1[31:0] - tmp[31:0]
    IF IsInf(tmp[31:0])
        tmp[31:0] := FP32(0.0)
    FI
    RETURN tmp[31:0]
}
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := ReduceArgumentPS(a[i+31:i], imm8[7:0])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VREDUCEPS - _mm_reduce_ps

| VREDUCEPS_XMMf32_MASKmskw_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract the reduced argument of packed single-precision (32-bit) floating-point elements in "a" by the number
of bits specified by "imm8", and store the results in "dst". [round_imm_note]

[algorithm]

DEFINE ReduceArgumentPS(src1[31:0], imm8[7:0]) {
    m[31:0] := FP32(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[31:0] := POW(FP32(2.0), -m) * ROUND(POW(FP32(2.0), m) * src1[31:0], imm8[3:0])
    tmp[31:0] := src1[31:0] - tmp[31:0]
    IF IsInf(tmp[31:0])
        tmp[31:0] := FP32(0.0)
    FI
    RETURN tmp[31:0]
}
FOR j := 0 to 3
    i := j*32
    dst[i+31:i] := ReduceArgumentPS(a[i+31:i], imm8[7:0])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VREDUCESD - _mm_mask_reduce_sd

| VREDUCESD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract the reduced argument of the lower double-precision (64-bit) floating-point element in "b" by the
number of bits specified by "imm8", store the result in the lower element of "dst" using writemask "k" (the
element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper
element of "dst". [round_imm_note]

[algorithm]

DEFINE ReduceArgumentPD(src1[63:0], imm8[7:0]) {
    m[63:0] := FP64(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[63:0] := POW(2.0, -m) * ROUND(POW(2.0, m) * src1[63:0], imm8[3:0])
    tmp[63:0] := src1[63:0] - tmp[63:0]
    IF IsInf(tmp[63:0])
        tmp[63:0] := FP64(0.0)
    FI
    RETURN tmp[63:0]
}
IF k[0]
    dst[63:0] := ReduceArgumentPD(b[63:0], imm8[7:0])
ELSE
    dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VREDUCESD - _mm_mask_reduce_round_sd

| VREDUCESD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract the reduced argument of the lower double-precision (64-bit) floating-point element in "b" by the
number of bits specified by "imm8", store the result in the lower element of "dst" using writemask "k" (the
element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper
element of "dst". [round_imm_note][sae_note]

[algorithm]

DEFINE ReduceArgumentPD(src1[63:0], imm8[7:0]) {
    m[63:0] := FP64(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[63:0] := POW(2.0, -m) * ROUND(POW(2.0, m) * src1[63:0], imm8[3:0])
    tmp[63:0] := src1[63:0] - tmp[63:0]
    IF IsInf(tmp[63:0])
        tmp[63:0] := FP64(0.0)
    FI
    RETURN tmp[63:0]
}
IF k[0]
    dst[63:0] := ReduceArgumentPD(b[63:0], imm8[7:0])
ELSE
    dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VREDUCESD - _mm_maskz_reduce_sd

| VREDUCESD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract the reduced argument of the lower double-precision (64-bit) floating-point element in "b" by the
number of bits specified by "imm8", store the result in the lower element of "dst" using zeromask "k" (the
element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of
"dst". [round_imm_note]

[algorithm]

DEFINE ReduceArgumentPD(src1[63:0], imm8[7:0]) {
    m[63:0] := FP64(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[63:0] := POW(2.0, -m) * ROUND(POW(2.0, m) * src1[63:0], imm8[3:0])
    tmp[63:0] := src1[63:0] - tmp[63:0]
    IF IsInf(tmp[63:0])
        tmp[63:0] := FP64(0.0)
    FI
    RETURN tmp[63:0]
}
IF k[0]
    dst[63:0] := ReduceArgumentPD(b[63:0], imm8[7:0])
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VREDUCESD - _mm_maskz_reduce_round_sd

| VREDUCESD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract the reduced argument of the lower double-precision (64-bit) floating-point element in "b" by the
number of bits specified by "imm8", store the result in the lower element of "dst" using zeromask "k" (the
element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of
"dst". [round_imm_note][sae_note]

[algorithm]

DEFINE ReduceArgumentPD(src1[63:0], imm8[7:0]) {
    m[63:0] := FP64(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[63:0] := POW(2.0, -m) * ROUND(POW(2.0, m) * src1[63:0], imm8[3:0])
    tmp[63:0] := src1[63:0] - tmp[63:0]
    IF IsInf(tmp[63:0])
        tmp[63:0] := FP64(0.0)
    FI
    RETURN tmp[63:0]
}
IF k[0]
    dst[63:0] := ReduceArgumentPD(b[63:0], imm8[7:0])
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VREDUCESD - _mm_reduce_sd

| VREDUCESD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract the reduced argument of the lower double-precision (64-bit) floating-point element in "b" by the
number of bits specified by "imm8", store the result in the lower element of "dst", and copy the upper element
from "a" to the upper element of "dst". [round_imm_note]

[algorithm]

DEFINE ReduceArgumentPD(src1[63:0], imm8[7:0]) {
    m[63:0] := FP64(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[63:0] := POW(2.0, -m) * ROUND(POW(2.0, m) * src1[63:0], imm8[3:0])
    tmp[63:0] := src1[63:0] - tmp[63:0]
    IF IsInf(tmp[63:0])
        tmp[63:0] := FP64(0.0)
    FI
    RETURN tmp[63:0]
}
dst[63:0] := ReduceArgumentPD(b[63:0], imm8[7:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VREDUCESD - _mm_reduce_round_sd

| VREDUCESD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract the reduced argument of the lower double-precision (64-bit) floating-point element in "b" by the
number of bits specified by "imm8", store the result in the lower element of "dst", and copy the upper element
from "a" to the upper element of "dst". [round_imm_note][sae_note]

[algorithm]

DEFINE ReduceArgumentPD(src1[63:0], imm8[7:0]) {
    m[63:0] := FP64(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[63:0] := POW(2.0, -m) * ROUND(POW(2.0, m) * src1[63:0], imm8[3:0])
    tmp[63:0] := src1[63:0] - tmp[63:0]
    IF IsInf(tmp[63:0])
        tmp[63:0] := FP64(0.0)
    FI
    RETURN tmp[63:0]
}
dst[63:0] := ReduceArgumentPD(b[63:0], imm8[7:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VREDUCESS - _mm_mask_reduce_ss

| VREDUCESS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract the reduced argument of the lower single-precision (32-bit) floating-point element in "b" by the
number of bits specified by "imm8", store the result in the lower element of "dst" using writemask "k" (the
element is copied from "src" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the
upper elements of "dst". [round_imm_note]

[algorithm]

DEFINE ReduceArgumentPS(src1[31:0], imm8[7:0]) {
    m[31:0] := FP32(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[31:0] := POW(FP32(2.0), -m) * ROUND(POW(FP32(2.0), m) * src1[31:0], imm8[3:0])
    tmp[31:0] := src1[31:0] - tmp[31:0]
    IF IsInf(tmp[31:0])
        tmp[31:0] := FP32(0.0)
    FI
    RETURN tmp[31:0]
}
IF k[0]
    dst[31:0] := ReduceArgumentPS(b[31:0], imm8[7:0])
ELSE
    dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VREDUCESS - _mm_mask_reduce_round_ss

| VREDUCESS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract the reduced argument of the lower single-precision (32-bit) floating-point element in "b" by the
number of bits specified by "imm8", store the result in the lower element of "dst" using writemask "k" (the
element is copied from "src" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the
upper elements of "dst". [round_imm_note][sae_note]

[algorithm]

DEFINE ReduceArgumentPS(src1[31:0], imm8[7:0]) {
    m[31:0] := FP32(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[31:0] := POW(FP32(2.0), -m) * ROUND(POW(FP32(2.0), m) * src1[31:0], imm8[3:0])
    tmp[31:0] := src1[31:0] - tmp[31:0]
    IF IsInf(tmp[31:0])
        tmp[31:0] := FP32(0.0)
    FI
    RETURN tmp[31:0]
}
IF k[0]
    dst[31:0] := ReduceArgumentPS(b[31:0], imm8[7:0])
ELSE
    dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VREDUCESS - _mm_maskz_reduce_ss

| VREDUCESS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract the reduced argument of the lower single-precision (32-bit) floating-point element in "b" by the
number of bits specified by "imm8", store the result in the lower element of "dst" using zeromask "k" (the
element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper
elements of "dst". [round_imm_note]

[algorithm]

DEFINE ReduceArgumentPS(src1[31:0], imm8[7:0]) {
    m[31:0] := FP32(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[31:0] := POW(FP32(2.0), -m) * ROUND(POW(FP32(2.0), m) * src1[31:0], imm8[3:0])
    tmp[31:0] := src1[31:0] - tmp[31:0]
    IF IsInf(tmp[31:0])
        tmp[31:0] := FP32(0.0)
    FI
    RETURN tmp[31:0]
}
IF k[0]
    dst[31:0] := ReduceArgumentPS(b[31:0], imm8[7:0])
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VREDUCESS - _mm_maskz_reduce_round_ss

| VREDUCESS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract the reduced argument of the lower single-precision (32-bit) floating-point element in "b" by the
number of bits specified by "imm8", store the result in the lower element of "dst" using zeromask "k" (the
element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper
elements of "dst". [round_imm_note][sae_note]

[algorithm]

DEFINE ReduceArgumentPS(src1[31:0], imm8[7:0]) {
    m[31:0] := FP32(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[31:0] := POW(FP32(2.0), -m) * ROUND(POW(FP32(2.0), m) * src1[31:0], imm8[3:0])
    tmp[31:0] := src1[31:0] - tmp[31:0]
    IF IsInf(tmp[31:0])
        tmp[31:0] := FP32(0.0)
    FI
    RETURN tmp[31:0]
}
IF k[0]
    dst[31:0] := ReduceArgumentPS(b[31:0], imm8[7:0])
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VREDUCESS - _mm_reduce_ss

| VREDUCESS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract the reduced argument of the lower single-precision (32-bit) floating-point element in "b" by the
number of bits specified by "imm8", store the result in the lower element of "dst", and copy the upper 3 packed
elements from "a" to the upper elements of "dst". [round_imm_note]

[algorithm]

DEFINE ReduceArgumentPS(src1[31:0], imm8[7:0]) {
    m[31:0] := FP32(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[31:0] := POW(FP32(2.0), -m) * ROUND(POW(FP32(2.0), m) * src1[31:0], imm8[3:0])
    tmp[31:0] := src1[31:0] - tmp[31:0]
    IF IsInf(tmp[31:0])
        tmp[31:0] := FP32(0.0)
    FI
    RETURN tmp[31:0]
}
dst[31:0] := ReduceArgumentPS(b[31:0], imm8[7:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VREDUCESS - _mm_reduce_round_ss

| VREDUCESS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract the reduced argument of the lower single-precision (32-bit) floating-point element in "b" by the
number of bits specified by "imm8", store the result in the lower element of "dst", and copy the upper 3 packed
elements from "a" to the upper elements of "dst". [round_imm_note][sae_note]

[algorithm]

DEFINE ReduceArgumentPS(src1[31:0], imm8[7:0]) {
    m[31:0] := FP32(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[31:0] := POW(FP32(2.0), -m) * ROUND(POW(FP32(2.0), m) * src1[31:0], imm8[3:0])
    tmp[31:0] := src1[31:0] - tmp[31:0]
    IF IsInf(tmp[31:0])
        tmp[31:0] := FP32(0.0)
    FI
    RETURN tmp[31:0]
}
dst[31:0] := ReduceArgumentPS(b[31:0], imm8[7:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VXORPD - _mm256_mask_xor_pd

| VXORPD_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XOR of packed double-precision (64-bit) floating-point elements in "a" and "b", and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VXORPD - _mm256_maskz_xor_pd

| VXORPD_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XOR of packed double-precision (64-bit) floating-point elements in "a" and "b", and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VXORPD - _mm512_mask_xor_pd

| VXORPD_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XOR of packed double-precision (64-bit) floating-point elements in "a" and "b", and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VXORPD - _mm512_maskz_xor_pd

| VXORPD_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XOR of packed double-precision (64-bit) floating-point elements in "a" and "b", and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VXORPD - _mm512_xor_pd

| VXORPD_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XOR of packed double-precision (64-bit) floating-point elements in "a" and "b", and store
the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VXORPD - _mm_mask_xor_pd

| VXORPD_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XOR of packed double-precision (64-bit) floating-point elements in "a" and "b", and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VXORPD - _mm_maskz_xor_pd

| VXORPD_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XOR of packed double-precision (64-bit) floating-point elements in "a" and "b", and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VXORPS - _mm256_mask_xor_ps

| VXORPS_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XOR of packed single-precision (32-bit) floating-point elements in "a" and "b", and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VXORPS - _mm256_maskz_xor_ps

| VXORPS_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XOR of packed single-precision (32-bit) floating-point elements in "a" and "b", and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VXORPS - _mm512_mask_xor_ps

| VXORPS_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XOR of packed single-precision (32-bit) floating-point elements in "a" and "b", and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VXORPS - _mm512_maskz_xor_ps

| VXORPS_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XOR of packed single-precision (32-bit) floating-point elements in "a" and "b", and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VXORPS - _mm512_xor_ps

| VXORPS_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XOR of packed single-precision (32-bit) floating-point elements in "a" and "b", and store
the results in "dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VXORPS - _mm_mask_xor_ps

| VXORPS_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XOR of packed single-precision (32-bit) floating-point elements in "a" and "b", and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VXORPS - _mm_maskz_xor_ps

| VXORPS_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XOR of packed single-precision (32-bit) floating-point elements in "a" and "b", and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## KADDB - _kadd_mask8

| KADDB_MASKmskw_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Add 8-bit masks in "a" and "b", and store the result in "k".

[algorithm]

k[7:0] := a[7:0] + b[7:0]
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## KADDW - _kadd_mask16

| KADDW_MASKmskw_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Add 16-bit masks in "a" and "b", and store the result in "k".

[algorithm]

k[15:0] := a[15:0] + b[15:0]
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## KANDB - _kand_mask8

| KANDB_MASKmskw_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of 8-bit masks "a" and "b", and store the result in "k".

[algorithm]

k[7:0] := a[7:0] AND b[7:0]
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## KANDNB - _kandn_mask8

| KANDNB_MASKmskw_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of 8-bit masks "a" and then AND with "b", and store the result in "k".

[algorithm]

k[7:0] := (NOT a[7:0]) AND b[7:0]
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## KNOTB - _knot_mask8

| KNOTB_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of 8-bit mask "a", and store the result in "k".

[algorithm]

k[7:0] := NOT a[7:0]
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## KORB - _kor_mask8

| KORB_MASKmskw_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of 8-bit masks "a" and "b", and store the result in "k".

[algorithm]

k[7:0] := a[7:0] OR b[7:0]
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## KXNORB - _kxnor_mask8

| KXNORB_MASKmskw_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XNOR of 8-bit masks "a" and "b", and store the result in "k".

[algorithm]

k[7:0] := NOT (a[7:0] XOR b[7:0])
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## KXORB - _kxor_mask8

| KXORB_MASKmskw_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XOR of 8-bit masks "a" and "b", and store the result in "k".

[algorithm]

k[7:0] := a[7:0] XOR b[7:0]
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## KSHIFTLB - _kshiftli_mask8

| KSHIFTLB_MASKmskw_MASKmskw_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift the bits of 8-bit mask "a" left by "count" while shifting in zeros, and store the least significant 8
bits of the result in "k".

[algorithm]

k[MAX:0] := 0
IF count[7:0] &lt;= 7
    k[7:0] := a[7:0] &lt;&lt; count[7:0]
FI

--------------------------------------------------------------------------------------------------------------

## KSHIFTRB - _kshiftri_mask8

| KSHIFTRB_MASKmskw_MASKmskw_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift the bits of 8-bit mask "a" right by "count" while shifting in zeros, and store the least significant 8
bits of the result in "k".

[algorithm]

k[MAX:0] := 0
IF count[7:0] &lt;= 7
    k[7:0] := a[7:0] &gt;&gt; count[7:0]
FI

--------------------------------------------------------------------------------------------------------------

## KMOVB - _load_mask8

| KMOVB_MASKmskw_MEMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Load 8-bit mask from memory into "k".

[algorithm]

k[7:0] := MEM[mem_addr+7:mem_addr]

--------------------------------------------------------------------------------------------------------------

## KMOVB - _store_mask8

| KMOVB_MEMu8_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Store 8-bit mask from "a" into memory.

[algorithm]

MEM[mem_addr+7:mem_addr] := a[7:0]

--------------------------------------------------------------------------------------------------------------

## KORTESTB - _kortest_mask8_u8

| KORTESTB_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of 8-bit masks "a" and "b". If the result is all zeros, store 1 in "dst", otherwise
store 0 in "dst". If the result is all ones, store 1 in "all_ones", otherwise store 0 in "all_ones".

[algorithm]

tmp[7:0] := a[7:0] OR b[7:0]
IF tmp[7:0] == 0x0
    dst := 1
ELSE
    dst := 0
FI
IF tmp[7:0] == 0xFF
    MEM[all_ones+7:all_ones] := 1
ELSE
    MEM[all_ones+7:all_ones] := 0
FI

--------------------------------------------------------------------------------------------------------------

## KORTESTB - _kortestz_mask8_u8

| KORTESTB_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of 8-bit masks "a" and "b". If the result is all zeroes, store 1 in "dst", otherwise
store 0 in "dst".

[algorithm]

tmp[7:0] := a[7:0] OR b[7:0]
IF tmp[7:0] == 0x0
    dst := 1
ELSE
    dst := 0
FI

--------------------------------------------------------------------------------------------------------------

## KORTESTB - _kortestc_mask8_u8

| KORTESTB_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of 8-bit masks "a" and "b". If the result is all ones, store 1 in "dst", otherwise
store 0 in "dst".

[algorithm]

tmp[7:0] := a[7:0] OR b[7:0]
IF tmp[7:0] == 0xFF
    dst := 1
ELSE
    dst := 0
FI

--------------------------------------------------------------------------------------------------------------

## KTESTB - _ktest_mask8_u8

| KTESTB_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of 8-bit masks "a" and "b", and if the result is all zeros, store 1 in "dst",
otherwise store 0 in "dst". Compute the bitwise NOT of "a" and then AND with "b", if the result is all zeros,
store 1 in "and_not", otherwise store 0 in "and_not".

[algorithm]

tmp1[7:0] := a[7:0] AND b[7:0]
IF tmp1[7:0] == 0x0
    dst := 1
ELSE
    dst := 0
FI
tmp2[7:0] := (NOT a[7:0]) AND b[7:0]
IF tmp2[7:0] == 0x0
    MEM[and_not+7:and_not] := 1
ELSE
    MEM[and_not+7:and_not] := 0
FI

--------------------------------------------------------------------------------------------------------------

## KTESTB - _ktestz_mask8_u8

| KTESTB_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of 8-bit masks "a" and "b", and if the result is all zeros, store 1 in "dst",
otherwise store 0 in "dst".

[algorithm]

tmp[7:0] := a[7:0] AND b[7:0]
IF tmp[7:0] == 0x0
    dst := 1
ELSE
    dst := 0
FI

--------------------------------------------------------------------------------------------------------------

## KTESTB - _ktestc_mask8_u8

| KTESTB_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of 8-bit mask "a" and then AND with "b", if the result is all zeroes, store 1 in
"dst", otherwise store 0 in "dst".

[algorithm]

tmp[7:0] := (NOT a[7:0]) AND b[7:0]
IF tmp[7:0] == 0x0
    dst := 1
ELSE
    dst := 0
FI

--------------------------------------------------------------------------------------------------------------

## KTESTW - _ktest_mask16_u8

| KTESTW_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of 16-bit masks "a" and "b", and if the result is all zeros, store 1 in "dst",
otherwise store 0 in "dst". Compute the bitwise NOT of "a" and then AND with "b", if the result is all zeros,
store 1 in "and_not", otherwise store 0 in "and_not".

[algorithm]

tmp1[15:0] := a[15:0] AND b[15:0]
IF tmp1[15:0] == 0x0
    dst := 1
ELSE
    dst := 0
FI
tmp2[15:0] := (NOT a[15:0]) AND b[15:0]
IF tmp2[15:0] == 0x0
    MEM[and_not+7:and_not] := 1
ELSE
    MEM[and_not+7:and_not] := 0
FI

--------------------------------------------------------------------------------------------------------------

## KTESTW - _ktestz_mask16_u8

| KTESTW_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of 16-bit masks "a" and "b", and if the result is all zeros, store 1 in "dst",
otherwise store 0 in "dst".

[algorithm]

tmp[15:0] := a[15:0] AND b[15:0]
IF tmp[15:0] == 0x0
    dst := 1
ELSE
    dst := 0
FI

--------------------------------------------------------------------------------------------------------------

## KTESTW - _ktestc_mask16_u8

| KTESTW_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of 16-bit mask "a" and then AND with "b", if the result is all zeroes, store 1 in
"dst", otherwise store 0 in "dst".

[algorithm]

tmp[15:0] := (NOT a[15:0]) AND b[15:0]
IF tmp[15:0] == 0x0
    dst := 1
ELSE
    dst := 0
FI

--------------------------------------------------------------------------------------------------------------

## KMOVB - _cvtmask8_u32

| KMOVB_GPR32u32_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Convert 8-bit mask "a" into an integer value, and store the result in "dst".

[algorithm]

dst := ZeroExtend32(a[7:0])

--------------------------------------------------------------------------------------------------------------

## KMOVB - _cvtu32_mask8

| KMOVB_MASKmskw_GPR32u32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert integer value "a" into an 8-bit mask, and store the result in "k".

[algorithm]

k := a[7:0]

--------------------------------------------------------------------------------------------------------------

## VEXP2PS - _mm512_exp2a23_round_ps

| VEXP2PS_ZMMf32_MASKmskw_ZMMf32_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate exponential value of 2 raised to the power of packed single-precision (32-bit)
floating-point elements in "a", and store the results in "dst". The maximum relative error for this
approximation is less than 2^-23. [sae_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := POW(FP32(2.0), a[i+31:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VEXP2PS - _mm512_exp2a23_ps

| VEXP2PS_ZMMf32_MASKmskw_ZMMf32_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate exponential value of 2 raised to the power of packed single-precision (32-bit)
floating-point elements in "a", and store the results in "dst". The maximum relative error for this
approximation is less than 2^-23.

[algorithm]

FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := POW(FP32(2.0), a[i+31:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VEXP2PS - _mm512_mask_exp2a23_round_ps

| VEXP2PS_ZMMf32_MASKmskw_ZMMf32_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate exponential value of 2 raised to the power of packed single-precision (32-bit)
floating-point elements in "a", and store the results in "dst" using writemask "k" (elements are copied from
"src" when the corresponding mask bit is not set). The maximum relative error for this approximation is less
than 2^-23. [sae_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := POW(FP32(2.0), a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VEXP2PS - _mm512_mask_exp2a23_ps

| VEXP2PS_ZMMf32_MASKmskw_ZMMf32_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate exponential value of 2 raised to the power of packed single-precision (32-bit)
floating-point elements in "a", and store the results in "dst" using writemask "k" (elements are copied from
"src" when the corresponding mask bit is not set). The maximum relative error for this approximation is less
than 2^-23.

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := POW(FP32(2.0), a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VEXP2PS - _mm512_maskz_exp2a23_round_ps

| VEXP2PS_ZMMf32_MASKmskw_ZMMf32_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate exponential value of 2 raised to the power of packed single-precision (32-bit)
floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-23.
[sae_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := POW(FP32(2.0), a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VEXP2PS - _mm512_maskz_exp2a23_ps

| VEXP2PS_ZMMf32_MASKmskw_ZMMf32_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate exponential value of 2 raised to the power of packed single-precision (32-bit)
floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-23.

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := POW(FP32(2.0), a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VEXP2PD - _mm512_exp2a23_round_pd

| VEXP2PD_ZMMf64_MASKmskw_ZMMf64_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate exponential value of 2 raised to the power of packed double-precision (64-bit)
floating-point elements in "a", and store the results in "dst". The maximum relative error for this
approximation is less than 2^-23. [sae_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := POW(2.0, a[i+63:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VEXP2PD - _mm512_exp2a23_pd

| VEXP2PD_ZMMf64_MASKmskw_ZMMf64_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate exponential value of 2 raised to the power of packed double-precision (64-bit)
floating-point elements in "a", and store the results in "dst". The maximum relative error for this
approximation is less than 2^-23.

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := POW(2.0, a[i+63:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VEXP2PD - _mm512_mask_exp2a23_round_pd

| VEXP2PD_ZMMf64_MASKmskw_ZMMf64_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate exponential value of 2 raised to the power of packed double-precision (64-bit)
floating-point elements in "a", and store the results in "dst" using writemask "k" (elements are copied from
"src" when the corresponding mask bit is not set). The maximum relative error for this approximation is less
than 2^-23. [sae_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := POW(2.0, a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VEXP2PD - _mm512_mask_exp2a23_pd

| VEXP2PD_ZMMf64_MASKmskw_ZMMf64_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate exponential value of 2 raised to the power of packed double-precision (64-bit)
floating-point elements in "a", and store the results in "dst" using writemask "k" (elements are copied from
"src" when the corresponding mask bit is not set). The maximum relative error for this approximation is less
than 2^-23.

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := POW(2.0, a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VEXP2PD - _mm512_maskz_exp2a23_round_pd

| VEXP2PD_ZMMf64_MASKmskw_ZMMf64_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate exponential value of 2 raised to the power of packed double-precision (64-bit)
floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-23.
[sae_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := POW(2.0, a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VEXP2PD - _mm512_maskz_exp2a23_pd

| VEXP2PD_ZMMf64_MASKmskw_ZMMf64_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate exponential value of 2 raised to the power of packed double-precision (64-bit)
floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-23.

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := POW(2.0, a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP28SD - _mm_rcp28_round_sd

| VRCP28SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of the lower double-precision (64-bit) floating-point element in "b", store
the result in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst".
The maximum relative error for this approximation is less than 2^-28. [sae_note]

[algorithm]

dst[63:0] := (1.0 / b[63:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP28SD - _mm_rcp28_sd

| VRCP28SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of the lower double-precision (64-bit) floating-point element in "b", store
the result in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst".
The maximum relative error for this approximation is less than 2^-28.

[algorithm]

dst[63:0] := (1.0 / b[63:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP28SD - _mm_mask_rcp28_round_sd

| VRCP28SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of the lower double-precision (64-bit) floating-point element in "b", store
the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0
is not set), and copy the upper element from "a" to the upper element of "dst". The maximum relative error for
this approximation is less than 2^-28. [sae_note]

[algorithm]

IF k[0]
    dst[63:0] := (1.0 / b[63:0])
ELSE
    dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP28SD - _mm_mask_rcp28_sd

| VRCP28SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of the lower double-precision (64-bit) floating-point element in "b", store
the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0
is not set), and copy the upper element from "a" to the upper element of "dst". The maximum relative error for
this approximation is less than 2^-28.

[algorithm]

IF k[0]
    dst[63:0] := (1.0 / b[63:0])
ELSE
    dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP28SD - _mm_maskz_rcp28_round_sd

| VRCP28SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of the lower double-precision (64-bit) floating-point element in "b", store
the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not
set), and copy the upper element from "a" to the upper element of "dst". The maximum relative error for this
approximation is less than 2^-28. [sae_note]

[algorithm]

IF k[0]
    dst[63:0] := (1.0 / b[63:0])
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP28SD - _mm_maskz_rcp28_sd

| VRCP28SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of the lower double-precision (64-bit) floating-point element in "b", store
the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not
set), and copy the upper element from "a" to the upper element of "dst". The maximum relative error for this
approximation is less than 2^-28.

[algorithm]

IF k[0]
    dst[63:0] := (1.0 / b[63:0])
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP28SS - _mm_rcp28_round_ss

| VRCP28SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of the lower single-precision (32-bit) floating-point element in "b", store
the result in the lower element of "dst". The maximum relative error for this approximation is less than 2^-28,
and copy the upper 3 packed elements from "a" to the upper elements of "dst". [sae_note]

[algorithm]

dst[31:0] := (1.0 / b[31:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP28SS - _mm_rcp28_ss

| VRCP28SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of the lower single-precision (32-bit) floating-point element in "b", store
the result in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements
of "dst". The maximum relative error for this approximation is less than 2^-28.

[algorithm]

dst[31:0] := (1.0 / b[31:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP28SS - _mm_mask_rcp28_round_ss

| VRCP28SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of the lower single-precision (32-bit) floating-point element in "b", store
the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0
is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". The maximum relative
error for this approximation is less than 2^-28. [sae_note]

[algorithm]

IF k[0]
    dst[31:0] := (1.0 / b[31:0])
ELSE
    dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP28SS - _mm_mask_rcp28_ss

| VRCP28SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of the lower single-precision (32-bit) floating-point element in "b", store
the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0
is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". The maximum relative
error for this approximation is less than 2^-28.

[algorithm]

IF k[0]
    dst[31:0] := (1.0 / b[31:0])
ELSE
    dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP28SS - _mm_maskz_rcp28_round_ss

| VRCP28SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of the lower single-precision (32-bit) floating-point element in "b", store
the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not
set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". The maximum relative error
for this approximation is less than 2^-28. [sae_note]

[algorithm]

IF k[0]
    dst[31:0] := (1.0 / b[31:0])
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP28SS - _mm_maskz_rcp28_ss

| VRCP28SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of the lower single-precision (32-bit) floating-point element in "b", store
the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not
set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". The maximum relative error
for this approximation is less than 2^-28.

[algorithm]

IF k[0]
    dst[31:0] := (1.0 / b[31:0])
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP28PS - _mm512_rcp28_round_ps

| VRCP28PS_ZMMf32_MASKmskw_ZMMf32_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in "a", and
store the results in "dst". The maximum relative error for this approximation is less than 2^-28. [sae_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := (1.0 / a[i+31:i])
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VRCP28PS - _mm512_rcp28_ps

| VRCP28PS_ZMMf32_MASKmskw_ZMMf32_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in "a", and
store the results in "dst". The maximum relative error for this approximation is less than 2^-28.

[algorithm]

FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := (1.0 / a[i+31:i])
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VRCP28PS - _mm512_mask_rcp28_round_ps

| VRCP28PS_ZMMf32_MASKmskw_ZMMf32_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in "a", and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set). The maximum relative error for this approximation is less than 2^-28. [sae_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := (1.0 / a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VRCP28PS - _mm512_mask_rcp28_ps

| VRCP28PS_ZMMf32_MASKmskw_ZMMf32_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in "a", and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set). The maximum relative error for this approximation is less than 2^-28.

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := (1.0 / a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VRCP28PS - _mm512_maskz_rcp28_round_ps

| VRCP28PS_ZMMf32_MASKmskw_ZMMf32_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in "a", and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set). The maximum relative error for this approximation is less than 2^-28. [sae_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := (1.0 / a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VRCP28PS - _mm512_maskz_rcp28_ps

| VRCP28PS_ZMMf32_MASKmskw_ZMMf32_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in "a", and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set). The maximum relative error for this approximation is less than 2^-28.

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := (1.0 / a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VRCP28PD - _mm512_rcp28_round_pd

| VRCP28PD_ZMMf64_MASKmskw_ZMMf64_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in "a", and
store the results in "dst". The maximum relative error for this approximation is less than 2^-28. [sae_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := (1.0 / a[i+63:i])
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VRCP28PD - _mm512_rcp28_pd

| VRCP28PD_ZMMf64_MASKmskw_ZMMf64_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in "a", and
store the results in "dst". The maximum relative error for this approximation is less than 2^-28.

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := (1.0 / a[i+63:i])
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VRCP28PD - _mm512_mask_rcp28_round_pd

| VRCP28PD_ZMMf64_MASKmskw_ZMMf64_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in "a", and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set). The maximum relative error for this approximation is less than 2^-28. [sae_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := (1.0 / a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VRCP28PD - _mm512_mask_rcp28_pd

| VRCP28PD_ZMMf64_MASKmskw_ZMMf64_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in "a", and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set). The maximum relative error for this approximation is less than 2^-28.

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := (1.0 / a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VRCP28PD - _mm512_maskz_rcp28_round_pd

| VRCP28PD_ZMMf64_MASKmskw_ZMMf64_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in "a", and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set). The maximum relative error for this approximation is less than 2^-28. [sae_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := (1.0 / a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VRCP28PD - _mm512_maskz_rcp28_pd

| VRCP28PD_ZMMf64_MASKmskw_ZMMf64_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in "a", and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set). The maximum relative error for this approximation is less than 2^-28.

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := (1.0 / a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VRSQRT28SD - _mm_rsqrt28_round_sd

| VRSQRT28SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of the lower double-precision (64-bit) floating-point element
in "b", store the result in the lower element of "dst", and copy the upper element from "a" to the upper
element of "dst". The maximum relative error for this approximation is less than 2^-28. [sae_note]

[algorithm]

dst[63:0] := (1.0 / SQRT(b[63:0]))
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRSQRT28SD - _mm_rsqrt28_sd

| VRSQRT28SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of the lower double-precision (64-bit) floating-point element
in "b", store the result in the lower element of "dst", and copy the upper element from "a" to the upper
element of "dst". The maximum relative error for this approximation is less than 2^-28.

[algorithm]

dst[63:0] := (1.0 / SQRT(b[63:0]))
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRSQRT28SD - _mm_mask_rsqrt28_round_sd

| VRSQRT28SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of the lower double-precision (64-bit) floating-point element
in "b", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src"
when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst". The maximum
relative error for this approximation is less than 2^-28. [sae_note]

[algorithm]

IF k[0]
    dst[63:0] := (1.0 / SQRT(b[63:0]))
ELSE
    dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRSQRT28SD - _mm_mask_rsqrt28_sd

| VRSQRT28SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of the lower double-precision (64-bit) floating-point element
in "b", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src"
when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst". The maximum
relative error for this approximation is less than 2^-28.

[algorithm]

IF k[0]
    dst[63:0] := (1.0 / SQRT(b[63:0]))
ELSE
    dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRSQRT28SD - _mm_maskz_rsqrt28_round_sd

| VRSQRT28SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of the lower double-precision (64-bit) floating-point element
in "b", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask
bit 0 is not set), and copy the upper element from "a" to the upper element of "dst". The maximum relative
error for this approximation is less than 2^-28. [sae_note]

[algorithm]

IF k[0]
    dst[63:0] := (1.0 / SQRT(b[63:0]))
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRSQRT28SD - _mm_maskz_rsqrt28_sd

| VRSQRT28SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of the lower double-precision (64-bit) floating-point element
in "b", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask
bit 0 is not set), and copy the upper element from "a" to the upper element of "dst". The maximum relative
error for this approximation is less than 2^-28.

[algorithm]

IF k[0]
    dst[63:0] := (1.0 / SQRT(b[63:0]))
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRSQRT28SS - _mm_rsqrt28_round_ss

| VRSQRT28SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of the lower single-precision (32-bit) floating-point element
in "b", store the result in the lower element of "dst", and copy the upper 3 packed elements from "a" to the
upper elements of "dst". The maximum relative error for this approximation is less than 2^-28. [sae_note]

[algorithm]

dst[31:0] := (1.0 / SQRT(b[31:0]))
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRSQRT28SS - _mm_rsqrt28_ss

| VRSQRT28SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of the lower single-precision (32-bit) floating-point element
in "b", store the result in the lower element of "dst", and copy the upper 3 packed elements from "a" to the
upper elements of "dst". The maximum relative error for this approximation is less than 2^-28.

[algorithm]

dst[31:0] := (1.0 / SQRT(b[31:0]))
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRSQRT28SS - _mm_mask_rsqrt28_round_ss

| VRSQRT28SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of the lower single-precision (32-bit) floating-point element
in "b", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src"
when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". The
maximum relative error for this approximation is less than 2^-28. [sae_note]

[algorithm]

IF k[0]
    dst[31:0] := (1.0 / SQRT(b[31:0]))
ELSE
    dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRSQRT28SS - _mm_mask_rsqrt28_ss

| VRSQRT28SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of the lower single-precision (32-bit) floating-point element
in "b", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src"
when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". The
maximum relative error for this approximation is less than 2^-28.

[algorithm]

IF k[0]
    dst[31:0] := (1.0 / SQRT(b[31:0]))
ELSE
    dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRSQRT28SS - _mm_maskz_rsqrt28_round_ss

| VRSQRT28SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of the lower single-precision (32-bit) floating-point element
in "b", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask
bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". The maximum
relative error for this approximation is less than 2^-28. [sae_note]

[algorithm]

IF k[0]
    dst[31:0] := (1.0 / SQRT(b[31:0]))
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRSQRT28SS - _mm_maskz_rsqrt28_ss

| VRSQRT28SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of the lower single-precision (32-bit) floating-point element
in "b", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask
bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". The maximum
relative error for this approximation is less than 2^-28.

[algorithm]

IF k[0]
    dst[31:0] := (1.0 / SQRT(b[31:0]))
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRSQRT28PS - _mm512_rsqrt28_round_ps

| VRSQRT28PS_ZMMf32_MASKmskw_ZMMf32_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of packed single-precision (32-bit) floating-point elements in
"a", store the results in "dst". The maximum relative error for this approximation is less than 2^-28.
[sae_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := (1.0 / SQRT(a[i+31:i]))
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VRSQRT28PS - _mm512_rsqrt28_ps

| VRSQRT28PS_ZMMf32_MASKmskw_ZMMf32_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of packed single-precision (32-bit) floating-point elements in
"a", store the results in "dst". The maximum relative error for this approximation is less than 2^-28.

[algorithm]

FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := (1.0 / SQRT(a[i+31:i]))
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VRSQRT28PS - _mm512_mask_rsqrt28_round_ps

| VRSQRT28PS_ZMMf32_MASKmskw_ZMMf32_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of packed single-precision (32-bit) floating-point elements in
"a", store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set). The maximum relative error for this approximation is less than 2^-28. [sae_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := (1.0 / SQRT(a[i+31:i]))
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VRSQRT28PS - _mm512_mask_rsqrt28_ps

| VRSQRT28PS_ZMMf32_MASKmskw_ZMMf32_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of packed single-precision (32-bit) floating-point elements in
"a", store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set). The maximum relative error for this approximation is less than 2^-28.

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := (1.0 / SQRT(a[i+31:i]))
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VRSQRT28PS - _mm512_maskz_rsqrt28_round_ps

| VRSQRT28PS_ZMMf32_MASKmskw_ZMMf32_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of packed single-precision (32-bit) floating-point elements in
"a", store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set). The maximum relative error for this approximation is less than 2^-28. [sae_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := (1.0 / SQRT(a[i+31:i]))
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VRSQRT28PS - _mm512_maskz_rsqrt28_ps

| VRSQRT28PS_ZMMf32_MASKmskw_ZMMf32_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of packed single-precision (32-bit) floating-point elements in
"a", store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set). The maximum relative error for this approximation is less than 2^-28.

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := (1.0 / SQRT(a[i+31:i]))
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VRSQRT28PD - _mm512_rsqrt28_round_pd

| VRSQRT28PD_ZMMf64_MASKmskw_ZMMf64_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of packed double-precision (64-bit) floating-point elements in
"a", store the results in "dst". The maximum relative error for this approximation is less than 2^-28.
[sae_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := (1.0 / SQRT(a[i+63:i]))
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VRSQRT28PD - _mm512_rsqrt28_pd

| VRSQRT28PD_ZMMf64_MASKmskw_ZMMf64_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of packed double-precision (64-bit) floating-point elements in
"a", store the results in "dst". The maximum relative error for this approximation is less than 2^-28.

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := (1.0 / SQRT(a[i+63:i]))
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VRSQRT28PD - _mm512_mask_rsqrt28_round_pd

| VRSQRT28PD_ZMMf64_MASKmskw_ZMMf64_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of packed double-precision (64-bit) floating-point elements in
"a", store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set). The maximum relative error for this approximation is less than 2^-28. [sae_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := (1.0 / SQRT(a[i+63:i]))
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VRSQRT28PD - _mm512_mask_rsqrt28_pd

| VRSQRT28PD_ZMMf64_MASKmskw_ZMMf64_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of packed double-precision (64-bit) floating-point elements in
"a", store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set). The maximum relative error for this approximation is less than 2^-28.

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := (1.0 / SQRT(a[i+63:i]))
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VRSQRT28PD - _mm512_maskz_rsqrt28_round_pd

| VRSQRT28PD_ZMMf64_MASKmskw_ZMMf64_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of packed double-precision (64-bit) floating-point elements in
"a", store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set). The maximum relative error for this approximation is less than 2^-28. [sae_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := (1.0 / SQRT(a[i+63:i]))
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VRSQRT28PD - _mm512_maskz_rsqrt28_pd

| VRSQRT28PD_ZMMf64_MASKmskw_ZMMf64_AVX512ER

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of packed double-precision (64-bit) floating-point elements in
"a", store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set). The maximum relative error for this approximation is less than 2^-28.

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := (1.0 / SQRT(a[i+63:i]))
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VADDPD - _mm256_mask_add_pd

| VADDPD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] + b[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VADDPD - _mm256_maskz_add_pd

| VADDPD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] + b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VADDPD - _mm_mask_add_pd

| VADDPD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] + b[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VADDPD - _mm_maskz_add_pd

| VADDPD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] + b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VADDPS - _mm256_mask_add_ps

| VADDPS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] + b[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VADDPS - _mm256_maskz_add_ps

| VADDPS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] + b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VADDPS - _mm_mask_add_ps

| VADDPS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] + b[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VADDPS - _mm_maskz_add_ps

| VADDPS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] + b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VALIGND - _mm256_alignr_epi32

| VALIGND_YMMu32_MASKmskw_YMMu32_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate "a" and "b" into a 64-byte immediate result, shift the result right by "imm8" 32-bit elements, and
store the low 32 bytes (8 elements) in "dst".

[algorithm]

temp[511:256] := a[255:0]
temp[255:0] := b[255:0]
temp[511:0] := temp[511:0] &gt;&gt; (32*imm8[2:0])
dst[255:0] := temp[255:0]
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VALIGND - _mm256_mask_alignr_epi32

| VALIGND_YMMu32_MASKmskw_YMMu32_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate "a" and "b" into a 64-byte immediate result, shift the result right by "imm8" 32-bit elements, and
store the low 32 bytes (8 elements) in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

temp[511:256] := a[255:0]
temp[255:0] := b[255:0]
temp[511:0] := temp[511:0] &gt;&gt; (32*imm8[2:0])
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := temp[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VALIGND - _mm256_maskz_alignr_epi32

| VALIGND_YMMu32_MASKmskw_YMMu32_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate "a" and "b" into a 64-byte immediate result, shift the result right by "imm8" 32-bit elements, and
store the low 32 bytes (8 elements) in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set).

[algorithm]

temp[511:256] := a[255:0]
temp[255:0] := b[255:0]
temp[511:0] := temp[511:0] &gt;&gt; (32*imm8[2:0])
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := temp[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VALIGND - _mm_alignr_epi32

| VALIGND_XMMu32_MASKmskw_XMMu32_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate "a" and "b" into a 32-byte immediate result, shift the result right by "imm8" 32-bit elements, and
store the low 16 bytes (4 elements) in "dst".

[algorithm]

temp[255:128] := a[127:0]
temp[127:0] := b[127:0]
temp[255:0] := temp[255:0] &gt;&gt; (32*imm8[1:0])
dst[127:0] := temp[127:0]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VALIGND - _mm_mask_alignr_epi32

| VALIGND_XMMu32_MASKmskw_XMMu32_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate "a" and "b" into a 32-byte immediate result, shift the result right by "imm8" 32-bit elements, and
store the low 16 bytes (4 elements) in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

temp[255:128] := a[127:0]
temp[127:0] := b[127:0]
temp[255:0] := temp[255:0] &gt;&gt; (32*imm8[1:0])
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := temp[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VALIGND - _mm_maskz_alignr_epi32

| VALIGND_XMMu32_MASKmskw_XMMu32_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate "a" and "b" into a 32-byte immediate result, shift the result right by "imm8" 32-bit elements, and
store the low 16 bytes (4 elements) in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set).

[algorithm]

temp[255:128] := a[127:0]
temp[127:0] := b[127:0]
temp[255:0] := temp[255:0] &gt;&gt; (32*imm8[1:0])
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := temp[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VALIGNQ - _mm256_alignr_epi64

| VALIGNQ_YMMu64_MASKmskw_YMMu64_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate "a" and "b" into a 64-byte immediate result, shift the result right by "imm8" 64-bit elements, and
store the low 32 bytes (4 elements) in "dst".

[algorithm]

temp[511:256] := a[255:0]
temp[255:0] := b[255:0]
temp[511:0] := temp[511:0] &gt;&gt; (64*imm8[1:0])
dst[255:0] := temp[255:0]
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VALIGNQ - _mm256_mask_alignr_epi64

| VALIGNQ_YMMu64_MASKmskw_YMMu64_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate "a" and "b" into a 64-byte immediate result, shift the result right by "imm8" 64-bit elements, and
store the low 32 bytes (4 elements) in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

temp[511:256] := a[255:0]
temp[255:0] := b[255:0]
temp[511:0] := temp[511:0] &gt;&gt; (64*imm8[1:0])
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := temp[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VALIGNQ - _mm256_maskz_alignr_epi64

| VALIGNQ_YMMu64_MASKmskw_YMMu64_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate "a" and "b" into a 64-byte immediate result, shift the result right by "imm8" 64-bit elements, and
store the low 32 bytes (4 elements) in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set).

[algorithm]

temp[511:256] := a[255:0]
temp[255:0] := b[255:0]
temp[511:0] := temp[511:0] &gt;&gt; (64*imm8[1:0])
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := temp[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VALIGNQ - _mm_alignr_epi64

| VALIGNQ_XMMu64_MASKmskw_XMMu64_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate "a" and "b" into a 32-byte immediate result, shift the result right by "imm8" 64-bit elements, and
store the low 16 bytes (2 elements) in "dst".

[algorithm]

temp[255:128] := a[127:0]
temp[127:0] := b[127:0]
temp[255:0] := temp[255:0] &gt;&gt; (64*imm8[0])
dst[127:0] := temp[127:0]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VALIGNQ - _mm_mask_alignr_epi64

| VALIGNQ_XMMu64_MASKmskw_XMMu64_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate "a" and "b" into a 32-byte immediate result, shift the result right by "imm8" 64-bit elements, and
store the low 16 bytes (2 elements) in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

temp[255:128] := a[127:0]
temp[127:0] := b[127:0]
temp[255:0] := temp[255:0] &gt;&gt; (64*imm8[0])
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := temp[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VALIGNQ - _mm_maskz_alignr_epi64

| VALIGNQ_XMMu64_MASKmskw_XMMu64_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate "a" and "b" into a 32-byte immediate result, shift the result right by "imm8" 64-bit elements, and
store the low 16 bytes (2 elements) in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set).

[algorithm]

temp[255:128] := a[127:0]
temp[127:0] := b[127:0]
temp[255:0] := temp[255:0] &gt;&gt; (64*imm8[0])
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := temp[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VBLENDMPD - _mm256_mask_blend_pd

| VBLENDMPD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Blend packed double-precision (64-bit) floating-point elements from "a" and "b" using control mask "k", and
store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := b[i+63:i]
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VBLENDMPD - _mm_mask_blend_pd

| VBLENDMPD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Blend packed double-precision (64-bit) floating-point elements from "a" and "b" using control mask "k", and
store the results in "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := b[i+63:i]
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VBLENDMPS - _mm256_mask_blend_ps

| VBLENDMPS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Blend packed single-precision (32-bit) floating-point elements from "a" and "b" using control mask "k", and
store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := b[i+31:i]
    ELSE
        dst[i+31:i] := a[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VBLENDMPS - _mm_mask_blend_ps

| VBLENDMPS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Blend packed single-precision (32-bit) floating-point elements from "a" and "b" using control mask "k", and
store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := b[i+31:i]
    ELSE
        dst[i+31:i] := a[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTF32X4 - _mm256_broadcast_f32x4

| VBROADCASTF32X4_YMMf32_MASKmskw_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the 4 packed single-precision (32-bit) floating-point elements from "a" to all elements of "dst".

[algorithm]

FOR j := 0 to 7
    i := j*32
    n := (j % 4)*32
    dst[i+31:i] := a[n+31:n]
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTF32X4 - _mm256_mask_broadcast_f32x4

| VBROADCASTF32X4_YMMf32_MASKmskw_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the 4 packed single-precision (32-bit) floating-point elements from "a" to all elements of "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    n := (j % 4)*32
    IF k[j]
        dst[i+31:i] := a[n+31:n]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTF32X4 - _mm256_maskz_broadcast_f32x4

| VBROADCASTF32X4_YMMf32_MASKmskw_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the 4 packed single-precision (32-bit) floating-point elements from "a" to all elements of "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    n := (j % 4)*32
    IF k[j]
        dst[i+31:i] := a[n+31:n]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTI32X4 - _mm256_broadcast_i32x4

| VBROADCASTI32X4_YMMu32_MASKmskw_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the 4 packed 32-bit integers from "a" to all elements of "dst".

[algorithm]

FOR j := 0 to 7
    i := j*32
    n := (j % 4)*32
    dst[i+31:i] := a[n+31:n]
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTI32X4 - _mm256_mask_broadcast_i32x4

| VBROADCASTI32X4_YMMu32_MASKmskw_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the 4 packed 32-bit integers from "a" to all elements of "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    n := (j % 4)*32
    IF k[j]
        dst[i+31:i] := a[n+31:n]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTI32X4 - _mm256_maskz_broadcast_i32x4

| VBROADCASTI32X4_YMMu32_MASKmskw_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the 4 packed 32-bit integers from "a" to all elements of "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    n := (j % 4)*32
    IF k[j]
        dst[i+31:i] := a[n+31:n]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTSD - _mm256_mask_broadcastsd_pd

| VBROADCASTSD_YMMf64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low double-precision (64-bit) floating-point element from "a" to all elements of "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := a[63:0]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTSD - _mm256_maskz_broadcastsd_pd

| VBROADCASTSD_YMMf64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low double-precision (64-bit) floating-point element from "a" to all elements of "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := a[63:0]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTSS - _mm256_mask_broadcastss_ps

| VBROADCASTSS_YMMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low single-precision (32-bit) floating-point element from "a" to all elements of "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := a[31:0]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTSS - _mm256_maskz_broadcastss_ps

| VBROADCASTSS_YMMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low single-precision (32-bit) floating-point element from "a" to all elements of "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := a[31:0]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTSS - _mm_mask_broadcastss_ps

| VBROADCASTSS_XMMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low single-precision (32-bit) floating-point element from "a" to all elements of "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := a[31:0]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTSS - _mm_maskz_broadcastss_ps

| VBROADCASTSS_XMMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low single-precision (32-bit) floating-point element from "a" to all elements of "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := a[31:0]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCMPPD - _mm256_cmp_pd_mask

| VCMPPD_MASKmskw_MASKmskw_YMMf64_YMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed double-precision (64-bit) floating-point elements in "a" and "b" based on the comparison
operand specified by "imm8", and store the results in mask vector "k".

[algorithm]

CASE (imm8[4:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
FOR j := 0 to 3
    i := j*64
    k[j] := (a[i+63:i] OP b[i+63:i]) ? 1 : 0
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VCMPPD - _mm256_mask_cmp_pd_mask

| VCMPPD_MASKmskw_MASKmskw_YMMf64_YMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed double-precision (64-bit) floating-point elements in "a" and "b" based on the comparison
operand specified by "imm8", and store the results in mask vector "k" using zeromask "k1" (elements are zeroed
out when the corresponding mask bit is not set).

[algorithm]

CASE (imm8[4:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
FOR j := 0 to 3
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VCMPPD - _mm_cmp_pd_mask

| VCMPPD_MASKmskw_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed double-precision (64-bit) floating-point elements in "a" and "b" based on the comparison
operand specified by "imm8", and store the results in mask vector "k".

[algorithm]

CASE (imm8[4:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
FOR j := 0 to 1
    i := j*64
    k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VCMPPD - _mm_mask_cmp_pd_mask

| VCMPPD_MASKmskw_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed double-precision (64-bit) floating-point elements in "a" and "b" based on the comparison
operand specified by "imm8", and store the results in mask vector "k" using zeromask "k1" (elements are zeroed
out when the corresponding mask bit is not set).

[algorithm]

CASE (imm8[4:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
FOR j := 0 to 1
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VCMPPS - _mm256_cmp_ps_mask

| VCMPPS_MASKmskw_MASKmskw_YMMf32_YMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed single-precision (32-bit) floating-point elements in "a" and "b" based on the comparison
operand specified by "imm8", and store the results in mask vector "k".

[algorithm]

CASE (imm8[4:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
FOR j := 0 to 7
    i := j*32
    k[j] := (a[i+31:i] OP b[i+31:i]) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VCMPPS - _mm256_mask_cmp_ps_mask

| VCMPPS_MASKmskw_MASKmskw_YMMf32_YMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed single-precision (32-bit) floating-point elements in "a" and "b" based on the comparison
operand specified by "imm8", and store the results in mask vector "k" using zeromask "k1" (elements are zeroed
out when the corresponding mask bit is not set).

[algorithm]

CASE (imm8[4:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
FOR j := 0 to 7
    i := j*32
    IF k1[j]
        k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VCMPPS - _mm_cmp_ps_mask

| VCMPPS_MASKmskw_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed single-precision (32-bit) floating-point elements in "a" and "b" based on the comparison
operand specified by "imm8", and store the results in mask vector "k".

[algorithm]

CASE (imm8[4:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
FOR j := 0 to 3
    i := j*32
    k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VCMPPS - _mm_mask_cmp_ps_mask

| VCMPPS_MASKmskw_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed single-precision (32-bit) floating-point elements in "a" and "b" based on the comparison
operand specified by "imm8", and store the results in mask vector "k" using zeromask "k1" (elements are zeroed
out when the corresponding mask bit is not set).

[algorithm]

CASE (imm8[4:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
FOR j := 0 to 3
    i := j*32
    IF k1[j]
        k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VCOMPRESSPD - _mm256_mask_compress_pd

| VCOMPRESSPD_YMMf64_MASKmskw_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active double-precision (64-bit) floating-point elements in "a" (those with their
respective bit set in writemask "k") to "dst", and pass through the remaining elements from "src".

[algorithm]

size := 64
m := 0
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[m+size-1:m] := a[i+63:i]
        m := m + size
    FI
ENDFOR
dst[255:m] := src[255:m]
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCOMPRESSPD - _mm256_mask_compressstoreu_pd

| VCOMPRESSPD_MEMf64_MASKmskw_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active double-precision (64-bit) floating-point elements in "a" (those with their
respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

size := 64
m := base_addr
FOR j := 0 to 3
    i := j*64
    IF k[j]
        MEM[m+size-1:m] := a[i+63:i]
        m := m + size
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VCOMPRESSPD - _mm256_maskz_compress_pd

| VCOMPRESSPD_YMMf64_MASKmskw_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active double-precision (64-bit) floating-point elements in "a" (those with their
respective bit set in zeromask "k") to "dst", and set the remaining elements to zero.

[algorithm]

size := 64
m := 0
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[m+size-1:m] := a[i+63:i]
        m := m + size
    FI
ENDFOR
dst[255:m] := 0
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCOMPRESSPD - _mm_mask_compress_pd

| VCOMPRESSPD_XMMf64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active double-precision (64-bit) floating-point elements in "a" (those with their
respective bit set in writemask "k") to "dst", and pass through the remaining elements from "src".

[algorithm]

size := 64
m := 0
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[m+size-1:m] := a[i+63:i]
        m := m + size
    FI
ENDFOR
dst[127:m] := src[127:m]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCOMPRESSPD - _mm_mask_compressstoreu_pd

| VCOMPRESSPD_MEMf64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active double-precision (64-bit) floating-point elements in "a" (those with their
respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

size := 64
m := base_addr
FOR j := 0 to 1
    i := j*64
    IF k[j]
        MEM[m+size-1:m] := a[i+63:i]
        m := m + size
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VCOMPRESSPD - _mm_maskz_compress_pd

| VCOMPRESSPD_XMMf64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active double-precision (64-bit) floating-point elements in "a" (those with their
respective bit set in zeromask "k") to "dst", and set the remaining elements to zero.

[algorithm]

size := 64
m := 0
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[m+size-1:m] := a[i+63:i]
        m := m + size
    FI
ENDFOR
dst[127:m] := 0
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCOMPRESSPS - _mm256_mask_compress_ps

| VCOMPRESSPS_YMMf32_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active single-precision (32-bit) floating-point elements in "a" (those with their
respective bit set in writemask "k") to "dst", and pass through the remaining elements from "src".

[algorithm]

size := 32
m := 0
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[m+size-1:m] := a[i+31:i]
        m := m + size
    FI
ENDFOR
dst[255:m] := src[255:m]
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCOMPRESSPS - _mm256_mask_compressstoreu_ps

| VCOMPRESSPS_MEMf32_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active single-precision (32-bit) floating-point elements in "a" (those with their
respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

size := 32
m := base_addr
FOR j := 0 to 7
    i := j*32
    IF k[j]
        MEM[m+size-1:m] := a[i+31:i]
        m := m + size
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VCOMPRESSPS - _mm256_maskz_compress_ps

| VCOMPRESSPS_YMMf32_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active single-precision (32-bit) floating-point elements in "a" (those with their
respective bit set in zeromask "k") to "dst", and set the remaining elements to zero.

[algorithm]

size := 32
m := 0
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[m+size-1:m] := a[i+31:i]
        m := m + size
    FI
ENDFOR
dst[255:m] := 0
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCOMPRESSPS - _mm_mask_compress_ps

| VCOMPRESSPS_XMMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active single-precision (32-bit) floating-point elements in "a" (those with their
respective bit set in writemask "k") to "dst", and pass through the remaining elements from "src".

[algorithm]

size := 32
m := 0
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[m+size-1:m] := a[i+31:i]
        m := m + size
    FI
ENDFOR
dst[127:m] := src[127:m]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCOMPRESSPS - _mm_mask_compressstoreu_ps

| VCOMPRESSPS_MEMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active single-precision (32-bit) floating-point elements in "a" (those with their
respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

size := 32
m := base_addr
FOR j := 0 to 3
    i := j*32
    IF k[j]
        MEM[m+size-1:m] := a[i+31:i]
        m := m + size
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VCOMPRESSPS - _mm_maskz_compress_ps

| VCOMPRESSPS_XMMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active single-precision (32-bit) floating-point elements in "a" (those with their
respective bit set in zeromask "k") to "dst", and set the remaining elements to zero.

[algorithm]

size := 32
m := 0
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[m+size-1:m] := a[i+31:i]
        m := m + size
    FI
ENDFOR
dst[127:m] := 0
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTDQ2PD - _mm256_mask_cvtepi32_pd

| VCVTDQ2PD_YMMf64_MASKmskw_XMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    m := j*64
    IF k[j]
        dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
    ELSE
        dst[m+63:m] := src[m+63:m]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTDQ2PD - _mm256_maskz_cvtepi32_pd

| VCVTDQ2PD_YMMf64_MASKmskw_XMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    m := j*64
    IF k[j]
        dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
    ELSE
        dst[m+63:m] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTDQ2PD - _mm_mask_cvtepi32_pd

| VCVTDQ2PD_XMMf64_MASKmskw_XMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 1
    i := j*32
    m := j*64
    IF k[j]
        dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
    ELSE
        dst[m+63:m] := src[m+63:m]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTDQ2PD - _mm_maskz_cvtepi32_pd

| VCVTDQ2PD_XMMf64_MASKmskw_XMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 1
    i := j*32
    m := j*64
    IF k[j]
        dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
    ELSE
        dst[m+63:m] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTDQ2PS - _mm256_mask_cvtepi32_ps

| VCVTDQ2PS_YMMf32_MASKmskw_YMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTDQ2PS - _mm256_maskz_cvtepi32_ps

| VCVTDQ2PS_YMMf32_MASKmskw_YMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := 32*j
    IF k[j]
        dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTDQ2PS - _mm_mask_cvtepi32_ps

| VCVTDQ2PS_XMMf32_MASKmskw_XMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTDQ2PS - _mm_maskz_cvtepi32_ps

| VCVTDQ2PS_XMMf32_MASKmskw_XMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 3
    i := 32*j
    IF k[j]
        dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2DQ - _mm256_mask_cvtpd_epi32

| VCVTPD2DQ_XMMi32_MASKmskw_YMMf64_AVX512_VL256

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers, and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    l := j*64
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2DQ - _mm256_maskz_cvtpd_epi32

| VCVTPD2DQ_XMMi32_MASKmskw_YMMf64_AVX512_VL256

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers, and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 32*j
    l := 64*j
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2DQ - _mm_mask_cvtpd_epi32

| VCVTPD2DQ_XMMi32_MASKmskw_XMMf64_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers, and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 1
    i := j*32
    l := j*64
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2DQ - _mm_maskz_cvtpd_epi32

| VCVTPD2DQ_XMMi32_MASKmskw_XMMf64_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers, and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := 32*j
    l := 64*j
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2PS - _mm256_mask_cvtpd_ps

| VCVTPD2PS_XMMf32_MASKmskw_YMMf64_AVX512_VL256

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed single-precision (32-bit)
floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 32*j
    l := 64*j
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2PS - _mm256_maskz_cvtpd_ps

| VCVTPD2PS_XMMf32_MASKmskw_YMMf64_AVX512_VL256

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed single-precision (32-bit)
floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    l := j*64
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2PS - _mm_mask_cvtpd_ps

| VCVTPD2PS_XMMf32_MASKmskw_XMMf64_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed single-precision (32-bit)
floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := 32*j
    l := 64*j
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2PS - _mm_maskz_cvtpd_ps

| VCVTPD2PS_XMMf32_MASKmskw_XMMf64_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed single-precision (32-bit)
floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*32
    l := j*64
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2UDQ - _mm256_cvtpd_epu32

| VCVTPD2UDQ_XMMu32_MASKmskw_YMMf64_AVX512_VL256

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers,
and store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := 32*j
    k := 64*j
    dst[i+31:i] := Convert_FP64_To_UInt32(a[k+63:k])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2UDQ - _mm256_mask_cvtpd_epu32

| VCVTPD2UDQ_XMMu32_MASKmskw_YMMf64_AVX512_VL256

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    l := j*64
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_UInt32(a[l+63:l])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2UDQ - _mm256_maskz_cvtpd_epu32

| VCVTPD2UDQ_XMMu32_MASKmskw_YMMf64_AVX512_VL256

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 3
    i := 32*j
    l := 64*j
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_UInt32(a[l+63:l])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2UDQ - _mm_cvtpd_epu32

| VCVTPD2UDQ_XMMu32_MASKmskw_XMMf64_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers,
and store the results in "dst".

[algorithm]

FOR j := 0 to 1
    i := 32*j
    k := 64*j
    dst[i+31:i] := Convert_FP64_To_UInt32(a[k+63:k])
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2UDQ - _mm_mask_cvtpd_epu32

| VCVTPD2UDQ_XMMu32_MASKmskw_XMMf64_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*32
    l := j*64
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_UInt32(a[l+63:l])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2UDQ - _mm_maskz_cvtpd_epu32

| VCVTPD2UDQ_XMMu32_MASKmskw_XMMf64_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 1
    i := 32*j
    l := 64*j
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_UInt32(a[l+63:l])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPH2PS - _mm256_mask_cvtph_ps

| VCVTPH2PS_YMMf32_MASKmskw_XMMf16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed half-precision (16-bit) floating-point elements in "a" to packed single-precision (32-bit)
floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    m := j*16
    IF k[j]
        dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPH2PS - _mm256_maskz_cvtph_ps

| VCVTPH2PS_YMMf32_MASKmskw_XMMf16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed half-precision (16-bit) floating-point elements in "a" to packed single-precision (32-bit)
floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    m := j*16
    IF k[j]
        dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPH2PS - _mm_mask_cvtph_ps

| VCVTPH2PS_XMMf32_MASKmskw_XMMf16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed half-precision (16-bit) floating-point elements in "a" to packed single-precision (32-bit)
floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    m := j*16
    IF k[j]
        dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPH2PS - _mm_maskz_cvtph_ps

| VCVTPH2PS_XMMf32_MASKmskw_XMMf16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed half-precision (16-bit) floating-point elements in "a" to packed single-precision (32-bit)
floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    m := j*16
    IF k[j]
        dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2DQ - _mm256_mask_cvtps_epi32

| VCVTPS2DQ_YMMi32_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers, and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2DQ - _mm256_maskz_cvtps_epi32

| VCVTPS2DQ_YMMi32_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers, and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 32*j
    IF k[j]
        dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2DQ - _mm_mask_cvtps_epi32

| VCVTPS2DQ_XMMi32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers, and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2DQ - _mm_maskz_cvtps_epi32

| VCVTPS2DQ_XMMi32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers, and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 32*j
    IF k[j]
        dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2PH - _mm256_mask_cvt_roundps_ph

| VCVTPS2PH_XMMf16_MASKmskw_YMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed half-precision (16-bit)
floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set). [round_imm_note]

[algorithm]

FOR j := 0 to 7
    i := 16*j
    l := 32*j
    IF k[j]
        dst[i+15:i] := Convert_FP32_To_FP16(a[l+31:l])
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2PH - _mm256_mask_cvtps_ph

| VCVTPS2PH_XMMf16_MASKmskw_YMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed half-precision (16-bit)
floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set). [round_imm_note]

[algorithm]

FOR j := 0 to 7
    i := 16*j
    l := 32*j
    IF k[j]
        dst[i+15:i] := Convert_FP32_To_FP16(a[l+31:l])
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2PH - _mm256_maskz_cvt_roundps_ph

| VCVTPS2PH_XMMf16_MASKmskw_YMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed half-precision (16-bit)
floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set). [round_imm_note]

[algorithm]

FOR j := 0 to 7
    i := 16*j
    l := 32*j
    IF k[j]
        dst[i+15:i] := Convert_FP32_To_FP16(a[l+31:l])
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2PH - _mm256_maskz_cvtps_ph

| VCVTPS2PH_XMMf16_MASKmskw_YMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed half-precision (16-bit)
floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set). [round_imm_note]

[algorithm]

FOR j := 0 to 7
    i := 16*j
    l := 32*j
    IF k[j]
        dst[i+15:i] := Convert_FP32_To_FP16(a[l+31:l])
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2PH - _mm_mask_cvt_roundps_ph

| VCVTPS2PH_XMMf16_MASKmskw_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed half-precision (16-bit)
floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set). [round_imm_note]

[algorithm]

FOR j := 0 to 3
    i := 16*j
    l := 32*j
    IF k[j]
        dst[i+15:i] := Convert_FP32_To_FP16(a[l+31:l])
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2PH - _mm_mask_cvtps_ph

| VCVTPS2PH_XMMf16_MASKmskw_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed half-precision (16-bit)
floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set). [round_imm_note]

[algorithm]

FOR j := 0 to 3
    i := 16*j
    l := 32*j
    IF k[j]
        dst[i+15:i] := Convert_FP32_To_FP16(a[l+31:l])
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2PH - _mm_maskz_cvt_roundps_ph

| VCVTPS2PH_XMMf16_MASKmskw_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed half-precision (16-bit)
floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set). [round_imm_note]

[algorithm]

FOR j := 0 to 3
    i := 16*j
    l := 32*j
    IF k[j]
        dst[i+15:i] := Convert_FP32_To_FP16(a[l+31:l])
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2PH - _mm_maskz_cvtps_ph

| VCVTPS2PH_XMMf16_MASKmskw_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed half-precision (16-bit)
floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set). [round_imm_note]

[algorithm]

FOR j := 0 to 3
    i := 16*j
    l := 32*j
    IF k[j]
        dst[i+15:i] := Convert_FP32_To_FP16(a[l+31:l])
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2UDQ - _mm256_cvtps_epu32

| VCVTPS2UDQ_YMMu32_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers,
and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := 32*j
    dst[i+31:i] := Convert_FP32_To_UInt32(a[i+31:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2UDQ - _mm256_mask_cvtps_epu32

| VCVTPS2UDQ_YMMu32_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 32*j
    IF k[j]
        dst[i+31:i] := Convert_FP32_To_UInt32(a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2UDQ - _mm256_maskz_cvtps_epu32

| VCVTPS2UDQ_YMMu32_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 7
    i := 32*j
    IF k[j]
        dst[i+31:i] := Convert_FP32_To_UInt32(a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2UDQ - _mm_cvtps_epu32

| VCVTPS2UDQ_XMMu32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers,
and store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := 32*j
    dst[i+31:i] := Convert_FP32_To_UInt32(a[i+31:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2UDQ - _mm_mask_cvtps_epu32

| VCVTPS2UDQ_XMMu32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 32*j
    IF k[j]
        dst[i+31:i] := Convert_FP32_To_UInt32(a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2UDQ - _mm_maskz_cvtps_epu32

| VCVTPS2UDQ_XMMu32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 3
    i := 32*j
    IF k[j]
        dst[i+31:i] := Convert_FP32_To_UInt32(a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2DQ - _mm256_mask_cvttpd_epi32

| VCVTTPD2DQ_XMMi32_MASKmskw_YMMf64_AVX512_VL256

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers with
truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 32*j
    l := 64*j
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[l+63:l])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2DQ - _mm256_maskz_cvttpd_epi32

| VCVTTPD2DQ_XMMi32_MASKmskw_YMMf64_AVX512_VL256

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers with
truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 32*j
    l := 64*j
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[l+63:l])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2DQ - _mm_mask_cvttpd_epi32

| VCVTTPD2DQ_XMMi32_MASKmskw_XMMf64_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers with
truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := 32*j
    l := 64*j
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[l+63:l])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2DQ - _mm_maskz_cvttpd_epi32

| VCVTTPD2DQ_XMMi32_MASKmskw_XMMf64_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers with
truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := 32*j
    l := 64*j
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[l+63:l])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2UDQ - _mm256_cvttpd_epu32

| VCVTTPD2UDQ_XMMu32_MASKmskw_YMMf64_AVX512_VL256

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers
with truncation, and store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := 32*j
    k := 64*j
    dst[i+31:i] := Convert_FP64_To_UInt32_Truncate(a[k+63:k])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2UDQ - _mm256_mask_cvttpd_epu32

| VCVTTPD2UDQ_XMMu32_MASKmskw_YMMf64_AVX512_VL256

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers
with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 32*j
    l := 64*j
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_UInt32_Truncate(a[l+63:l])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2UDQ - _mm256_maskz_cvttpd_epu32

| VCVTTPD2UDQ_XMMu32_MASKmskw_YMMf64_AVX512_VL256

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers
with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 32*j
    l := 64*j
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_UInt32_Truncate(a[l+63:l])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2UDQ - _mm_cvttpd_epu32

| VCVTTPD2UDQ_XMMu32_MASKmskw_XMMf64_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers
with truncation, and store the results in "dst".

[algorithm]

FOR j := 0 to 1
    i := 32*j
    k := 64*j
    dst[i+31:i] := Convert_FP64_To_UInt32_Truncate(a[k+63:k])
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2UDQ - _mm_mask_cvttpd_epu32

| VCVTTPD2UDQ_XMMu32_MASKmskw_XMMf64_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers
with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := 32*j
    l := 64*j
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_UInt32_Truncate(a[l+63:l])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2UDQ - _mm_maskz_cvttpd_epu32

| VCVTTPD2UDQ_XMMu32_MASKmskw_XMMf64_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers
with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := 32*j
    l := 64*j
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_UInt32_Truncate(a[l+63:l])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2DQ - _mm256_mask_cvttps_epi32

| VCVTTPS2DQ_YMMi32_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers with
truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 32*j
    IF k[j]
        dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2DQ - _mm256_maskz_cvttps_epi32

| VCVTTPS2DQ_YMMi32_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers with
truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 32*j
    IF k[j]
        dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2DQ - _mm_mask_cvttps_epi32

| VCVTTPS2DQ_XMMi32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers with
truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 32*j
    IF k[j]
        dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2DQ - _mm_maskz_cvttps_epi32

| VCVTTPS2DQ_XMMi32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers with
truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 32*j
    IF k[j]
        dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2UDQ - _mm256_cvttps_epu32

| VCVTTPS2UDQ_YMMu32_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers
with truncation, and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := 32*j
    dst[i+31:i] := Convert_FP32_To_UInt32_Truncate(a[i+31:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2UDQ - _mm256_mask_cvttps_epu32

| VCVTTPS2UDQ_YMMu32_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers
with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 32*j
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_UInt32_Truncate(a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2UDQ - _mm256_maskz_cvttps_epu32

| VCVTTPS2UDQ_YMMu32_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers
with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 32*j
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_UInt32_Truncate(a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2UDQ - _mm_cvttps_epu32

| VCVTTPS2UDQ_XMMu32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers
with truncation, and store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := 32*j
    dst[i+31:i] := Convert_FP32_To_UInt32_Truncate(a[i+31:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2UDQ - _mm_mask_cvttps_epu32

| VCVTTPS2UDQ_XMMu32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers
with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 32*j
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_UInt32_Truncate(a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2UDQ - _mm_maskz_cvttps_epu32

| VCVTTPS2UDQ_XMMu32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers
with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 32*j
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_UInt32_Truncate(a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUDQ2PD - _mm256_cvtepu32_pd

| VCVTUDQ2PD_YMMf64_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed double-precision (64-bit) floating-point elements,
and store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    l := j*32
    dst[i+63:i] := Convert_Int32_To_FP64(a[l+31:l])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUDQ2PD - _mm256_mask_cvtepu32_pd

| VCVTUDQ2PD_YMMf64_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed double-precision (64-bit) floating-point elements,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_Int32_To_FP64(a[l+31:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUDQ2PD - _mm256_maskz_cvtepu32_pd

| VCVTUDQ2PD_YMMf64_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed double-precision (64-bit) floating-point elements,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_Int64_To_FP64(a[l+31:l])
    ELSE
        dst[i+63:i] := 0
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUDQ2PD - _mm_cvtepu32_pd

| VCVTUDQ2PD_XMMf64_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed double-precision (64-bit) floating-point elements,
and store the results in "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    l := j*32
    dst[i+63:i] := Convert_Int64_To_FP64(a[l+31:l])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUDQ2PD - _mm_mask_cvtepu32_pd

| VCVTUDQ2PD_XMMf64_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed double-precision (64-bit) floating-point elements,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_Int64_To_FP64(a[l+31:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUDQ2PD - _mm_maskz_cvtepu32_pd

| VCVTUDQ2PD_XMMf64_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed double-precision (64-bit) floating-point elements,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_Int64_To_FP64(a[l+31:l])
    ELSE
        dst[i+63:i] := 0
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VDIVPD - _mm256_mask_div_pd

| VDIVPD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Divide packed double-precision (64-bit) floating-point elements in "a" by packed elements in "b", and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 3
    i := 64*j
    IF k[j]
        dst[i+63:i] := a[i+63:i] / b[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VDIVPD - _mm256_maskz_div_pd

| VDIVPD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Divide packed double-precision (64-bit) floating-point elements in "a" by packed elements in "b", and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 64*j
    IF k[j]
        dst[i+63:i] := a[i+63:i] / b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VDIVPD - _mm_mask_div_pd

| VDIVPD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Divide packed double-precision (64-bit) floating-point elements in "a" by packed elements in "b", and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 1
    i := 64*j
    IF k[j]
        dst[i+63:i] := a[i+63:i] / b[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VDIVPD - _mm_maskz_div_pd

| VDIVPD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Divide packed double-precision (64-bit) floating-point elements in "a" by packed elements in "b", and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := 64*j
    IF k[j]
        dst[i+63:i] := a[i+63:i] / b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VDIVPS - _mm256_mask_div_ps

| VDIVPS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Divide packed single-precision (32-bit) floating-point elements in "a" by packed elements in "b", and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := 32*j
    IF k[j]
        dst[i+31:i] := a[i+31:i] / b[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VDIVPS - _mm256_maskz_div_ps

| VDIVPS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Divide packed single-precision (32-bit) floating-point elements in "a" by packed elements in "b", and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 32*j
    IF k[j]
        dst[i+31:i] := a[i+31:i] / b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VDIVPS - _mm_mask_div_ps

| VDIVPS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Divide packed single-precision (32-bit) floating-point elements in "a" by packed elements in "b", and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 3
    i := 32*j
    IF k[j]
        dst[i+31:i] := a[i+31:i] / b[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VDIVPS - _mm_maskz_div_ps

| VDIVPS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Divide packed single-precision (32-bit) floating-point elements in "a" by packed elements in "b", and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 32*j
    IF k[j]
        dst[i+31:i] := a[i+31:i] / b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VEXPANDPD - _mm256_mask_expand_pd

| VEXPANDPD_YMMf64_MASKmskw_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active double-precision (64-bit) floating-point elements from "a" (those with their respective
bit set in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := a[m+63:m]
        m := m + 64
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VEXPANDPD - _mm256_mask_expandloadu_pd

| VEXPANDPD_YMMf64_MASKmskw_MEMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active double-precision (64-bit) floating-point elements from unaligned memory at "mem_addr"
(those with their respective bit set in mask "k"), and store the results in "dst" using writemask "k" (elements
are copied from "src" when the corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
        m := m + 64
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VEXPANDPD - _mm256_maskz_expand_pd

| VEXPANDPD_YMMf64_MASKmskw_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active double-precision (64-bit) floating-point elements from "a" (those with their respective
bit set in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := a[m+63:m]
        m := m + 64
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VEXPANDPD - _mm256_maskz_expandloadu_pd

| VEXPANDPD_YMMf64_MASKmskw_MEMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active double-precision (64-bit) floating-point elements from unaligned memory at "mem_addr"
(those with their respective bit set in mask "k"), and store the results in "dst" using zeromask "k" (elements
are zeroed out when the corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
        m := m + 64
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VEXPANDPD - _mm_mask_expand_pd

| VEXPANDPD_XMMf64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active double-precision (64-bit) floating-point elements from "a" (those with their respective
bit set in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := a[m+63:m]
        m := m + 64
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VEXPANDPD - _mm_mask_expandloadu_pd

| VEXPANDPD_XMMf64_MASKmskw_MEMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active double-precision (64-bit) floating-point elements from unaligned memory at "mem_addr"
(those with their respective bit set in mask "k"), and store the results in "dst" using writemask "k" (elements
are copied from "src" when the corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
        m := m + 64
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VEXPANDPD - _mm_maskz_expand_pd

| VEXPANDPD_XMMf64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active double-precision (64-bit) floating-point elements from "a" (those with their respective
bit set in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := a[m+63:m]
        m := m + 64
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VEXPANDPD - _mm_maskz_expandloadu_pd

| VEXPANDPD_XMMf64_MASKmskw_MEMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active double-precision (64-bit) floating-point elements from unaligned memory at "mem_addr"
(those with their respective bit set in mask "k"), and store the results in "dst" using zeromask "k" (elements
are zeroed out when the corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
        m := m + 64
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VEXPANDPS - _mm256_mask_expand_ps

| VEXPANDPS_YMMf32_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active single-precision (32-bit) floating-point elements from "a" (those with their respective
bit set in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := a[m+31:m]
        m := m + 32
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VEXPANDPS - _mm256_mask_expandloadu_ps

| VEXPANDPS_YMMf32_MASKmskw_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active single-precision (32-bit) floating-point elements from unaligned memory at "mem_addr"
(those with their respective bit set in mask "k"), and store the results in "dst" using writemask "k" (elements
are copied from "src" when the corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
        m := m + 32
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VEXPANDPS - _mm256_maskz_expand_ps

| VEXPANDPS_YMMf32_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active single-precision (32-bit) floating-point elements from "a" (those with their respective
bit set in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := a[m+31:m]
        m := m + 32
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VEXPANDPS - _mm256_maskz_expandloadu_ps

| VEXPANDPS_YMMf32_MASKmskw_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active single-precision (32-bit) floating-point elements from unaligned memory at "mem_addr"
(those with their respective bit set in mask "k"), and store the results in "dst" using zeromask "k" (elements
are zeroed out when the corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
        m := m + 32
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VEXPANDPS - _mm_mask_expand_ps

| VEXPANDPS_XMMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active single-precision (32-bit) floating-point elements from "a" (those with their respective
bit set in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := a[m+31:m]
        m := m + 32
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VEXPANDPS - _mm_mask_expandloadu_ps

| VEXPANDPS_XMMf32_MASKmskw_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active single-precision (32-bit) floating-point elements from unaligned memory at "mem_addr"
(those with their respective bit set in mask "k"), and store the results in "dst" using writemask "k" (elements
are copied from "src" when the corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
        m := m + 32
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VEXPANDPS - _mm_maskz_expand_ps

| VEXPANDPS_XMMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active single-precision (32-bit) floating-point elements from "a" (those with their respective
bit set in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := a[m+31:m]
        m := m + 32
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VEXPANDPS - _mm_maskz_expandloadu_ps

| VEXPANDPS_XMMf32_MASKmskw_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active single-precision (32-bit) floating-point elements from unaligned memory at "mem_addr"
(those with their respective bit set in mask "k"), and store the results in "dst" using zeromask "k" (elements
are zeroed out when the corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
        m := m + 32
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VEXTRACTF32X4 - _mm256_extractf32x4_ps

| VEXTRACTF32X4_XMMf32_MASKmskw_YMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract 128 bits (composed of 4 packed single-precision (32-bit) floating-point elements) from "a", selected
with "imm8", and store the result in "dst".

[algorithm]

CASE imm8[0] OF
0: dst[127:0] := a[127:0]
1: dst[127:0] := a[255:128]
ESAC
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VEXTRACTF32X4 - _mm256_mask_extractf32x4_ps

| VEXTRACTF32X4_XMMf32_MASKmskw_YMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract 128 bits (composed of 4 packed single-precision (32-bit) floating-point elements) from "a", selected
with "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

CASE imm8[0] OF
0: tmp[127:0] := a[127:0]
1: tmp[127:0] := a[255:128]
ESAC
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VEXTRACTF32X4 - _mm256_maskz_extractf32x4_ps

| VEXTRACTF32X4_XMMf32_MASKmskw_YMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract 128 bits (composed of 4 packed single-precision (32-bit) floating-point elements) from "a", selected
with "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set).

[algorithm]

CASE imm8[0] OF
0: tmp[127:0] := a[127:0]
1: tmp[127:0] := a[255:128]
ESAC
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VEXTRACTI32X4 - _mm256_extracti32x4_epi32

| VEXTRACTI32X4_XMMu32_MASKmskw_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract 128 bits (composed of 4 packed 32-bit integers) from "a", selected with "imm8", and store the result
in "dst".

[algorithm]

CASE imm8[0] OF
0: dst[127:0] := a[127:0]
1: dst[127:0] := a[255:128]
ESAC
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VEXTRACTI32X4 - _mm256_mask_extracti32x4_epi32

| VEXTRACTI32X4_XMMu32_MASKmskw_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract 128 bits (composed of 4 packed 32-bit integers) from "a", selected with "imm8", and store the results
in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

CASE imm8[0] OF
0: tmp[127:0] := a[127:0]
1: tmp[127:0] := a[255:128]
ESAC
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VEXTRACTI32X4 - _mm256_maskz_extracti32x4_epi32

| VEXTRACTI32X4_XMMu32_MASKmskw_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract 128 bits (composed of 4 packed 32-bit integers) from "a", selected with "imm8", and store the results
in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

CASE imm8[0] OF
0: tmp[127:0] := a[127:0]
1: tmp[127:0] := a[255:128]
ESAC
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFIXUPIMMPD - _mm256_fixupimm_pd

| VFIXUPIMMPD_YMMf64_MASKmskw_YMMf64_YMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Fix up packed double-precision (64-bit) floating-point elements in "a" and "b" using packed 64-bit integers in
"c", and store the results in "dst". "imm8" is used to set the required flags reporting.

[algorithm]

enum TOKEN_TYPE {
    QNAN_TOKEN := 0, \
    SNAN_TOKEN := 1, \
    ZERO_VALUE_TOKEN := 2, \
    ONE_VALUE_TOKEN := 3, \
    NEG_INF_TOKEN := 4, \
    POS_INF_TOKEN := 5, \
    NEG_VALUE_TOKEN := 6, \
    POS_VALUE_TOKEN := 7
}
DEFINE FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]) {
    tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
    CASE(tsrc[63:0]) OF
    QNAN_TOKEN: j := 0
    SNAN_TOKEN: j := 1
    ZERO_VALUE_TOKEN: j := 2
    ONE_VALUE_TOKEN: j := 3
    NEG_INF_TOKEN: j := 4
    POS_INF_TOKEN: j := 5
    NEG_VALUE_TOKEN: j := 6
    POS_VALUE_TOKEN: j := 7
    ESAC
    
    token_response[3:0] := src3[3+4*j:4*j]
    
    CASE(token_response[3:0]) OF
    0 : dest[63:0] := src1[63:0]
    1 : dest[63:0] := tsrc[63:0]
    2 : dest[63:0] := QNaN(tsrc[63:0])
    3 : dest[63:0] := QNAN_Indefinite
    4 : dest[63:0] := -INF
    5 : dest[63:0] := +INF
    6 : dest[63:0] := tsrc.sign? -INF : +INF
    7 : dest[63:0] := -0
    8 : dest[63:0] := +0
    9 : dest[63:0] := -1
    10: dest[63:0] := +1
    11: dest[63:0] := 1/2
    12: dest[63:0] := 90.0
    13: dest[63:0] := PI/2
    14: dest[63:0] := MAX_FLOAT
    15: dest[63:0] := -MAX_FLOAT
    ESAC
    
    CASE(tsrc[31:0]) OF
    ZERO_VALUE_TOKEN:
        IF (imm8[0]) #ZE; FI
    ZERO_VALUE_TOKEN:
        IF (imm8[1]) #IE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[2]) #ZE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[3]) #IE; FI
    SNAN_TOKEN:
        IF (imm8[4]) #IE; FI
    NEG_INF_TOKEN:
        IF (imm8[5]) #IE; FI
    NEG_VALUE_TOKEN:
        IF (imm8[6]) #IE; FI
    POS_INF_TOKEN:
        IF (imm8[7]) #IE; FI
    ESAC
    RETURN dest[63:0]
}
FOR j := 0 to 3
    i := j*64
    dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFIXUPIMMPD - _mm256_mask_fixupimm_pd

| VFIXUPIMMPD_YMMf64_MASKmskw_YMMf64_YMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Fix up packed double-precision (64-bit) floating-point elements in "a" and "b" using packed 64-bit integers in
"c", and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding
mask bit is not set). "imm8" is used to set the required flags reporting.

[algorithm]

enum TOKEN_TYPE {
    QNAN_TOKEN := 0, \
    SNAN_TOKEN := 1, \
    ZERO_VALUE_TOKEN := 2, \
    ONE_VALUE_TOKEN := 3, \
    NEG_INF_TOKEN := 4, \
    POS_INF_TOKEN := 5, \
    NEG_VALUE_TOKEN := 6, \
    POS_VALUE_TOKEN := 7
}
DEFINE FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]) {
    tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
    CASE(tsrc[63:0]) OF
    QNAN_TOKEN:j := 0
    SNAN_TOKEN:j := 1
    ZERO_VALUE_TOKEN: j := 2
    ONE_VALUE_TOKEN: j := 3
    NEG_INF_TOKEN: j := 4
    POS_INF_TOKEN: j := 5
    NEG_VALUE_TOKEN: j := 6
    POS_VALUE_TOKEN: j := 7
    ESAC
    
    token_response[3:0] := src3[3+4*j:4*j]
    
    CASE(token_response[3:0]) OF
    0 : dest[63:0] := src1[63:0]
    1 : dest[63:0] := tsrc[63:0]
    2 : dest[63:0] := QNaN(tsrc[63:0])
    3 : dest[63:0] := QNAN_Indefinite
    4 : dest[63:0] := -INF
    5 : dest[63:0] := +INF
    6 : dest[63:0] := tsrc.sign? -INF : +INF
    7 : dest[63:0] := -0
    8 : dest[63:0] := +0
    9 : dest[63:0] := -1
    10: dest[63:0] := +1
    11: dest[63:0] := 1/2
    12: dest[63:0] := 90.0
    13: dest[63:0] := PI/2
    14: dest[63:0] := MAX_FLOAT
    15: dest[63:0] := -MAX_FLOAT
    ESAC
    
    CASE(tsrc[31:0]) OF
    ZERO_VALUE_TOKEN:
        IF (imm8[0]) #ZE; FI
    ZERO_VALUE_TOKEN:
        IF (imm8[1]) #IE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[2]) #ZE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[3]) #IE; FI
    SNAN_TOKEN:
        IF (imm8[4]) #IE; FI
    NEG_INF_TOKEN:
        IF (imm8[5]) #IE; FI
    NEG_VALUE_TOKEN:
        IF (imm8[6]) #IE; FI
    POS_INF_TOKEN:
        IF (imm8[7]) #IE; FI
    ESAC
    RETURN dest[63:0]
}
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFIXUPIMMPD - _mm256_maskz_fixupimm_pd

| VFIXUPIMMPD_YMMf64_MASKmskw_YMMf64_YMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Fix up packed double-precision (64-bit) floating-point elements in "a" and "b" using packed 64-bit integers in
"c", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit
is not set). "imm8" is used to set the required flags reporting.

[algorithm]

enum TOKEN_TYPE {
    QNAN_TOKEN := 0, \
    SNAN_TOKEN := 1, \
    ZERO_VALUE_TOKEN := 2, \
    ONE_VALUE_TOKEN := 3, \
    NEG_INF_TOKEN := 4, \
    POS_INF_TOKEN := 5, \
    NEG_VALUE_TOKEN := 6, \
    POS_VALUE_TOKEN := 7
}
DEFINE FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]) {
    tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
    CASE(tsrc[63:0]) OF
    QNAN_TOKEN:j := 0
    SNAN_TOKEN:j := 1
    ZERO_VALUE_TOKEN: j := 2
    ONE_VALUE_TOKEN: j := 3
    NEG_INF_TOKEN: j := 4
    POS_INF_TOKEN: j := 5
    NEG_VALUE_TOKEN: j := 6
    POS_VALUE_TOKEN: j := 7
    ESAC
    
    token_response[3:0] := src3[3+4*j:4*j]
    
    CASE(token_response[3:0]) OF
    0 : dest[63:0] := src1[63:0]
    1 : dest[63:0] := tsrc[63:0]
    2 : dest[63:0] := QNaN(tsrc[63:0])
    3 : dest[63:0] := QNAN_Indefinite
    4 : dest[63:0] := -INF
    5 : dest[63:0] := +INF
    6 : dest[63:0] := tsrc.sign? -INF : +INF
    7 : dest[63:0] := -0
    8 : dest[63:0] := +0
    9 : dest[63:0] := -1
    10: dest[63:0] := +1
    11: dest[63:0] := 1/2
    12: dest[63:0] := 90.0
    13: dest[63:0] := PI/2
    14: dest[63:0] := MAX_FLOAT
    15: dest[63:0] := -MAX_FLOAT
    ESAC
    
    CASE(tsrc[31:0]) OF
    ZERO_VALUE_TOKEN:
        IF (imm8[0]) #ZE; FI
    ZERO_VALUE_TOKEN:
        IF (imm8[1]) #IE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[2]) #ZE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[3]) #IE; FI
    SNAN_TOKEN:
        IF (imm8[4]) #IE; FI
    NEG_INF_TOKEN:
        IF (imm8[5]) #IE; FI
    NEG_VALUE_TOKEN:
        IF (imm8[6]) #IE; FI
    POS_INF_TOKEN:
        IF (imm8[7]) #IE; FI
    ESAC
    RETURN dest[63:0]
}
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFIXUPIMMPD - _mm_fixupimm_pd

| VFIXUPIMMPD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Fix up packed double-precision (64-bit) floating-point elements in "a" and "b" using packed 64-bit integers in
"c", and store the results in "dst". "imm8" is used to set the required flags reporting.

[algorithm]

enum TOKEN_TYPE {
    QNAN_TOKEN := 0, \
    SNAN_TOKEN := 1, \
    ZERO_VALUE_TOKEN := 2, \
    ONE_VALUE_TOKEN := 3, \
    NEG_INF_TOKEN := 4, \
    POS_INF_TOKEN := 5, \
    NEG_VALUE_TOKEN := 6, \
    POS_VALUE_TOKEN := 7
}
DEFINE FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]) {
    tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
    CASE(tsrc[63:0]) OF
    QNAN_TOKEN:j := 0
    SNAN_TOKEN:j := 1
    ZERO_VALUE_TOKEN: j := 2
    ONE_VALUE_TOKEN: j := 3
    NEG_INF_TOKEN: j := 4
    POS_INF_TOKEN: j := 5
    NEG_VALUE_TOKEN: j := 6
    POS_VALUE_TOKEN: j := 7
    ESAC
    
    token_response[3:0] := src3[3+4*j:4*j]
    
    CASE(token_response[3:0]) OF
    0 : dest[63:0] := src1[63:0]
    1 : dest[63:0] := tsrc[63:0]
    2 : dest[63:0] := QNaN(tsrc[63:0])
    3 : dest[63:0] := QNAN_Indefinite
    4 : dest[63:0] := -INF
    5 : dest[63:0] := +INF
    6 : dest[63:0] := tsrc.sign? -INF : +INF
    7 : dest[63:0] := -0
    8 : dest[63:0] := +0
    9 : dest[63:0] := -1
    10: dest[63:0] := +1
    11: dest[63:0] := 1/2
    12: dest[63:0] := 90.0
    13: dest[63:0] := PI/2
    14: dest[63:0] := MAX_FLOAT
    15: dest[63:0] := -MAX_FLOAT
    ESAC
    
    CASE(tsrc[31:0]) OF
    ZERO_VALUE_TOKEN:
        IF (imm8[0]) #ZE; FI
    ZERO_VALUE_TOKEN:
        IF (imm8[1]) #IE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[2]) #ZE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[3]) #IE; FI
    SNAN_TOKEN:
        IF (imm8[4]) #IE; FI
    NEG_INF_TOKEN:
        IF (imm8[5]) #IE; FI
    NEG_VALUE_TOKEN:
        IF (imm8[6]) #IE; FI
    POS_INF_TOKEN:
        IF (imm8[7]) #IE; FI
    ESAC
    RETURN dest[63:0]
}
FOR j := 0 to 1
    i := j*64
    dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFIXUPIMMPD - _mm_mask_fixupimm_pd

| VFIXUPIMMPD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Fix up packed double-precision (64-bit) floating-point elements in "a" and "b" using packed 64-bit integers in
"c", and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding
mask bit is not set). "imm8" is used to set the required flags reporting.

[algorithm]

enum TOKEN_TYPE {
    QNAN_TOKEN := 0, \
    SNAN_TOKEN := 1, \
    ZERO_VALUE_TOKEN := 2, \
    ONE_VALUE_TOKEN := 3, \
    NEG_INF_TOKEN := 4, \
    POS_INF_TOKEN := 5, \
    NEG_VALUE_TOKEN := 6, \
    POS_VALUE_TOKEN := 7
}
DEFINE FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]) {
    tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
    CASE(tsrc[63:0]) OF
    QNAN_TOKEN:j := 0
    SNAN_TOKEN:j := 1
    ZERO_VALUE_TOKEN: j := 2
    ONE_VALUE_TOKEN: j := 3
    NEG_INF_TOKEN: j := 4
    POS_INF_TOKEN: j := 5
    NEG_VALUE_TOKEN: j := 6
    POS_VALUE_TOKEN: j := 7
    ESAC
    
    token_response[3:0] := src3[3+4*j:4*j]
    
    CASE(token_response[3:0]) OF
    0 : dest[63:0] := src1[63:0]
    1 : dest[63:0] := tsrc[63:0]
    2 : dest[63:0] := QNaN(tsrc[63:0])
    3 : dest[63:0] := QNAN_Indefinite
    4 : dest[63:0] := -INF
    5 : dest[63:0] := +INF
    6 : dest[63:0] := tsrc.sign? -INF : +INF
    7 : dest[63:0] := -0
    8 : dest[63:0] := +0
    9 : dest[63:0] := -1
    10: dest[63:0] := +1
    11: dest[63:0] := 1/2
    12: dest[63:0] := 90.0
    13: dest[63:0] := PI/2
    14: dest[63:0] := MAX_FLOAT
    15: dest[63:0] := -MAX_FLOAT
    ESAC
    
    CASE(tsrc[31:0]) OF
    ZERO_VALUE_TOKEN:
        IF (imm8[0]) #ZE; FI
    ZERO_VALUE_TOKEN:
        IF (imm8[1]) #IE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[2]) #ZE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[3]) #IE; FI
    SNAN_TOKEN:
        IF (imm8[4]) #IE; FI
    NEG_INF_TOKEN:
        IF (imm8[5]) #IE; FI
    NEG_VALUE_TOKEN:
        IF (imm8[6]) #IE; FI
    POS_INF_TOKEN:
        IF (imm8[7]) #IE; FI
    ESAC
    RETURN dest[63:0]
}
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFIXUPIMMPD - _mm_maskz_fixupimm_pd

| VFIXUPIMMPD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Fix up packed double-precision (64-bit) floating-point elements in "a" and "b" using packed 64-bit integers in
"c", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit
is not set). "imm8" is used to set the required flags reporting.

[algorithm]

enum TOKEN_TYPE {
    QNAN_TOKEN := 0, \
    SNAN_TOKEN := 1, \
    ZERO_VALUE_TOKEN := 2, \
    ONE_VALUE_TOKEN := 3, \
    NEG_INF_TOKEN := 4, \
    POS_INF_TOKEN := 5, \
    NEG_VALUE_TOKEN := 6, \
    POS_VALUE_TOKEN := 7
}
DEFINE FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]) {
    tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
    CASE(tsrc[63:0]) OF
    QNAN_TOKEN:j := 0
    SNAN_TOKEN:j := 1
    ZERO_VALUE_TOKEN: j := 2
    ONE_VALUE_TOKEN: j := 3
    NEG_INF_TOKEN: j := 4
    POS_INF_TOKEN: j := 5
    NEG_VALUE_TOKEN: j := 6
    POS_VALUE_TOKEN: j := 7
    ESAC
    
    token_response[3:0] := src3[3+4*j:4*j]
    
    CASE(token_response[3:0]) OF
    0 : dest[63:0] := src1[63:0]
    1 : dest[63:0] := tsrc[63:0]
    2 : dest[63:0] := QNaN(tsrc[63:0])
    3 : dest[63:0] := QNAN_Indefinite
    4 : dest[63:0] := -INF
    5 : dest[63:0] := +INF
    6 : dest[63:0] := tsrc.sign? -INF : +INF
    7 : dest[63:0] := -0
    8 : dest[63:0] := +0
    9 : dest[63:0] := -1
    10: dest[63:0] := +1
    11: dest[63:0] := 1/2
    12: dest[63:0] := 90.0
    13: dest[63:0] := PI/2
    14: dest[63:0] := MAX_FLOAT
    15: dest[63:0] := -MAX_FLOAT
    ESAC
    
    CASE(tsrc[31:0]) OF
    ZERO_VALUE_TOKEN:
        IF (imm8[0]) #ZE; FI
    ZERO_VALUE_TOKEN:
        IF (imm8[1]) #IE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[2]) #ZE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[3]) #IE; FI
    SNAN_TOKEN:
        IF (imm8[4]) #IE; FI
    NEG_INF_TOKEN:
        IF (imm8[5]) #IE; FI
    NEG_VALUE_TOKEN:
        IF (imm8[6]) #IE; FI
    POS_INF_TOKEN:
        IF (imm8[7]) #IE; FI
    ESAC
    RETURN dest[63:0]
}
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFIXUPIMMPS - _mm256_fixupimm_ps

| VFIXUPIMMPS_YMMf32_MASKmskw_YMMf32_YMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Fix up packed single-precision (32-bit) floating-point elements in "a" and "b" using packed 32-bit integers in
"c", and store the results in "dst". "imm8" is used to set the required flags reporting.

[algorithm]

enum TOKEN_TYPE {
    QNAN_TOKEN := 0, \
    SNAN_TOKEN := 1, \
    ZERO_VALUE_TOKEN := 2, \
    ONE_VALUE_TOKEN := 3, \
    NEG_INF_TOKEN := 4, \
    POS_INF_TOKEN := 5, \
    NEG_VALUE_TOKEN := 6, \
    POS_VALUE_TOKEN := 7
}
DEFINE FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]) {
    tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
    CASE(tsrc[31:0]) OF
    QNAN_TOKEN:j := 0
    SNAN_TOKEN:j := 1
    ZERO_VALUE_TOKEN: j := 2
    ONE_VALUE_TOKEN: j := 3
    NEG_INF_TOKEN: j := 4
    POS_INF_TOKEN: j := 5
    NEG_VALUE_TOKEN: j := 6
    POS_VALUE_TOKEN: j := 7
    ESAC
    
    token_response[3:0] := src3[3+4*j:4*j]
    
    CASE(token_response[3:0]) OF
    0 : dest[31:0] := src1[31:0]
    1 : dest[31:0] := tsrc[31:0]
    2 : dest[31:0] := QNaN(tsrc[31:0])
    3 : dest[31:0] := QNAN_Indefinite
    4 : dest[31:0] := -INF
    5 : dest[31:0] := +INF
    6 : dest[31:0] := tsrc.sign? -INF : +INF
    7 : dest[31:0] := -0
    8 : dest[31:0] := +0
    9 : dest[31:0] := -1
    10: dest[31:0] := +1
    11: dest[31:0] := 1/2
    12: dest[31:0] := 90.0
    13: dest[31:0] := PI/2
    14: dest[31:0] := MAX_FLOAT
    15: dest[31:0] := -MAX_FLOAT
    ESAC
    
    CASE(tsrc[31:0]) OF
    ZERO_VALUE_TOKEN:
        IF (imm8[0]) #ZE; FI
    ZERO_VALUE_TOKEN:
        IF (imm8[1]) #IE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[2]) #ZE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[3]) #IE; FI
    SNAN_TOKEN:
        IF (imm8[4]) #IE; FI
    NEG_INF_TOKEN:
        IF (imm8[5]) #IE; FI
    NEG_VALUE_TOKEN:
        IF (imm8[6]) #IE; FI
    POS_INF_TOKEN:
        IF (imm8[7]) #IE; FI
    ESAC
    RETURN dest[31:0]
}
FOR j := 0 to 7
    i := j*32
    dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFIXUPIMMPS - _mm256_mask_fixupimm_ps

| VFIXUPIMMPS_YMMf32_MASKmskw_YMMf32_YMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Fix up packed single-precision (32-bit) floating-point elements in "a" and "b" using packed 32-bit integers in
"c", and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding
mask bit is not set). "imm8" is used to set the required flags reporting.

[algorithm]

enum TOKEN_TYPE {
    QNAN_TOKEN := 0, \
    SNAN_TOKEN := 1, \
    ZERO_VALUE_TOKEN := 2, \
    ONE_VALUE_TOKEN := 3, \
    NEG_INF_TOKEN := 4, \
    POS_INF_TOKEN := 5, \
    NEG_VALUE_TOKEN := 6, \
    POS_VALUE_TOKEN := 7
}
DEFINE FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]) {
    tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
    CASE(tsrc[31:0]) OF
    QNAN_TOKEN:j := 0
    SNAN_TOKEN:j := 1
    ZERO_VALUE_TOKEN: j := 2
    ONE_VALUE_TOKEN: j := 3
    NEG_INF_TOKEN: j := 4
    POS_INF_TOKEN: j := 5
    NEG_VALUE_TOKEN: j := 6
    POS_VALUE_TOKEN: j := 7
    ESAC
    
    token_response[3:0] := src3[3+4*j:4*j]
    
    CASE(token_response[3:0]) OF
    0 : dest[31:0] := src1[31:0]
    1 : dest[31:0] := tsrc[31:0]
    2 : dest[31:0] := QNaN(tsrc[31:0])
    3 : dest[31:0] := QNAN_Indefinite
    4 : dest[31:0] := -INF
    5 : dest[31:0] := +INF
    6 : dest[31:0] := tsrc.sign? -INF : +INF
    7 : dest[31:0] := -0
    8 : dest[31:0] := +0
    9 : dest[31:0] := -1
    10: dest[31:0] := +1
    11: dest[31:0] := 1/2
    12: dest[31:0] := 90.0
    13: dest[31:0] := PI/2
    14: dest[31:0] := MAX_FLOAT
    15: dest[31:0] := -MAX_FLOAT
    ESAC
    
    CASE(tsrc[31:0]) OF
    ZERO_VALUE_TOKEN:
        IF (imm8[0]) #ZE; FI
    ZERO_VALUE_TOKEN:
        IF (imm8[1]) #IE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[2]) #ZE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[3]) #IE; FI
    SNAN_TOKEN:
        IF (imm8[4]) #IE; FI
    NEG_INF_TOKEN:
        IF (imm8[5]) #IE; FI
    NEG_VALUE_TOKEN:
        IF (imm8[6]) #IE; FI
    POS_INF_TOKEN:
        IF (imm8[7]) #IE; FI
    ESAC
    RETURN dest[31:0]
}
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
    ELSE
        dst[i+31:i] := a[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFIXUPIMMPS - _mm256_maskz_fixupimm_ps

| VFIXUPIMMPS_YMMf32_MASKmskw_YMMf32_YMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Fix up packed single-precision (32-bit) floating-point elements in "a" and "b" using packed 32-bit integers in
"c", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit
is not set). "imm8" is used to set the required flags reporting.

[algorithm]

enum TOKEN_TYPE {
    QNAN_TOKEN := 0, \
    SNAN_TOKEN := 1, \
    ZERO_VALUE_TOKEN := 2, \
    ONE_VALUE_TOKEN := 3, \
    NEG_INF_TOKEN := 4, \
    POS_INF_TOKEN := 5, \
    NEG_VALUE_TOKEN := 6, \
    POS_VALUE_TOKEN := 7
}
DEFINE FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]) {
    tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
    CASE(tsrc[31:0]) OF
    QNAN_TOKEN:j := 0
    SNAN_TOKEN:j := 1
    ZERO_VALUE_TOKEN: j := 2
    ONE_VALUE_TOKEN: j := 3
    NEG_INF_TOKEN: j := 4
    POS_INF_TOKEN: j := 5
    NEG_VALUE_TOKEN: j := 6
    POS_VALUE_TOKEN: j := 7
    ESAC
    
    token_response[3:0] := src3[3+4*j:4*j]
    
    CASE(token_response[3:0]) OF
    0 : dest[31:0] := src1[31:0]
    1 : dest[31:0] := tsrc[31:0]
    2 : dest[31:0] := QNaN(tsrc[31:0])
    3 : dest[31:0] := QNAN_Indefinite
    4 : dest[31:0] := -INF
    5 : dest[31:0] := +INF
    6 : dest[31:0] := tsrc.sign? -INF : +INF
    7 : dest[31:0] := -0
    8 : dest[31:0] := +0
    9 : dest[31:0] := -1
    10: dest[31:0] := +1
    11: dest[31:0] := 1/2
    12: dest[31:0] := 90.0
    13: dest[31:0] := PI/2
    14: dest[31:0] := MAX_FLOAT
    15: dest[31:0] := -MAX_FLOAT
    ESAC
    
    CASE(tsrc[31:0]) OF
    ZERO_VALUE_TOKEN:
        IF (imm8[0]) #ZE; FI
    ZERO_VALUE_TOKEN:
        IF (imm8[1]) #IE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[2]) #ZE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[3]) #IE; FI
    SNAN_TOKEN:
        IF (imm8[4]) #IE; FI
    NEG_INF_TOKEN:
        IF (imm8[5]) #IE; FI
    NEG_VALUE_TOKEN:
        IF (imm8[6]) #IE; FI
    POS_INF_TOKEN:
        IF (imm8[7]) #IE; FI
    ESAC
    RETURN dest[31:0]
}
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFIXUPIMMPS - _mm_fixupimm_ps

| VFIXUPIMMPS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Fix up packed single-precision (32-bit) floating-point elements in "a" and "b" using packed 32-bit integers in
"c", and store the results in "dst". "imm8" is used to set the required flags reporting.

[algorithm]

enum TOKEN_TYPE {
    QNAN_TOKEN := 0, \
    SNAN_TOKEN := 1, \
    ZERO_VALUE_TOKEN := 2, \
    ONE_VALUE_TOKEN := 3, \
    NEG_INF_TOKEN := 4, \
    POS_INF_TOKEN := 5, \
    NEG_VALUE_TOKEN := 6, \
    POS_VALUE_TOKEN := 7
}
DEFINE FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]) {
    tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
    CASE(tsrc[31:0]) OF
    QNAN_TOKEN:j := 0
    SNAN_TOKEN:j := 1
    ZERO_VALUE_TOKEN: j := 2
    ONE_VALUE_TOKEN: j := 3
    NEG_INF_TOKEN: j := 4
    POS_INF_TOKEN: j := 5
    NEG_VALUE_TOKEN: j := 6
    POS_VALUE_TOKEN: j := 7
    ESAC
    
    token_response[3:0] := src3[3+4*j:4*j]
    
    CASE(token_response[3:0]) OF
    0 : dest[31:0] := src1[31:0]
    1 : dest[31:0] := tsrc[31:0]
    2 : dest[31:0] := QNaN(tsrc[31:0])
    3 : dest[31:0] := QNAN_Indefinite
    4 : dest[31:0] := -INF
    5 : dest[31:0] := +INF
    6 : dest[31:0] := tsrc.sign? -INF : +INF
    7 : dest[31:0] := -0
    8 : dest[31:0] := +0
    9 : dest[31:0] := -1
    10: dest[31:0] := +1
    11: dest[31:0] := 1/2
    12: dest[31:0] := 90.0
    13: dest[31:0] := PI/2
    14: dest[31:0] := MAX_FLOAT
    15: dest[31:0] := -MAX_FLOAT
    ESAC
    
    CASE(tsrc[31:0]) OF
    ZERO_VALUE_TOKEN:
        IF (imm8[0]) #ZE; FI
    ZERO_VALUE_TOKEN:
        IF (imm8[1]) #IE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[2]) #ZE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[3]) #IE; FI
    SNAN_TOKEN:
        IF (imm8[4]) #IE; FI
    NEG_INF_TOKEN:
        IF (imm8[5]) #IE; FI
    NEG_VALUE_TOKEN:
        IF (imm8[6]) #IE; FI
    POS_INF_TOKEN:
        IF (imm8[7]) #IE; FI
    ESAC
    RETURN dest[31:0]
}
FOR j := 0 to 3
    i := j*32
    dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFIXUPIMMPS - _mm_mask_fixupimm_ps

| VFIXUPIMMPS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Fix up packed single-precision (32-bit) floating-point elements in "a" and "b" using packed 32-bit integers in
"c", and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding
mask bit is not set). "imm8" is used to set the required flags reporting.

[algorithm]

enum TOKEN_TYPE {
    QNAN_TOKEN := 0, \
    SNAN_TOKEN := 1, \
    ZERO_VALUE_TOKEN := 2, \
    ONE_VALUE_TOKEN := 3, \
    NEG_INF_TOKEN := 4, \
    POS_INF_TOKEN := 5, \
    NEG_VALUE_TOKEN := 6, \
    POS_VALUE_TOKEN := 7
}
DEFINE FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]) {
    tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
    CASE(tsrc[31:0]) OF
    QNAN_TOKEN:j := 0
    SNAN_TOKEN:j := 1
    ZERO_VALUE_TOKEN: j := 2
    ONE_VALUE_TOKEN: j := 3
    NEG_INF_TOKEN: j := 4
    POS_INF_TOKEN: j := 5
    NEG_VALUE_TOKEN: j := 6
    POS_VALUE_TOKEN: j := 7
    ESAC
    
    token_response[3:0] := src3[3+4*j:4*j]
    
    CASE(token_response[3:0]) OF
    0 : dest[31:0] := src1[31:0]
    1 : dest[31:0] := tsrc[31:0]
    2 : dest[31:0] := QNaN(tsrc[31:0])
    3 : dest[31:0] := QNAN_Indefinite
    4 : dest[31:0] := -INF
    5 : dest[31:0] := +INF
    6 : dest[31:0] := tsrc.sign? -INF : +INF
    7 : dest[31:0] := -0
    8 : dest[31:0] := +0
    9 : dest[31:0] := -1
    10: dest[31:0] := +1
    11: dest[31:0] := 1/2
    12: dest[31:0] := 90.0
    13: dest[31:0] := PI/2
    14: dest[31:0] := MAX_FLOAT
    15: dest[31:0] := -MAX_FLOAT
    ESAC
    
    CASE(tsrc[31:0]) OF
    ZERO_VALUE_TOKEN:
        IF (imm8[0]) #ZE; FI
    ZERO_VALUE_TOKEN:
        IF (imm8[1]) #IE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[2]) #ZE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[3]) #IE; FI
    SNAN_TOKEN:
        IF (imm8[4]) #IE; FI
    NEG_INF_TOKEN:
        IF (imm8[5]) #IE; FI
    NEG_VALUE_TOKEN:
        IF (imm8[6]) #IE; FI
    POS_INF_TOKEN:
        IF (imm8[7]) #IE; FI
    ESAC
    RETURN dest[31:0]
}
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
    ELSE
        dst[i+31:i] := a[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFIXUPIMMPS - _mm_maskz_fixupimm_ps

| VFIXUPIMMPS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Fix up packed single-precision (32-bit) floating-point elements in "a" and "b" using packed 32-bit integers in
"c", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit
is not set). "imm8" is used to set the required flags reporting.

[algorithm]

enum TOKEN_TYPE {
    QNAN_TOKEN := 0, \
    SNAN_TOKEN := 1, \
    ZERO_VALUE_TOKEN := 2, \
    ONE_VALUE_TOKEN := 3, \
    NEG_INF_TOKEN := 4, \
    POS_INF_TOKEN := 5, \
    NEG_VALUE_TOKEN := 6, \
    POS_VALUE_TOKEN := 7
}
DEFINE FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]) {
    tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
    CASE(tsrc[31:0]) OF
    QNAN_TOKEN:j := 0
    SNAN_TOKEN:j := 1
    ZERO_VALUE_TOKEN: j := 2
    ONE_VALUE_TOKEN: j := 3
    NEG_INF_TOKEN: j := 4
    POS_INF_TOKEN: j := 5
    NEG_VALUE_TOKEN: j := 6
    POS_VALUE_TOKEN: j := 7
    ESAC
    
    token_response[3:0] := src3[3+4*j:4*j]
    
    CASE(token_response[3:0]) OF
    0 : dest[31:0] := src1[31:0]
    1 : dest[31:0] := tsrc[31:0]
    2 : dest[31:0] := QNaN(tsrc[31:0])
    3 : dest[31:0] := QNAN_Indefinite
    4 : dest[31:0] := -INF
    5 : dest[31:0] := +INF
    6 : dest[31:0] := tsrc.sign? -INF : +INF
    7 : dest[31:0] := -0
    8 : dest[31:0] := +0
    9 : dest[31:0] := -1
    10: dest[31:0] := +1
    11: dest[31:0] := 1/2
    12: dest[31:0] := 90.0
    13: dest[31:0] := PI/2
    14: dest[31:0] := MAX_FLOAT
    15: dest[31:0] := -MAX_FLOAT
    ESAC
    
    CASE(tsrc[31:0]) OF
    ZERO_VALUE_TOKEN:
        IF (imm8[0]) #ZE; FI
    ZERO_VALUE_TOKEN:
        IF (imm8[1]) #IE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[2]) #ZE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[3]) #IE; FI
    SNAN_TOKEN:
        IF (imm8[4]) #IE; FI
    NEG_INF_TOKEN:
        IF (imm8[5]) #IE; FI
    NEG_VALUE_TOKEN:
        IF (imm8[6]) #IE; FI
    POS_INF_TOKEN:
        IF (imm8[7]) #IE; FI
    ESAC
    RETURN dest[31:0]
}
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADD132PD - _mm256_mask3_fmadd_pd

| VFMADD132PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VFMADD213PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VFMADD231PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", add the intermediate result
to packed elements in "c", and store the results in "dst" using writemask "k" (elements are copied from "c"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
    ELSE
        dst[i+63:i] := c[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADD132PD - _mm256_mask_fmadd_pd

| VFMADD132PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VFMADD213PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VFMADD231PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", add the intermediate result
to packed elements in "c", and store the results in "dst" using writemask "k" (elements are copied from "a"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADD132PD - _mm256_maskz_fmadd_pd

| VFMADD132PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VFMADD213PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VFMADD231PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", add the intermediate result
to packed elements in "c", and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADD132PD - _mm_mask3_fmadd_pd

| VFMADD132PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMADD213PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMADD231PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", add the intermediate result
to packed elements in "c", and store the results in "dst" using writemask "k" (elements are copied from "c"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
    ELSE
        dst[i+63:i] := c[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADD132PD - _mm_mask_fmadd_pd

| VFMADD132PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMADD213PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMADD231PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", add the intermediate result
to packed elements in "c", and store the results in "dst" using writemask "k" (elements are copied from "a"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADD132PD - _mm_maskz_fmadd_pd

| VFMADD132PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMADD213PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMADD231PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", add the intermediate result
to packed elements in "c", and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADD132PS - _mm256_mask3_fmadd_ps

| VFMADD132PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VFMADD213PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VFMADD231PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", add the intermediate result
to packed elements in "c", and store the results in "dst" using writemask "k" (elements are copied from "c"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
    ELSE
        dst[i+31:i] := c[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADD132PS - _mm256_mask_fmadd_ps

| VFMADD132PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VFMADD213PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VFMADD231PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", add the intermediate result
to packed elements in "c", and store the results in "dst" using writemask "k" (elements are copied from "a"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
    ELSE
        dst[i+31:i] := a[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADD132PS - _mm256_maskz_fmadd_ps

| VFMADD132PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VFMADD213PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VFMADD231PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", add the intermediate result
to packed elements in "c", and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADD132PS - _mm_mask3_fmadd_ps

| VFMADD132PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMADD213PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMADD231PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", add the intermediate result
to packed elements in "c", and store the results in "dst" using writemask "k" (elements are copied from "c"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
    ELSE
        dst[i+31:i] := c[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADD132PS - _mm_mask_fmadd_ps

| VFMADD132PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMADD213PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMADD231PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", add the intermediate result
to packed elements in "c", and store the results in "dst" using writemask "k" (elements are copied from "a"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
    ELSE
        dst[i+31:i] := a[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADD132PS - _mm_maskz_fmadd_ps

| VFMADD132PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMADD213PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMADD231PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", add the intermediate result
to packed elements in "c", and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADDSUB132PD - _mm256_mask3_fmaddsub_pd

| VFMADDSUB132PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VFMADDSUB213PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VFMADDSUB231PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively add and
subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using writemask
"k" (elements are copied from "c" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        IF ((j &amp; 1) == 0) 
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
        ELSE
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
        FI
    ELSE
        dst[i+63:i] := c[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADDSUB132PD - _mm256_mask_fmaddsub_pd

| VFMADDSUB132PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VFMADDSUB213PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VFMADDSUB231PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively add and
subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using writemask
"k" (elements are copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        IF ((j &amp; 1) == 0) 
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
        ELSE
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
        FI
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADDSUB132PD - _mm256_maskz_fmaddsub_pd

| VFMADDSUB132PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VFMADDSUB213PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VFMADDSUB231PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively add and
subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        IF ((j &amp; 1) == 0) 
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
        ELSE
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
        FI
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADDSUB132PD - _mm_mask3_fmaddsub_pd

| VFMADDSUB132PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMADDSUB213PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMADDSUB231PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively add and
subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using writemask
"k" (elements are copied from "c" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        IF ((j &amp; 1) == 0) 
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
        ELSE
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
        FI
    ELSE
        dst[i+63:i] := c[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADDSUB132PD - _mm_mask_fmaddsub_pd

| VFMADDSUB132PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMADDSUB213PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMADDSUB231PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively add and
subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using writemask
"k" (elements are copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        IF ((j &amp; 1) == 0) 
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
        ELSE
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
        FI
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADDSUB132PD - _mm_maskz_fmaddsub_pd

| VFMADDSUB132PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMADDSUB213PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMADDSUB231PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively add and
subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        IF ((j &amp; 1) == 0) 
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
        ELSE
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
        FI
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADDSUB132PS - _mm256_mask3_fmaddsub_ps

| VFMADDSUB132PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VFMADDSUB213PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VFMADDSUB231PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively add and
subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using writemask
"k" (elements are copied from "c" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        IF ((j &amp; 1) == 0) 
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
        ELSE
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
        FI
    ELSE
        dst[i+31:i] := c[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADDSUB132PS - _mm256_mask_fmaddsub_ps

| VFMADDSUB132PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VFMADDSUB213PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VFMADDSUB231PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively add and
subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using writemask
"k" (elements are copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        IF ((j &amp; 1) == 0) 
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
        ELSE
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
        FI
    ELSE
        dst[i+31:i] := a[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADDSUB132PS - _mm256_maskz_fmaddsub_ps

| VFMADDSUB132PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VFMADDSUB213PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VFMADDSUB231PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively add and
subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        IF ((j &amp; 1) == 0) 
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
        ELSE
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
        FI
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADDSUB132PS - _mm_mask3_fmaddsub_ps

| VFMADDSUB132PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMADDSUB213PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMADDSUB231PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively add and
subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using writemask
"k" (elements are copied from "c" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        IF ((j &amp; 1) == 0) 
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
        ELSE
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
        FI
    ELSE
        dst[i+31:i] := c[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADDSUB132PS - _mm_mask_fmaddsub_ps

| VFMADDSUB132PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMADDSUB213PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMADDSUB231PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively add and
subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using writemask
"k" (elements are copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        IF ((j &amp; 1) == 0) 
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
        ELSE
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
        FI
    ELSE
        dst[i+31:i] := a[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADDSUB132PS - _mm_maskz_fmaddsub_ps

| VFMADDSUB132PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMADDSUB213PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMADDSUB231PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively add and
subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        IF ((j &amp; 1) == 0) 
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
        ELSE
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
        FI
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUB132PD - _mm256_mask3_fmsub_pd

| VFMSUB132PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VFMSUB213PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VFMSUB231PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", subtract packed elements in
"c" from the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from
"c" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
    ELSE
        dst[i+63:i] := c[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUB132PD - _mm256_mask_fmsub_pd

| VFMSUB132PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VFMSUB213PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VFMSUB231PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", subtract packed elements in
"c" from the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from
"a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUB132PD - _mm256_maskz_fmsub_pd

| VFMSUB132PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VFMSUB213PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VFMSUB231PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", subtract packed elements in
"c" from the intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUB132PD - _mm_mask3_fmsub_pd

| VFMSUB132PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMSUB213PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMSUB231PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", subtract packed elements in
"c" from the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from
"c" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
    ELSE
        dst[i+63:i] := c[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUB132PD - _mm_mask_fmsub_pd

| VFMSUB132PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMSUB213PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMSUB231PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", subtract packed elements in
"c" from the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from
"a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUB132PD - _mm_maskz_fmsub_pd

| VFMSUB132PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMSUB213PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMSUB231PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", subtract packed elements in
"c" from the intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUB132PS - _mm256_mask3_fmsub_ps

| VFMSUB132PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VFMSUB213PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VFMSUB231PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", subtract packed elements in
"c" from the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from
"c" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
    ELSE
        dst[i+31:i] := c[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUB132PS - _mm256_mask_fmsub_ps

| VFMSUB132PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VFMSUB213PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VFMSUB231PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", subtract packed elements in
"c" from the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from
"a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
    ELSE
        dst[i+31:i] := a[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUB132PS - _mm256_maskz_fmsub_ps

| VFMSUB132PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VFMSUB213PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VFMSUB231PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", subtract packed elements in
"c" from the intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUB132PS - _mm_mask3_fmsub_ps

| VFMSUB132PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMSUB213PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMSUB231PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", subtract packed elements in
"c" from the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from
"c" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
    ELSE
        dst[i+31:i] := c[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUB132PS - _mm_mask_fmsub_ps

| VFMSUB132PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMSUB213PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMSUB231PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", subtract packed elements in
"c" from the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from
"a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
    ELSE
        dst[i+31:i] := a[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUB132PS - _mm_maskz_fmsub_ps

| VFMSUB132PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMSUB213PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMSUB231PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", subtract packed elements in
"c" from the intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUBADD132PD - _mm256_mask3_fmsubadd_pd

| VFMSUBADD132PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VFMSUBADD213PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VFMSUBADD231PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively subtract and
add packed elements in "c" from/to the intermediate result, and store the results in "dst" using writemask "k"
(elements are copied from "c" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        IF ((j &amp; 1) == 0) 
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
        ELSE
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
        FI
    ELSE
        dst[i+63:i] := c[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUBADD132PD - _mm256_mask_fmsubadd_pd

| VFMSUBADD132PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VFMSUBADD213PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VFMSUBADD231PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively subtract and
add packed elements in "c" from/to the intermediate result, and store the results in "dst" using writemask "k"
(elements are copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        IF ((j &amp; 1) == 0) 
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
        ELSE
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
        FI
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUBADD132PD - _mm256_maskz_fmsubadd_pd

| VFMSUBADD132PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VFMSUBADD213PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VFMSUBADD231PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively subtract and
add packed elements in "c" from/to the intermediate result, and store the results in "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        IF ((j &amp; 1) == 0) 
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
        ELSE
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
        FI
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUBADD132PD - _mm_mask3_fmsubadd_pd

| VFMSUBADD132PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMSUBADD213PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMSUBADD231PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively subtract and
add packed elements in "c" from/to the intermediate result, and store the results in "dst" using writemask "k"
(elements are copied from "c" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        IF ((j &amp; 1) == 0) 
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
        ELSE
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
        FI
    ELSE
        dst[i+63:i] := c[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUBADD132PD - _mm_mask_fmsubadd_pd

| VFMSUBADD132PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMSUBADD213PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMSUBADD231PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively subtract and
add packed elements in "c" from/to the intermediate result, and store the results in "dst" using writemask "k"
(elements are copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1 
    i := j*64
    IF k[j]
        IF ((j &amp; 1) == 0) 
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
        ELSE
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
        FI
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUBADD132PD - _mm_maskz_fmsubadd_pd

| VFMSUBADD132PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMSUBADD213PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMSUBADD231PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively subtract and
add packed elements in "c" from/to the intermediate result, and store the results in "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        IF ((j &amp; 1) == 0) 
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
        ELSE
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
        FI
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUBADD132PS - _mm256_mask3_fmsubadd_ps

| VFMSUBADD132PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VFMSUBADD213PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VFMSUBADD231PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively subtract and
add packed elements in "c" from/to the intermediate result, and store the results in "dst" using writemask "k"
(elements are copied from "c" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        IF ((j &amp; 1) == 0) 
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
        ELSE
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
        FI
    ELSE
        dst[i+31:i] := c[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUBADD132PS - _mm256_mask_fmsubadd_ps

| VFMSUBADD132PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VFMSUBADD213PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VFMSUBADD231PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively subtract and
add packed elements in "c" from/to the intermediate result, and store the results in "dst" using writemask "k"
(elements are copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        IF ((j &amp; 1) == 0) 
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
        ELSE
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
        FI
    ELSE
        dst[i+31:i] := a[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUBADD132PS - _mm256_maskz_fmsubadd_ps

| VFMSUBADD132PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VFMSUBADD213PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VFMSUBADD231PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively subtract and
add packed elements in "c" from/to the intermediate result, and store the results in "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        IF ((j &amp; 1) == 0) 
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
        ELSE
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
        FI
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUBADD132PS - _mm_mask3_fmsubadd_ps

| VFMSUBADD132PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMSUBADD213PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMSUBADD231PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively subtract and
add packed elements in "c" from/to the intermediate result, and store the results in "dst" using writemask "k"
(elements are copied from "c" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        IF ((j &amp; 1) == 0) 
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
        ELSE
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
        FI
    ELSE
        dst[i+31:i] := c[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUBADD132PS - _mm_mask_fmsubadd_ps

| VFMSUBADD132PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMSUBADD213PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMSUBADD231PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively subtract and
add packed elements in "c" from/to the intermediate result, and store the results in "dst" using writemask "k"
(elements are copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        IF ((j &amp; 1) == 0) 
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
        ELSE
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
        FI
    ELSE
        dst[i+31:i] := a[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUBADD132PS - _mm_maskz_fmsubadd_ps

| VFMSUBADD132PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMSUBADD213PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMSUBADD231PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively subtract and
add packed elements in "c" from/to the intermediate result, and store the results in "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        IF ((j &amp; 1) == 0) 
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
        ELSE
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
        FI
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMADD132PD - _mm256_mask3_fnmadd_pd

| VFNMADD132PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VFNMADD213PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VFNMADD231PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", add the negated intermediate
result to packed elements in "c", and store the results in "dst" using writemask "k" (elements are copied from
"c" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
    ELSE
        dst[i+63:i] := c[i+63:i]
    FI
ENDFOR    
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMADD132PD - _mm256_mask_fnmadd_pd

| VFNMADD132PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VFNMADD213PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VFNMADD231PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", add the negated intermediate
result to packed elements in "c", and store the results in "dst" using writemask "k" (elements are copied from
"a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR    
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMADD132PD - _mm256_maskz_fnmadd_pd

| VFNMADD132PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VFNMADD213PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VFNMADD231PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", add the negated intermediate
result to packed elements in "c", and store the results in "dst" using zeromask "k" (elements are zeroed out
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR    
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMADD132PD - _mm_mask3_fnmadd_pd

| VFNMADD132PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMADD213PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMADD231PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", add the negated intermediate
result to packed elements in "c", and store the results in "dst" using writemask "k" (elements are copied from
"c" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
    ELSE
        dst[i+63:i] := c[i+63:i]
    FI
ENDFOR    
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMADD132PD - _mm_mask_fnmadd_pd

| VFNMADD132PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMADD213PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMADD231PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", add the negated intermediate
result to packed elements in "c", and store the results in "dst" using writemask "k" (elements are copied from
"a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR    
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMADD132PD - _mm_maskz_fnmadd_pd

| VFNMADD132PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMADD213PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMADD231PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", add the negated intermediate
result to packed elements in "c", and store the results in "dst" using zeromask "k" (elements are zeroed out
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR    
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMADD132PS - _mm256_mask3_fnmadd_ps

| VFNMADD132PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VFNMADD213PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VFNMADD231PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", add the negated intermediate
result to packed elements in "c", and store the results in "dst" using writemask "k" (elements are copied from
"c" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
    ELSE
        dst[i+31:i] := c[i+31:i]
    FI
ENDFOR    
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMADD132PS - _mm256_mask_fnmadd_ps

| VFNMADD132PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VFNMADD213PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VFNMADD231PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", add the negated intermediate
result to packed elements in "c", and store the results in "dst" using writemask "k" (elements are copied from
"a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
    ELSE
        dst[i+31:i] := a[i+31:i]
    FI
ENDFOR    
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMADD132PS - _mm256_maskz_fnmadd_ps

| VFNMADD132PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VFNMADD213PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VFNMADD231PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", add the negated intermediate
result to packed elements in "c", and store the results in "dst" using zeromask "k" (elements are zeroed out
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR    
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMADD132PS - _mm_mask3_fnmadd_ps

| VFNMADD132PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMADD213PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMADD231PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", add the negated intermediate
result to packed elements in "c", and store the results in "dst" using writemask "k" (elements are copied from
"c" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
    ELSE
        dst[i+31:i] := c[i+31:i]
    FI
ENDFOR    
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMADD132PS - _mm_mask_fnmadd_ps

| VFNMADD132PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMADD213PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMADD231PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", add the negated intermediate
result to packed elements in "c", and store the results in "dst" using writemask "k" (elements are copied from
"a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
    ELSE
        dst[i+31:i] := a[i+31:i]
    FI
ENDFOR    
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMADD132PS - _mm_maskz_fnmadd_ps

| VFNMADD132PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMADD213PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMADD231PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", add the negated intermediate
result to packed elements in "c", and store the results in "dst" using zeromask "k" (elements are zeroed out
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR    
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMSUB132PD - _mm256_mask3_fnmsub_pd

| VFNMSUB132PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VFNMSUB213PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VFNMSUB231PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", subtract packed elements in
"c" from the negated intermediate result, and store the results in "dst" using writemask "k" (elements are
copied from "c" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
    ELSE
        dst[i+63:i] := c[i+63:i]
    FI
ENDFOR    
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMSUB132PD - _mm256_mask_fnmsub_pd

| VFNMSUB132PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VFNMSUB213PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VFNMSUB231PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", subtract packed elements in
"c" from the negated intermediate result, and store the results in "dst" using writemask "k" (elements are
copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR    
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMSUB132PD - _mm256_maskz_fnmsub_pd

| VFNMSUB132PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VFNMSUB213PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VFNMSUB231PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", subtract packed elements in
"c" from the negated intermediate result, and store the results in "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR    
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMSUB132PD - _mm_mask3_fnmsub_pd

| VFNMSUB132PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMSUB213PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMSUB231PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", subtract packed elements in
"c" from the negated intermediate result, and store the results in "dst" using writemask "k" (elements are
copied from "c" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
    ELSE
        dst[i+63:i] := c[i+63:i]
    FI
ENDFOR    
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMSUB132PD - _mm_mask_fnmsub_pd

| VFNMSUB132PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMSUB213PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMSUB231PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", subtract packed elements in
"c" from the negated intermediate result, and store the results in "dst" using writemask "k" (elements are
copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR    
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMSUB132PD - _mm_maskz_fnmsub_pd

| VFNMSUB132PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMSUB213PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMSUB231PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", subtract packed elements in
"c" from the negated intermediate result, and store the results in "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR    
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMSUB132PS - _mm256_mask3_fnmsub_ps

| VFNMSUB132PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VFNMSUB213PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VFNMSUB231PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", subtract packed elements in
"c" from the negated intermediate result, and store the results in "dst" using writemask "k" (elements are
copied from "c" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
    ELSE
        dst[i+31:i] := c[i+31:i]
    FI
ENDFOR    
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMSUB132PS - _mm256_mask_fnmsub_ps

| VFNMSUB132PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VFNMSUB213PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VFNMSUB231PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", subtract packed elements in
"c" from the negated intermediate result, and store the results in "dst" using writemask "k" (elements are
copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
    ELSE
        dst[i+31:i] := a[i+31:i]
    FI
ENDFOR    
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMSUB132PS - _mm256_maskz_fnmsub_ps

| VFNMSUB132PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VFNMSUB213PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VFNMSUB231PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", subtract packed elements in
"c" from the negated intermediate result, and store the results in "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR    
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMSUB132PS - _mm_mask3_fnmsub_ps

| VFNMSUB132PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMSUB213PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMSUB231PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", subtract packed elements in
"c" from the negated intermediate result, and store the results in "dst" using writemask "k" (elements are
copied from "c" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
    ELSE
        dst[i+31:i] := c[i+31:i]
    FI
ENDFOR    
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMSUB132PS - _mm_mask_fnmsub_ps

| VFNMSUB132PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMSUB213PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMSUB231PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", subtract packed elements in
"c" from the negated intermediate result, and store the results in "dst" using writemask "k" (elements are
copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
    ELSE
        dst[i+31:i] := a[i+31:i]
    FI
ENDFOR    
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMSUB132PS - _mm_maskz_fnmsub_ps

| VFNMSUB132PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMSUB213PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMSUB231PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", subtract packed elements in
"c" from the negated intermediate result, and store the results in "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR    
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGATHERDPD - _mm256_mmask_i32gather_pd

| VGATHERDPD_YMMf64_MASKmskw_MEMf64_AVX512_VL256

--------------------------------------------------------------------------------------------------------------
Gather double-precision (64-bit) floating-point elements from memory using 32-bit indices. 64-bit elements are
loaded from addresses starting at "base_addr" and offset by each 32-bit element in "vindex" (each index is
scaled by the factor in "scale"). Gathered elements are merged into "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 3
    i := j*64
    m := j*32
    IF k[j]
        addr := base_addr + SignExtend64(vindex[m+31:m]) * ZeroExtend64(scale) * 8
        dst[i+63:i] := MEM[addr+63:addr]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VGATHERDPD - _mm_mmask_i32gather_pd

| VGATHERDPD_XMMf64_MASKmskw_MEMf64_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Gather double-precision (64-bit) floating-point elements from memory using 32-bit indices. 64-bit elements are
loaded from addresses starting at "base_addr" and offset by each 32-bit element in "vindex" (each index is
scaled by the factor in "scale"). Gathered elements are merged into "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 1
    i := j*64
    m := j*32
    IF k[j]
        addr := base_addr + SignExtend64(vindex[m+31:m]) * ZeroExtend64(scale) * 8
        dst[i+63:i] := MEM[addr+63:addr]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGATHERDPS - _mm256_mmask_i32gather_ps

| VGATHERDPS_YMMf32_MASKmskw_MEMf32_AVX512_VL256

--------------------------------------------------------------------------------------------------------------
Gather single-precision (32-bit) floating-point elements from memory using 32-bit indices. 32-bit elements are
loaded from addresses starting at "base_addr" and offset by each 32-bit element in "vindex" (each index is
scaled by the factor in "scale"). Gathered elements are merged into "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 7
    i := j*32
    m := j*32
    IF k[j]
        addr := base_addr + SignExtend64(vindex[m+31:m]) * ZeroExtend64(scale) * 8
        dst[i+31:i] := MEM[addr+31:addr]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VGATHERDPS - _mm_mmask_i32gather_ps

| VGATHERDPS_XMMf32_MASKmskw_MEMf32_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Gather single-precision (32-bit) floating-point elements from memory using 32-bit indices. 32-bit elements are
loaded from addresses starting at "base_addr" and offset by each 32-bit element in "vindex" (each index is
scaled by the factor in "scale"). Gathered elements are merged into "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 3
    i := j*32
    m := j*32
    IF k[j]
        addr := base_addr + SignExtend64(vindex[m+31:m]) * ZeroExtend64(scale) * 8
        dst[i+31:i] := MEM[addr+31:addr]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGATHERQPD - _mm256_mmask_i64gather_pd

| VGATHERQPD_YMMf64_MASKmskw_MEMf64_AVX512_VL256

--------------------------------------------------------------------------------------------------------------
Gather double-precision (64-bit) floating-point elements from memory using 64-bit indices. 64-bit elements are
loaded from addresses starting at "base_addr" and offset by each 64-bit element in "vindex" (each index is
scaled by the factor in "scale"). Gathered elements are merged into "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 3
    i := j*64
    m := j*64
    IF k[j]
        addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
        dst[i+63:i] := MEM[addr+63:addr]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VGATHERQPD - _mm_mmask_i64gather_pd

| VGATHERQPD_XMMf64_MASKmskw_MEMf64_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Gather double-precision (64-bit) floating-point elements from memory using 64-bit indices. 64-bit elements are
loaded from addresses starting at "base_addr" and offset by each 64-bit element in "vindex" (each index is
scaled by the factor in "scale"). Gathered elements are merged into "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 1
    i := j*64
    m := j*64
    IF k[j]
        addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
        dst[i+63:i] := MEM[addr+63:addr]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGATHERQPS - _mm256_mmask_i64gather_ps

| VGATHERQPS_YMMf32_MASKmskw_MEMf32_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Gather single-precision (32-bit) floating-point elements from memory using 64-bit indices. 32-bit elements are
loaded from addresses starting at "base_addr" and offset by each 64-bit element in "vindex" (each index is
scaled by the factor in "scale"). Gathered elements are merged into "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 3
    i := j*32
    m := j*64
    IF k[j]
        addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
        dst[i+31:i] := MEM[addr+31:addr]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGATHERQPS - _mm_mmask_i64gather_ps

| VGATHERQPS_XMMf32_MASKmskw_MEMf32_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Gather single-precision (32-bit) floating-point elements from memory using 64-bit indices. 32-bit elements are
loaded from addresses starting at "base_addr" and offset by each 64-bit element in "vindex" (each index is
scaled by the factor in "scale"). Gathered elements are merged into "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 1
    i := j*32
    m := j*64
    IF k[j]
        addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
        dst[i+31:i] := MEM[addr+31:addr]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VGETEXPPD - _mm256_getexp_pd

| VGETEXPPD_YMMf64_MASKmskw_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the exponent of each packed double-precision (64-bit) floating-point element in "a" to a
double-precision (64-bit) floating-point number representing the integer exponent, and store the results in
"dst". This intrinsic essentially calculates "floor(log2(x))" for each element.

[algorithm]

FOR j := 0 to 3
    i := j*64
    dst[i+63:i] := ConvertExpFP64(a[i+63:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VGETEXPPD - _mm256_mask_getexp_pd

| VGETEXPPD_YMMf64_MASKmskw_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the exponent of each packed double-precision (64-bit) floating-point element in "a" to a
double-precision (64-bit) floating-point number representing the integer exponent, and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). This
intrinsic essentially calculates "floor(log2(x))" for each element.

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := ConvertExpFP64(a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VGETEXPPD - _mm256_maskz_getexp_pd

| VGETEXPPD_YMMf64_MASKmskw_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the exponent of each packed double-precision (64-bit) floating-point element in "a" to a
double-precision (64-bit) floating-point number representing the integer exponent, and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). This intrinsic
essentially calculates "floor(log2(x))" for each element.

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := ConvertExpFP64(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VGETEXPPD - _mm_getexp_pd

| VGETEXPPD_XMMf64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the exponent of each packed double-precision (64-bit) floating-point element in "a" to a
double-precision (64-bit) floating-point number representing the integer exponent, and store the results in
"dst". This intrinsic essentially calculates "floor(log2(x))" for each element.

[algorithm]

FOR j := 0 to 1
    i := j*64
    dst[i+63:i] := ConvertExpFP64(a[i+63:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGETEXPPD - _mm_mask_getexp_pd

| VGETEXPPD_XMMf64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the exponent of each packed double-precision (64-bit) floating-point element in "a" to a
double-precision (64-bit) floating-point number representing the integer exponent, and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). This
intrinsic essentially calculates "floor(log2(x))" for each element.

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := ConvertExpFP64(a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGETEXPPD - _mm_maskz_getexp_pd

| VGETEXPPD_XMMf64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the exponent of each packed double-precision (64-bit) floating-point element in "a" to a
double-precision (64-bit) floating-point number representing the integer exponent, and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). This intrinsic
essentially calculates "floor(log2(x))" for each element.

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := ConvertExpFP64(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGETEXPPS - _mm256_getexp_ps

| VGETEXPPS_YMMf32_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the exponent of each packed single-precision (32-bit) floating-point element in "a" to a
single-precision (32-bit) floating-point number representing the integer exponent, and store the results in
"dst". This intrinsic essentially calculates "floor(log2(x))" for each element.

[algorithm]

FOR j := 0 to 7
    i := j*32
    dst[i+31:i] := ConvertExpFP32(a[i+31:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VGETEXPPS - _mm256_mask_getexp_ps

| VGETEXPPS_YMMf32_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the exponent of each packed single-precision (32-bit) floating-point element in "a" to a
single-precision (32-bit) floating-point number representing the integer exponent, and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). This
intrinsic essentially calculates "floor(log2(x))" for each element.

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := ConvertExpFP32(a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VGETEXPPS - _mm256_maskz_getexp_ps

| VGETEXPPS_YMMf32_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the exponent of each packed single-precision (32-bit) floating-point element in "a" to a
single-precision (32-bit) floating-point number representing the integer exponent, and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). This intrinsic
essentially calculates "floor(log2(x))" for each element.

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := ConvertExpFP32(a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VGETEXPPS - _mm_getexp_ps

| VGETEXPPS_XMMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the exponent of each packed single-precision (32-bit) floating-point element in "a" to a
single-precision (32-bit) floating-point number representing the integer exponent, and store the results in
"dst". This intrinsic essentially calculates "floor(log2(x))" for each element.

[algorithm]

FOR j := 0 to 3
    i := j*32
    dst[i+31:i] := ConvertExpFP32(a[i+31:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGETEXPPS - _mm_mask_getexp_ps

| VGETEXPPS_XMMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the exponent of each packed single-precision (32-bit) floating-point element in "a" to a
single-precision (32-bit) floating-point number representing the integer exponent, and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). This
intrinsic essentially calculates "floor(log2(x))" for each element.

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := ConvertExpFP32(a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGETEXPPS - _mm_maskz_getexp_ps

| VGETEXPPS_XMMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the exponent of each packed single-precision (32-bit) floating-point element in "a" to a
single-precision (32-bit) floating-point number representing the integer exponent, and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). This intrinsic
essentially calculates "floor(log2(x))" for each element.

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := ConvertExpFP32(a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGETMANTPD - _mm256_getmant_pd

| VGETMANTPD_YMMf64_MASKmskw_YMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Normalize the mantissas of packed double-precision (64-bit) floating-point elements in "a", and store the
results in "dst". This intrinsic essentially calculates "(2^k)*|x.significand|", where "k" depends on the
interval range defined by "interv" and the sign depends on "sc" and the source sign.
	[getmant_note]

[algorithm]

FOR j := 0 to 3
    i := j*64
    dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VGETMANTPD - _mm256_mask_getmant_pd

| VGETMANTPD_YMMf64_MASKmskw_YMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Normalize the mantissas of packed double-precision (64-bit) floating-point elements in "a", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set). This intrinsic essentially calculates "(2^k)*|x.significand|", where "k" depends on the interval range
defined by "interv" and the sign depends on "sc" and the source sign.
	[getmant_note]

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VGETMANTPD - _mm256_maskz_getmant_pd

| VGETMANTPD_YMMf64_MASKmskw_YMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Normalize the mantissas of packed double-precision (64-bit) floating-point elements in "a", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). This
intrinsic essentially calculates "(2^k)*|x.significand|", where "k" depends on the interval range defined by
"interv" and the sign depends on "sc" and the source sign.
	[getmant_note]

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VGETMANTPD - _mm_getmant_pd

| VGETMANTPD_XMMf64_MASKmskw_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Normalize the mantissas of packed double-precision (64-bit) floating-point elements in "a", and store the
results in "dst". This intrinsic essentially calculates "(2^k)*|x.significand|", where "k" depends on the
interval range defined by "interv" and the sign depends on "sc" and the source sign.
	[getmant_note]

[algorithm]

FOR j := 0 to 1
    i := j*64
    dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGETMANTPD - _mm_mask_getmant_pd

| VGETMANTPD_XMMf64_MASKmskw_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Normalize the mantissas of packed double-precision (64-bit) floating-point elements in "a", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set). This intrinsic essentially calculates "(2^k)*|x.significand|", where "k" depends on the interval range
defined by "interv" and the sign depends on "sc" and the source sign.
	[getmant_note]

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGETMANTPD - _mm_maskz_getmant_pd

| VGETMANTPD_XMMf64_MASKmskw_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Normalize the mantissas of packed double-precision (64-bit) floating-point elements in "a", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). This
intrinsic essentially calculates "(2^k)*|x.significand|", where "k" depends on the interval range defined by
"interv" and the sign depends on "sc" and the source sign.
	[getmant_note]

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGETMANTPS - _mm256_getmant_ps

| VGETMANTPS_YMMf32_MASKmskw_YMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Normalize the mantissas of packed single-precision (32-bit) floating-point elements in "a", and store the
results in "dst". This intrinsic essentially calculates "(2^k)*|x.significand|", where "k" depends on the
interval range defined by "interv" and the sign depends on "sc" and the source sign.
	[getmant_note]

[algorithm]

FOR j := 0 to 7
    i := j*32
    dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VGETMANTPS - _mm256_mask_getmant_ps

| VGETMANTPS_YMMf32_MASKmskw_YMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Normalize the mantissas of packed single-precision (32-bit) floating-point elements in "a", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set). This intrinsic essentially calculates "(2^k)*|x.significand|", where "k" depends on the interval range
defined by "interv" and the sign depends on "sc" and the source sign.
	[getmant_note]

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VGETMANTPS - _mm256_maskz_getmant_ps

| VGETMANTPS_YMMf32_MASKmskw_YMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Normalize the mantissas of packed single-precision (32-bit) floating-point elements in "a", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). This
intrinsic essentially calculates "(2^k)*|x.significand|", where "k" depends on the interval range defined by
"interv" and the sign depends on "sc" and the source sign.
	[getmant_note]

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VGETMANTPS - _mm_getmant_ps

| VGETMANTPS_XMMf32_MASKmskw_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Normalize the mantissas of packed single-precision (32-bit) floating-point elements in "a", and store the
results in "dst". This intrinsic essentially calculates "(2^k)*|x.significand|", where "k" depends on the
interval range defined by "interv" and the sign depends on "sc" and the source sign.
	[getmant_note]

[algorithm]

FOR j := 0 to 3
    i := j*32
    dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGETMANTPS - _mm_mask_getmant_ps

| VGETMANTPS_XMMf32_MASKmskw_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Normalize the mantissas of packed single-precision (32-bit) floating-point elements in "a", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set). This intrinsic essentially calculates "(2^k)*|x.significand|", where "k" depends on the interval range
defined by "interv" and the sign depends on "sc" and the source sign.
	[getmant_note]

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGETMANTPS - _mm_maskz_getmant_ps

| VGETMANTPS_XMMf32_MASKmskw_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Normalize the mantissas of packed single-precision (32-bit) floating-point elements in "a", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). This
intrinsic essentially calculates "(2^k)*|x.significand|", where "k" depends on the interval range defined by
"interv" and the sign depends on "sc" and the source sign.
	[getmant_note]

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VINSERTF32X4 - _mm256_insertf32x4

| VINSERTF32X4_YMMf32_MASKmskw_YMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Copy "a" to "dst", then insert 128 bits (composed of 4 packed single-precision (32-bit) floating-point
elements) from "b" into "dst" at the location specified by "imm8".

[algorithm]

dst[255:0] := a[255:0]
CASE (imm8[0]) OF
0: dst[127:0] := b[127:0]
1: dst[255:128] := b[127:0]
ESAC
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VINSERTF32X4 - _mm256_mask_insertf32x4

| VINSERTF32X4_YMMf32_MASKmskw_YMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Copy "a" to "tmp", then insert 128 bits (composed of 4 packed single-precision (32-bit) floating-point
elements) from "b" into "tmp" at the location specified by "imm8". Store "tmp" to "dst" using writemask "k"
(elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

tmp[255:0] := a[255:0]
CASE (imm8[0]) OF
0: tmp[127:0] := b[127:0]
1: tmp[255:128] := b[127:0]
ESAC
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VINSERTF32X4 - _mm256_maskz_insertf32x4

| VINSERTF32X4_YMMf32_MASKmskw_YMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Copy "a" to "tmp", then insert 128 bits (composed of 4 packed single-precision (32-bit) floating-point
elements) from "b" into "tmp" at the location specified by "imm8". Store "tmp" to "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

tmp[255:0] := a[255:0]
CASE (imm8[0]) OF
0: tmp[127:0] := b[127:0]
1: tmp[255:128] := b[127:0]
ESAC
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VINSERTI32X4 - _mm256_inserti32x4

| VINSERTI32X4_YMMu32_MASKmskw_YMMu32_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Copy "a" to "dst", then insert 128 bits (composed of 4 packed 32-bit integers) from "b" into "dst" at the
location specified by "imm8".

[algorithm]

dst[255:0] := a[255:0]
CASE (imm8[0]) OF
0: dst[127:0] := b[127:0]
1: dst[255:128] := b[127:0]
ESAC
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VINSERTI32X4 - _mm256_mask_inserti32x4

| VINSERTI32X4_YMMu32_MASKmskw_YMMu32_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Copy "a" to "tmp", then insert 128 bits (composed of 4 packed 32-bit integers) from "b" into "tmp" at the
location specified by "imm8". Store "tmp" to "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

tmp[255:0] := a[255:0]
CASE (imm8[0]) OF
0: tmp[127:0] := b[127:0]
1: tmp[255:128] := b[127:0]
ESAC
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VINSERTI32X4 - _mm256_maskz_inserti32x4

| VINSERTI32X4_YMMu32_MASKmskw_YMMu32_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Copy "a" to "tmp", then insert 128 bits (composed of 4 packed 32-bit integers) from "b" into "tmp" at the
location specified by "imm8". Store "tmp" to "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

tmp[255:0] := a[255:0]
CASE (imm8[0]) OF
0: tmp[127:0] := b[127:0]
1: tmp[255:128] := b[127:0]
ESAC
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMAXPD - _mm256_mask_max_pd

| VMAXPD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed maximum
values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMAXPD - _mm256_maskz_max_pd

| VMAXPD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed maximum
values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMAXPD - _mm_mask_max_pd

| VMAXPD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed maximum
values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMAXPD - _mm_maskz_max_pd

| VMAXPD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed maximum
values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMAXPS - _mm256_mask_max_ps

| VMAXPS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed maximum
values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMAXPS - _mm256_maskz_max_ps

| VMAXPS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed maximum
values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMAXPS - _mm_mask_max_ps

| VMAXPS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed maximum
values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMAXPS - _mm_maskz_max_ps

| VMAXPS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed maximum
values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMINPD - _mm256_mask_min_pd

| VMINPD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed minimum
values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMINPD - _mm256_maskz_min_pd

| VMINPD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed minimum
values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMINPD - _mm_mask_min_pd

| VMINPD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed minimum
values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMINPD - _mm_maskz_min_pd

| VMINPD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed minimum
values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMINPS - _mm256_mask_min_ps

| VMINPS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed minimum
values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMINPS - _mm256_maskz_min_ps

| VMINPS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed minimum
values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMINPS - _mm_mask_min_ps

| VMINPS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed minimum
values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMINPS - _mm_maskz_min_ps

| VMINPS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed minimum
values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVAPD - _mm256_mask_load_pd

| VMOVAPD_YMMf64_MASKmskw_MEMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed double-precision (64-bit) floating-point elements from memory into "dst" using writemask "k"
(elements are copied from "src" when the corresponding mask bit is not set). "mem_addr" must be aligned on a
32-byte boundary or a general-protection exception may be generated.

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVAPD - _mm256_mask_mov_pd

| VMOVAPD_YMMf64_MASKmskw_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Move packed double-precision (64-bit) floating-point elements from "a" to "dst" using writemask "k" (elements
are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVAPD - _mm256_mask_store_pd

| VMOVAPD_MEMf64_MASKmskw_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Store packed double-precision (64-bit) floating-point elements from "a" into memory using writemask
"k".
	"mem_addr" must be aligned on a 32-byte boundary or a general-protection exception may be generated.

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VMOVAPD - _mm256_maskz_load_pd

| VMOVAPD_YMMf64_MASKmskw_MEMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed double-precision (64-bit) floating-point elements from memory into "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set). "mem_addr" must be aligned on a 32-byte
boundary or a general-protection exception may be generated.

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVAPD - _mm256_maskz_mov_pd

| VMOVAPD_YMMf64_MASKmskw_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Move packed double-precision (64-bit) floating-point elements from "a" into "dst" using zeromask "k" (elements
are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVAPD - _mm_mask_load_pd

| VMOVAPD_XMMf64_MASKmskw_MEMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed double-precision (64-bit) floating-point elements from memory into "dst" using writemask "k"
(elements are copied from "src" when the corresponding mask bit is not set). "mem_addr" must be aligned on a
16-byte boundary or a general-protection exception may be generated.

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVAPD - _mm_mask_mov_pd

| VMOVAPD_XMMf64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Move packed double-precision (64-bit) floating-point elements from "a" to "dst" using writemask "k" (elements
are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVAPD - _mm_mask_store_pd

| VMOVAPD_MEMf64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Store packed double-precision (64-bit) floating-point elements from "a" into memory using writemask
"k".
	"mem_addr" must be aligned on a 16-byte boundary or a general-protection exception may be generated.

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VMOVAPD - _mm_maskz_load_pd

| VMOVAPD_XMMf64_MASKmskw_MEMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed double-precision (64-bit) floating-point elements from memory into "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set). "mem_addr" must be aligned on a 16-byte
boundary or a general-protection exception may be generated.

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVAPD - _mm_maskz_mov_pd

| VMOVAPD_XMMf64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Move packed double-precision (64-bit) floating-point elements from "a" into "dst" using zeromask "k" (elements
are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVAPS - _mm256_mask_load_ps

| VMOVAPS_YMMf32_MASKmskw_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed single-precision (32-bit) floating-point elements from memory into "dst" using writemask "k"
(elements are copied from "src" when the corresponding mask bit is not set). "mem_addr" must be aligned on a
32-byte boundary or a general-protection exception may be generated.

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVAPS - _mm256_mask_mov_ps

| VMOVAPS_YMMf32_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Move packed single-precision (32-bit) floating-point elements from "a" to "dst" using writemask "k" (elements
are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVAPS - _mm256_mask_store_ps

| VMOVAPS_MEMf32_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Store packed single-precision (32-bit) floating-point elements from "a" into memory using writemask
"k".
	"mem_addr" must be aligned on a 32-byte boundary or a general-protection exception may be generated.

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VMOVAPS - _mm256_maskz_load_ps

| VMOVAPS_YMMf32_MASKmskw_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed single-precision (32-bit) floating-point elements from memory into "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set). "mem_addr" must be aligned on a 32-byte
boundary or a general-protection exception may be generated.

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVAPS - _mm256_maskz_mov_ps

| VMOVAPS_YMMf32_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Move packed single-precision (32-bit) floating-point elements from "a" into "dst" using zeromask "k" (elements
are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVAPS - _mm_mask_load_ps

| VMOVAPS_XMMf32_MASKmskw_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed single-precision (32-bit) floating-point elements from memory into "dst" using writemask "k"
(elements are copied from "src" when the corresponding mask bit is not set). "mem_addr" must be aligned on a
16-byte boundary or a general-protection exception may be generated.

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVAPS - _mm_mask_mov_ps

| VMOVAPS_XMMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Move packed single-precision (32-bit) floating-point elements from "a" to "dst" using writemask "k" (elements
are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVAPS - _mm_mask_store_ps

| VMOVAPS_MEMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Store packed single-precision (32-bit) floating-point elements from "a" into memory using writemask
"k".
	"mem_addr" must be aligned on a 16-byte boundary or a general-protection exception may be generated.

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VMOVAPS - _mm_maskz_load_ps

| VMOVAPS_XMMf32_MASKmskw_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed single-precision (32-bit) floating-point elements from memory into "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set). "mem_addr" must be aligned on a 16-byte
boundary or a general-protection exception may be generated.

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVAPS - _mm_maskz_mov_ps

| VMOVAPS_XMMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Move packed single-precision (32-bit) floating-point elements from "a" into "dst" using zeromask "k" (elements
are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDDUP - _mm256_mask_movedup_pd

| VMOVDDUP_YMMf64_MASKmskw_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Duplicate even-indexed double-precision (64-bit) floating-point elements from "a", and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

tmp[63:0] := a[63:0]
tmp[127:64] := a[63:0]
tmp[191:128] := a[191:128]
tmp[255:192] := a[191:128]
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDDUP - _mm256_maskz_movedup_pd

| VMOVDDUP_YMMf64_MASKmskw_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Duplicate even-indexed double-precision (64-bit) floating-point elements from "a", and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

tmp[63:0] := a[63:0]
tmp[127:64] := a[63:0]
tmp[191:128] := a[191:128]
tmp[255:192] := a[191:128]
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDDUP - _mm_mask_movedup_pd

| VMOVDDUP_XMMf64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Duplicate even-indexed double-precision (64-bit) floating-point elements from "a", and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

tmp[63:0] := a[63:0]
tmp[127:64] := a[63:0]
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDDUP - _mm_maskz_movedup_pd

| VMOVDDUP_XMMf64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Duplicate even-indexed double-precision (64-bit) floating-point elements from "a", and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

tmp[63:0] := a[63:0]
tmp[127:64] := a[63:0]
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQA32 - _mm256_mask_load_epi32

| VMOVDQA32_YMMu32_MASKmskw_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed 32-bit integers from memory into "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set). 
	"mem_addr" must be aligned on a 32-byte boundary or a
general-protection exception may be generated.

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQA32 - _mm256_mask_mov_epi32

| VMOVDQA32_YMMu32_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Move packed 32-bit integers from "a" to "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQA32 - _mm256_mask_store_epi32

| VMOVDQA32_MEMu32_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Store packed 32-bit integers from "a" into memory using writemask "k".
	"mem_addr" must be aligned on a
32-byte boundary or a general-protection exception may be generated.

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VMOVDQA32 - _mm256_maskz_load_epi32

| VMOVDQA32_YMMu32_MASKmskw_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed 32-bit integers from memory into "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set). 
	"mem_addr" must be aligned on a 32-byte boundary or a general-protection
exception may be generated.

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQA32 - _mm256_maskz_mov_epi32

| VMOVDQA32_YMMu32_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Move packed 32-bit integers from "a" into "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQA32 - _mm_mask_load_epi32

| VMOVDQA32_XMMu32_MASKmskw_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed 32-bit integers from memory into "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set). 
	"mem_addr" must be aligned on a 16-byte boundary or a
general-protection exception may be generated.

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQA32 - _mm_mask_mov_epi32

| VMOVDQA32_XMMu32_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Move packed 32-bit integers from "a" to "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQA32 - _mm_mask_store_epi32

| VMOVDQA32_MEMu32_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Store packed 32-bit integers from "a" into memory using writemask "k".
	"mem_addr" must be aligned on a
16-byte boundary or a general-protection exception may be generated.

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VMOVDQA32 - _mm_maskz_load_epi32

| VMOVDQA32_XMMu32_MASKmskw_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed 32-bit integers from memory into "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set). 
	"mem_addr" must be aligned on a 16-byte boundary or a general-protection
exception may be generated.

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQA32 - _mm_maskz_mov_epi32

| VMOVDQA32_XMMu32_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Move packed 32-bit integers from "a" into "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQA64 - _mm256_mask_load_epi64

| VMOVDQA64_YMMu64_MASKmskw_MEMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed 64-bit integers from memory into "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set). 
	"mem_addr" must be aligned on a 32-byte boundary or a
general-protection exception may be generated.

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQA64 - _mm256_mask_mov_epi64

| VMOVDQA64_YMMu64_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Move packed 64-bit integers from "a" to "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQA64 - _mm256_mask_store_epi64

| VMOVDQA64_MEMu64_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Store packed 64-bit integers from "a" into memory using writemask "k".
	"mem_addr" must be aligned on a
32-byte boundary or a general-protection exception may be generated.

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VMOVDQA64 - _mm256_maskz_load_epi64

| VMOVDQA64_YMMu64_MASKmskw_MEMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed 64-bit integers from memory into "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set). 
	"mem_addr" must be aligned on a 32-byte boundary or a general-protection
exception may be generated.

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQA64 - _mm256_maskz_mov_epi64

| VMOVDQA64_YMMu64_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Move packed 64-bit integers from "a" into "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQA64 - _mm_mask_load_epi64

| VMOVDQA64_XMMu64_MASKmskw_MEMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed 64-bit integers from memory into "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set). 
	"mem_addr" must be aligned on a 16-byte boundary or a
general-protection exception may be generated.

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQA64 - _mm_mask_mov_epi64

| VMOVDQA64_XMMu64_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Move packed 64-bit integers from "a" to "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQA64 - _mm_mask_store_epi64

| VMOVDQA64_MEMu64_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Store packed 64-bit integers from "a" into memory using writemask "k".
	"mem_addr" must be aligned on a
16-byte boundary or a general-protection exception may be generated.

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VMOVDQA64 - _mm_maskz_load_epi64

| VMOVDQA64_XMMu64_MASKmskw_MEMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed 64-bit integers from memory into "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set). 
	"mem_addr" must be aligned on a 16-byte boundary or a general-protection
exception may be generated.

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQA64 - _mm_maskz_mov_epi64

| VMOVDQA64_XMMu64_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Move packed 64-bit integers from "a" into "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU32 - _mm256_mask_loadu_epi32

| VMOVDQU32_YMMu32_MASKmskw_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed 32-bit integers from memory into "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set). 
	"mem_addr" does not need to be aligned on any particular boundary.

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU32 - _mm256_mask_storeu_epi32

| VMOVDQU32_MEMu32_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Store packed 32-bit integers from "a" into memory using writemask "k".
	"mem_addr" does not need to be aligned
on any particular boundary.

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VMOVDQU32 - _mm256_maskz_loadu_epi32

| VMOVDQU32_YMMu32_MASKmskw_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed 32-bit integers from memory into "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).
	"mem_addr" does not need to be aligned on any particular boundary.

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU32 - _mm_mask_loadu_epi32

| VMOVDQU32_XMMu32_MASKmskw_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed 32-bit integers from memory into "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set). 
	"mem_addr" does not need to be aligned on any particular boundary.

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU32 - _mm_mask_storeu_epi32

| VMOVDQU32_MEMu32_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Store packed 32-bit integers from "a" into memory using writemask "k".
	"mem_addr" does not need to be aligned
on any particular boundary.

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VMOVDQU32 - _mm_maskz_loadu_epi32

| VMOVDQU32_XMMu32_MASKmskw_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed 32-bit integers from memory into "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).
	"mem_addr" does not need to be aligned on any particular boundary.

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU64 - _mm256_mask_loadu_epi64

| VMOVDQU64_YMMu64_MASKmskw_MEMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed 64-bit integers from memory into "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set). 
	"mem_addr" does not need to be aligned on any particular boundary.

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU64 - _mm256_mask_storeu_epi64

| VMOVDQU64_MEMu64_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Store packed 64-bit integers from "a" into memory using writemask "k".
	"mem_addr" does not need to be aligned
on any particular boundary.

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VMOVDQU64 - _mm256_maskz_loadu_epi64

| VMOVDQU64_YMMu64_MASKmskw_MEMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed 64-bit integers from memory into "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).
	"mem_addr" does not need to be aligned on any particular boundary.

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU64 - _mm_mask_loadu_epi64

| VMOVDQU64_XMMu64_MASKmskw_MEMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed 64-bit integers from memory into "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set). 
	"mem_addr" does not need to be aligned on any particular boundary.

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU64 - _mm_mask_storeu_epi64

| VMOVDQU64_MEMu64_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Store packed 64-bit integers from "a" into memory using writemask "k".
	"mem_addr" does not need to be aligned
on any particular boundary.

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VMOVDQU64 - _mm_maskz_loadu_epi64

| VMOVDQU64_XMMu64_MASKmskw_MEMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed 64-bit integers from memory into "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).
	"mem_addr" does not need to be aligned on any particular boundary.

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVSHDUP - _mm256_mask_movehdup_ps

| VMOVSHDUP_YMMf32_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Duplicate odd-indexed single-precision (32-bit) floating-point elements from "a", and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

tmp[31:0] := a[63:32] 
tmp[63:32] := a[63:32] 
tmp[95:64] := a[127:96] 
tmp[127:96] := a[127:96]
tmp[159:128] := a[191:160] 
tmp[191:160] := a[191:160] 
tmp[223:192] := a[255:224] 
tmp[255:224] := a[255:224]
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVSHDUP - _mm256_maskz_movehdup_ps

| VMOVSHDUP_YMMf32_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Duplicate odd-indexed single-precision (32-bit) floating-point elements from "a", and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

tmp[31:0] := a[63:32] 
tmp[63:32] := a[63:32] 
tmp[95:64] := a[127:96] 
tmp[127:96] := a[127:96]
tmp[159:128] := a[191:160] 
tmp[191:160] := a[191:160] 
tmp[223:192] := a[255:224] 
tmp[255:224] := a[255:224]
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVSHDUP - _mm_mask_movehdup_ps

| VMOVSHDUP_XMMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Duplicate odd-indexed single-precision (32-bit) floating-point elements from "a", and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

tmp[31:0] := a[63:32] 
tmp[63:32] := a[63:32] 
tmp[95:64] := a[127:96] 
tmp[127:96] := a[127:96]
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVSHDUP - _mm_maskz_movehdup_ps

| VMOVSHDUP_XMMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Duplicate odd-indexed single-precision (32-bit) floating-point elements from "a", and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

tmp[31:0] := a[63:32] 
tmp[63:32] := a[63:32] 
tmp[95:64] := a[127:96] 
tmp[127:96] := a[127:96]
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVSLDUP - _mm256_mask_moveldup_ps

| VMOVSLDUP_YMMf32_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Duplicate even-indexed single-precision (32-bit) floating-point elements from "a", and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

tmp[31:0] := a[31:0] 
tmp[63:32] := a[31:0] 
tmp[95:64] := a[95:64] 
tmp[127:96] := a[95:64]
tmp[159:128] := a[159:128] 
tmp[191:160] := a[159:128] 
tmp[223:192] := a[223:192] 
tmp[255:224] := a[223:192]
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR    
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVSLDUP - _mm256_maskz_moveldup_ps

| VMOVSLDUP_YMMf32_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Duplicate even-indexed single-precision (32-bit) floating-point elements from "a", and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

tmp[31:0] := a[31:0] 
tmp[63:32] := a[31:0] 
tmp[95:64] := a[95:64] 
tmp[127:96] := a[95:64]
tmp[159:128] := a[159:128] 
tmp[191:160] := a[159:128] 
tmp[223:192] := a[223:192] 
tmp[255:224] := a[223:192]
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVSLDUP - _mm_mask_moveldup_ps

| VMOVSLDUP_XMMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Duplicate even-indexed single-precision (32-bit) floating-point elements from "a", and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

tmp[31:0] := a[31:0] 
tmp[63:32] := a[31:0] 
tmp[95:64] := a[95:64] 
tmp[127:96] := a[95:64]
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR    
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVSLDUP - _mm_maskz_moveldup_ps

| VMOVSLDUP_XMMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Duplicate even-indexed single-precision (32-bit) floating-point elements from "a", and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

tmp[31:0] := a[31:0] 
tmp[63:32] := a[31:0] 
tmp[95:64] := a[95:64] 
tmp[127:96] := a[95:64]
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVUPD - _mm256_mask_loadu_pd

| VMOVUPD_YMMf64_MASKmskw_MEMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed double-precision (64-bit) floating-point elements from memoy into "dst" using writemask "k"
(elements are copied from "src" when the corresponding mask bit is not set).
	"mem_addr" does not need to be
aligned on any particular boundary.

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVUPD - _mm256_mask_storeu_pd

| VMOVUPD_MEMf64_MASKmskw_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Store packed double-precision (64-bit) floating-point elements from "a" into memory using writemask
"k".
	"mem_addr" does not need to be aligned on any particular boundary.

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VMOVUPD - _mm256_maskz_loadu_pd

| VMOVUPD_YMMf64_MASKmskw_MEMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed double-precision (64-bit) floating-point elements from memoy into "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set).
	"mem_addr" does not need to be aligned
on any particular boundary.

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVUPD - _mm_mask_loadu_pd

| VMOVUPD_XMMf64_MASKmskw_MEMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed double-precision (64-bit) floating-point elements from memoy into "dst" using writemask "k"
(elements are copied from "src" when the corresponding mask bit is not set).
	"mem_addr" does not need to be
aligned on any particular boundary.

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVUPD - _mm_mask_storeu_pd

| VMOVUPD_MEMf64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Store packed double-precision (64-bit) floating-point elements from "a" into memory using writemask
"k".
	"mem_addr" does not need to be aligned on any particular boundary.

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VMOVUPD - _mm_maskz_loadu_pd

| VMOVUPD_XMMf64_MASKmskw_MEMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed double-precision (64-bit) floating-point elements from memoy into "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set).
	"mem_addr" does not need to be aligned
on any particular boundary.

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVUPS - _mm256_mask_loadu_ps

| VMOVUPS_YMMf32_MASKmskw_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed single-precision (32-bit) floating-point elements from memory into "dst" using writemask "k"
(elements are copied from "src" when the corresponding mask bit is not set).
	"mem_addr" does not need to be
aligned on any particular boundary.

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVUPS - _mm256_mask_storeu_ps

| VMOVUPS_MEMf32_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Store packed single-precision (32-bit) floating-point elements from "a" into memory using writemask
"k".
	"mem_addr" does not need to be aligned on any particular boundary.

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VMOVUPS - _mm256_maskz_loadu_ps

| VMOVUPS_YMMf32_MASKmskw_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed single-precision (32-bit) floating-point elements from memory into "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set).
	"mem_addr" does not need to be aligned
on any particular boundary.

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVUPS - _mm_mask_loadu_ps

| VMOVUPS_XMMf32_MASKmskw_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed single-precision (32-bit) floating-point elements from memory into "dst" using writemask "k"
(elements are copied from "src" when the corresponding mask bit is not set).
	"mem_addr" does not need to be
aligned on any particular boundary.

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVUPS - _mm_mask_storeu_ps

| VMOVUPS_MEMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Store packed single-precision (32-bit) floating-point elements from "a" into memory using writemask
"k".
	"mem_addr" does not need to be aligned on any particular boundary.

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VMOVUPS - _mm_maskz_loadu_ps

| VMOVUPS_XMMf32_MASKmskw_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed single-precision (32-bit) floating-point elements from memory into "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set).
	"mem_addr" does not need to be aligned
on any particular boundary.

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMULPD - _mm256_mask_mul_pd

| VMULPD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] * b[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMULPD - _mm256_maskz_mul_pd

| VMULPD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] * b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMULPD - _mm_mask_mul_pd

| VMULPD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] * b[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMULPD - _mm_maskz_mul_pd

| VMULPD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] * b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMULPS - _mm256_mask_mul_ps

| VMULPS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). RM.

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] * b[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMULPS - _mm256_maskz_mul_ps

| VMULPS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] * b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMULPS - _mm_mask_mul_ps

| VMULPS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] * b[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMULPS - _mm_maskz_mul_ps

| VMULPS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] * b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPABSD - _mm256_mask_abs_epi32

| VPABSD_YMMi32_MASKmskw_YMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the absolute value of packed signed 32-bit integers in "a", and store the unsigned results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := ABS(a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPABSD - _mm256_maskz_abs_epi32

| VPABSD_YMMi32_MASKmskw_YMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the absolute value of packed signed 32-bit integers in "a", and store the unsigned results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := ABS(a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPABSD - _mm_mask_abs_epi32

| VPABSD_XMMi32_MASKmskw_XMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the absolute value of packed signed 32-bit integers in "a", and store the unsigned results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := ABS(a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPABSD - _mm_maskz_abs_epi32

| VPABSD_XMMi32_MASKmskw_XMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the absolute value of packed signed 32-bit integers in "a", and store the unsigned results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := ABS(a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPABSQ - _mm256_abs_epi64

| VPABSQ_YMMi64_MASKmskw_YMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the absolute value of packed signed 64-bit integers in "a", and store the unsigned results in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    dst[i+63:i] := ABS(a[i+63:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPABSQ - _mm256_mask_abs_epi64

| VPABSQ_YMMi64_MASKmskw_YMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the absolute value of packed signed 64-bit integers in "a", and store the unsigned results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := ABS(a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPABSQ - _mm256_maskz_abs_epi64

| VPABSQ_YMMi64_MASKmskw_YMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the absolute value of packed signed 64-bit integers in "a", and store the unsigned results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := ABS(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPABSQ - _mm_abs_epi64

| VPABSQ_XMMi64_MASKmskw_XMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the absolute value of packed signed 64-bit integers in "a", and store the unsigned results in "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    dst[i+63:i] := ABS(a[i+63:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPABSQ - _mm_mask_abs_epi64

| VPABSQ_XMMi64_MASKmskw_XMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the absolute value of packed signed 64-bit integers in "a", and store the unsigned results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := ABS(a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPABSQ - _mm_maskz_abs_epi64

| VPABSQ_XMMi64_MASKmskw_XMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the absolute value of packed signed 64-bit integers in "a", and store the unsigned results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := ABS(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDD - _mm256_mask_add_epi32

| VPADDD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed 32-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] + b[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDD - _mm256_maskz_add_epi32

| VPADDD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed 32-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] + b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDD - _mm_mask_add_epi32

| VPADDD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed 32-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] + b[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDD - _mm_maskz_add_epi32

| VPADDD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed 32-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] + b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDQ - _mm256_mask_add_epi64

| VPADDQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed 64-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] + b[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDQ - _mm256_maskz_add_epi64

| VPADDQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed 64-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] + b[i+63:i]
    ELSE
        dst[i+63:i] :=0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDQ - _mm_mask_add_epi64

| VPADDQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed 64-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] + b[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDQ - _mm_maskz_add_epi64

| VPADDQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed 64-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] + b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPANDD - _mm256_mask_and_epi32

| VPANDD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed 32-bit integers in "a" and "b", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] AND b[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPANDD - _mm256_maskz_and_epi32

| VPANDD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed 32-bit integers in "a" and "b", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] AND b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPANDD - _mm_mask_and_epi32

| VPANDD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed 32-bit integers in "a" and "b", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] AND b[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPANDD - _mm_maskz_and_epi32

| VPANDD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed 32-bit integers in "a" and "b", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] AND b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPANDND - _mm256_mask_andnot_epi32

| VPANDND_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of packed 32-bit integers in "a" and then AND with "b", and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPANDND - _mm256_maskz_andnot_epi32

| VPANDND_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of packed 32-bit integers in "a" and then AND with "b", and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := (NOT a[i+31:i]) AND b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPANDND - _mm_mask_andnot_epi32

| VPANDND_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of packed 32-bit integers in "a" and then AND with "b", and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPANDND - _mm_maskz_andnot_epi32

| VPANDND_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of packed 32-bit integers in "a" and then AND with "b", and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := (NOT a[i+31:i]) AND b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPANDNQ - _mm256_mask_andnot_epi64

| VPANDNQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of packed 64-bit integers in "a" and then AND with "b", and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPANDNQ - _mm256_maskz_andnot_epi64

| VPANDNQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of packed 64-bit integers in "a" and then AND with "b", and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := (NOT a[i+63:i]) AND b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPANDNQ - _mm_mask_andnot_epi64

| VPANDNQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of packed 64-bit integers in "a" and then AND with "b", and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPANDNQ - _mm_maskz_andnot_epi64

| VPANDNQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of packed 64-bit integers in "a" and then AND with "b", and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := (NOT a[i+63:i]) AND b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPANDQ - _mm256_mask_and_epi64

| VPANDQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed 64-bit integers in "a" and "b", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] AND b[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPANDQ - _mm256_maskz_and_epi64

| VPANDQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed 64-bit integers in "a" and "b", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] AND b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPANDQ - _mm_mask_and_epi64

| VPANDQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed 64-bit integers in "a" and "b", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] AND b[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPANDQ - _mm_maskz_and_epi64

| VPANDQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed 64-bit integers in "a" and "b", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] AND b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPBLENDMD - _mm256_mask_blend_epi32

| VPBLENDMD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Blend packed 32-bit integers from "a" and "b" using control mask "k", and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := b[i+31:i]
    ELSE
        dst[i+31:i] := a[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPBLENDMD - _mm_mask_blend_epi32

| VPBLENDMD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Blend packed 32-bit integers from "a" and "b" using control mask "k", and store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := b[i+31:i]
    ELSE
        dst[i+31:i] := a[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPBLENDMQ - _mm256_mask_blend_epi64

| VPBLENDMQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Blend packed 64-bit integers from "a" and "b" using control mask "k", and store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := b[i+63:i]
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPBLENDMQ - _mm_mask_blend_epi64

| VPBLENDMQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Blend packed 64-bit integers from "a" and "b" using control mask "k", and store the results in "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := b[i+63:i]
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTD - _mm256_mask_broadcastd_epi32

| VPBROADCASTD_YMMu32_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low packed 32-bit integer from "a" to all elements of "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := a[31:0]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTD - _mm256_mask_set1_epi32

| VPBROADCASTD_YMMu32_MASKmskw_GPR32u32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast 32-bit integer "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := a[31:0]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTD - _mm256_maskz_broadcastd_epi32

| VPBROADCASTD_YMMu32_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low packed 32-bit integer from "a" to all elements of "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := a[31:0]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTD - _mm256_maskz_set1_epi32

| VPBROADCASTD_YMMu32_MASKmskw_GPR32u32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast 32-bit integer "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := a[31:0]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTD - _mm_mask_broadcastd_epi32

| VPBROADCASTD_XMMu32_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low packed 32-bit integer from "a" to all elements of "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := a[31:0]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTD - _mm_mask_set1_epi32

| VPBROADCASTD_XMMu32_MASKmskw_GPR32u32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast 32-bit integer "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := a[31:0]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTD - _mm_maskz_broadcastd_epi32

| VPBROADCASTD_XMMu32_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low packed 32-bit integer from "a" to all elements of "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := a[31:0]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTD - _mm_maskz_set1_epi32

| VPBROADCASTD_XMMu32_MASKmskw_GPR32u32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast 32-bit integer "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := a[31:0]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTQ - _mm256_mask_broadcastq_epi64

| VPBROADCASTQ_YMMu64_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low packed 64-bit integer from "a" to all elements of "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := a[63:0]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTQ - _mm256_mask_set1_epi64

| VPBROADCASTQ_YMMu64_MASKmskw_GPR64u64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast 64-bit integer "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := a[63:0]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTQ - _mm256_maskz_broadcastq_epi64

| VPBROADCASTQ_YMMu64_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low packed 64-bit integer from "a" to all elements of "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := a[63:0]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTQ - _mm256_maskz_set1_epi64

| VPBROADCASTQ_YMMu64_MASKmskw_GPR64u64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast 64-bit integer "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := a[63:0]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTQ - _mm_mask_broadcastq_epi64

| VPBROADCASTQ_XMMu64_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low packed 64-bit integer from "a" to all elements of "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := a[63:0]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTQ - _mm_mask_set1_epi64

| VPBROADCASTQ_XMMu64_MASKmskw_GPR64u64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast 64-bit integer "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := a[63:0]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTQ - _mm_maskz_broadcastq_epi64

| VPBROADCASTQ_XMMu64_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low packed 64-bit integer from "a" to all elements of "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := a[63:0]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTQ - _mm_maskz_set1_epi64

| VPBROADCASTQ_XMMu64_MASKmskw_GPR64u64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast 64-bit integer "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := a[63:0]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPD - _mm256_cmp_epi32_mask

| VPCMPD_MASKmskw_MASKmskw_YMMi32_YMMi32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b" based on the comparison operand specified by "imm8", and
store the results in mask vector "k".

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 7
    i := j*32
    k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPD - _mm256_cmpeq_epi32_mask

| VPCMPD_MASKmskw_MASKmskw_YMMi32_YMMi32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b" for equality, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 7
    i := j*32
    k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPD - _mm256_cmpge_epi32_mask

| VPCMPD_MASKmskw_MASKmskw_YMMi32_YMMi32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask
vector "k".

[algorithm]

FOR j := 0 to 7
    i := j*32
    k[j] := ( a[i+31:i] &gt;= b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPD - _mm256_cmpgt_epi32_mask

| VPCMPD_MASKmskw_MASKmskw_YMMi32_YMMi32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 7
    i := j*32
    k[j] := ( a[i+31:i] &gt; b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPD - _mm256_cmple_epi32_mask

| VPCMPD_MASKmskw_MASKmskw_YMMi32_YMMi32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k".

[algorithm]

FOR j := 0 to 7
    i := j*32
    k[j] := ( a[i+31:i] &lt;= b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPD - _mm256_cmplt_epi32_mask

| VPCMPD_MASKmskw_MASKmskw_YMMi32_YMMi32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 7
    i := j*32
    k[j] := ( a[i+31:i] &lt; b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPD - _mm256_cmpneq_epi32_mask

| VPCMPD_MASKmskw_MASKmskw_YMMi32_YMMi32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 7
    i := j*32
    k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPD - _mm256_mask_cmp_epi32_mask

| VPCMPD_MASKmskw_MASKmskw_YMMi32_YMMi32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b" based on the comparison operand specified by "imm8", and
store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask
bit is not set).

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 7
    i := j*32
    IF k1[j]
        k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPD - _mm256_mask_cmpeq_epi32_mask

| VPCMPD_MASKmskw_MASKmskw_YMMi32_YMMi32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b" for equality, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k1[j]
        k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPD - _mm256_mask_cmpge_epi32_mask

| VPCMPD_MASKmskw_MASKmskw_YMMi32_YMMi32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k1[j]
        k[j] := ( a[i+31:i] &gt;= b[i+31:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPD - _mm256_mask_cmpgt_epi32_mask

| VPCMPD_MASKmskw_MASKmskw_YMMi32_YMMi32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k1[j]
        k[j] := ( a[i+31:i] &gt; b[i+31:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPD - _mm256_mask_cmple_epi32_mask

| VPCMPD_MASKmskw_MASKmskw_YMMi32_YMMi32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k1[j]
        k[j] := ( a[i+31:i] &lt;= b[i+31:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPD - _mm256_mask_cmplt_epi32_mask

| VPCMPD_MASKmskw_MASKmskw_YMMi32_YMMi32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b" for less-than, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k1[j]
        k[j] := ( a[i+31:i] &lt; b[i+31:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPD - _mm256_mask_cmpneq_epi32_mask

| VPCMPD_MASKmskw_MASKmskw_YMMi32_YMMi32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k1[j]
        k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPD - _mm_cmp_epi32_mask

| VPCMPD_MASKmskw_MASKmskw_XMMi32_XMMi32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b" based on the comparison operand specified by "imm8", and
store the results in mask vector "k".

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 3
    i := j*32
    k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPD - _mm_cmpeq_epi32_mask

| VPCMPD_MASKmskw_MASKmskw_XMMi32_XMMi32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b" for equality, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 3
    i := j*32
    k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPD - _mm_cmpge_epi32_mask

| VPCMPD_MASKmskw_MASKmskw_XMMi32_XMMi32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask
vector "k".

[algorithm]

FOR j := 0 to 3
    i := j*32
    k[j] := ( a[i+31:i] &gt;= b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPD - _mm_cmpgt_epi32_mask

| VPCMPD_MASKmskw_MASKmskw_XMMi32_XMMi32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 3
    i := j*32
    k[j] := ( a[i+31:i] &gt; b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPD - _mm_cmple_epi32_mask

| VPCMPD_MASKmskw_MASKmskw_XMMi32_XMMi32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k".

[algorithm]

FOR j := 0 to 3
    i := j*32
    k[j] := ( a[i+31:i] &lt;= b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPD - _mm_cmplt_epi32_mask

| VPCMPD_MASKmskw_MASKmskw_XMMi32_XMMi32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 3
    i := j*32
    k[j] := ( a[i+31:i] &lt; b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPD - _mm_cmpneq_epi32_mask

| VPCMPD_MASKmskw_MASKmskw_XMMi32_XMMi32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 3
    i := j*32
    k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPD - _mm_mask_cmp_epi32_mask

| VPCMPD_MASKmskw_MASKmskw_XMMi32_XMMi32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b" based on the comparison operand specified by "imm8", and
store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask
bit is not set).

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 3
    i := j*32
    IF k1[j]
        k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPD - _mm_mask_cmpeq_epi32_mask

| VPCMPD_MASKmskw_MASKmskw_XMMi32_XMMi32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b" for equality, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k1[j]
        k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPD - _mm_mask_cmpge_epi32_mask

| VPCMPD_MASKmskw_MASKmskw_XMMi32_XMMi32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k1[j]
        k[j] := ( a[i+31:i] &gt;= b[i+31:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPD - _mm_mask_cmpgt_epi32_mask

| VPCMPD_MASKmskw_MASKmskw_XMMi32_XMMi32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k1[j]
        k[j] := ( a[i+31:i] &gt; b[i+31:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPD - _mm_mask_cmple_epi32_mask

| VPCMPD_MASKmskw_MASKmskw_XMMi32_XMMi32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k1[j]
        k[j] := ( a[i+31:i] &lt;= b[i+31:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPD - _mm_mask_cmplt_epi32_mask

| VPCMPD_MASKmskw_MASKmskw_XMMi32_XMMi32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b" for less-than, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k1[j]
        k[j] := ( a[i+31:i] &lt; b[i+31:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPD - _mm_mask_cmpneq_epi32_mask

| VPCMPD_MASKmskw_MASKmskw_XMMi32_XMMi32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k1[j]
        k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm256_cmp_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_YMMi64_YMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" based on the comparison operand specified by "imm8", and
store the results in mask vector "k".

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 3
    i := j*64
    k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm256_cmpeq_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_YMMi64_YMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" for equality, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 3
    i := j*64
    k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm256_cmpge_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_YMMi64_YMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask
vector "k".

[algorithm]

FOR j := 0 to 3
    i := j*64
    k[j] := ( a[i+63:i] &gt;= b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm256_cmpgt_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_YMMi64_YMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 3
    i := j*64
    k[j] := ( a[i+63:i] &gt; b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm256_cmple_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_YMMi64_YMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k".

[algorithm]

FOR j := 0 to 3
    i := j*64
    k[j] := ( a[i+63:i] &lt;= b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm256_cmplt_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_YMMi64_YMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 3
    i := j*64
    k[j] := ( a[i+63:i] &lt; b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm256_cmpneq_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_YMMi64_YMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 3
    i := j*64
    k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm256_mask_cmp_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_YMMi64_YMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" based on the comparison operand specified by "imm8", and
store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask
bit is not set).

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 3
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm256_mask_cmpeq_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_YMMi64_YMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" for equality, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm256_mask_cmpge_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_YMMi64_YMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] &gt;= b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm256_mask_cmpgt_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_YMMi64_YMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] &gt; b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm256_mask_cmple_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_YMMi64_YMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] &lt;= b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm256_mask_cmplt_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_YMMi64_YMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" for less-than, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] &lt; b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm256_mask_cmpneq_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_YMMi64_YMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm_cmp_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_XMMi64_XMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" based on the comparison operand specified by "imm8", and
store the results in mask vector "k".

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 1
    i := j*64
    k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm_cmpeq_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_XMMi64_XMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" for equality, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 1
    i := j*64
    k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm_cmpge_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_XMMi64_XMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask
vector "k".

[algorithm]

FOR j := 0 to 1
    i := j*64
    k[j] := ( a[i+63:i] &gt;= b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm_cmpgt_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_XMMi64_XMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 1
    i := j*64
    k[j] := ( a[i+63:i] &gt; b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm_cmple_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_XMMi64_XMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k".

[algorithm]

FOR j := 0 to 1
    i := j*64
    k[j] := ( a[i+63:i] &lt;= b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm_cmplt_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_XMMi64_XMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 1
    i := j*64
    k[j] := ( a[i+63:i] &lt; b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm_cmpneq_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_XMMi64_XMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 1
    i := j*64
    k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm_mask_cmp_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_XMMi64_XMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" based on the comparison operand specified by "imm8", and
store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask
bit is not set).

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 1
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm_mask_cmpeq_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_XMMi64_XMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" for equality, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm_mask_cmpge_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_XMMi64_XMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] &gt;= b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm_mask_cmpgt_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_XMMi64_XMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] &gt; b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm_mask_cmple_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_XMMi64_XMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] &lt;= b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm_mask_cmplt_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_XMMi64_XMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" for less-than, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] &lt; b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm_mask_cmpneq_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_XMMi64_XMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUD - _mm256_cmp_epu32_mask

| VPCMPUD_MASKmskw_MASKmskw_YMMu32_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b" based on the comparison operand specified by "imm8",
and store the results in mask vector "k".

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 7
    i := j*32
    k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUD - _mm256_cmpeq_epu32_mask

| VPCMPUD_MASKmskw_MASKmskw_YMMu32_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b" for equality, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 7
    i := j*32
    k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUD - _mm256_cmpge_epu32_mask

| VPCMPUD_MASKmskw_MASKmskw_YMMu32_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b" for greater-than-or-equal, and store the results in
mask vector "k".

[algorithm]

FOR j := 0 to 7
    i := j*32
    k[j] := ( a[i+31:i] &gt;= b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUD - _mm256_cmpgt_epu32_mask

| VPCMPUD_MASKmskw_MASKmskw_YMMu32_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 7
    i := j*32
    k[j] := ( a[i+31:i] &gt; b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUD - _mm256_cmple_epu32_mask

| VPCMPUD_MASKmskw_MASKmskw_YMMu32_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k".

[algorithm]

FOR j := 0 to 7
    i := j*32
    k[j] := ( a[i+31:i] &lt;= b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUD - _mm256_cmplt_epu32_mask

| VPCMPUD_MASKmskw_MASKmskw_YMMu32_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b" for less-than, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 7
    i := j*32
    k[j] := ( a[i+31:i] &lt; b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUD - _mm256_cmpneq_epu32_mask

| VPCMPUD_MASKmskw_MASKmskw_YMMu32_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b" for not-equal, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 7
    i := j*32
    k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUD - _mm256_mask_cmp_epu32_mask

| VPCMPUD_MASKmskw_MASKmskw_YMMu32_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b" based on the comparison operand specified by "imm8",
and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding
mask bit is not set).

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 7
    i := j*32
    IF k1[j]
        k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUD - _mm256_mask_cmpeq_epu32_mask

| VPCMPUD_MASKmskw_MASKmskw_YMMu32_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b" for equality, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k1[j]
        k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUD - _mm256_mask_cmpge_epu32_mask

| VPCMPUD_MASKmskw_MASKmskw_YMMu32_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b" for greater-than-or-equal, and store the results in
mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k1[j]
        k[j] := ( a[i+31:i] &gt;= b[i+31:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUD - _mm256_mask_cmpgt_epu32_mask

| VPCMPUD_MASKmskw_MASKmskw_YMMu32_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k1[j]
        k[j] := ( a[i+31:i] &gt; b[i+31:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUD - _mm256_mask_cmple_epu32_mask

| VPCMPUD_MASKmskw_MASKmskw_YMMu32_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k1[j]
        k[j] := ( a[i+31:i] &lt;= b[i+31:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUD - _mm256_mask_cmplt_epu32_mask

| VPCMPUD_MASKmskw_MASKmskw_YMMu32_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b" for less-than, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k1[j]
        k[j] := ( a[i+31:i] &lt; b[i+31:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUD - _mm256_mask_cmpneq_epu32_mask

| VPCMPUD_MASKmskw_MASKmskw_YMMu32_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k1[j]
        k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUD - _mm_cmp_epu32_mask

| VPCMPUD_MASKmskw_MASKmskw_XMMu32_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b" based on the comparison operand specified by "imm8",
and store the results in mask vector "k".

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 3
    i := j*32
    k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUD - _mm_cmpeq_epu32_mask

| VPCMPUD_MASKmskw_MASKmskw_XMMu32_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b" for equality, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 3
    i := j*32
    k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUD - _mm_cmpge_epu32_mask

| VPCMPUD_MASKmskw_MASKmskw_XMMu32_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b" for greater-than-or-equal, and store the results in
mask vector "k".

[algorithm]

FOR j := 0 to 3
    i := j*32
    k[j] := ( a[i+31:i] &gt;= b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUD - _mm_cmpgt_epu32_mask

| VPCMPUD_MASKmskw_MASKmskw_XMMu32_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 3
    i := j*32
    k[j] := ( a[i+31:i] &gt; b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUD - _mm_cmple_epu32_mask

| VPCMPUD_MASKmskw_MASKmskw_XMMu32_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k".

[algorithm]

FOR j := 0 to 3
    i := j*32
    k[j] := ( a[i+31:i] &lt;= b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUD - _mm_cmplt_epu32_mask

| VPCMPUD_MASKmskw_MASKmskw_XMMu32_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b" for less-than, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 3
    i := j*32
    k[j] := ( a[i+31:i] &lt; b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUD - _mm_cmpneq_epu32_mask

| VPCMPUD_MASKmskw_MASKmskw_XMMu32_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b" for not-equal, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 3
    i := j*32
    k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUD - _mm_mask_cmp_epu32_mask

| VPCMPUD_MASKmskw_MASKmskw_XMMu32_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b" based on the comparison operand specified by "imm8",
and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding
mask bit is not set).

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 3
    i := j*32
    IF k1[j]
        k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUD - _mm_mask_cmpeq_epu32_mask

| VPCMPUD_MASKmskw_MASKmskw_XMMu32_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b" for equality, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k1[j]
        k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUD - _mm_mask_cmpge_epu32_mask

| VPCMPUD_MASKmskw_MASKmskw_XMMu32_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b" for greater-than-or-equal, and store the results in
mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k1[j]
        k[j] := ( a[i+31:i] &gt;= b[i+31:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUD - _mm_mask_cmpgt_epu32_mask

| VPCMPUD_MASKmskw_MASKmskw_XMMu32_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k1[j]
        k[j] := ( a[i+31:i] &gt; b[i+31:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUD - _mm_mask_cmple_epu32_mask

| VPCMPUD_MASKmskw_MASKmskw_XMMu32_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k1[j]
        k[j] := ( a[i+31:i] &lt;= b[i+31:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUD - _mm_mask_cmplt_epu32_mask

| VPCMPUD_MASKmskw_MASKmskw_XMMu32_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b" for less-than, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k1[j]
        k[j] := ( a[i+31:i] &lt; b[i+31:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUD - _mm_mask_cmpneq_epu32_mask

| VPCMPUD_MASKmskw_MASKmskw_XMMu32_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k1[j]
        k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm256_cmp_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_YMMu64_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" based on the comparison operand specified by "imm8",
and store the results in mask vector "k".

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 3
    i := j*64
    k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm256_cmpeq_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_YMMu64_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" for equality, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 3
    i := j*64
    k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm256_cmpge_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_YMMu64_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" for greater-than-or-equal, and store the results in
mask vector "k".

[algorithm]

FOR j := 0 to 3
    i := j*64
    k[j] := ( a[i+63:i] &gt;= b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm256_cmpgt_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_YMMu64_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 3
    i := j*64
    k[j] := ( a[i+63:i] &gt; b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm256_cmple_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_YMMu64_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k".

[algorithm]

FOR j := 0 to 3
    i := j*64
    k[j] := ( a[i+63:i] &lt;= b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm256_cmplt_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_YMMu64_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" for less-than, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 3
    i := j*64
    k[j] := ( a[i+63:i] &lt; b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm256_cmpneq_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_YMMu64_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" for not-equal, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 3
    i := j*64
    k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm256_mask_cmp_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_YMMu64_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" based on the comparison operand specified by "imm8",
and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding
mask bit is not set).

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 3
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm256_mask_cmpeq_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_YMMu64_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" for equality, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm256_mask_cmpge_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_YMMu64_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" for greater-than-or-equal, and store the results in
mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] &gt;= b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm256_mask_cmpgt_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_YMMu64_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] &gt; b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm256_mask_cmple_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_YMMu64_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] &lt;= b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm256_mask_cmplt_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_YMMu64_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" for less-than, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] &lt; b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm256_mask_cmpneq_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_YMMu64_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm_cmp_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_XMMu64_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" based on the comparison operand specified by "imm8",
and store the results in mask vector "k".

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 1
    i := j*64
    k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm_cmpeq_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_XMMu64_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" for equality, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 1
    i := j*64
    k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm_cmpge_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_XMMu64_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" for greater-than-or-equal, and store the results in
mask vector "k".

[algorithm]

FOR j := 0 to 1
    i := j*64
    k[j] := ( a[i+63:i] &gt;= b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm_cmpgt_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_XMMu64_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 1
    i := j*64
    k[j] := ( a[i+63:i] &gt; b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm_cmple_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_XMMu64_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k".

[algorithm]

FOR j := 0 to 1
    i := j*64
    k[j] := ( a[i+63:i] &lt;= b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm_cmplt_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_XMMu64_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" for less-than, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 1
    i := j*64
    k[j] := ( a[i+63:i] &lt; b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm_cmpneq_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_XMMu64_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" for not-equal, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 1
    i := j*64
    k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm_mask_cmp_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_XMMu64_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" based on the comparison operand specified by "imm8",
and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding
mask bit is not set).

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 1
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm_mask_cmpeq_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_XMMu64_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" for equality, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm_mask_cmpge_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_XMMu64_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" for greater-than-or-equal, and store the results in
mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] &gt;= b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm_mask_cmpgt_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_XMMu64_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] &gt; b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm_mask_cmple_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_XMMu64_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] &lt;= b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm_mask_cmplt_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_XMMu64_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" for less-than, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] &lt; b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm_mask_cmpneq_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_XMMu64_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VPCOMPRESSD - _mm256_mask_compress_epi32

| VPCOMPRESSD_YMMu32_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active 32-bit integers in "a" (those with their respective bit set in writemask "k") to
"dst", and pass through the remaining elements from "src".

[algorithm]

size := 32
m := 0
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[m+size-1:m] := a[i+31:i]
        m := m + size
    FI
ENDFOR
dst[255:m] := src[255:m]
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPCOMPRESSD - _mm256_mask_compressstoreu_epi32

| VPCOMPRESSD_MEMu32_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active 32-bit integers in "a" (those with their respective bit set in writemask "k") to
unaligned memory at "base_addr".

[algorithm]

size := 32
m := base_addr
FOR j := 0 to 7
    i := j*32
    IF k[j]
        MEM[m+size-1:m] := a[i+31:i]
        m := m + size
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPCOMPRESSD - _mm256_maskz_compress_epi32

| VPCOMPRESSD_YMMu32_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active 32-bit integers in "a" (those with their respective bit set in zeromask "k") to
"dst", and set the remaining elements to zero.

[algorithm]

size := 32
m := 0
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[m+size-1:m] := a[i+31:i]
        m := m + size
    FI
ENDFOR
dst[255:m] := 0
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPCOMPRESSD - _mm_mask_compress_epi32

| VPCOMPRESSD_XMMu32_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active 32-bit integers in "a" (those with their respective bit set in writemask "k") to
"dst", and pass through the remaining elements from "src".

[algorithm]

size := 32
m := 0
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[m+size-1:m] := a[i+31:i]
        m := m + size
    FI
ENDFOR
dst[127:m] := src[127:m]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPCOMPRESSD - _mm_mask_compressstoreu_epi32

| VPCOMPRESSD_MEMu32_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active 32-bit integers in "a" (those with their respective bit set in writemask "k") to
unaligned memory at "base_addr".

[algorithm]

size := 32
m := base_addr
FOR j := 0 to 3
    i := j*32
    IF k[j]
        MEM[m+size-1:m] := a[i+31:i]
        m := m + size
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPCOMPRESSD - _mm_maskz_compress_epi32

| VPCOMPRESSD_XMMu32_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active 32-bit integers in "a" (those with their respective bit set in zeromask "k") to
"dst", and set the remaining elements to zero.

[algorithm]

size := 32
m := 0
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[m+size-1:m] := a[i+31:i]
        m := m + size
    FI
ENDFOR
dst[127:m] := 0
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPCOMPRESSQ - _mm256_mask_compress_epi64

| VPCOMPRESSQ_YMMu64_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active 64-bit integers in "a" (those with their respective bit set in writemask "k") to
"dst", and pass through the remaining elements from "src".

[algorithm]

size := 64
m := 0
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[m+size-1:m] := a[i+63:i]
        m := m + size
    FI
ENDFOR
dst[255:m] := src[255:m]
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPCOMPRESSQ - _mm256_mask_compressstoreu_epi64

| VPCOMPRESSQ_MEMu64_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active 64-bit integers in "a" (those with their respective bit set in writemask "k") to
unaligned memory at "base_addr".

[algorithm]

size := 64
m := base_addr
FOR j := 0 to 3
    i := j*64
    IF k[j]
        MEM[m+size-1:m] := a[i+63:i]
        m := m + size
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPCOMPRESSQ - _mm256_maskz_compress_epi64

| VPCOMPRESSQ_YMMu64_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active 64-bit integers in "a" (those with their respective bit set in zeromask "k") to
"dst", and set the remaining elements to zero.

[algorithm]

size := 64
m := 0
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[m+size-1:m] := a[i+63:i]
        m := m + size
    FI
ENDFOR
dst[255:m] := 0
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPCOMPRESSQ - _mm_mask_compress_epi64

| VPCOMPRESSQ_XMMu64_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active 64-bit integers in "a" (those with their respective bit set in writemask "k") to
"dst", and pass through the remaining elements from "src".

[algorithm]

size := 64
m := 0
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[m+size-1:m] := a[i+63:i]
        m := m + size
    FI
ENDFOR
dst[127:m] := src[127:m]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPCOMPRESSQ - _mm_mask_compressstoreu_epi64

| VPCOMPRESSQ_MEMu64_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active 64-bit integers in "a" (those with their respective bit set in writemask "k") to
unaligned memory at "base_addr".

[algorithm]

size := 64
m := base_addr
FOR j := 0 to 1
    i := j*64
    IF k[j]
        MEM[m+size-1:m] := a[i+63:i]
        m := m + size
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPCOMPRESSQ - _mm_maskz_compress_epi64

| VPCOMPRESSQ_XMMu64_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active 64-bit integers in "a" (those with their respective bit set in zeromask "k") to
"dst", and set the remaining elements to zero.

[algorithm]

size := 64
m := 0
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[m+size-1:m] := a[i+63:i]
        m := m + size
    FI
ENDFOR
dst[127:m] := 0
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMD - _mm256_mask_permutexvar_epi32

| VPERMD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 32-bit integers in "a" across lanes using the corresponding index in "idx", and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    id := idx[i+2:i]*32
    IF k[j]
        dst[i+31:i] := a[id+31:id]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMD - _mm256_maskz_permutexvar_epi32

| VPERMD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 32-bit integers in "a" across lanes using the corresponding index in "idx", and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    id := idx[i+2:i]*32
    IF k[j]
        dst[i+31:i] := a[id+31:id]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMD - _mm256_permutexvar_epi32

| VPERMD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 32-bit integers in "a" across lanes using the corresponding index in "idx", and store the results in
"dst".

[algorithm]

FOR j := 0 to 7
    i := j*32
    id := idx[i+2:i]*32
    dst[i+31:i] := a[id+31:id]
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2D - _mm256_mask2_permutex2var_epi32

| VPERMI2D_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 32-bit integers in "a" and "b" across lanes using the corresponding selector and index in "idx", and
store the results in "dst" using writemask "k" (elements are copied from "idx" when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    off := idx[i+2:i]*32
    IF k[j]
        dst[i+31:i] := idx[i+3] ? b[off+31:off] : a[off+31:off]
    ELSE
        dst[i+31:i] := idx[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMT2D - _mm256_mask_permutex2var_epi32

| VPERMT2D_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 32-bit integers in "a" and "b" across lanes using the corresponding selector and index in "idx", and
store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    off := idx[i+2:i]*32
    IF k[j]
        dst[i+31:i] := idx[i+3] ? b[off+31:off] : a[off+31:off]
    ELSE
        dst[i+31:i] := a[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2D - _mm256_maskz_permutex2var_epi32

| VPERMI2D_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512 | VPERMT2D_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 32-bit integers in "a" and "b" across lanes using the corresponding selector and index in "idx", and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    off := idx[i+2:i]*32
    IF k[j]
        dst[i+31:i] := (idx[i+3]) ? b[off+31:off] : a[off+31:off]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2D - _mm256_permutex2var_epi32

| VPERMI2D_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512 | VPERMT2D_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 32-bit integers in "a" and "b" across lanes using the corresponding selector and index in "idx", and
store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*32
    off := idx[i+2:i]*32
    dst[i+31:i] := idx[i+3] ? b[off+31:off] : a[off+31:off]
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2D - _mm_mask2_permutex2var_epi32

| VPERMI2D_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 32-bit integers in "a" and "b" using the corresponding selector and index in "idx", and store the
results in "dst" using writemask "k" (elements are copied from "idx" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    off := idx[i+1:i]*32
    IF k[j]
        dst[i+31:i] := idx[i+2] ? b[off+31:off] : a[off+31:off]
    ELSE
        dst[i+31:i] := idx[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMT2D - _mm_mask_permutex2var_epi32

| VPERMT2D_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 32-bit integers in "a" and "b" using the corresponding selector and index in "idx", and store the
results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    off := idx[i+1:i]*32
    IF k[j]
        dst[i+31:i] := idx[i+2] ? b[off+31:off] : a[off+31:off]
    ELSE
        dst[i+31:i] := a[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2D - _mm_maskz_permutex2var_epi32

| VPERMI2D_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512 | VPERMT2D_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 32-bit integers in "a" and "b" using the corresponding selector and index in "idx", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    off := idx[i+1:i]*32
    IF k[j]
        dst[i+31:i] := (idx[i+2]) ? b[off+31:off] : a[off+31:off]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2D - _mm_permutex2var_epi32

| VPERMI2D_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512 | VPERMT2D_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 32-bit integers in "a" and "b" using the corresponding selector and index in "idx", and store the
results in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*32
    off := idx[i+1:i]*32
    dst[i+31:i] := idx[i+2] ? b[off+31:off] : a[off+31:off]
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2PD - _mm256_mask2_permutex2var_pd

| VPERMI2PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" and "b" across lanes using the corresponding
selector and index in "idx", and store the results in "dst" using writemask "k" (elements are copied from "idx"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    off := idx[i+1:i]*64
    IF k[j]
        dst[i+63:i] := idx[i+2] ? b[off+63:off] : a[off+63:off]
    ELSE
        dst[i+63:i] := idx[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMT2PD - _mm256_mask_permutex2var_pd

| VPERMT2PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" and "b" across lanes using the corresponding
selector and index in "idx", and store the results in "dst" using writemask "k" (elements are copied from "a"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    off := idx[i+1:i]*64
    IF k[j]
        dst[i+63:i] := idx[i+2] ? b[off+63:off] : a[off+63:off]
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2PD - _mm256_maskz_permutex2var_pd

| VPERMI2PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VPERMT2PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" and "b" across lanes using the corresponding
selector and index in "idx", and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    off := idx[i+1:i]*64
    IF k[j]
        dst[i+63:i] := (idx[i+2]) ? b[off+63:off] : a[off+63:off]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2PD - _mm256_permutex2var_pd

| VPERMI2PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512 | VPERMT2PD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" and "b" across lanes using the corresponding
selector and index in "idx", and store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    off := idx[i+1:i]*64
    dst[i+63:i] := idx[i+2] ? b[off+63:off] : a[off+63:off]
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2PD - _mm_mask2_permutex2var_pd

| VPERMI2PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" and "b" using the corresponding selector and
index in "idx", and store the results in "dst" using writemask "k" (elements are copied from "idx" when the
corresponding mask bit is not set)

[algorithm]

FOR j := 0 to 1
    i := j*64
    off := idx[i]*64
    IF k[j]
        dst[i+63:i] := idx[i+1] ? b[off+63:off] : a[off+63:off]
    ELSE
        dst[i+63:i] := idx[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMT2PD - _mm_mask_permutex2var_pd

| VPERMT2PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" and "b" using the corresponding selector and
index in "idx", and store the results in "dst" using writemask "k" (elements are copied from "a" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    off := idx[i]*64
    IF k[j]
        dst[i+63:i] := idx[i+1] ? b[off+63:off] : a[off+63:off]
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2PD - _mm_maskz_permutex2var_pd

| VPERMI2PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VPERMT2PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" and "b" using the corresponding selector and
index in "idx", and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    off := idx[i]*64
    IF k[j]
        dst[i+63:i] := (idx[i+1]) ? b[off+63:off] : a[off+63:off]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2PD - _mm_permutex2var_pd

| VPERMI2PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VPERMT2PD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" and "b" using the corresponding selector and
index in "idx", and store the results in "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    off := idx[i]*64
    dst[i+63:i] := idx[i+1] ? b[off+63:off] : a[off+63:off]
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2PS - _mm256_mask2_permutex2var_ps

| VPERMI2PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" and "b" across lanes using the corresponding
selector and index in "idx", and store the results in "dst" using writemask "k" (elements are copied from "idx"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    off := idx[i+2:i]*32
    IF k[j]
        dst[i+31:i] := idx[i+3] ? b[off+31:off] : a[off+31:off]
    ELSE
        dst[i+31:i] := idx[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMT2PS - _mm256_mask_permutex2var_ps

| VPERMT2PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" and "b" across lanes using the corresponding
selector and index in "idx", and store the results in "dst" using writemask "k" (elements are copied from "a"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    off := idx[i+2:i]*32
    IF k[j]
        dst[i+31:i] := idx[i+3] ? b[off+31:off] : a[off+31:off]
    ELSE
        dst[i+31:i] := a[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2PS - _mm256_maskz_permutex2var_ps

| VPERMI2PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VPERMT2PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" and "b" across lanes using the corresponding
selector and index in "idx", and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    off := idx[i+2:i]*32
    IF k[j]
        dst[i+31:i] := (idx[i+3]) ? b[off+31:off] : a[off+31:off]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2PS - _mm256_permutex2var_ps

| VPERMI2PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512 | VPERMT2PS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" and "b" across lanes using the corresponding
selector and index in "idx", and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*32
    off := idx[i+2:i]*32
    dst[i+31:i] := idx[i+3] ? b[off+31:off] : a[off+31:off]
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2PS - _mm_mask2_permutex2var_ps

| VPERMI2PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" and "b" using the corresponding selector and
index in "idx", and store the results in "dst" using writemask "k" (elements are copied from "idx" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    off := idx[i+1:i]*32
    IF k[j]
        dst[i+31:i] := idx[i+2] ? b[off+31:off] : a[off+31:off]
    ELSE
        dst[i+31:i] := idx[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMT2PS - _mm_mask_permutex2var_ps

| VPERMT2PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" and "b" using the corresponding selector and
index in "idx", and store the results in "dst" using writemask "k" (elements are copied from "a" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    off := idx[i+1:i]*32
    IF k[j]
        dst[i+31:i] := idx[i+2] ? b[off+31:off] : a[off+31:off]
    ELSE
        dst[i+31:i] := a[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2PS - _mm_maskz_permutex2var_ps

| VPERMI2PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VPERMT2PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" and "b" using the corresponding selector and
index in "idx", and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    off := idx[i+1:i]*32
    IF k[j]
        dst[i+31:i] := (idx[i+2]) ? b[off+31:off] : a[off+31:off]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2PS - _mm_permutex2var_ps

| VPERMI2PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VPERMT2PS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" and "b" using the corresponding selector and
index in "idx", and store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*32
    off := idx[i+1:i]*32
    dst[i+31:i] := idx[i+2] ? b[off+31:off] : a[off+31:off]
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2Q - _mm256_mask2_permutex2var_epi64

| VPERMI2Q_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 64-bit integers in "a" and "b" across lanes using the corresponding selector and index in "idx", and
store the results in "dst" using writemask "k" (elements are copied from "idx" when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    off := idx[i+1:i]*64
    IF k[j]
        dst[i+63:i] := idx[i+2] ? b[off+63:off] : a[off+63:off]
    ELSE
        dst[i+63:i] := idx[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMT2Q - _mm256_mask_permutex2var_epi64

| VPERMT2Q_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 64-bit integers in "a" and "b" across lanes using the corresponding selector and index in "idx", and
store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    off := idx[i+1:i]*64
    IF k[j]
        dst[i+63:i] := idx[i+2] ? b[off+63:off] : a[off+63:off]
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2Q - _mm256_maskz_permutex2var_epi64

| VPERMI2Q_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512 | VPERMT2Q_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 64-bit integers in "a" and "b" across lanes using the corresponding selector and index in "idx", and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    off := idx[i+1:i]*64
    IF k[j]
        dst[i+63:i] := (idx[i+2]) ? b[off+63:off] : a[off+63:off]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2Q - _mm256_permutex2var_epi64

| VPERMI2Q_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512 | VPERMT2Q_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 64-bit integers in "a" and "b" across lanes using the corresponding selector and index in "idx", and
store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    off := idx[i+1:i]*64
    dst[i+63:i] := idx[i+2] ? b[off+63:off] : a[off+63:off]
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2Q - _mm_mask2_permutex2var_epi64

| VPERMI2Q_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 64-bit integers in "a" and "b" using the corresponding selector and index in "idx", and store the
results in "dst" using writemask "k" (elements are copied from "idx" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    off := idx[i]*64
    IF k[j]
        dst[i+63:i] := idx[i+1] ? b[off+63:off] : a[off+63:off]
    ELSE
        dst[i+63:i] := idx[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMT2Q - _mm_mask_permutex2var_epi64

| VPERMT2Q_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 64-bit integers in "a" and "b" using the corresponding selector and index in "idx", and store the
results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    off := idx[i]*64
    IF k[j]
        dst[i+63:i] := idx[i+1] ? b[off+63:off] : a[off+63:off]
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2Q - _mm_maskz_permutex2var_epi64

| VPERMI2Q_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512 | VPERMT2Q_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 64-bit integers in "a" and "b" using the corresponding selector and index in "idx", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    off := idx[i]*64
    IF k[j]
        dst[i+63:i] := (idx[i+1]) ? b[off+63:off] : a[off+63:off]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2Q - _mm_permutex2var_epi64

| VPERMI2Q_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512 | VPERMT2Q_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 64-bit integers in "a" and "b" using the corresponding selector and index in "idx", and store the
results in "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    off := idx[i]*64
    dst[i+63:i] := idx[i+1] ? b[off+63:off] : a[off+63:off]
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMILPD - _mm256_mask_permute_pd

| VPERMILPD_YMMf64_MASKmskw_YMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" within 128-bit lanes using the control in
"imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

IF (imm8[0] == 0) tmp_dst[63:0] := a[63:0]; FI
IF (imm8[0] == 1) tmp_dst[63:0] := a[127:64]; FI
IF (imm8[1] == 0) tmp_dst[127:64] := a[63:0]; FI
IF (imm8[1] == 1) tmp_dst[127:64] := a[127:64]; FI
IF (imm8[2] == 0) tmp_dst[191:128] := a[191:128]; FI
IF (imm8[2] == 1) tmp_dst[191:128] := a[255:192]; FI
IF (imm8[3] == 0) tmp_dst[255:192] := a[191:128]; FI
IF (imm8[3] == 1) tmp_dst[255:192] := a[255:192]; FI
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMILPD - _mm256_mask_permutevar_pd

| VPERMILPD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" within 128-bit lanes using the control in
"b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding
mask bit is not set).

[algorithm]

IF (b[1] == 0) tmp_dst[63:0] := a[63:0]; FI
IF (b[1] == 1) tmp_dst[63:0] := a[127:64]; FI
IF (b[65] == 0) tmp_dst[127:64] := a[63:0]; FI
IF (b[65] == 1) tmp_dst[127:64] := a[127:64]; FI
IF (b[129] == 0) tmp_dst[191:128] := a[191:128]; FI
IF (b[129] == 1) tmp_dst[191:128] := a[255:192]; FI
IF (b[193] == 0) tmp_dst[255:192] := a[191:128]; FI
IF (b[193] == 1) tmp_dst[255:192] := a[255:192]; FI
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMILPD - _mm256_maskz_permute_pd

| VPERMILPD_YMMf64_MASKmskw_YMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" within 128-bit lanes using the control in
"imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask
bit is not set).

[algorithm]

IF (imm8[0] == 0) tmp_dst[63:0] := a[63:0]; FI
IF (imm8[0] == 1) tmp_dst[63:0] := a[127:64]; FI
IF (imm8[1] == 0) tmp_dst[127:64] := a[63:0]; FI
IF (imm8[1] == 1) tmp_dst[127:64] := a[127:64]; FI
IF (imm8[2] == 0) tmp_dst[191:128] := a[191:128]; FI
IF (imm8[2] == 1) tmp_dst[191:128] := a[255:192]; FI
IF (imm8[3] == 0) tmp_dst[255:192] := a[191:128]; FI
IF (imm8[3] == 1) tmp_dst[255:192] := a[255:192]; FI
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMILPD - _mm256_maskz_permutevar_pd

| VPERMILPD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" within 128-bit lanes using the control in
"b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit
is not set).

[algorithm]

IF (b[1] == 0) tmp_dst[63:0] := a[63:0]; FI
IF (b[1] == 1) tmp_dst[63:0] := a[127:64]; FI
IF (b[65] == 0) tmp_dst[127:64] := a[63:0]; FI
IF (b[65] == 1) tmp_dst[127:64] := a[127:64]; FI
IF (b[129] == 0) tmp_dst[191:128] := a[191:128]; FI
IF (b[129] == 1) tmp_dst[191:128] := a[255:192]; FI
IF (b[193] == 0) tmp_dst[255:192] := a[191:128]; FI
IF (b[193] == 1) tmp_dst[255:192] := a[255:192]; FI
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMILPD - _mm_mask_permute_pd

| VPERMILPD_XMMf64_MASKmskw_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" using the control in "imm8", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

IF (imm8[0] == 0) tmp_dst[63:0] := a[63:0]; FI
IF (imm8[0] == 1) tmp_dst[63:0] := a[127:64]; FI
IF (imm8[1] == 0) tmp_dst[127:64] := a[63:0]; FI
IF (imm8[1] == 1) tmp_dst[127:64] := a[127:64]; FI
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMILPD - _mm_mask_permutevar_pd

| VPERMILPD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" using the control in "b", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

IF (b[1] == 0) tmp_dst[63:0] := a[63:0]; FI
IF (b[1] == 1) tmp_dst[63:0] := a[127:64]; FI
IF (b[65] == 0) tmp_dst[127:64] := a[63:0]; FI
IF (b[65] == 1) tmp_dst[127:64] := a[127:64]; FI
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMILPD - _mm_maskz_permute_pd

| VPERMILPD_XMMf64_MASKmskw_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" using the control in "imm8", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

IF (imm8[0] == 0) tmp_dst[63:0] := a[63:0]; FI
IF (imm8[0] == 1) tmp_dst[63:0] := a[127:64]; FI
IF (imm8[1] == 0) tmp_dst[127:64] := a[63:0]; FI
IF (imm8[1] == 1) tmp_dst[127:64] := a[127:64]; FI
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMILPD - _mm_maskz_permutevar_pd

| VPERMILPD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" using the control in "b", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

IF (b[1] == 0) tmp_dst[63:0] := a[63:0]; FI
IF (b[1] == 1) tmp_dst[63:0] := a[127:64]; FI
IF (b[65] == 0) tmp_dst[127:64] := a[63:0]; FI
IF (b[65] == 1) tmp_dst[127:64] := a[127:64]; FI
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMILPS - _mm256_mask_permute_ps

| VPERMILPS_YMMf32_MASKmskw_YMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" within 128-bit lanes using the control in
"imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[31:0] := src[31:0]
    1:    tmp[31:0] := src[63:32]
    2:    tmp[31:0] := src[95:64]
    3:    tmp[31:0] := src[127:96]
    ESAC
    RETURN tmp[31:0]
}
tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMILPS - _mm256_mask_permutevar_ps

| VPERMILPS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" within 128-bit lanes using the control in
"b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding
mask bit is not set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[31:0] := src[31:0]
    1:    tmp[31:0] := src[63:32]
    2:    tmp[31:0] := src[95:64]
    3:    tmp[31:0] := src[127:96]
    ESAC
    RETURN tmp[31:0]
}
tmp_dst[31:0] := SELECT4(a[127:0], b[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], b[33:32])
tmp_dst[95:64] := SELECT4(a[127:0], b[65:64])
tmp_dst[127:96] := SELECT4(a[127:0], b[97:96])
tmp_dst[159:128] := SELECT4(a[255:128], b[129:128])
tmp_dst[191:160] := SELECT4(a[255:128], b[161:160])
tmp_dst[223:192] := SELECT4(a[255:128], b[193:192])
tmp_dst[255:224] := SELECT4(a[255:128], b[225:224])
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMILPS - _mm256_maskz_permute_ps

| VPERMILPS_YMMf32_MASKmskw_YMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" within 128-bit lanes using the control in
"imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask
bit is not set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[31:0] := src[31:0]
    1:    tmp[31:0] := src[63:32]
    2:    tmp[31:0] := src[95:64]
    3:    tmp[31:0] := src[127:96]
    ESAC
    RETURN tmp[31:0]
}
tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMILPS - _mm256_maskz_permutevar_ps

| VPERMILPS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" within 128-bit lanes using the control in
"b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit
is not set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[31:0] := src[31:0]
    1:    tmp[31:0] := src[63:32]
    2:    tmp[31:0] := src[95:64]
    3:    tmp[31:0] := src[127:96]
    ESAC
    RETURN tmp[31:0]
}
tmp_dst[31:0] := SELECT4(a[127:0], b[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], b[33:32])
tmp_dst[95:64] := SELECT4(a[127:0], b[65:64])
tmp_dst[127:96] := SELECT4(a[127:0], b[97:96])
tmp_dst[159:128] := SELECT4(a[255:128], b[129:128])
tmp_dst[191:160] := SELECT4(a[255:128], b[161:160])
tmp_dst[223:192] := SELECT4(a[255:128], b[193:192])
tmp_dst[255:224] := SELECT4(a[255:128], b[225:224])
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMILPS - _mm_mask_permute_ps

| VPERMILPS_XMMf32_MASKmskw_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" using the control in "imm8", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[31:0] := src[31:0]
    1:    tmp[31:0] := src[63:32]
    2:    tmp[31:0] := src[95:64]
    3:    tmp[31:0] := src[127:96]
    ESAC
    RETURN tmp[31:0]
}
tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMILPS - _mm_mask_permutevar_ps

| VPERMILPS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" using the control in "b", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[31:0] := src[31:0]
    1:    tmp[31:0] := src[63:32]
    2:    tmp[31:0] := src[95:64]
    3:    tmp[31:0] := src[127:96]
    ESAC
    RETURN tmp[31:0]
}
tmp_dst[31:0] := SELECT4(a[127:0], b[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], b[33:32])
tmp_dst[95:64] := SELECT4(a[127:0], b[65:64])
tmp_dst[127:96] := SELECT4(a[127:0], b[97:96])
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMILPS - _mm_maskz_permute_ps

| VPERMILPS_XMMf32_MASKmskw_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" using the control in "imm8", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[31:0] := src[31:0]
    1:    tmp[31:0] := src[63:32]
    2:    tmp[31:0] := src[95:64]
    3:    tmp[31:0] := src[127:96]
    ESAC
    RETURN tmp[31:0]
}
tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMILPS - _mm_maskz_permutevar_ps

| VPERMILPS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" using the control in "b", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[31:0] := src[31:0]
    1:    tmp[31:0] := src[63:32]
    2:    tmp[31:0] := src[95:64]
    3:    tmp[31:0] := src[127:96]
    ESAC
    RETURN tmp[31:0]
}
tmp_dst[31:0] := SELECT4(a[127:0], b[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], b[33:32])
tmp_dst[95:64] := SELECT4(a[127:0], b[65:64])
tmp_dst[127:96] := SELECT4(a[127:0], b[97:96])
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMPD - _mm256_mask_permutex_pd

| VPERMPD_YMMf64_MASKmskw_YMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" across lanes using the control in "imm8", and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[63:0] := src[63:0]
    1:    tmp[63:0] := src[127:64]
    2:    tmp[63:0] := src[191:128]
    3:    tmp[63:0] := src[255:192]
    ESAC
    RETURN tmp[63:0]
}
tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMPD - _mm256_mask_permutexvar_pd

| VPERMPD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" across lanes using the corresponding index in
"idx", and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    id := idx[i+1:i]*64
    IF k[j]
        dst[i+63:i] := a[id+63:id]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMPD - _mm256_maskz_permutex_pd

| VPERMPD_YMMf64_MASKmskw_YMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" across lanes using the control in "imm8", and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[63:0] := src[63:0]
    1:    tmp[63:0] := src[127:64]
    2:    tmp[63:0] := src[191:128]
    3:    tmp[63:0] := src[255:192]
    ESAC
    RETURN tmp[63:0]
}
tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMPD - _mm256_maskz_permutexvar_pd

| VPERMPD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" across lanes using the corresponding index in
"idx", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    id := idx[i+1:i]*64
    IF k[j]
        dst[i+63:i] := a[id+63:id]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMPD - _mm256_permutex_pd

| VPERMPD_YMMf64_MASKmskw_YMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" across lanes using the control in "imm8", and
store the results in "dst".

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[63:0] := src[63:0]
    1:    tmp[63:0] := src[127:64]
    2:    tmp[63:0] := src[191:128]
    3:    tmp[63:0] := src[255:192]
    ESAC
    RETURN tmp[63:0]
}
dst[63:0] := SELECT4(a[255:0], imm8[1:0])
dst[127:64] := SELECT4(a[255:0], imm8[3:2])
dst[191:128] := SELECT4(a[255:0], imm8[5:4])
dst[255:192] := SELECT4(a[255:0], imm8[7:6])
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMPD - _mm256_permutexvar_pd

| VPERMPD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" across lanes using the corresponding index in
"idx", and store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    id := idx[i+1:i]*64
    dst[i+63:i] := a[id+63:id]
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMPS - _mm256_mask_permutexvar_ps

| VPERMPS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" across lanes using the corresponding index in
"idx", and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    id := idx[i+2:i]*32
    IF k[j]
        dst[i+31:i] := a[id+31:id]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMPS - _mm256_maskz_permutexvar_ps

| VPERMPS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" across lanes using the corresponding index in
"idx", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    id := idx[i+2:i]*32
    IF k[j]
        dst[i+31:i] := a[id+31:id]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMPS - _mm256_permutexvar_ps

| VPERMPS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" across lanes using the corresponding index in
"idx".

[algorithm]

FOR j := 0 to 7
    i := j*32
    id := idx[i+2:i]*32
    dst[i+31:i] := a[id+31:id]
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMQ - _mm256_mask_permutex_epi64

| VPERMQ_YMMu64_MASKmskw_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 64-bit integers in "a" across lanes lanes using the control in "imm8", and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[63:0] := src[63:0]
    1:    tmp[63:0] := src[127:64]
    2:    tmp[63:0] := src[191:128]
    3:    tmp[63:0] := src[255:192]
    ESAC
    RETURN tmp[63:0]
}
tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMQ - _mm256_mask_permutexvar_epi64

| VPERMQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 64-bit integers in "a" across lanes using the corresponding index in "idx", and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    id := idx[i+1:i]*64
    IF k[j]
        dst[i+63:i] := a[id+63:id]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMQ - _mm256_maskz_permutex_epi64

| VPERMQ_YMMu64_MASKmskw_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 64-bit integers in "a" across lanes using the control in "imm8", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[63:0] := src[63:0]
    1:    tmp[63:0] := src[127:64]
    2:    tmp[63:0] := src[191:128]
    3:    tmp[63:0] := src[255:192]
    ESAC
    RETURN tmp[63:0]
}
tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMQ - _mm256_maskz_permutexvar_epi64

| VPERMQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 64-bit integers in "a" across lanes using the corresponding index in "idx", and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    id := idx[i+1:i]*64
    IF k[j]
        dst[i+63:i] := a[id+63:id]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMQ - _mm256_permutex_epi64

| VPERMQ_YMMu64_MASKmskw_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 64-bit integers in "a" across lanes using the control in "imm8", and store the results in "dst".

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[63:0] := src[63:0]
    1:    tmp[63:0] := src[127:64]
    2:    tmp[63:0] := src[191:128]
    3:    tmp[63:0] := src[255:192]
    ESAC
    RETURN tmp[63:0]
}
dst[63:0] := SELECT4(a[255:0], imm8[1:0])
dst[127:64] := SELECT4(a[255:0], imm8[3:2])
dst[191:128] := SELECT4(a[255:0], imm8[5:4])
dst[255:192] := SELECT4(a[255:0], imm8[7:6])
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMQ - _mm256_permutexvar_epi64

| VPERMQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 64-bit integers in "a" across lanes using the corresponding index in "idx", and store the results in
"dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    id := idx[i+1:i]*64
    dst[i+63:i] := a[id+63:id]
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDD - _mm256_mask_expand_epi32

| VPEXPANDD_YMMu32_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 32-bit integers from "a" (those with their respective bit set in mask "k"), and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

m := 0
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := a[m+31:m]
        m := m + 32
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDD - _mm256_mask_expandloadu_epi32

| VPEXPANDD_YMMu32_MASKmskw_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 32-bit integers from unaligned memory at "mem_addr" (those with their respective bit
set in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
        m := m + 32
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDD - _mm256_maskz_expand_epi32

| VPEXPANDD_YMMu32_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 32-bit integers from "a" (those with their respective bit set in mask "k"), and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := a[m+31:m]
        m := m + 32
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDD - _mm256_maskz_expandloadu_epi32

| VPEXPANDD_YMMu32_MASKmskw_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 32-bit integers from unaligned memory at "mem_addr" (those with their respective bit
set in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
        m := m + 32
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDD - _mm_mask_expand_epi32

| VPEXPANDD_XMMu32_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 32-bit integers from "a" (those with their respective bit set in mask "k"), and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

m := 0
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := a[m+31:m]
        m := m + 32
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDD - _mm_mask_expandloadu_epi32

| VPEXPANDD_XMMu32_MASKmskw_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 32-bit integers from unaligned memory at "mem_addr" (those with their respective bit
set in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
        m := m + 32
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDD - _mm_maskz_expand_epi32

| VPEXPANDD_XMMu32_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 32-bit integers from "a" (those with their respective bit set in mask "k"), and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := a[m+31:m]
        m := m + 32
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDD - _mm_maskz_expandloadu_epi32

| VPEXPANDD_XMMu32_MASKmskw_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 32-bit integers from unaligned memory at "mem_addr" (those with their respective bit
set in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
        m := m + 32
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDQ - _mm256_mask_expand_epi64

| VPEXPANDQ_YMMu64_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 64-bit integers from "a" (those with their respective bit set in mask "k"), and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

m := 0
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := a[m+63:m]
        m := m + 64
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDQ - _mm256_mask_expandloadu_epi64

| VPEXPANDQ_YMMu64_MASKmskw_MEMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 64-bit integers from unaligned memory at "mem_addr" (those with their respective bit
set in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
        m := m + 64
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDQ - _mm256_maskz_expand_epi64

| VPEXPANDQ_YMMu64_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 64-bit integers from "a" (those with their respective bit set in mask "k"), and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := a[m+63:m]
        m := m + 64
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDQ - _mm256_maskz_expandloadu_epi64

| VPEXPANDQ_YMMu64_MASKmskw_MEMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 64-bit integers from unaligned memory at "mem_addr" (those with their respective bit
set in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
        m := m + 64
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDQ - _mm_mask_expand_epi64

| VPEXPANDQ_XMMu64_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 64-bit integers from "a" (those with their respective bit set in mask "k"), and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

m := 0
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := a[m+63:m]
        m := m + 64
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDQ - _mm_mask_expandloadu_epi64

| VPEXPANDQ_XMMu64_MASKmskw_MEMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 64-bit integers from unaligned memory at "mem_addr" (those with their respective bit
set in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
        m := m + 64
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDQ - _mm_maskz_expand_epi64

| VPEXPANDQ_XMMu64_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 64-bit integers from "a" (those with their respective bit set in mask "k"), and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := a[m+63:m]
        m := m + 64
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDQ - _mm_maskz_expandloadu_epi64

| VPEXPANDQ_XMMu64_MASKmskw_MEMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 64-bit integers from unaligned memory at "mem_addr" (those with their respective bit
set in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
        m := m + 64
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPGATHERDD - _mm256_mmask_i32gather_epi32

| VPGATHERDD_YMMu32_MASKmskw_MEMu32_AVX512_VL256

--------------------------------------------------------------------------------------------------------------
Gather 32-bit integers from memory using 32-bit indices. 32-bit elements are loaded from addresses starting at
"base_addr" and offset by each 32-bit element in "vindex" (each index is scaled by the factor in "scale").
Gathered elements are merged into "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 7
    i := j*32
    m := j*32
    IF k[j]
        addr := base_addr + SignExtend64(vindex[m+31:m]) * ZeroExtend64(scale) * 8
        dst[i+31:i] := MEM[addr+31:addr]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPGATHERDD - _mm_mmask_i32gather_epi32

| VPGATHERDD_XMMu32_MASKmskw_MEMu32_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Gather 32-bit integers from memory using 32-bit indices. 32-bit elements are loaded from addresses starting at
"base_addr" and offset by each 32-bit element in "vindex" (each index is scaled by the factor in "scale").
Gathered elements are merged into "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 3
    i := j*32
    m := j*32
    IF k[j]
        addr := base_addr + SignExtend64(vindex[m+31:m]) * ZeroExtend64(scale) * 8
        dst[i+31:i] := MEM[addr+31:addr]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPGATHERDQ - _mm256_mmask_i32gather_epi64

| VPGATHERDQ_YMMu64_MASKmskw_MEMu64_AVX512_VL256

--------------------------------------------------------------------------------------------------------------
Gather 64-bit integers from memory using 32-bit indices. 64-bit elements are loaded from addresses starting at
"base_addr" and offset by each 32-bit element in "vindex" (each index is scaled by the factor in "scale").
Gathered elements are merged into "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 3
    i := j*64
    m := j*32
    IF k[j]
        addr := base_addr + SignExtend64(vindex[m+31:m]) * ZeroExtend64(scale) * 8
        dst[i+63:i] := MEM[addr+63:addr]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPGATHERDQ - _mm_mmask_i32gather_epi64

| VPGATHERDQ_XMMu64_MASKmskw_MEMu64_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Gather 64-bit integers from memory using 32-bit indices. 64-bit elements are loaded from addresses starting at
"base_addr" and offset by each 32-bit element in "vindex" (each index is scaled by the factor in "scale").
Gathered elements are merged into "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 1
    i := j*64
    m := j*32
    IF k[j]
        addr := base_addr + SignExtend64(vindex[m+31:m]) * ZeroExtend64(scale) * 8
        dst[i+63:i] := MEM[addr+63:addr]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPGATHERQD - _mm256_mmask_i64gather_epi32

| VPGATHERQD_XMMu32_MASKmskw_MEMu32_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Gather 32-bit integers from memory using 64-bit indices. 32-bit elements are loaded from addresses starting at
"base_addr" and offset by each 64-bit element in "vindex" (each index is scaled by the factor in "scale").
Gathered elements are merged into "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 3
    i := j*32
    m := j*64
    IF k[j]
        addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
        dst[i+31:i] := MEM[addr+31:addr]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPGATHERQD - _mm_mmask_i64gather_epi32

| VPGATHERQD_XMMu32_MASKmskw_MEMu32_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Gather 32-bit integers from memory using 64-bit indices. 32-bit elements are loaded from addresses starting at
"base_addr" and offset by each 64-bit element in "vindex" (each index is scaled by the factor in "scale").
Gathered elements are merged into "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 1
    i := j*32
    m := j*64
    IF k[j]
        addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
        dst[i+31:i] := MEM[addr+31:addr]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPGATHERQQ - _mm256_mmask_i64gather_epi64

| VPGATHERQQ_YMMu64_MASKmskw_MEMu64_AVX512_VL256

--------------------------------------------------------------------------------------------------------------
Gather 64-bit integers from memory using 64-bit indices. 64-bit elements are loaded from addresses starting at
"base_addr" and offset by each 64-bit element in "vindex" (each index is scaled by the factor in "scale").
Gathered elements are merged into "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 3
    i := j*64
    m := j*64
    IF k[j]
        addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
        dst[i+63:i] := MEM[addr+63:addr]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPGATHERQQ - _mm_mmask_i64gather_epi64

| VPGATHERQQ_XMMu64_MASKmskw_MEMu64_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Gather 64-bit integers from memory using 64-bit indices. 64-bit elements are loaded from addresses starting at
"base_addr" and offset by each 64-bit element in "vindex" (each index is scaled by the factor in "scale").
Gathered elements are merged into "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 1
    i := j*64
    m := j*64
    IF k[j]
        addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
        dst[i+63:i] := MEM[addr+63:addr]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXSD - _mm256_mask_max_epi32

| VPMAXSD_YMMi32_MASKmskw_YMMi32_YMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b", and store packed maximum values in "dst" using writemask
"k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXSD - _mm256_maskz_max_epi32

| VPMAXSD_YMMi32_MASKmskw_YMMi32_YMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXSD - _mm_mask_max_epi32

| VPMAXSD_XMMi32_MASKmskw_XMMi32_XMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b", and store packed maximum values in "dst" using writemask
"k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXSD - _mm_maskz_max_epi32

| VPMAXSD_XMMi32_MASKmskw_XMMi32_XMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXSQ - _mm256_mask_max_epi64

| VPMAXSQ_YMMi64_MASKmskw_YMMi64_YMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b", and store packed maximum values in "dst" using writemask
"k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXSQ - _mm256_maskz_max_epi64

| VPMAXSQ_YMMi64_MASKmskw_YMMi64_YMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXSQ - _mm256_max_epi64

| VPMAXSQ_YMMi64_MASKmskw_YMMi64_YMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b", and store packed maximum values in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXSQ - _mm_mask_max_epi64

| VPMAXSQ_XMMi64_MASKmskw_XMMi64_XMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b", and store packed maximum values in "dst" using writemask
"k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXSQ - _mm_maskz_max_epi64

| VPMAXSQ_XMMi64_MASKmskw_XMMi64_XMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXSQ - _mm_max_epi64

| VPMAXSQ_XMMi64_MASKmskw_XMMi64_XMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b", and store packed maximum values in "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXUD - _mm256_mask_max_epu32

| VPMAXUD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b", and store packed maximum values in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXUD - _mm256_maskz_max_epu32

| VPMAXUD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b", and store packed maximum values in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXUD - _mm_mask_max_epu32

| VPMAXUD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b", and store packed maximum values in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXUD - _mm_maskz_max_epu32

| VPMAXUD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b", and store packed maximum values in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXUQ - _mm256_mask_max_epu64

| VPMAXUQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b", and store packed maximum values in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXUQ - _mm256_maskz_max_epu64

| VPMAXUQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b", and store packed maximum values in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXUQ - _mm256_max_epu64

| VPMAXUQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b", and store packed maximum values in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXUQ - _mm_mask_max_epu64

| VPMAXUQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b", and store packed maximum values in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXUQ - _mm_maskz_max_epu64

| VPMAXUQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b", and store packed maximum values in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXUQ - _mm_max_epu64

| VPMAXUQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b", and store packed maximum values in "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINSD - _mm256_mask_min_epi32

| VPMINSD_YMMi32_MASKmskw_YMMi32_YMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b", and store packed minimum values in "dst" using writemask
"k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINSD - _mm256_maskz_min_epi32

| VPMINSD_YMMi32_MASKmskw_YMMi32_YMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINSD - _mm_mask_min_epi32

| VPMINSD_XMMi32_MASKmskw_XMMi32_XMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b", and store packed minimum values in "dst" using writemask
"k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINSD - _mm_maskz_min_epi32

| VPMINSD_XMMi32_MASKmskw_XMMi32_XMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINSQ - _mm256_mask_min_epi64

| VPMINSQ_YMMi64_MASKmskw_YMMi64_YMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b", and store packed minimum values in "dst" using writemask
"k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINSQ - _mm256_maskz_min_epi64

| VPMINSQ_YMMi64_MASKmskw_YMMi64_YMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINSQ - _mm256_min_epi64

| VPMINSQ_YMMi64_MASKmskw_YMMi64_YMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b", and store packed minimum values in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINSQ - _mm_mask_min_epi64

| VPMINSQ_XMMi64_MASKmskw_XMMi64_XMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b", and store packed minimum values in "dst" using writemask
"k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINSQ - _mm_maskz_min_epi64

| VPMINSQ_XMMi64_MASKmskw_XMMi64_XMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINSQ - _mm_min_epi64

| VPMINSQ_XMMi64_MASKmskw_XMMi64_XMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b", and store packed minimum values in "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINUD - _mm256_mask_min_epu32

| VPMINUD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b", and store packed minimum values in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINUD - _mm256_maskz_min_epu32

| VPMINUD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b", and store packed minimum values in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINUD - _mm_mask_min_epu32

| VPMINUD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b", and store packed minimum values in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINUD - _mm_maskz_min_epu32

| VPMINUD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b", and store packed minimum values in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINUQ - _mm256_mask_min_epu64

| VPMINUQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b", and store packed minimum values in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINUQ - _mm256_maskz_min_epu64

| VPMINUQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b", and store packed minimum values in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINUQ - _mm256_min_epu64

| VPMINUQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b", and store packed minimum values in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINUQ - _mm_mask_min_epu64

| VPMINUQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b", and store packed minimum values in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINUQ - _mm_maskz_min_epu64

| VPMINUQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b", and store packed minimum values in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINUQ - _mm_min_epu64

| VPMINUQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b", and store packed minimum values in "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVDB - _mm256_cvtepi32_epi8

| VPMOVDB_XMMu8_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 32-bit integers in "a" to packed 8-bit integers with truncation, and store the results in
"dst".

[algorithm]

FOR j := 0 to 7
    i := 32*j
    k := 8*j
    dst[k+7:k] := Truncate8(a[i+31:i])
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVDB - _mm256_mask_cvtepi32_epi8

| VPMOVDB_XMMu8_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 32-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := Truncate8(a[i+31:i])
    ELSE
        dst[l+7:l] := src[l+7:l]
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVDB - _mm256_mask_cvtepi32_storeu_epi8

| VPMOVDB_MEMu8_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 32-bit integers in "a" to packed 8-bit integers with truncation, and store the active results
(those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 8*j
    IF k[j]
        MEM[base_addr+l+7:base_addr+l] := Truncate8(a[i+31:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVDB - _mm256_maskz_cvtepi32_epi8

| VPMOVDB_XMMu8_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 32-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := Truncate8(a[i+31:i])
    ELSE
        dst[l+7:l] := 0
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVDB - _mm_cvtepi32_epi8

| VPMOVDB_XMMu8_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 32-bit integers in "a" to packed 8-bit integers with truncation, and store the results in
"dst".

[algorithm]

FOR j := 0 to 3
    i := 32*j
    k := 8*j
    dst[k+7:k] := Truncate8(a[i+31:i])
ENDFOR
dst[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVDB - _mm_mask_cvtepi32_epi8

| VPMOVDB_XMMu8_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 32-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 32*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := Truncate8(a[i+31:i])
    ELSE
        dst[l+7:l] := src[l+7:l]
    FI
ENDFOR
dst[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVDB - _mm_mask_cvtepi32_storeu_epi8

| VPMOVDB_MEMu8_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 32-bit integers in "a" to packed 8-bit integers with truncation, and store the active results
(those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 3
    i := 32*j
    l := 8*j
    IF k[j]
        MEM[base_addr+l+7:base_addr+l] := Truncate8(a[i+31:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVDB - _mm_maskz_cvtepi32_epi8

| VPMOVDB_XMMu8_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 32-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 32*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := Truncate8(a[i+31:i])
    ELSE
        dst[l+7:l] := 0
    FI
ENDFOR
dst[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVDW - _mm256_cvtepi32_epi16

| VPMOVDW_XMMu16_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 32-bit integers in "a" to packed 16-bit integers with truncation, and store the results in
"dst".

[algorithm]

FOR j := 0 to 7
    i := 32*j
    k := 16*j
    dst[k+15:k] := Truncate16(a[i+31:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVDW - _mm256_mask_cvtepi32_epi16

| VPMOVDW_XMMu16_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 32-bit integers in "a" to packed 16-bit integers with truncation, and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 16*j
    IF k[j]
        dst[l+15:l] := Truncate16(a[i+31:i])
    ELSE
        dst[l+15:l] := src[l+15:l]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVDW - _mm256_mask_cvtepi32_storeu_epi16

| VPMOVDW_MEMu16_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 32-bit integers in "a" to packed 16-bit integers with truncation, and store the active results
(those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 16*j
    IF k[j]
        MEM[base_addr+l+15:base_addr+l] := Truncate16(a[i+31:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVDW - _mm256_maskz_cvtepi32_epi16

| VPMOVDW_XMMu16_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 32-bit integers in "a" to packed 16-bit integers with truncation, and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 16*j
    IF k[j]
        dst[l+15:l] := Truncate16(a[i+31:i])
    ELSE
        dst[l+15:l] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVDW - _mm_cvtepi32_epi16

| VPMOVDW_XMMu16_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 32-bit integers in "a" to packed 16-bit integers with truncation, and store the results in
"dst".

[algorithm]

FOR j := 0 to 3
    i := 32*j
    k := 16*j
    dst[k+15:k] := Truncate16(a[i+31:i])
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVDW - _mm_mask_cvtepi32_epi16

| VPMOVDW_XMMu16_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 32-bit integers in "a" to packed 16-bit integers with truncation, and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 32*j
    l := 16*j
    IF k[j]
        dst[l+15:l] := Truncate16(a[i+31:i])
    ELSE
        dst[l+15:l] := src[l+15:l]
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVDW - _mm_mask_cvtepi32_storeu_epi16

| VPMOVDW_MEMu16_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 32-bit integers in "a" to packed 16-bit integers with truncation, and store the active results
(those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 3
    i := 32*j
    l := 16*j
    IF k[j]
        MEM[base_addr+l+15:base_addr+l] := Truncate16(a[i+31:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVDW - _mm_maskz_cvtepi32_epi16

| VPMOVDW_XMMu16_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 32-bit integers in "a" to packed 16-bit integers with truncation, and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 32*j
    l := 16*j
    IF k[j]
        dst[l+15:l] := Truncate16(a[i+31:i])
    ELSE
        dst[l+15:l] := 0
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVQB - _mm256_cvtepi64_epi8

| VPMOVQB_XMMu8_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 64-bit integers in "a" to packed 8-bit integers with truncation, and store the results in
"dst".

[algorithm]

FOR j := 0 to 3
    i := 64*j
    k := 8*j
    dst[k+7:k] := Truncate8(a[i+63:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVQB - _mm256_mask_cvtepi64_epi8

| VPMOVQB_XMMu8_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 64-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := Truncate8(a[i+63:i])
    ELSE
        dst[l+7:l] := src[l+7:l]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVQB - _mm256_mask_cvtepi64_storeu_epi8

| VPMOVQB_MEMu8_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 64-bit integers in "a" to packed 8-bit integers with truncation, and store the active results
(those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 8*j
    IF k[j]
        MEM[base_addr+l+7:base_addr+l] := Truncate8(a[i+63:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVQB - _mm256_maskz_cvtepi64_epi8

| VPMOVQB_XMMu8_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 64-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := Truncate8(a[i+63:i])
    ELSE
        dst[l+7:l] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVQB - _mm_cvtepi64_epi8

| VPMOVQB_XMMu8_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 64-bit integers in "a" to packed 8-bit integers with truncation, and store the results in
"dst".

[algorithm]

FOR j := 0 to 1
    i := 64*j
    k := 8*j
    dst[k+7:k] := Truncate8(a[i+63:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVQB - _mm_mask_cvtepi64_epi8

| VPMOVQB_XMMu8_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 64-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := Truncate8(a[i+63:i])
    ELSE
        dst[l+7:l] := src[l+7:l]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVQB - _mm_mask_cvtepi64_storeu_epi8

| VPMOVQB_MEMu8_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 64-bit integers in "a" to packed 8-bit integers with truncation, and store the active results
(those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 8*j
    IF k[j]
        MEM[base_addr+l+7:base_addr+l] := Truncate8(a[i+63:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVQB - _mm_maskz_cvtepi64_epi8

| VPMOVQB_XMMu8_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 64-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := Truncate8(a[i+63:i])
    ELSE
        dst[l+7:l] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVQD - _mm256_cvtepi64_epi32

| VPMOVQD_XMMu32_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 64-bit integers in "a" to packed 32-bit integers with truncation, and store the results in
"dst".

[algorithm]

FOR j := 0 to 3
    i := 64*j
    k := 32*j
    dst[k+31:k] := Truncate32(a[i+63:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVQD - _mm256_mask_cvtepi64_epi32

| VPMOVQD_XMMu32_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 64-bit integers in "a" to packed 32-bit integers with truncation, and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 32*j
    IF k[j]
        dst[l+31:l] := Truncate32(a[i+63:i])
    ELSE
        dst[l+31:l] := src[l+31:l]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVQD - _mm256_mask_cvtepi64_storeu_epi32

| VPMOVQD_MEMu32_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 64-bit integers in "a" to packed 32-bit integers with truncation, and store the active results
(those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 32*j
    IF k[j]
        MEM[base_addr+l+31:base_addr+l] := Truncate32(a[i+63:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVQD - _mm256_maskz_cvtepi64_epi32

| VPMOVQD_XMMu32_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 64-bit integers in "a" to packed 32-bit integers with truncation, and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 32*j
    IF k[j]
        dst[l+31:l] := Truncate32(a[i+63:i])
    ELSE
        dst[l+31:l] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVQD - _mm_cvtepi64_epi32

| VPMOVQD_XMMu32_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 64-bit integers in "a" to packed 32-bit integers with truncation, and store the results in
"dst".

[algorithm]

FOR j := 0 to 1
    i := 64*j
    k := 32*j
    dst[k+31:k] := Truncate32(a[i+63:i])
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVQD - _mm_mask_cvtepi64_epi32

| VPMOVQD_XMMu32_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 64-bit integers in "a" to packed 32-bit integers with truncation, and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 32*j
    IF k[j]
        dst[l+31:l] := Truncate32(a[i+63:i])
    ELSE
        dst[l+31:l] := src[l+31:l]
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVQD - _mm_mask_cvtepi64_storeu_epi32

| VPMOVQD_MEMu32_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 64-bit integers in "a" to packed 32-bit integers with truncation, and store the active results
(those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 32*j
    IF k[j]
        MEM[base_addr+l+31:base_addr+l] := Truncate32(a[i+63:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVQD - _mm_maskz_cvtepi64_epi32

| VPMOVQD_XMMu32_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 64-bit integers in "a" to packed 32-bit integers with truncation, and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 32*j
    IF k[j]
        dst[l+31:l] := Truncate32(a[i+63:i])
    ELSE
        dst[l+31:l] := 0
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVQW - _mm256_cvtepi64_epi16

| VPMOVQW_XMMu16_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 64-bit integers in "a" to packed 16-bit integers with truncation, and store the results in
"dst".

[algorithm]

FOR j := 0 to 3
    i := 64*j
    k := 16*j
    dst[k+15:k] := Truncate16(a[i+63:i])
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVQW - _mm256_mask_cvtepi64_epi16

| VPMOVQW_XMMu16_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 64-bit integers in "a" to packed 16-bit integers with truncation, and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 16*j
    IF k[j]
        dst[l+15:l] := Truncate16(a[i+63:i])
    ELSE
        dst[l+15:l] := src[l+15:l]
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVQW - _mm256_mask_cvtepi64_storeu_epi16

| VPMOVQW_MEMu16_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 64-bit integers in "a" to packed 16-bit integers with truncation, and store the active results
(those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 16*j
    IF k[j]
        MEM[base_addr+l+15:base_addr+l] := Truncate16(a[i+63:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVQW - _mm256_maskz_cvtepi64_epi16

| VPMOVQW_XMMu16_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 64-bit integers in "a" to packed 16-bit integers with truncation, and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 16*j
    IF k[j]
        dst[l+15:l] := Truncate16(a[i+63:i])
    ELSE
        dst[l+15:l] := 0
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVQW - _mm_cvtepi64_epi16

| VPMOVQW_XMMu16_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 64-bit integers in "a" to packed 16-bit integers with truncation, and store the results in
"dst".

[algorithm]

FOR j := 0 to 1
    i := 64*j
    k := 16*j
    dst[k+15:k] := Truncate16(a[i+63:i])
ENDFOR
dst[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVQW - _mm_mask_cvtepi64_epi16

| VPMOVQW_XMMu16_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 64-bit integers in "a" to packed 16-bit integers with truncation, and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 16*j
    IF k[j]
        dst[l+15:l] := Truncate16(a[i+63:i])
    ELSE
        dst[l+15:l] := src[l+15:l]
    FI
ENDFOR
dst[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVQW - _mm_mask_cvtepi64_storeu_epi16

| VPMOVQW_MEMu16_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 64-bit integers in "a" to packed 16-bit integers with truncation, and store the active results
(those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 16*j
    IF k[j]
        MEM[base_addr+l+15:base_addr+l] := Truncate16(a[i+63:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVQW - _mm_maskz_cvtepi64_epi16

| VPMOVQW_XMMu16_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 64-bit integers in "a" to packed 16-bit integers with truncation, and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 16*j
    IF k[j]
        dst[l+15:l] := Truncate16(a[i+63:i])
    ELSE
        dst[l+15:l] := 0
    FI
ENDFOR
dst[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSDB - _mm256_cvtsepi32_epi8

| VPMOVSDB_XMMi8_MASKmskw_YMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed 8-bit integers with signed saturation, and store the
results in "dst".

[algorithm]

FOR j := 0 to 7
    i := 32*j
    k := 8*j
    dst[k+7:k] := Saturate8(a[i+31:i])
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSDB - _mm256_mask_cvtsepi32_epi8

| VPMOVSDB_XMMi8_MASKmskw_YMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed 8-bit integers with signed saturation, and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := Saturate8(a[i+31:i])
    ELSE
        dst[l+7:l] := src[l+7:l]
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSDB - _mm256_mask_cvtsepi32_storeu_epi8

| VPMOVSDB_MEMi8_MASKmskw_YMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed 8-bit integers with signed saturation, and store the
active results (those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 8*j
    IF k[j]
        MEM[base_addr+l+7:base_addr+l] := Saturate8(a[i+31:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVSDB - _mm256_maskz_cvtsepi32_epi8

| VPMOVSDB_XMMi8_MASKmskw_YMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed 8-bit integers with signed saturation, and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := Saturate8(a[i+31:i])
    ELSE
        dst[l+7:l] := 0
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSDB - _mm_cvtsepi32_epi8

| VPMOVSDB_XMMi8_MASKmskw_XMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed 8-bit integers with signed saturation, and store the
results in "dst".

[algorithm]

FOR j := 0 to 3
    i := 32*j
    k := 8*j
    dst[k+7:k] := Saturate8(a[i+31:i])
ENDFOR
dst[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSDB - _mm_mask_cvtsepi32_epi8

| VPMOVSDB_XMMi8_MASKmskw_XMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed 8-bit integers with signed saturation, and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 3
    i := 32*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := Saturate8(a[i+31:i])
    ELSE
        dst[l+7:l] := src[l+7:l]
    FI
ENDFOR
dst[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSDB - _mm_mask_cvtsepi32_storeu_epi8

| VPMOVSDB_MEMi8_MASKmskw_XMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed 8-bit integers with signed saturation, and store the
active results (those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 3
    i := 32*j
    l := 8*j
    IF k[j]
        MEM[base_addr+l+7:base_addr+l] := Saturate8(a[i+31:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVSDB - _mm_maskz_cvtsepi32_epi8

| VPMOVSDB_XMMi8_MASKmskw_XMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed 8-bit integers with signed saturation, and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 32*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := Saturate8(a[i+31:i])
    ELSE
        dst[l+7:l] := 0
    FI
ENDFOR
dst[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSDW - _mm256_cvtsepi32_epi16

| VPMOVSDW_XMMi16_MASKmskw_YMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed 16-bit integers with signed saturation, and store the
results in "dst".

[algorithm]

FOR j := 0 to 7
    i := 32*j
    k := 16*j
    dst[k+15:k] := Saturate16(a[i+31:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSDW - _mm256_mask_cvtsepi32_epi16

| VPMOVSDW_XMMi16_MASKmskw_YMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed 16-bit integers with signed saturation, and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 16*j
    IF k[j]
        dst[l+15:l] := Saturate16(a[i+31:i])
    ELSE
        dst[l+15:l] := src[l+15:l]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSDW - _mm256_mask_cvtsepi32_storeu_epi16

| VPMOVSDW_MEMi16_MASKmskw_YMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed 16-bit integers with signed saturation, and store the
active results (those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 16*j
    IF k[j]
        MEM[base_addr+l+15:base_addr+l] := Saturate16(a[i+31:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVSDW - _mm256_maskz_cvtsepi32_epi16

| VPMOVSDW_XMMi16_MASKmskw_YMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed 16-bit integers with signed saturation, and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 16*j
    IF k[j]
        dst[l+15:l] := Saturate16(a[i+31:i])
    ELSE
        dst[l+15:l] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSDW - _mm_cvtsepi32_epi16

| VPMOVSDW_XMMi16_MASKmskw_XMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed 16-bit integers with signed saturation, and store the
results in "dst".

[algorithm]

FOR j := 0 to 3
    i := 32*j
    k := 16*j
    dst[k+15:k] := Saturate16(a[i+31:i])
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSDW - _mm_mask_cvtsepi32_epi16

| VPMOVSDW_XMMi16_MASKmskw_XMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed 16-bit integers with signed saturation, and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 3
    i := 32*j
    l := 16*j
    IF k[j]
        dst[l+15:l] := Saturate16(a[i+31:i])
    ELSE
        dst[l+15:l] := src[l+15:l]
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSDW - _mm_mask_cvtsepi32_storeu_epi16

| VPMOVSDW_MEMi16_MASKmskw_XMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed 16-bit integers with signed saturation, and store the
active results (those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 3
    i := 32*j
    l := 16*j
    IF k[j]
        MEM[base_addr+l+15:base_addr+l] := Saturate16(a[i+31:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVSDW - _mm_maskz_cvtsepi32_epi16

| VPMOVSDW_XMMi16_MASKmskw_XMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed 16-bit integers with signed saturation, and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 32*j
    l := 16*j
    IF k[j]
        dst[l+15:l] := Saturate16(a[i+31:i])
    ELSE
        dst[l+15:l] := 0
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSQB - _mm256_cvtsepi64_epi8

| VPMOVSQB_XMMi8_MASKmskw_YMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed 8-bit integers with signed saturation, and store the
results in "dst".

[algorithm]

FOR j := 0 to 3
    i := 64*j
    k := 8*j
    dst[k+7:k] := Saturate8(a[i+63:i])
ENDFOR
dst[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSQB - _mm256_mask_cvtsepi64_epi8

| VPMOVSQB_XMMi8_MASKmskw_YMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed 8-bit integers with signed saturation, and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := Saturate8(a[i+63:i])
    ELSE
        dst[l+7:l] := src[l+7:l]
    FI
ENDFOR
dst[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSQB - _mm256_mask_cvtsepi64_storeu_epi8

| VPMOVSQB_MEMi8_MASKmskw_YMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed 8-bit integers with signed saturation, and store the
active results (those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 8*j
    IF k[j]
        MEM[base_addr+l+7:base_addr+l] := Saturate8(a[i+63:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVSQB - _mm256_maskz_cvtsepi64_epi8

| VPMOVSQB_XMMi8_MASKmskw_YMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed 8-bit integers with signed saturation, and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := Saturate8(a[i+63:i])
    ELSE
        dst[l+7:l] := 0
    FI
ENDFOR
dst[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSQB - _mm_cvtsepi64_epi8

| VPMOVSQB_XMMi8_MASKmskw_XMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed 8-bit integers with signed saturation, and store the
results in "dst".

[algorithm]

FOR j := 0 to 1
    i := 64*j
    k := 8*j
    dst[k+7:k] := Saturate8(a[i+63:i])
ENDFOR
dst[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSQB - _mm_mask_cvtsepi64_epi8

| VPMOVSQB_XMMi8_MASKmskw_XMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed 8-bit integers with signed saturation, and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := Saturate8(a[i+63:i])
    ELSE
        dst[l+7:l] := src[l+7:l]
    FI
ENDFOR
dst[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSQB - _mm_mask_cvtsepi64_storeu_epi8

| VPMOVSQB_MEMi8_MASKmskw_XMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed 8-bit integers with signed saturation, and store the
active results (those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 8*j
    IF k[j]
        MEM[base_addr+l+7:base_addr+l] := Saturate8(a[i+63:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVSQB - _mm_maskz_cvtsepi64_epi8

| VPMOVSQB_XMMi8_MASKmskw_XMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed 8-bit integers with signed saturation, and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := Saturate8(a[i+63:i])
    ELSE
        dst[l+7:l] := 0
    FI
ENDFOR
dst[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSQD - _mm256_cvtsepi64_epi32

| VPMOVSQD_XMMi32_MASKmskw_YMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed 32-bit integers with signed saturation, and store the
results in "dst".

[algorithm]

FOR j := 0 to 3
    i := 64*j
    k := 32*j
    dst[k+31:k] := Saturate32(a[i+63:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSQD - _mm256_mask_cvtsepi64_epi32

| VPMOVSQD_XMMi32_MASKmskw_YMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed 32-bit integers with signed saturation, and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 32*j
    IF k[j]
        dst[l+31:l] := Saturate32(a[i+63:i])
    ELSE
        dst[l+31:l] := src[l+31:l]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSQD - _mm256_mask_cvtsepi64_storeu_epi32

| VPMOVSQD_MEMi32_MASKmskw_YMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed 32-bit integers with signed saturation, and store the
active results (those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 32*j
    IF k[j]
        MEM[base_addr+l+31:base_addr+l] := Saturate32(a[i+63:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVSQD - _mm256_maskz_cvtsepi64_epi32

| VPMOVSQD_XMMi32_MASKmskw_YMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed 32-bit integers with signed saturation, and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 32*j
    IF k[j]
        dst[l+31:l] := Saturate32(a[i+63:i])
    ELSE
        dst[l+31:l] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSQD - _mm_cvtsepi64_epi32

| VPMOVSQD_XMMi32_MASKmskw_XMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed 32-bit integers with signed saturation, and store the
results in "dst".

[algorithm]

FOR j := 0 to 1
    i := 64*j
    k := 32*j
    dst[k+31:k] := Saturate32(a[i+63:i])
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSQD - _mm_mask_cvtsepi64_epi32

| VPMOVSQD_XMMi32_MASKmskw_XMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed 32-bit integers with signed saturation, and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 32*j
    IF k[j]
        dst[l+31:l] := Saturate32(a[i+63:i])
    ELSE
        dst[l+31:l] := src[l+31:l]
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSQD - _mm_mask_cvtsepi64_storeu_epi32

| VPMOVSQD_MEMi32_MASKmskw_XMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed 32-bit integers with signed saturation, and store the
active results (those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 32*j
    IF k[j]
        MEM[base_addr+l+31:base_addr+l] := Saturate32(a[i+63:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVSQD - _mm_maskz_cvtsepi64_epi32

| VPMOVSQD_XMMi32_MASKmskw_XMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed 32-bit integers with signed saturation, and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 32*j
    IF k[j]
        dst[l+31:l] := Saturate32(a[i+63:i])
    ELSE
        dst[l+31:l] := 0
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSQW - _mm256_cvtsepi64_epi16

| VPMOVSQW_XMMi16_MASKmskw_YMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed 16-bit integers with signed saturation, and store the
results in "dst".

[algorithm]

FOR j := 0 to 3
    i := 64*j
    k := 16*j
    dst[k+15:k] := Saturate16(a[i+63:i])
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSQW - _mm256_mask_cvtsepi64_epi16

| VPMOVSQW_XMMi16_MASKmskw_YMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed 16-bit integers with signed saturation, and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 16*j
    IF k[j]
        dst[l+15:l] := Saturate16(a[i+63:i])
    ELSE
        dst[l+15:l] := src[l+15:l]
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSQW - _mm256_mask_cvtsepi64_storeu_epi16

| VPMOVSQW_MEMi16_MASKmskw_YMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed 16-bit integers with signed saturation, and store the
active results (those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 16*j
    IF k[j]
        MEM[base_addr+l+15:base_addr+l] := Saturate16(a[i+63:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVSQW - _mm256_maskz_cvtsepi64_epi16

| VPMOVSQW_XMMi16_MASKmskw_YMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed 16-bit integers with signed saturation, and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 16*j
    IF k[j]
        dst[l+15:l] := Saturate16(a[i+63:i])
    ELSE
        dst[l+15:l] := 0
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSQW - _mm_cvtsepi64_epi16

| VPMOVSQW_XMMi16_MASKmskw_XMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed 16-bit integers with signed saturation, and store the
results in "dst".

[algorithm]

FOR j := 0 to 1
    i := 64*j
    k := 16*j
    dst[k+15:k] := Saturate16(a[i+63:i])
ENDFOR
dst[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSQW - _mm_mask_cvtsepi64_epi16

| VPMOVSQW_XMMi16_MASKmskw_XMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed 16-bit integers with signed saturation, and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 16*j
    IF k[j]
        dst[l+15:l] := Saturate16(a[i+63:i])
    ELSE
        dst[l+15:l] := src[l+15:l]
    FI
ENDFOR
dst[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSQW - _mm_mask_cvtsepi64_storeu_epi16

| VPMOVSQW_MEMi16_MASKmskw_XMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed 16-bit integers with signed saturation, and store the
active results (those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 16*j
    IF k[j]
        MEM[base_addr+l+15:base_addr+l] := Saturate16(a[i+63:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVSQW - _mm_maskz_cvtsepi64_epi16

| VPMOVSQW_XMMi16_MASKmskw_XMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed 16-bit integers with signed saturation, and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 16*j
    IF k[j]
        dst[l+15:l] := Saturate16(a[i+63:i])
    ELSE
        dst[l+15:l] := 0
    FI
ENDFOR
dst[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXBD - _mm256_mask_cvtepi8_epi32

| VPMOVSXBD_YMMi32_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 8-bit integers in the low 8 bytes of "a" to packed 32-bit integers, and store the results
in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 8*j
    IF k[j]
        dst[i+31:i] := SignExtend32(a[l+7:l])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXBD - _mm256_maskz_cvtepi8_epi32

| VPMOVSXBD_YMMi32_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 8-bit integers in the low 8 bytes of "a" to packed 32-bit integers, and store the results
in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 8*j
    IF k[j]
        dst[i+31:i] := SignExtend32(a[l+7:l])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXBD - _mm_mask_cvtepi8_epi32

| VPMOVSXBD_XMMi32_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 8-bit integers in the low 4 bytes of "a" to packed 32-bit integers, and store the results
in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 32*j
    l := 8*j
    IF k[j]
        dst[i+31:i] := SignExtend32(a[l+7:l])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXBD - _mm_maskz_cvtepi8_epi32

| VPMOVSXBD_XMMi32_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 8-bit integers in the low 4 bytes of "a" to packed 32-bit integers, and store the results
in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 32*j
    l := 8*j
    IF k[j]
        dst[i+31:i] := SignExtend32(a[l+7:l])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXBQ - _mm256_mask_cvtepi8_epi64

| VPMOVSXBQ_YMMi64_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 8-bit integers in the low 4 bytes of "a" to packed 64-bit integers, and store the results
in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 8*j
    IF k[j]
        dst[i+63:i] := SignExtend64(a[l+7:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXBQ - _mm256_maskz_cvtepi8_epi64

| VPMOVSXBQ_YMMi64_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 8-bit integers in the low 4 bytes of "a" to packed 64-bit integers, and store the results
in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 8*j
    IF k[j]
        dst[i+63:i] := SignExtend64(a[l+7:l])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXBQ - _mm_mask_cvtepi8_epi64

| VPMOVSXBQ_XMMi64_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 8-bit integers in the low 2 bytes of "a" to packed 64-bit integers, and store the results
in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 8*j
    IF k[j]
        dst[i+63:i] := SignExtend64(a[l+7:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXBQ - _mm_maskz_cvtepi8_epi64

| VPMOVSXBQ_XMMi64_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 8-bit integers in the low 2 bytes of "a" to packed 64-bit integers, and store the results
in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 8*j
    IF k[j]
        dst[i+63:i] := SignExtend64(a[l+7:l])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXDQ - _mm256_mask_cvtepi32_epi64

| VPMOVSXDQ_YMMi64_MASKmskw_XMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 32-bit integers in "a" to packed 64-bit integers, and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 32*j
    IF k[j]
        dst[i+63:i] := SignExtend64(a[l+31:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXDQ - _mm256_maskz_cvtepi32_epi64

| VPMOVSXDQ_YMMi64_MASKmskw_XMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 32-bit integers in "a" to packed 64-bit integers, and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 32*j
    IF k[j]
        dst[i+63:i] := SignExtend64(a[l+31:l])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXDQ - _mm_mask_cvtepi32_epi64

| VPMOVSXDQ_XMMi64_MASKmskw_XMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 32-bit integers in "a" to packed 64-bit integers, and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 32*j
    IF k[j]
        dst[i+63:i] := SignExtend64(a[l+31:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXDQ - _mm_maskz_cvtepi32_epi64

| VPMOVSXDQ_XMMi64_MASKmskw_XMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 32-bit integers in "a" to packed 64-bit integers, and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 32*j
    IF k[j]
        dst[i+63:i] := SignExtend64(a[l+31:l])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXWD - _mm256_mask_cvtepi16_epi32

| VPMOVSXWD_YMMi32_MASKmskw_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 16-bit integers in "a" to packed 32-bit integers, and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    l := j*16
    IF k[j]
        dst[i+31:i] := SignExtend32(a[l+15:l])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXWD - _mm256_maskz_cvtepi16_epi32

| VPMOVSXWD_YMMi32_MASKmskw_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 16-bit integers in "a" to packed 32-bit integers, and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 16*j
    IF k[j]
        dst[i+31:i] := SignExtend32(a[l+15:l])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXWD - _mm_mask_cvtepi16_epi32

| VPMOVSXWD_XMMi32_MASKmskw_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 16-bit integers in "a" to packed 32-bit integers, and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    l := j*16
    IF k[j]
        dst[i+31:i] := SignExtend32(a[l+15:l])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXWD - _mm_maskz_cvtepi16_epi32

| VPMOVSXWD_XMMi32_MASKmskw_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 16-bit integers in "a" to packed 32-bit integers, and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 32*j
    l := 16*j
    IF k[j]
        dst[i+31:i] := SignExtend32(a[l+15:l])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXWQ - _mm256_mask_cvtepi16_epi64

| VPMOVSXWQ_YMMi64_MASKmskw_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 16-bit integers in the low 8 bytes of "a" to packed 64-bit integers, and store the results
in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 16*j
    IF k[j]
        dst[i+63:i] := SignExtend64(a[l+15:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXWQ - _mm256_maskz_cvtepi16_epi64

| VPMOVSXWQ_YMMi64_MASKmskw_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 16-bit integers in the low 8 bytes of "a" to packed 64-bit integers, and store the results
in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 16*j
    IF k[j]
        dst[i+63:i] := SignExtend64(a[l+15:l])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXWQ - _mm_mask_cvtepi16_epi64

| VPMOVSXWQ_XMMi64_MASKmskw_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 16-bit integers in the low 4 bytes of "a" to packed 64-bit integers, and store the results
in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 16*j
    IF k[j]
        dst[i+63:i] := SignExtend64(a[l+15:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXWQ - _mm_maskz_cvtepi16_epi64

| VPMOVSXWQ_XMMi64_MASKmskw_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 16-bit integers in the low 4 bytes of "a" to packed 64-bit integers, and store the results
in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 16*j
    IF k[j]
        dst[i+63:i] := SignExtend64(a[l+15:l])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSDB - _mm256_cvtusepi32_epi8

| VPMOVUSDB_XMMu8_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and
store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := 32*j
    k := 8*j
    dst[k+7:k] := SaturateU8(a[i+31:i])
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSDB - _mm256_mask_cvtusepi32_epi8

| VPMOVUSDB_XMMu8_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := SaturateU8(a[i+31:i])
    ELSE
        dst[l+7:l] := src[l+7:l]
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSDB - _mm256_mask_cvtusepi32_storeu_epi8

| VPMOVUSDB_MEMu8_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and
store the active results (those with their respective bit set in writemask "k") to unaligned memory at
"base_addr".

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 8*j
    IF k[j]
        MEM[base_addr+l+7:base_addr+l] := SaturateU8(a[i+31:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVUSDB - _mm256_maskz_cvtusepi32_epi8

| VPMOVUSDB_XMMu8_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := SaturateU8(a[i+31:i])
    ELSE
        dst[l+7:l] := 0
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSDB - _mm_cvtusepi32_epi8

| VPMOVUSDB_XMMu8_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and
store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := 32*j
    k := 8*j
    dst[k+7:k] := SaturateU8(a[i+31:i])
ENDFOR
dst[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSDB - _mm_mask_cvtusepi32_epi8

| VPMOVUSDB_XMMu8_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 3
    i := 32*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := SaturateU8(a[i+31:i])
    ELSE
        dst[l+7:l] := src[l+7:l]
    FI
ENDFOR
dst[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSDB - _mm_mask_cvtusepi32_storeu_epi8

| VPMOVUSDB_MEMu8_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and
store the active results (those with their respective bit set in writemask "k") to unaligned memory at
"base_addr".

[algorithm]

FOR j := 0 to 3
    i := 32*j
    l := 8*j
    IF k[j]
        MEM[base_addr+l+7:base_addr+l] := SaturateU8(a[i+31:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVUSDB - _mm_maskz_cvtusepi32_epi8

| VPMOVUSDB_XMMu8_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 3
    i := 32*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := SaturateU8(a[i+31:i])
    ELSE
        dst[l+7:l] := 0
    FI
ENDFOR
dst[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSDW - _mm256_cvtusepi32_epi16

| VPMOVUSDW_XMMu16_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation,
and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := 32*j
    k := 16*j
    dst[k+15:k] := SaturateU16(a[i+31:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSDW - _mm256_mask_cvtusepi32_epi16

| VPMOVUSDW_XMMu16_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 16*j
    IF k[j]
        dst[l+15:l] := SaturateU16(a[i+31:i])
    ELSE
        dst[l+15:l] := src[l+15:l]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSDW - _mm256_mask_cvtusepi32_storeu_epi16

| VPMOVUSDW_MEMu16_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation,
and store the active results (those with their respective bit set in writemask "k") to unaligned memory at
"base_addr".

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 16*j
    IF k[j]
        MEM[base_addr+l+15:base_addr+l] := SaturateU16(a[i+31:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVUSDW - _mm256_maskz_cvtusepi32_epi16

| VPMOVUSDW_XMMu16_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 16*j
    IF k[j]
        dst[l+15:l] := SaturateU16(a[i+31:i])
    ELSE
        dst[l+15:l] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSDW - _mm_cvtusepi32_epi16

| VPMOVUSDW_XMMu16_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation,
and store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := 32*j
    k := 16*j
    dst[k+15:k] := SaturateU16(a[i+31:i])
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSDW - _mm_mask_cvtusepi32_epi16

| VPMOVUSDW_XMMu16_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 32*j
    l := 16*j
    IF k[j]
        dst[l+15:l] := SaturateU16(a[i+31:i])
    ELSE
        dst[l+15:l] := src[l+15:l]
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSDW - _mm_mask_cvtusepi32_storeu_epi16

| VPMOVUSDW_MEMu16_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation,
and store the active results (those with their respective bit set in writemask "k") to unaligned memory at
"base_addr".

[algorithm]

FOR j := 0 to 3
    i := 32*j
    l := 16*j
    IF k[j]
        MEM[base_addr+l+15:base_addr+l] := SaturateU16(a[i+31:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVUSDW - _mm_maskz_cvtusepi32_epi16

| VPMOVUSDW_XMMu16_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 3
    i := 32*j
    l := 16*j
    IF k[j]
        dst[l+15:l] := SaturateU16(a[i+31:i])
    ELSE
        dst[l+15:l] := 0
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSQB - _mm256_cvtusepi64_epi8

| VPMOVUSQB_XMMu8_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and
store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := 64*j
    k := 8*j
    dst[k+7:k] := SaturateU8(a[i+63:i])
ENDFOR
dst[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSQB - _mm256_mask_cvtusepi64_epi8

| VPMOVUSQB_XMMu8_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := SaturateU8(a[i+63:i])
    ELSE
        dst[l+7:l] := src[l+7:l]
    FI
ENDFOR
dst[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSQB - _mm256_mask_cvtusepi64_storeu_epi8

| VPMOVUSQB_MEMu8_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and
store the active results (those with their respective bit set in writemask "k") to unaligned memory at
"base_addr".

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 8*j
    IF k[j]
        MEM[base_addr+l+7:base_addr+l] := SaturateU8(a[i+63:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVUSQB - _mm256_maskz_cvtusepi64_epi8

| VPMOVUSQB_XMMu8_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := SaturateU8(a[i+63:i])
    ELSE
        dst[l+7:l] := 0
    FI
ENDFOR
dst[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSQB - _mm_cvtusepi64_epi8

| VPMOVUSQB_XMMu8_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and
store the results in "dst".

[algorithm]

FOR j := 0 to 1
    i := 64*j
    k := 8*j
    dst[k+7:k] := SaturateU8(a[i+63:i])
ENDFOR
dst[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSQB - _mm_mask_cvtusepi64_epi8

| VPMOVUSQB_XMMu8_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := SaturateU8(a[i+63:i])
    ELSE
        dst[l+7:l] := src[l+7:l]
    FI
ENDFOR
dst[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSQB - _mm_mask_cvtusepi64_storeu_epi8

| VPMOVUSQB_MEMu8_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and
store the active results (those with their respective bit set in writemask "k") to unaligned memory at
"base_addr".

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 8*j
    IF k[j]
        MEM[base_addr+l+7:base_addr+l] := SaturateU8(a[i+63:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVUSQB - _mm_maskz_cvtusepi64_epi8

| VPMOVUSQB_XMMu8_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := SaturateU8(a[i+63:i])
    ELSE
        dst[l+7:l] := 0
    FI
ENDFOR
dst[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSQD - _mm256_cvtusepi64_epi32

| VPMOVUSQD_XMMu32_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed unsigned 32-bit integers with unsigned saturation,
and store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := 64*j
    k := 32*j
    dst[k+31:k] := SaturateU32(a[i+63:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSQD - _mm256_mask_cvtusepi64_epi32

| VPMOVUSQD_XMMu32_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed unsigned 32-bit integers with unsigned saturation,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 32*j
    IF k[j]
        dst[l+31:l] := SaturateU32(a[i+63:i])
    ELSE
        dst[l+31:l] := src[l+31:l]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSQD - _mm256_mask_cvtusepi64_storeu_epi32

| VPMOVUSQD_MEMu32_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed unsigned 32-bit integers with unsigned saturation,
and store the active results (those with their respective bit set in writemask "k") to unaligned memory at
"base_addr".

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 32*j
    IF k[j]
        MEM[base_addr+l+31:base_addr+l] := SaturateU32(a[i+63:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVUSQD - _mm256_maskz_cvtusepi64_epi32

| VPMOVUSQD_XMMu32_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed unsigned 32-bit integers with unsigned saturation,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 32*j
    IF k[j]
        dst[l+31:l] := SaturateU32(a[i+63:i])
    ELSE
        dst[l+31:l] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSQD - _mm_cvtusepi64_epi32

| VPMOVUSQD_XMMu32_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed unsigned 32-bit integers with unsigned saturation,
and store the results in "dst".

[algorithm]

FOR j := 0 to 1
    i := 64*j
    k := 32*j
    dst[k+31:k] := SaturateU32(a[i+63:i])
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSQD - _mm_mask_cvtusepi64_epi32

| VPMOVUSQD_XMMu32_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed unsigned 32-bit integers with unsigned saturation,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 32*j
    IF k[j]
        dst[l+31:l] := SaturateU32(a[i+63:i])
    ELSE
        dst[l+31:l] := src[l+31:l]
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSQD - _mm_mask_cvtusepi64_storeu_epi32

| VPMOVUSQD_MEMu32_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed unsigned 32-bit integers with unsigned saturation,
and store the active results (those with their respective bit set in writemask "k") to unaligned memory at
"base_addr".

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 32*j
    IF k[j]
        MEM[base_addr+l+31:base_addr+l] := SaturateU32(a[i+63:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVUSQD - _mm_maskz_cvtusepi64_epi32

| VPMOVUSQD_XMMu32_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed unsigned 32-bit integers with unsigned saturation,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 32*j
    IF k[j]
        dst[l+31:l] := SaturateU32(a[i+63:i])
    ELSE
        dst[l+31:l] := 0
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSQW - _mm256_cvtusepi64_epi16

| VPMOVUSQW_XMMu16_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation,
and store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := 64*j
    k := 16*j
    dst[k+15:k] := SaturateU16(a[i+63:i])
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSQW - _mm256_mask_cvtusepi64_epi16

| VPMOVUSQW_XMMu16_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 16*j
    IF k[j]
        dst[l+15:l] := SaturateU16(a[i+63:i])
    ELSE
        dst[l+15:l] := src[l+15:l]
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSQW - _mm256_mask_cvtusepi64_storeu_epi16

| VPMOVUSQW_MEMu16_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation,
and store the active results (those with their respective bit set in writemask "k") to unaligned memory at
"base_addr".

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 16*j
    IF k[j]
        MEM[base_addr+l+15:base_addr+l] := SaturateU16(a[i+63:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVUSQW - _mm256_maskz_cvtusepi64_epi16

| VPMOVUSQW_XMMu16_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 16*j
    IF k[j]
        dst[l+15:l] := SaturateU16(a[i+63:i])
    ELSE
        dst[l+15:l] := 0
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSQW - _mm_cvtusepi64_epi16

| VPMOVUSQW_XMMu16_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation,
and store the results in "dst".

[algorithm]

FOR j := 0 to 1
    i := 64*j
    k := 16*j
    dst[k+15:k] := SaturateU16(a[i+63:i])
ENDFOR
dst[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSQW - _mm_mask_cvtusepi64_epi16

| VPMOVUSQW_XMMu16_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 16*j
    IF k[j]
        dst[l+15:l] := SaturateU16(a[i+63:i])
    ELSE
        dst[l+15:l] := src[l+15:l]
    FI
ENDFOR
dst[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSQW - _mm_mask_cvtusepi64_storeu_epi16

| VPMOVUSQW_MEMu16_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation,
and store the active results (those with their respective bit set in writemask "k") to unaligned memory at
"base_addr".

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 16*j
    IF k[j]
        MEM[base_addr+l+15:base_addr+l] := SaturateU16(a[i+63:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVUSQW - _mm_maskz_cvtusepi64_epi16

| VPMOVUSQW_XMMu16_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 16*j
    IF k[j]
        dst[l+15:l] := SaturateU16(a[i+63:i])
    ELSE
        dst[l+15:l] := 0
    FI
ENDFOR
dst[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXBD - _mm256_mask_cvtepu8_epi32

| VPMOVZXBD_YMMi32_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 8-bit integers in the low 8 bytes of "a" to packed 32-bit integers, and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 8*j
    IF k[j]
        dst[i+31:i] := ZeroExtend32(a[l+7:l])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXBD - _mm256_maskz_cvtepu8_epi32

| VPMOVZXBD_YMMi32_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 8-bit integers in the low 8 bytes of "a" to packed 32-bit integers, and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 8*j
    IF k[j]
        dst[i+31:i] := ZeroExtend32(a[l+7:l])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXBD - _mm_mask_cvtepu8_epi32

| VPMOVZXBD_XMMi32_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 8-bit integers in the low 4 bytes of "a" to packed 32-bit integers, and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 3
    i := 32*j
    l := 8*j
    IF k[j]
        dst[i+31:i] := ZeroExtend32(a[l+7:l])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXBD - _mm_maskz_cvtepu8_epi32

| VPMOVZXBD_XMMi32_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 8-bit integers in th elow 4 bytes of "a" to packed 32-bit integers, and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 32*j
    l := 8*j
    IF k[j]
        dst[i+31:i] := ZeroExtend32(a[l+7:l])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXBQ - _mm256_mask_cvtepu8_epi64

| VPMOVZXBQ_YMMi64_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 8-bit integers in the low 4 bytes of "a" to packed 64-bit integers, and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 8*j
    IF k[j]
        dst[i+63:i] := ZeroExtend64(a[l+7:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXBQ - _mm256_maskz_cvtepu8_epi64

| VPMOVZXBQ_YMMi64_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 8-bit integers in the low 4 bytes of "a" to packed 64-bit integers, and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 8*j
    IF k[j]
        dst[i+63:i] := ZeroExtend64(a[l+7:l])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXBQ - _mm_mask_cvtepu8_epi64

| VPMOVZXBQ_XMMi64_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 8-bit integers in the low 2 bytes of "a" to packed 64-bit integers, and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 8*j
    IF k[j]
        dst[i+63:i] := ZeroExtend64(a[l+7:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXBQ - _mm_maskz_cvtepu8_epi64

| VPMOVZXBQ_XMMi64_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 8-bit integers in the low 2 bytes of "a" to packed 64-bit integers, and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 8*j
    IF k[j]
        dst[i+63:i] := ZeroExtend64(a[l+7:l])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXDQ - _mm256_mask_cvtepu32_epi64

| VPMOVZXDQ_YMMi64_MASKmskw_XMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 32-bit integers in "a" to packed 64-bit integers, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 32*j
    IF k[j]
        dst[i+63:i] := ZeroExtend64(a[l+31:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXDQ - _mm256_maskz_cvtepu32_epi64

| VPMOVZXDQ_YMMi64_MASKmskw_XMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 32-bit integers in "a" to packed 64-bit integers, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 32*j
    IF k[j]
        dst[i+63:i] := ZeroExtend64(a[l+31:l])
    ELSE 
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXDQ - _mm_mask_cvtepu32_epi64

| VPMOVZXDQ_XMMi64_MASKmskw_XMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 32-bit integers in "a" to packed 64-bit integers, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 32*j
    IF k[j]
        dst[i+63:i] := ZeroExtend64(a[l+31:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXDQ - _mm_maskz_cvtepu32_epi64

| VPMOVZXDQ_XMMi64_MASKmskw_XMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 32-bit integers in "a" to packed 64-bit integers, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 32*j
    IF k[j]
        dst[i+63:i] := ZeroExtend64(a[l+31:l])
    ELSE 
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXWD - _mm256_mask_cvtepu16_epi32

| VPMOVZXWD_YMMi32_MASKmskw_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 16-bit integers in "a" to packed 32-bit integers, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 16*j
    IF k[j]
        dst[i+31:i] := ZeroExtend32(a[l+15:l])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXWD - _mm256_maskz_cvtepu16_epi32

| VPMOVZXWD_YMMi32_MASKmskw_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 16-bit integers in "a" to packed 32-bit integers, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 16*j
    IF k[j]
        dst[i+31:i] := ZeroExtend32(a[l+15:l])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXWD - _mm_mask_cvtepu16_epi32

| VPMOVZXWD_XMMi32_MASKmskw_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 16-bit integers in "a" to packed 32-bit integers, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 32*j
    l := 16*j
    IF k[j]
        dst[i+31:i] := ZeroExtend32(a[l+15:l])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXWD - _mm_maskz_cvtepu16_epi32

| VPMOVZXWD_XMMi32_MASKmskw_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 16-bit integers in "a" to packed 32-bit integers, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 32*j
    l := 16*j
    IF k[j]
        dst[i+31:i] := ZeroExtend32(a[l+15:l])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXWQ - _mm256_mask_cvtepu16_epi64

| VPMOVZXWQ_YMMi64_MASKmskw_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 16-bit integers in the low 8 bytes of "a" to packed 64-bit integers, and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 16*j
    IF k[j]
        dst[i+63:i] := ZeroExtend64(a[l+15:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXWQ - _mm256_maskz_cvtepu16_epi64

| VPMOVZXWQ_YMMi64_MASKmskw_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 16-bit integers in the low 8 bytes of "a" to packed 64-bit integers, and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := 64*j
    l := 16*j
    IF k[j]
        dst[i+63:i] := ZeroExtend64(a[l+15:l])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXWQ - _mm_mask_cvtepu16_epi64

| VPMOVZXWQ_XMMi64_MASKmskw_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 16-bit integers in the low 4 bytes of "a" to packed 64-bit integers, and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 16*j
    IF k[j]
        dst[i+63:i] := ZeroExtend64(a[l+15:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXWQ - _mm_maskz_cvtepu16_epi64

| VPMOVZXWQ_XMMi64_MASKmskw_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 16-bit integers in the low 4 bytes of "a" to packed 64-bit integers, and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := 64*j
    l := 16*j
    IF k[j]
        dst[i+63:i] := ZeroExtend64(a[l+15:l])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULDQ - _mm256_mask_mul_epi32

| VPMULDQ_YMMi64_MASKmskw_YMMi32_YMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the low signed 32-bit integers from each packed 64-bit element in "a" and "b", and store the signed
64-bit results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := SignExtend64(a[i+31:i]) * SignExtend64(b[i+31:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULDQ - _mm256_maskz_mul_epi32

| VPMULDQ_YMMi64_MASKmskw_YMMi32_YMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the low signed 32-bit integers from each packed 64-bit element in "a" and "b", and store the signed
64-bit results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := SignExtend64(a[i+31:i]) * SignExtend64(b[i+31:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULDQ - _mm_mask_mul_epi32

| VPMULDQ_XMMi64_MASKmskw_XMMi32_XMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the low signed 32-bit integers from each packed 64-bit element in "a" and "b", and store the signed
64-bit results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := SignExtend64(a[i+31:i]) * SignExtend64(b[i+31:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULDQ - _mm_maskz_mul_epi32

| VPMULDQ_XMMi64_MASKmskw_XMMi32_XMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the low signed 32-bit integers from each packed 64-bit element in "a" and "b", and store the signed
64-bit results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := SignExtend64(a[i+31:i]) * SignExtend64(b[i+31:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULLD - _mm256_mask_mullo_epi32

| VPMULLD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the packed 32-bit integers in "a" and "b", producing intermediate 64-bit integers, and store the low
32 bits of the intermediate integers in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        tmp[63:0] := a[i+31:i] * b[i+31:i]
        dst[i+31:i] := tmp[31:0]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULLD - _mm256_maskz_mullo_epi32

| VPMULLD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the packed 32-bit integers in "a" and "b", producing intermediate 64-bit integers, and store the low
32 bits of the intermediate integers in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        tmp[63:0] := a[i+31:i] * b[i+31:i]
        dst[i+31:i] := tmp[31:0]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULLD - _mm512_maskz_mullo_epi32

| VPMULLD_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the packed 32-bit integers in "a" and "b", producing intermediate 64-bit integers, and store the low
32 bits of the intermediate integers in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        tmp[63:0] := a[i+31:i] * b[i+31:i]
        dst[i+31:i] := tmp[31:0]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULLD - _mm_mask_mullo_epi32

| VPMULLD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the packed 32-bit integers in "a" and "b", producing intermediate 64-bit integers, and store the low
32 bits of the intermediate integers in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        tmp[63:0] := a[i+31:i] * b[i+31:i]
        dst[i+31:i] := tmp[31:0]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULLD - _mm_maskz_mullo_epi32

| VPMULLD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the packed 32-bit integers in "a" and "b", producing intermediate 64-bit integers, and store the low
32 bits of the intermediate integers in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        tmp[63:0] := a[i+31:i] * b[i+31:i]
        dst[i+31:i] := tmp[31:0]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULUDQ - _mm256_mask_mul_epu32

| VPMULUDQ_YMMu64_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the low unsigned 32-bit integers from each packed 64-bit element in "a" and "b", and store the
unsigned 64-bit results in "dst" using writemask "k" (elements are copied from "src" when the corresponding
mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+31:i] * b[i+31:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULUDQ - _mm256_maskz_mul_epu32

| VPMULUDQ_YMMu64_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the low unsigned 32-bit integers from each packed 64-bit element in "a" and "b", and store the
unsigned 64-bit results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+31:i] * b[i+31:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULUDQ - _mm_mask_mul_epu32

| VPMULUDQ_XMMu64_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the low unsigned 32-bit integers from each packed 64-bit element in "a" and "b", and store the
unsigned 64-bit results in "dst" using writemask "k" (elements are copied from "src" when the corresponding
mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+31:i] * b[i+31:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULUDQ - _mm_maskz_mul_epu32

| VPMULUDQ_XMMu64_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the low unsigned 32-bit integers from each packed 64-bit element in "a" and "b", and store the
unsigned 64-bit results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+31:i] * b[i+31:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPORD - _mm256_mask_or_epi32

| VPORD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of packed 32-bit integers in "a" and "b", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] OR b[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPORD - _mm256_maskz_or_epi32

| VPORD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of packed 32-bit integers in "a" and "b", and store the results in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] OR b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPORD - _mm_mask_or_epi32

| VPORD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of packed 32-bit integers in "a" and "b", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] OR b[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPORD - _mm_maskz_or_epi32

| VPORD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of packed 32-bit integers in "a" and "b", and store the results in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] OR b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPORQ - _mm256_mask_or_epi64

| VPORQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of packed 64-bit integers in "a" and "b", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] OR b[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPORQ - _mm256_maskz_or_epi64

| VPORQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of packed 64-bit integers in "a" and "b", and store the results in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] OR b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPORQ - _mm_mask_or_epi64

| VPORQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of packed 64-bit integers in "a" and "b", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] OR b[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPORQ - _mm_maskz_or_epi64

| VPORQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of packed 64-bit integers in "a" and "b", and store the results in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] OR b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPROLD - _mm256_mask_rol_epi32

| VPROLD_YMMu32_MASKmskw_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 32-bit integer in "a" to the left by the number of bits specified in "imm8",
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

DEFINE LEFT_ROTATE_DWORDS(src, count_src) {
    count := count_src % 32
    RETURN (src &lt;&lt; count) OR (src &gt;&gt; (32 - count))
}
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPROLD - _mm256_maskz_rol_epi32

| VPROLD_YMMu32_MASKmskw_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 32-bit integer in "a" to the left by the number of bits specified in "imm8",
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

DEFINE LEFT_ROTATE_DWORDS(src, count_src) {
    count := count_src % 32
    RETURN (src &lt;&lt; count) OR (src &gt;&gt; (32 - count))
}
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPROLD - _mm256_rol_epi32

| VPROLD_YMMu32_MASKmskw_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 32-bit integer in "a" to the left by the number of bits specified in "imm8",
and store the results in "dst".

[algorithm]

DEFINE LEFT_ROTATE_DWORDS(src, count_src) {
    count := count_src % 32
    RETURN (src &lt;&lt; count) OR (src &gt;&gt; (32 - count))
}
FOR j := 0 to 7
    i := j*32
    dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPROLD - _mm_mask_rol_epi32

| VPROLD_XMMu32_MASKmskw_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 32-bit integer in "a" to the left by the number of bits specified in "imm8",
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

DEFINE LEFT_ROTATE_DWORDS(src, count_src) {
    count := count_src % 32
    RETURN (src &lt;&lt; count) OR (src &gt;&gt; (32 - count))
}
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPROLD - _mm_maskz_rol_epi32

| VPROLD_XMMu32_MASKmskw_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 32-bit integer in "a" to the left by the number of bits specified in "imm8",
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

DEFINE LEFT_ROTATE_DWORDS(src, count_src) {
    count := count_src % 32
    RETURN (src &lt;&lt; count) OR (src &gt;&gt; (32 - count))
}
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPROLD - _mm_rol_epi32

| VPROLD_XMMu32_MASKmskw_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 32-bit integer in "a" to the left by the number of bits specified in "imm8",
and store the results in "dst".

[algorithm]

DEFINE LEFT_ROTATE_DWORDS(src, count_src) {
    count := count_src % 32
    RETURN (src &lt;&lt; count) OR (src &gt;&gt; (32 - count))
}
FOR j := 0 to 3
    i := j*32
    dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPROLQ - _mm256_mask_rol_epi64

| VPROLQ_YMMu64_MASKmskw_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 64-bit integer in "a" to the left by the number of bits specified in "imm8",
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

DEFINE LEFT_ROTATE_QWORDS(src, count_src) {
    count := count_src % 64
    RETURN (src &lt;&lt; count) OR (src &gt;&gt; (64 - count))
}
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPROLQ - _mm256_maskz_rol_epi64

| VPROLQ_YMMu64_MASKmskw_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 64-bit integer in "a" to the left by the number of bits specified in "imm8",
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

DEFINE LEFT_ROTATE_QWORDS(src, count_src) {
    count := count_src % 64
    RETURN (src &lt;&lt; count) OR (src &gt;&gt; (64 - count))
}
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPROLQ - _mm256_rol_epi64

| VPROLQ_YMMu64_MASKmskw_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 64-bit integer in "a" to the left by the number of bits specified in "imm8",
and store the results in "dst".

[algorithm]

DEFINE LEFT_ROTATE_QWORDS(src, count_src) {
    count := count_src % 64
    RETURN (src &lt;&lt; count) OR (src &gt;&gt; (64 - count))
}
FOR j := 0 to 3
    i := j*64
    dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPROLQ - _mm_mask_rol_epi64

| VPROLQ_XMMu64_MASKmskw_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 64-bit integer in "a" to the left by the number of bits specified in "imm8",
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

DEFINE LEFT_ROTATE_QWORDS(src, count_src) {
    count := count_src % 64
    RETURN (src &lt;&lt; count) OR (src &gt;&gt; (64 - count))
}
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPROLQ - _mm_maskz_rol_epi64

| VPROLQ_XMMu64_MASKmskw_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 64-bit integer in "a" to the left by the number of bits specified in "imm8",
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

DEFINE LEFT_ROTATE_QWORDS(src, count_src) {
    count := count_src % 64
    RETURN (src &lt;&lt; count) OR (src &gt;&gt; (64 - count))
}
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPROLQ - _mm_rol_epi64

| VPROLQ_XMMu64_MASKmskw_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 64-bit integer in "a" to the left by the number of bits specified in "imm8",
and store the results in "dst".

[algorithm]

DEFINE LEFT_ROTATE_QWORDS(src, count_src) {
    count := count_src % 64
    RETURN (src &lt;&lt; count) OR (src &gt;&gt; (64 - count))
}
FOR j := 0 to 1
    i := j*64
    dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPROLVD - _mm256_mask_rolv_epi32

| VPROLVD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 32-bit integer in "a" to the left by the number of bits specified in the
corresponding element of "b", and store the results in "dst" using writemask "k" (elements are copied from
"src" when the corresponding mask bit is not set).

[algorithm]

DEFINE LEFT_ROTATE_DWORDS(src, count_src) {
    count := count_src % 32
    RETURN (src &lt;&lt; count) OR (src &gt;&gt; (32 - count))
}
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPROLVD - _mm256_maskz_rolv_epi32

| VPROLVD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 32-bit integer in "a" to the left by the number of bits specified in the
corresponding element of "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).

[algorithm]

DEFINE LEFT_ROTATE_DWORDS(src, count_src) {
    count := count_src % 32
    RETURN (src &lt;&lt; count) OR (src &gt;&gt; (32 - count))
}
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPROLVD - _mm256_rolv_epi32

| VPROLVD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 32-bit integer in "a" to the left by the number of bits specified in the
corresponding element of "b", and store the results in "dst".

[algorithm]

DEFINE LEFT_ROTATE_DWORDS(src, count_src) {
    count := count_src % 32
    RETURN (src &lt;&lt; count) OR (src &gt;&gt; (32 - count))
}
FOR j := 0 to 7
    i := j*32
    dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPROLVD - _mm_mask_rolv_epi32

| VPROLVD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 32-bit integer in "a" to the left by the number of bits specified in the
corresponding element of "b", and store the results in "dst" using writemask "k" (elements are copied from
"src" when the corresponding mask bit is not set).

[algorithm]

DEFINE LEFT_ROTATE_DWORDS(src, count_src) {
    count := count_src % 32
    RETURN (src &lt;&lt; count) OR (src &gt;&gt; (32 - count))
}
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPROLVD - _mm_maskz_rolv_epi32

| VPROLVD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 32-bit integer in "a" to the left by the number of bits specified in the
corresponding element of "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).

[algorithm]

DEFINE LEFT_ROTATE_DWORDS(src, count_src) {
    count := count_src % 32
    RETURN (src &lt;&lt; count) OR (src &gt;&gt; (32 - count))
}
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPROLVD - _mm_rolv_epi32

| VPROLVD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 32-bit integer in "a" to the left by the number of bits specified in the
corresponding element of "b", and store the results in "dst".

[algorithm]

DEFINE LEFT_ROTATE_DWORDS(src, count_src) {
    count := count_src % 32
    RETURN (src &lt;&lt; count) OR (src &gt;&gt; (32 - count))
}
FOR j := 0 to 3
    i := j*32
    dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPROLVQ - _mm256_mask_rolv_epi64

| VPROLVQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 64-bit integer in "a" to the left by the number of bits specified in the
corresponding element of "b", and store the results in "dst" using writemask "k" (elements are copied from
"src" when the corresponding mask bit is not set).

[algorithm]

DEFINE LEFT_ROTATE_QWORDS(src, count_src) {
    count := count_src % 64
    RETURN (src &lt;&lt; count) OR (src &gt;&gt; (64 - count))
}
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPROLVQ - _mm256_maskz_rolv_epi64

| VPROLVQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 64-bit integer in "a" to the left by the number of bits specified in the
corresponding element of "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).

[algorithm]

DEFINE LEFT_ROTATE_QWORDS(src, count_src) {
    count := count_src % 64
    RETURN (src &lt;&lt; count) OR (src &gt;&gt; (64 - count))
}
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPROLVQ - _mm256_rolv_epi64

| VPROLVQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 64-bit integer in "a" to the left by the number of bits specified in the
corresponding element of "b", and store the results in "dst".

[algorithm]

DEFINE LEFT_ROTATE_QWORDS(src, count_src) {
    count := count_src % 64
    RETURN (src &lt;&lt; count) OR (src &gt;&gt; (64 - count))
}
FOR j := 0 to 3
    i := j*64
    dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPROLVQ - _mm_mask_rolv_epi64

| VPROLVQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 64-bit integer in "a" to the left by the number of bits specified in the
corresponding element of "b", and store the results in "dst" using writemask "k" (elements are copied from
"src" when the corresponding mask bit is not set).

[algorithm]

DEFINE LEFT_ROTATE_QWORDS(src, count_src) {
    count := count_src % 64
    RETURN (src &lt;&lt; count) OR (src &gt;&gt; (64 - count))
}
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPROLVQ - _mm_maskz_rolv_epi64

| VPROLVQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 64-bit integer in "a" to the left by the number of bits specified in the
corresponding element of "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).

[algorithm]

DEFINE LEFT_ROTATE_QWORDS(src, count_src) {
    count := count_src % 64
    RETURN (src &lt;&lt; count) OR (src &gt;&gt; (64 - count))
}
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPROLVQ - _mm_rolv_epi64

| VPROLVQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 64-bit integer in "a" to the left by the number of bits specified in the
corresponding element of "b", and store the results in "dst".

[algorithm]

DEFINE LEFT_ROTATE_QWORDS(src, count_src) {
    count := count_src % 64
    RETURN (src &lt;&lt; count) OR (src &gt;&gt; (64 - count))
}
FOR j := 0 to 1
    i := j*64
    dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPRORD - _mm256_mask_ror_epi32

| VPRORD_YMMu32_MASKmskw_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 32-bit integer in "a" to the right by the number of bits specified in "imm8",
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

DEFINE RIGHT_ROTATE_DWORDS(src, count_src) {
    count := count_src % 32
    RETURN (src &gt;&gt;count) OR (src &lt;&lt; (32 - count))
}
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPRORD - _mm256_maskz_ror_epi32

| VPRORD_YMMu32_MASKmskw_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 32-bit integer in "a" to the right by the number of bits specified in "imm8",
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

DEFINE RIGHT_ROTATE_DWORDS(src, count_src) {
    count := count_src % 32
    RETURN (src &gt;&gt;count) OR (src &lt;&lt; (32 - count))
}
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPRORD - _mm256_ror_epi32

| VPRORD_YMMu32_MASKmskw_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 32-bit integer in "a" to the right by the number of bits specified in "imm8",
and store the results in "dst".

[algorithm]

DEFINE RIGHT_ROTATE_DWORDS(src, count_src) {
    count := count_src % 32
    RETURN (src &gt;&gt;count) OR (src &lt;&lt; (32 - count))
}
FOR j := 0 to 7
    i := j*32
    dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPRORD - _mm_mask_ror_epi32

| VPRORD_XMMu32_MASKmskw_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 32-bit integer in "a" to the right by the number of bits specified in "imm8",
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

DEFINE RIGHT_ROTATE_DWORDS(src, count_src) {
    count := count_src % 32
    RETURN (src &gt;&gt;count) OR (src &lt;&lt; (32 - count))
}
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPRORD - _mm_maskz_ror_epi32

| VPRORD_XMMu32_MASKmskw_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 32-bit integer in "a" to the right by the number of bits specified in "imm8",
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

DEFINE RIGHT_ROTATE_DWORDS(src, count_src) {
    count := count_src % 32
    RETURN (src &gt;&gt;count) OR (src &lt;&lt; (32 - count))
}
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPRORD - _mm_ror_epi32

| VPRORD_XMMu32_MASKmskw_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 32-bit integer in "a" to the right by the number of bits specified in "imm8",
and store the results in "dst".

[algorithm]

DEFINE RIGHT_ROTATE_DWORDS(src, count_src) {
    count := count_src % 32
    RETURN (src &gt;&gt;count) OR (src &lt;&lt; (32 - count))
}
FOR j := 0 to 3
    i := j*32
    dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPRORQ - _mm256_mask_ror_epi64

| VPRORQ_YMMu64_MASKmskw_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 64-bit integer in "a" to the right by the number of bits specified in "imm8",
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

DEFINE RIGHT_ROTATE_QWORDS(src, count_src) {
    count := count_src % 64
    RETURN (src &gt;&gt; count) OR (src &lt;&lt; (64 - count))
}
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPRORQ - _mm256_maskz_ror_epi64

| VPRORQ_YMMu64_MASKmskw_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 64-bit integer in "a" to the right by the number of bits specified in "imm8",
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

DEFINE RIGHT_ROTATE_QWORDS(src, count_src) {
    count := count_src % 64
    RETURN (src &gt;&gt; count) OR (src &lt;&lt; (64 - count))
}
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPRORQ - _mm256_ror_epi64

| VPRORQ_YMMu64_MASKmskw_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 64-bit integer in "a" to the right by the number of bits specified in "imm8",
and store the results in "dst".

[algorithm]

DEFINE RIGHT_ROTATE_QWORDS(src, count_src) {
    count := count_src % 64
    RETURN (src &gt;&gt; count) OR (src &lt;&lt; (64 - count))
}
FOR j := 0 to 3
    i := j*64
    dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPRORQ - _mm_mask_ror_epi64

| VPRORQ_XMMu64_MASKmskw_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 64-bit integer in "a" to the right by the number of bits specified in "imm8",
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

DEFINE RIGHT_ROTATE_QWORDS(src, count_src) {
    count := count_src % 64
    RETURN (src &gt;&gt; count) OR (src &lt;&lt; (64 - count))
}
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPRORQ - _mm_maskz_ror_epi64

| VPRORQ_XMMu64_MASKmskw_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 64-bit integer in "a" to the right by the number of bits specified in "imm8",
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

DEFINE RIGHT_ROTATE_QWORDS(src, count_src) {
    count := count_src % 64
    RETURN (src &gt;&gt; count) OR (src &lt;&lt; (64 - count))
}
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPRORQ - _mm_ror_epi64

| VPRORQ_XMMu64_MASKmskw_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 64-bit integer in "a" to the right by the number of bits specified in "imm8",
and store the results in "dst".

[algorithm]

DEFINE RIGHT_ROTATE_QWORDS(src, count_src) {
    count := count_src % 64
    RETURN (src &gt;&gt; count) OR (src &lt;&lt; (64 - count))
}
FOR j := 0 to 1
    i := j*64
    dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPRORVD - _mm256_mask_rorv_epi32

| VPRORVD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 32-bit integer in "a" to the right by the number of bits specified in the
corresponding element of "b", and store the results in "dst" using writemask "k" (elements are copied from
"src" when the corresponding mask bit is not set).

[algorithm]

DEFINE RIGHT_ROTATE_DWORDS(src, count_src) {
    count := count_src % 32
    RETURN (src &gt;&gt;count) OR (src &lt;&lt; (32 - count))
}
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPRORVD - _mm256_maskz_rorv_epi32

| VPRORVD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 32-bit integer in "a" to the right by the number of bits specified in the
corresponding element of "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).

[algorithm]

DEFINE RIGHT_ROTATE_DWORDS(src, count_src) {
    count := count_src % 32
    RETURN (src &gt;&gt;count) OR (src &lt;&lt; (32 - count))
}
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPRORVD - _mm256_rorv_epi32

| VPRORVD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 32-bit integer in "a" to the right by the number of bits specified in the
corresponding element of "b", and store the results in "dst".

[algorithm]

DEFINE RIGHT_ROTATE_DWORDS(src, count_src) {
    count := count_src % 32
    RETURN (src &gt;&gt;count) OR (src &lt;&lt; (32 - count))
}
FOR j := 0 to 7
    i := j*32
    dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPRORVD - _mm_mask_rorv_epi32

| VPRORVD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 32-bit integer in "a" to the right by the number of bits specified in the
corresponding element of "b", and store the results in "dst" using writemask "k" (elements are copied from
"src" when the corresponding mask bit is not set).

[algorithm]

DEFINE RIGHT_ROTATE_DWORDS(src, count_src) {
    count := count_src % 32
    RETURN (src &gt;&gt;count) OR (src &lt;&lt; (32 - count))
}
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPRORVD - _mm_maskz_rorv_epi32

| VPRORVD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 32-bit integer in "a" to the right by the number of bits specified in the
corresponding element of "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).

[algorithm]

DEFINE RIGHT_ROTATE_DWORDS(src, count_src) {
    count := count_src % 32
    RETURN (src &gt;&gt;count) OR (src &lt;&lt; (32 - count))
}
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPRORVD - _mm_rorv_epi32

| VPRORVD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 32-bit integer in "a" to the right by the number of bits specified in the
corresponding element of "b", and store the results in "dst".

[algorithm]

DEFINE RIGHT_ROTATE_DWORDS(src, count_src) {
    count := count_src % 32
    RETURN (src &gt;&gt;count) OR (src &lt;&lt; (32 - count))
}
FOR j := 0 to 3
    i := j*32
    dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPRORVQ - _mm256_mask_rorv_epi64

| VPRORVQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 64-bit integer in "a" to the right by the number of bits specified in the
corresponding element of "b", and store the results in "dst" using writemask "k" (elements are copied from
"src" when the corresponding mask bit is not set).

[algorithm]

DEFINE RIGHT_ROTATE_QWORDS(src, count_src) {
    count := count_src % 64
    RETURN (src &gt;&gt; count) OR (src &lt;&lt; (64 - count))
}
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPRORVQ - _mm256_maskz_rorv_epi64

| VPRORVQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 64-bit integer in "a" to the right by the number of bits specified in the
corresponding element of "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).

[algorithm]

DEFINE RIGHT_ROTATE_QWORDS(src, count_src) {
    count := count_src % 64
    RETURN (src &gt;&gt; count) OR (src &lt;&lt; (64 - count))
}
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPRORVQ - _mm256_rorv_epi64

| VPRORVQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 64-bit integer in "a" to the right by the number of bits specified in the
corresponding element of "b", and store the results in "dst".

[algorithm]

DEFINE RIGHT_ROTATE_QWORDS(src, count_src) {
    count := count_src % 64
    RETURN (src &gt;&gt; count) OR (src &lt;&lt; (64 - count))
}
FOR j := 0 to 3
    i := j*64
    dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPRORVQ - _mm_mask_rorv_epi64

| VPRORVQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 64-bit integer in "a" to the right by the number of bits specified in the
corresponding element of "b", and store the results in "dst" using writemask "k" (elements are copied from
"src" when the corresponding mask bit is not set).

[algorithm]

DEFINE RIGHT_ROTATE_QWORDS(src, count_src) {
    count := count_src % 64
    RETURN (src &gt;&gt; count) OR (src &lt;&lt; (64 - count))
}
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPRORVQ - _mm_maskz_rorv_epi64

| VPRORVQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 64-bit integer in "a" to the right by the number of bits specified in the
corresponding element of "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).

[algorithm]

DEFINE RIGHT_ROTATE_QWORDS(src, count_src) {
    count := count_src % 64
    RETURN (src &gt;&gt; count) OR (src &lt;&lt; (64 - count))
}
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPRORVQ - _mm_rorv_epi64

| VPRORVQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 64-bit integer in "a" to the right by the number of bits specified in the
corresponding element of "b", and store the results in "dst".

[algorithm]

DEFINE RIGHT_ROTATE_QWORDS(src, count_src) {
    count := count_src % 64
    RETURN (src &gt;&gt; count) OR (src &lt;&lt; (64 - count))
}
FOR j := 0 to 1
    i := j*64
    dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSCATTERDD - _mm256_i32scatter_epi32

| VPSCATTERDD_MEMu32_MASKmskw_YMMu32_AVX512_VL256

--------------------------------------------------------------------------------------------------------------
Scatter 32-bit integers from "a" into memory using 32-bit indices. 32-bit elements are stored at addresses
starting at "base_addr" and offset by each 32-bit element in "vindex" (each index is scaled by the factor in
"scale"). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 7
    i := j*32
    m := j*32
    addr := base_addr + SignExtend64(vindex[m+31:m]) * ZeroExtend64(scale) * 8
    MEM[addr+31:addr] := a[i+31:i]
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPSCATTERDD - _mm256_mask_i32scatter_epi32

| VPSCATTERDD_MEMu32_MASKmskw_YMMu32_AVX512_VL256

--------------------------------------------------------------------------------------------------------------
Scatter 32-bit integers from "a" into memory using 32-bit indices. 32-bit elements are stored at addresses
starting at "base_addr" and offset by each 32-bit element in "vindex" (each index is scaled by the factor in
"scale") subject to mask "k" (elements are not stored when the corresponding mask bit is not set). "scale"
should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 7
    i := j*32
    m := j*32
    IF k[j]
        addr := base_addr + SignExtend64(vindex[m+31:m]) * ZeroExtend64(scale) * 8
        MEM[addr+31:addr] := a[i+31:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPSCATTERDD - _mm_i32scatter_epi32

| VPSCATTERDD_MEMu32_MASKmskw_XMMu32_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Scatter 32-bit integers from "a" into memory using 32-bit indices. 32-bit elements are stored at addresses
starting at "base_addr" and offset by each 32-bit element in "vindex" (each index is scaled by the factor in
"scale"). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 3
    i := j*32
    m := j*32
    addr := base_addr + SignExtend64(vindex[m+31:m]) * ZeroExtend64(scale) * 8
    MEM[addr+31:addr] := a[i+31:i]
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPSCATTERDD - _mm_mask_i32scatter_epi32

| VPSCATTERDD_MEMu32_MASKmskw_XMMu32_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Scatter 32-bit integers from "a" into memory using 32-bit indices. 32-bit elements are stored at addresses
starting at "base_addr" and offset by each 32-bit element in "vindex" (each index is scaled by the factor in
"scale") subject to mask "k" (elements are not stored when the corresponding mask bit is not set). "scale"
should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 3
    i := j*32
    m := j*32
    IF k[j]
        addr := base_addr + SignExtend64(vindex[m+31:m]) * ZeroExtend64(scale) * 8
        MEM[addr+31:addr] := a[i+31:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPSCATTERDQ - _mm256_i32scatter_epi64

| VPSCATTERDQ_MEMu64_MASKmskw_YMMu64_AVX512_VL256

--------------------------------------------------------------------------------------------------------------
Scatter 64-bit integers from "a" into memory using 32-bit indices. 64-bit elements are stored at addresses
starting at "base_addr" and offset by each 32-bit element in "vindex" (each index is scaled by the factor in
"scale"). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 3
    i := j*64
    m := j*32
    addr := base_addr + SignExtend64(vindex[m+31:m]) * ZeroExtend64(scale) * 8
    MEM[addr+63:addr] := a[i+63:i]
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPSCATTERDQ - _mm256_mask_i32scatter_epi64

| VPSCATTERDQ_MEMu64_MASKmskw_YMMu64_AVX512_VL256

--------------------------------------------------------------------------------------------------------------
Scatter 64-bit integers from "a" into memory using 32-bit indices. 64-bit elements are stored at addresses
starting at "base_addr" and offset by each 32-bit element in "vindex" (each index is scaled by the factor in
"scale") subject to mask "k" (elements are not stored when the corresponding mask bit is not set). "scale"
should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 3
    i := j*64
    m := j*32
    IF k[j]
        addr := base_addr + SignExtend64(vindex[m+31:m]) * ZeroExtend64(scale) * 8
        MEM[addr+63:addr] := a[i+63:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPSCATTERDQ - _mm_i32scatter_epi64

| VPSCATTERDQ_MEMu64_MASKmskw_XMMu64_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Scatter 64-bit integers from "a" into memory using 32-bit indices. 64-bit elements are stored at addresses
starting at "base_addr" and offset by each 32-bit element in "vindex" (each index is scaled by the factor in
"scale"). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 1
    i := j*64
    m := j*32
    addr := base_addr + SignExtend64(vindex[m+31:m]) * ZeroExtend64(scale) * 8
    MEM[addr+63:addr] := a[i+63:i]
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPSCATTERDQ - _mm_mask_i32scatter_epi64

| VPSCATTERDQ_MEMu64_MASKmskw_XMMu64_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Scatter 64-bit integers from "a" into memory using 32-bit indices. 64-bit elements are stored at addresses
starting at "base_addr" and offset by each 32-bit element in "vindex" (each index is scaled by the factor in
"scale") subject to mask "k" (elements are not stored when the corresponding mask bit is not set). "scale"
should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 1
    i := j*64
    m := j*32
    IF k[j]
        addr := base_addr + SignExtend64(vindex[m+31:m]) * ZeroExtend64(scale) * 8
        MEM[addr+63:addr] := a[i+63:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPSCATTERQD - _mm256_i64scatter_epi32

| VPSCATTERQD_MEMu32_MASKmskw_XMMu32_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Scatter 32-bit integers from "a" into memory using 64-bit indices. 32-bit elements are stored at addresses
starting at "base_addr" and offset by each 64-bit element in "vindex" (each index is scaled by the factor in
"scale"). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 3
    i := j*32
    m := j*64
    addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
    MEM[addr+31:addr] := a[i+31:i]
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPSCATTERQD - _mm256_mask_i64scatter_epi32

| VPSCATTERQD_MEMu32_MASKmskw_XMMu32_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Scatter 32-bit integers from "a" into memory using 64-bit indices. 32-bit elements are stored at addresses
starting at "base_addr" and offset by each 64-bit element in "vindex" (each index is scaled by the factor in
"scale") subject to mask "k" (elements are not stored when the corresponding mask bit is not set). "scale"
should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 3
    i := j*32
    m := j*64
    IF k[j]
        addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
        MEM[addr+31:addr] := a[i+31:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPSCATTERQD - _mm_i64scatter_epi32

| VPSCATTERQD_MEMu32_MASKmskw_XMMu32_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Scatter 32-bit integers from "a" into memory using 64-bit indices. 32-bit elements are stored at addresses
starting at "base_addr" and offset by each 64-bit element in "vindex" (each index is scaled by the factor in
"scale"). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 1
    i := j*32
    m := j*64
    addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
    MEM[addr+31:addr] := a[i+31:i]
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPSCATTERQD - _mm_mask_i64scatter_epi32

| VPSCATTERQD_MEMu32_MASKmskw_XMMu32_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Scatter 32-bit integers from "a" into memory using 64-bit indices. 32-bit elements are stored at addresses
starting at "base_addr" and offset by each 64-bit element in "vindex" (each index is scaled by the factor in
"scale") subject to mask "k" (elements are not stored when the corresponding mask bit is not set). "scale"
should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 1
    i := j*32
    m := j*64
    IF k[j]
        addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
        MEM[addr+31:addr] := a[i+31:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPSCATTERQQ - _mm256_i64scatter_epi64

| VPSCATTERQQ_MEMu64_MASKmskw_YMMu64_AVX512_VL256

--------------------------------------------------------------------------------------------------------------
Scatter 64-bit integers from "a" into memory using 64-bit indices. 64-bit elements are stored at addresses
starting at "base_addr" and offset by each 64-bit element in "vindex" (each index is scaled by the factor in
"scale"). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 3
    i := j*64
    m := j*64
    addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
    MEM[addr+63:addr] := a[i+63:i]
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPSCATTERQQ - _mm256_mask_i64scatter_epi64

| VPSCATTERQQ_MEMu64_MASKmskw_YMMu64_AVX512_VL256

--------------------------------------------------------------------------------------------------------------
Scatter 64-bit integers from "a" into memory using 64-bit indices. 64-bit elements are stored at addresses
starting at "base_addr" and offset by each 64-bit element in "vindex" (each index is scaled by the factor in
"scale") subject to mask "k" (elements are not stored when the corresponding mask bit is not set). "scale"
should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 3
    i := j*64
    m := j*64
    IF k[j]
        addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
        MEM[addr+63:addr] := a[i+63:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPSCATTERQQ - _mm_i64scatter_epi64

| VPSCATTERQQ_MEMu64_MASKmskw_XMMu64_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Scatter 64-bit integers from "a" into memory using 64-bit indices. 64-bit elements are stored at addresses
starting at "base_addr" and offset by each 64-bit element in "vindex" (each index is scaled by the factor in
"scale"). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 1
    i := j*64
    m := j*64
    addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
    MEM[addr+63:addr] := a[i+63:i]
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPSCATTERQQ - _mm_mask_i64scatter_epi64

| VPSCATTERQQ_MEMu64_MASKmskw_XMMu64_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Scatter 64-bit integers from "a" into memory using 64-bit indices. 64-bit elements are stored at addresses
starting at "base_addr" and offset by each 64-bit element in "vindex" (each index is scaled by the factor in
"scale") subject to mask "k" (elements are not stored when the corresponding mask bit is not set). "scale"
should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 1
    i := j*64
    m := j*64
    IF k[j]
        addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
        MEM[addr+63:addr] := a[i+63:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPSHUFD - _mm256_mask_shuffle_epi32

| VPSHUFD_YMMu32_MASKmskw_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 32-bit integers in "a" within 128-bit lanes using the control in "imm8", and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[31:0] := src[31:0]
    1:    tmp[31:0] := src[63:32]
    2:    tmp[31:0] := src[95:64]
    3:    tmp[31:0] := src[127:96]
    ESAC
    RETURN tmp[31:0]
}
tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHUFD - _mm256_maskz_shuffle_epi32

| VPSHUFD_YMMu32_MASKmskw_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 32-bit integers in "a" within 128-bit lanes using the control in "imm8", and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[31:0] := src[31:0]
    1:    tmp[31:0] := src[63:32]
    2:    tmp[31:0] := src[95:64]
    3:    tmp[31:0] := src[127:96]
    ESAC
    RETURN tmp[31:0]
}
tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHUFD - _mm_mask_shuffle_epi32

| VPSHUFD_XMMu32_MASKmskw_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 32-bit integers in "a" using the control in "imm8", and store the results in "dst" using writemask "k"
(elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[31:0] := src[31:0]
    1:    tmp[31:0] := src[63:32]
    2:    tmp[31:0] := src[95:64]
    3:    tmp[31:0] := src[127:96]
    ESAC
    RETURN tmp[31:0]
}
tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHUFD - _mm_maskz_shuffle_epi32

| VPSHUFD_XMMu32_MASKmskw_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 32-bit integers in "a" using the control in "imm8", and store the results in "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[31:0] := src[31:0]
    1:    tmp[31:0] := src[63:32]
    2:    tmp[31:0] := src[95:64]
    3:    tmp[31:0] := src[127:96]
    ESAC
    RETURN tmp[31:0]
}
tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLD - _mm256_mask_sll_epi32

| VPSLLD_YMMu32_MASKmskw_YMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        IF count[63:0] &gt; 31
            dst[i+31:i] := 0
        ELSE
            dst[i+31:i] := ZeroExtend32(a[i+31:i] &lt;&lt; count[63:0])
        FI
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLD - _mm256_mask_slli_epi32

| VPSLLD_YMMu32_MASKmskw_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" left by "imm8" while shifting in zeros, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        IF imm8[7:0] &gt; 31
            dst[i+31:i] := 0
        ELSE
            dst[i+31:i] := ZeroExtend32(a[i+31:i] &lt;&lt; imm8[7:0])
        FI
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLD - _mm256_maskz_sll_epi32

| VPSLLD_YMMu32_MASKmskw_YMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        IF count[63:0] &gt; 31
            dst[i+31:i] := 0
        ELSE
            dst[i+31:i] := ZeroExtend32(a[i+31:i] &lt;&lt; count[63:0])
        FI
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLD - _mm256_maskz_slli_epi32

| VPSLLD_YMMu32_MASKmskw_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" left by "imm8" while shifting in zeros, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        IF imm8[7:0] &gt; 31
            dst[i+31:i] := 0
        ELSE
            dst[i+31:i] := ZeroExtend32(a[i+31:i] &lt;&lt; imm8[7:0])
        FI
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLD - _mm_mask_sll_epi32

| VPSLLD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        IF count[63:0] &gt; 31
            dst[i+31:i] := 0
        ELSE
            dst[i+31:i] := ZeroExtend32(a[i+31:i] &lt;&lt; count[63:0])
        FI
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLD - _mm_mask_slli_epi32

| VPSLLD_XMMu32_MASKmskw_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" left by "imm8" while shifting in zeros, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        IF imm8[7:0] &gt; 31
            dst[i+31:i] := 0
        ELSE
            dst[i+31:i] := ZeroExtend32(a[i+31:i] &lt;&lt; imm8[7:0])
        FI
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLD - _mm_maskz_sll_epi32

| VPSLLD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        IF count[63:0] &gt; 31
            dst[i+31:i] := 0
        ELSE
            dst[i+31:i] := ZeroExtend32(a[i+31:i] &lt;&lt; count[63:0])
        FI
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLD - _mm_maskz_slli_epi32

| VPSLLD_XMMu32_MASKmskw_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" left by "imm8" while shifting in zeros, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        IF imm8[7:0] &gt; 31
            dst[i+31:i] := 0
        ELSE
            dst[i+31:i] := ZeroExtend32(a[i+31:i] &lt;&lt; imm8[7:0])
        FI
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLQ - _mm256_mask_sll_epi64

| VPSLLQ_YMMu64_MASKmskw_YMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        IF count[63:0] &gt; 63
            dst[i+63:i] := 0
        ELSE
            dst[i+63:i] := ZeroExtend64(a[i+63:i] &lt;&lt; count[63:0])
        FI
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLQ - _mm256_mask_slli_epi64

| VPSLLQ_YMMu64_MASKmskw_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" left by "imm8" while shifting in zeros, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        IF imm8[7:0] &gt; 63
            dst[i+63:i] := 0
        ELSE
            dst[i+63:i] := ZeroExtend64(a[i+63:i] &lt;&lt; imm8[7:0])
        FI
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLQ - _mm256_maskz_sll_epi64

| VPSLLQ_YMMu64_MASKmskw_YMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        IF count[63:0] &gt; 63
            dst[i+63:i] := 0
        ELSE
            dst[i+63:i] := ZeroExtend64(a[i+63:i] &lt;&lt; count[63:0])
        FI
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLQ - _mm256_maskz_slli_epi64

| VPSLLQ_YMMu64_MASKmskw_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" left by "imm8" while shifting in zeros, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        IF imm8[7:0] &gt; 63
            dst[i+63:i] := 0
        ELSE
            dst[i+63:i] := ZeroExtend64(a[i+63:i] &lt;&lt; imm8[7:0])
        FI
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLQ - _mm_mask_sll_epi64

| VPSLLQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        IF count[63:0] &gt; 63
            dst[i+63:i] := 0
        ELSE
            dst[i+63:i] := ZeroExtend64(a[i+63:i] &lt;&lt; count[63:0])
        FI
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLQ - _mm_mask_slli_epi64

| VPSLLQ_XMMu64_MASKmskw_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" left by "imm8" while shifting in zeros, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        IF imm8[7:0] &gt; 63
            dst[i+63:i] := 0
        ELSE
            dst[i+63:i] := ZeroExtend64(a[i+63:i] &lt;&lt; imm8[7:0])
        FI
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLQ - _mm_maskz_sll_epi64

| VPSLLQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        IF count[63:0] &gt; 63
            dst[i+63:i] := 0
        ELSE
            dst[i+63:i] := ZeroExtend64(a[i+63:i] &lt;&lt; count[63:0])
        FI
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLQ - _mm_maskz_slli_epi64

| VPSLLQ_XMMu64_MASKmskw_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" left by "imm8" while shifting in zeros, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        IF imm8[7:0] &gt; 63
            dst[i+63:i] := 0
        ELSE
            dst[i+63:i] := ZeroExtend64(a[i+63:i] &lt;&lt; imm8[7:0])
        FI
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLVD - _mm256_mask_sllv_epi32

| VPSLLVD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" left by the amount specified by the corresponding element in "count" while
shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        IF count[i+31:i] &lt; 32
            dst[i+31:i] := ZeroExtend32(a[i+31:i] &lt;&lt; count[i+31:i])
        ELSE
            dst[i+31:i] := 0
        FI
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLVD - _mm256_maskz_sllv_epi32

| VPSLLVD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" left by the amount specified by the corresponding element in "count" while
shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        IF count[i+31:i] &lt; 32
            dst[i+31:i] := ZeroExtend32(a[i+31:i] &lt;&lt; count[i+31:i])
        ELSE
            dst[i+31:i] := 0
        FI
    ELSE
        dst[i+31:i] := 0
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLVD - _mm_mask_sllv_epi32

| VPSLLVD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" left by the amount specified by the corresponding element in "count" while
shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        IF count[i+31:i] &lt; 32
            dst[i+31:i] := ZeroExtend32(a[i+31:i] &lt;&lt; count[i+31:i])
        ELSE
            dst[i+31:i] := 0
        FI
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLVD - _mm_maskz_sllv_epi32

| VPSLLVD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" left by the amount specified by the corresponding element in "count" while
shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        IF count[i+31:i] &lt; 32
            dst[i+31:i] := ZeroExtend32(a[i+31:i] &lt;&lt; count[i+31:i])
        ELSE
            dst[i+31:i] := 0
        FI
    ELSE
        dst[i+31:i] := 0
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLVQ - _mm256_mask_sllv_epi64

| VPSLLVQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" left by the amount specified by the corresponding element in "count" while
shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        IF count[i+63:i] &lt; 64
            dst[i+63:i] := ZeroExtend64(a[i+63:i] &lt;&lt; count[i+63:i])
        ELSE
            dst[i+63:i] := 0
        FI
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLVQ - _mm256_maskz_sllv_epi64

| VPSLLVQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" left by the amount specified by the corresponding element in "count" while
shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        IF count[i+63:i] &lt; 64
            dst[i+63:i] := ZeroExtend64(a[i+63:i] &lt;&lt; count[i+63:i])
        ELSE
            dst[i+63:i] := 0
        FI
    ELSE
        dst[i+63:i] := 0
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLVQ - _mm_mask_sllv_epi64

| VPSLLVQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" left by the amount specified by the corresponding element in "count" while
shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        IF count[i+63:i] &lt; 64
            dst[i+63:i] := ZeroExtend64(a[i+63:i] &lt;&lt; count[i+63:i])
        ELSE
            dst[i+63:i] := 0
        FI
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLVQ - _mm_maskz_sllv_epi64

| VPSLLVQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" left by the amount specified by the corresponding element in "count" while
shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        IF count[i+63:i] &lt; 64
            dst[i+63:i] := ZeroExtend64(a[i+63:i] &lt;&lt; count[i+63:i])
        ELSE
            dst[i+63:i] := 0
        FI
    ELSE
        dst[i+63:i] := 0
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAD - _mm256_mask_sra_epi32

| VPSRAD_YMMu32_MASKmskw_YMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" right by "count" while shifting in sign bits, and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        IF count[63:0] &gt; 31
            dst[i+31:i] := (a[i+31] ? 0xFFFFFFFF : 0x0)
        ELSE
            dst[i+31:i] := SignExtend32(a[i+31:i] &gt;&gt; count[63:0])
        FI
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAD - _mm256_mask_srai_epi32

| VPSRAD_YMMu32_MASKmskw_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" right by "imm8" while shifting in sign bits, and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        IF imm8[7:0] &gt; 31
            dst[i+31:i] := (a[i+31] ? 0xFFFFFFFF : 0x0)
        ELSE
            dst[i+31:i] := SignExtend32(a[i+31:i] &gt;&gt; imm8[7:0])
        FI
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAD - _mm256_maskz_sra_epi32

| VPSRAD_YMMu32_MASKmskw_YMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" right by "count" while shifting in sign bits, and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        IF count[63:0] &gt; 31
            dst[i+31:i] := (a[i+31] ? 0xFFFFFFFF : 0x0)
        ELSE
            dst[i+31:i] := SignExtend32(a[i+31:i] &gt;&gt; count[63:0])
        FI
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAD - _mm256_maskz_srai_epi32

| VPSRAD_YMMu32_MASKmskw_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" right by "imm8" while shifting in sign bits, and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        IF imm8[7:0] &gt; 31
            dst[i+31:i] := (a[i+31] ? 0xFFFFFFFF : 0x0)
        ELSE
            dst[i+31:i] := SignExtend32(a[i+31:i] &gt;&gt; imm8[7:0])
        FI
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAD - _mm_mask_sra_epi32

| VPSRAD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" right by "count" while shifting in sign bits, and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        IF count[63:0] &gt; 31
            dst[i+31:i] := (a[i+31] ? 0xFFFFFFFF : 0x0)
        ELSE
            dst[i+31:i] := SignExtend32(a[i+31:i] &gt;&gt; count[63:0])
        FI
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAD - _mm_mask_srai_epi32

| VPSRAD_XMMu32_MASKmskw_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" right by "imm8" while shifting in sign bits, and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        IF imm8[7:0] &gt; 31
            dst[i+31:i] := (a[i+31] ? 0xFFFFFFFF : 0x0)
        ELSE
            dst[i+31:i] := SignExtend32(a[i+31:i] &gt;&gt; imm8[7:0])
        FI
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAD - _mm_maskz_sra_epi32

| VPSRAD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" right by "count" while shifting in sign bits, and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        IF count[63:0] &gt; 31
            dst[i+31:i] := (a[i+31] ? 0xFFFFFFFF : 0x0)
        ELSE
            dst[i+31:i] := SignExtend32(a[i+31:i] &gt;&gt; count[63:0])
        FI
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAD - _mm_maskz_srai_epi32

| VPSRAD_XMMu32_MASKmskw_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" right by "imm8" while shifting in sign bits, and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        IF imm8[7:0] &gt; 31
            dst[i+31:i] := (a[i+31] ? 0xFFFFFFFF : 0x0)
        ELSE
            dst[i+31:i] := SignExtend32(a[i+31:i] &gt;&gt; imm8[7:0])
        FI
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAQ - _mm256_mask_sra_epi64

| VPSRAQ_YMMu64_MASKmskw_YMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by "count" while shifting in sign bits, and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        IF count[63:0] &gt; 63
            dst[i+63:i] := (a[i+63] ? 0xFFFFFFFFFFFFFFFF : 0x0)
        ELSE
            dst[i+63:i] := SignExtend64(a[i+63:i] &gt;&gt; count[63:0])
        FI
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAQ - _mm256_mask_srai_epi64

| VPSRAQ_YMMu64_MASKmskw_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by "imm8" while shifting in sign bits, and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        IF imm8[7:0] &gt; 63
            dst[i+63:i] := (a[i+63] ? 0xFFFFFFFFFFFFFFFF : 0x0)
        ELSE
            dst[i+63:i] := SignExtend64(a[i+63:i] &gt;&gt; imm8[7:0])
        FI
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAQ - _mm256_maskz_sra_epi64

| VPSRAQ_YMMu64_MASKmskw_YMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by "count" while shifting in sign bits, and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        IF count[63:0] &gt; 63
            dst[i+63:i] := (a[i+63] ? 0xFFFFFFFFFFFFFFFF : 0x0)
        ELSE
            dst[i+63:i] := SignExtend64(a[i+63:i] &gt;&gt; count[63:0])
        FI
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAQ - _mm256_maskz_srai_epi64

| VPSRAQ_YMMu64_MASKmskw_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by "imm8" while shifting in sign bits, and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        IF imm8[7:0] &gt; 63
            dst[i+63:i] := (a[i+63] ? 0xFFFFFFFFFFFFFFFF : 0x0)
        ELSE
            dst[i+63:i] := SignExtend64(a[i+63:i] &gt;&gt; imm8[7:0])
        FI
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAQ - _mm256_sra_epi64

| VPSRAQ_YMMu64_MASKmskw_YMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by "count" while shifting in sign bits, and store the results in
"dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF count[63:0] &gt; 63
        dst[i+63:i] := (a[i+63] ? 0xFFFFFFFFFFFFFFFF : 0x0)
    ELSE
        dst[i+63:i] := SignExtend64(a[i+63:i] &gt;&gt; count[63:0])
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAQ - _mm256_srai_epi64

| VPSRAQ_YMMu64_MASKmskw_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by "imm8" while shifting in sign bits, and store the results in
"dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF imm8[7:0] &gt; 63
        dst[i+63:i] := (a[i+63] ? 0xFFFFFFFFFFFFFFFF : 0x0)
    ELSE
        dst[i+63:i] := SignExtend64(a[i+63:i] &gt;&gt; imm8[7:0])
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAQ - _mm_mask_sra_epi64

| VPSRAQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by "count" while shifting in sign bits, and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        IF count[63:0] &gt; 63
            dst[i+63:i] := (a[i+63] ? 0xFFFFFFFFFFFFFFFF : 0x0)
        ELSE
            dst[i+63:i] := SignExtend64(a[i+63:i] &gt;&gt; count[63:0])
        FI
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAQ - _mm_mask_srai_epi64

| VPSRAQ_XMMu64_MASKmskw_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by "imm8" while shifting in sign bits, and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        IF imm8[7:0] &gt; 63
            dst[i+63:i] := (a[i+63] ? 0xFFFFFFFFFFFFFFFF : 0x0)
        ELSE
            dst[i+63:i] := SignExtend64(a[i+63:i] &gt;&gt; imm8[7:0])
        FI
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAQ - _mm_maskz_sra_epi64

| VPSRAQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by "count" while shifting in sign bits, and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        IF count[63:0] &gt; 63
            dst[i+63:i] := (a[i+63] ? 0xFFFFFFFFFFFFFFFF : 0x0)
        ELSE
            dst[i+63:i] := SignExtend64(a[i+63:i] &gt;&gt; count[63:0])
        FI
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAQ - _mm_maskz_srai_epi64

| VPSRAQ_XMMu64_MASKmskw_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by "imm8" while shifting in sign bits, and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        IF imm8[7:0] &gt; 63
            dst[i+63:i] := (a[i+63] ? 0xFFFFFFFFFFFFFFFF : 0x0)
        ELSE
            dst[i+63:i] := SignExtend64(a[i+63:i] &gt;&gt; imm8[7:0])
        FI
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAQ - _mm_sra_epi64

| VPSRAQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by "count" while shifting in sign bits, and store the results in
"dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF count[63:0] &gt; 63
        dst[i+63:i] := (a[i+63] ? 0xFFFFFFFFFFFFFFFF : 0x0)
    ELSE
        dst[i+63:i] := SignExtend64(a[i+63:i] &gt;&gt; count[63:0])
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAQ - _mm_srai_epi64

| VPSRAQ_XMMu64_MASKmskw_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by "imm8" while shifting in sign bits, and store the results in
"dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF imm8[7:0] &gt; 63
        dst[i+63:i] := (a[i+63] ? 0xFFFFFFFFFFFFFFFF : 0x0)
    ELSE
        dst[i+63:i] := SignExtend64(a[i+63:i] &gt;&gt; imm8[7:0])
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAVD - _mm256_mask_srav_epi32

| VPSRAVD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in sign bits, and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        IF count[i+31:i] &lt; 32
            dst[i+31:i] := SignExtend32(a[i+31:i] &gt;&gt; count[i+31:i])
        ELSE
            dst[i+31:i] := (a[i+31] ? 0xFFFFFFFF : 0)
        FI
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAVD - _mm256_maskz_srav_epi32

| VPSRAVD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in sign bits, and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        IF count[i+31:i] &lt; 32
            dst[i+31:i] := SignExtend32(a[i+31:i] &gt;&gt; count[i+31:i])
        ELSE
            dst[i+31:i] := (a[i+31] ? 0xFFFFFFFF : 0)
        FI
    ELSE
        dst[i+31:i] := 0
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAVD - _mm_mask_srav_epi32

| VPSRAVD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in sign bits, and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        IF count[i+31:i] &lt; 32
            dst[i+31:i] := SignExtend32(a[i+31:i] &gt;&gt; count[i+31:i])
        ELSE
            dst[i+31:i] := (a[i+31] ? 0xFFFFFFFF : 0)
        FI
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAVD - _mm_maskz_srav_epi32

| VPSRAVD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in sign bits, and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        IF count[i+31:i] &lt; 32
            dst[i+31:i] := SignExtend32(a[i+31:i] &gt;&gt; count[i+31:i])
        ELSE
            dst[i+31:i] := (a[i+31] ? 0xFFFFFFFF : 0)
        FI
    ELSE
        dst[i+31:i] := 0
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAVQ - _mm256_mask_srav_epi64

| VPSRAVQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in sign bits, and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        IF count[i+63:i] &lt; 64
            dst[i+63:i] := SignExtend64(a[i+63:i] &gt;&gt; count[i+63:i])
        ELSE
            dst[i+63:i] := (a[i+63] ? 0xFFFFFFFFFFFFFFFF : 0)
        FI
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAVQ - _mm256_maskz_srav_epi64

| VPSRAVQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in sign bits, and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        IF count[i+63:i] &lt; 64
            dst[i+63:i] := SignExtend64(a[i+63:i] &gt;&gt; count[i+63:i])
        ELSE
            dst[i+63:i] := (a[i+63] ? 0xFFFFFFFFFFFFFFFF : 0)
        FI
    ELSE
        dst[i+63:i] := 0
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAVQ - _mm256_srav_epi64

| VPSRAVQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in sign bits, and store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF count[i+63:i] &lt; 64
        dst[i+63:i] := SignExtend64(a[i+63:i] &gt;&gt; count[i+63:i])
    ELSE
        dst[i+63:i] := (a[i+63] ? 0xFFFFFFFFFFFFFFFF : 0)
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAVQ - _mm_mask_srav_epi64

| VPSRAVQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in sign bits, and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        IF count[i+63:i] &lt; 64
            dst[i+63:i] := SignExtend64(a[i+63:i] &gt;&gt; count[i+63:i])
        ELSE
            dst[i+63:i] := (a[i+63] ? 0xFFFFFFFFFFFFFFFF : 0)
        FI
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAVQ - _mm_maskz_srav_epi64

| VPSRAVQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in sign bits, and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        IF count[i+63:i] &lt; 64
            dst[i+63:i] := SignExtend64(a[i+63:i] &gt;&gt; count[i+63:i])
        ELSE
            dst[i+63:i] := (a[i+63] ? 0xFFFFFFFFFFFFFFFF : 0)
        FI
    ELSE
        dst[i+63:i] := 0
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAVQ - _mm_srav_epi64

| VPSRAVQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in sign bits, and store the results in "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF count[i+63:i] &lt; 64
        dst[i+63:i] := SignExtend64(a[i+63:i] &gt;&gt; count[i+63:i])
    ELSE
        dst[i+63:i] := (a[i+63] ? 0xFFFFFFFFFFFFFFFF : 0)
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLD - _mm256_mask_srl_epi32

| VPSRLD_YMMu32_MASKmskw_YMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        IF count[63:0] &gt; 31
            dst[i+31:i] := 0
        ELSE
            dst[i+31:i] := ZeroExtend32(a[i+31:i] &gt;&gt; count[63:0])
        FI
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLD - _mm256_mask_srli_epi32

| VPSRLD_YMMu32_MASKmskw_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" right by "imm8" while shifting in zeros, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        IF imm8[7:0] &gt; 31
            dst[i+31:i] := 0
        ELSE
            dst[i+31:i] := ZeroExtend32(a[i+31:i] &gt;&gt; imm8[7:0])
        FI
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLD - _mm256_maskz_srl_epi32

| VPSRLD_YMMu32_MASKmskw_YMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        IF count[63:0] &gt; 31
            dst[i+31:i] := 0
        ELSE
            dst[i+31:i] := ZeroExtend32(a[i+31:i] &gt;&gt; count[63:0])
        FI
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLD - _mm256_maskz_srli_epi32

| VPSRLD_YMMu32_MASKmskw_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" right by "imm8" while shifting in zeros, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        IF imm8[7:0] &gt; 31
            dst[i+31:i] := 0
        ELSE
            dst[i+31:i] := ZeroExtend32(a[i+31:i] &gt;&gt; imm8[7:0])
        FI
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLD - _mm_mask_srl_epi32

| VPSRLD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        IF count[63:0] &gt; 31
            dst[i+31:i] := 0
        ELSE
            dst[i+31:i] := ZeroExtend32(a[i+31:i] &gt;&gt; count[63:0])
        FI
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLD - _mm_mask_srli_epi32

| VPSRLD_XMMu32_MASKmskw_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" right by "imm8" while shifting in zeros, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        IF imm8[7:0] &gt; 31
            dst[i+31:i] := 0
        ELSE
            dst[i+31:i] := ZeroExtend32(a[i+31:i] &gt;&gt; imm8[7:0])
        FI
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLD - _mm_maskz_srl_epi32

| VPSRLD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        IF count[63:0] &gt; 31
            dst[i+31:i] := 0
        ELSE
            dst[i+31:i] := ZeroExtend32(a[i+31:i] &gt;&gt; count[63:0])
        FI
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLD - _mm_maskz_srli_epi32

| VPSRLD_XMMu32_MASKmskw_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" right by "imm8" while shifting in zeros, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        IF imm8[7:0] &gt; 31
            dst[i+31:i] := 0
        ELSE
            dst[i+31:i] := ZeroExtend32(a[i+31:i] &gt;&gt; imm8[7:0])
        FI
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLQ - _mm256_mask_srl_epi64

| VPSRLQ_YMMu64_MASKmskw_YMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        IF count[63:0] &gt; 63
            dst[i+63:i] := 0
        ELSE
            dst[i+63:i] := ZeroExtend64(a[i+63:i] &gt;&gt; count[63:0])
        FI
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLQ - _mm256_mask_srli_epi64

| VPSRLQ_YMMu64_MASKmskw_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by "imm8" while shifting in zeros, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        IF imm8[7:0] &gt; 63
            dst[i+63:i] := 0
        ELSE
            dst[i+63:i] := ZeroExtend64(a[i+63:i] &gt;&gt; imm8[7:0])
        FI
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLQ - _mm256_maskz_srl_epi64

| VPSRLQ_YMMu64_MASKmskw_YMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        IF count[63:0] &gt; 63
            dst[i+63:i] := 0
        ELSE
            dst[i+63:i] := ZeroExtend64(a[i+63:i] &gt;&gt; count[63:0])
        FI
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLQ - _mm256_maskz_srli_epi64

| VPSRLQ_YMMu64_MASKmskw_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by "imm8" while shifting in zeros, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        IF imm8[7:0] &gt; 63
            dst[i+63:i] := 0
        ELSE
            dst[i+63:i] := ZeroExtend64(a[i+63:i] &gt;&gt; imm8[7:0])
        FI
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLQ - _mm_mask_srl_epi64

| VPSRLQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        IF count[63:0] &gt; 63
            dst[i+63:i] := 0
        ELSE
            dst[i+63:i] := ZeroExtend64(a[i+63:i] &gt;&gt; count[63:0])
        FI
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLQ - _mm_mask_srli_epi64

| VPSRLQ_XMMu64_MASKmskw_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by "imm8" while shifting in zeros, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        IF imm8[7:0] &gt; 63
            dst[i+63:i] := 0
        ELSE
            dst[i+63:i] := ZeroExtend64(a[i+63:i] &gt;&gt; imm8[7:0])
        FI
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLQ - _mm_maskz_srl_epi64

| VPSRLQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        IF count[63:0] &gt; 63
            dst[i+63:i] := 0
        ELSE
            dst[i+63:i] := ZeroExtend64(a[i+63:i] &gt;&gt; count[63:0])
        FI
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLQ - _mm_maskz_srli_epi64

| VPSRLQ_XMMu64_MASKmskw_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by "imm8" while shifting in zeros, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        IF imm8[7:0] &gt; 63
            dst[i+63:i] := 0
        ELSE
            dst[i+63:i] := ZeroExtend64(a[i+63:i] &gt;&gt; imm8[7:0])
        FI
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLVD - _mm256_mask_srlv_epi32

| VPSRLVD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        IF count[i+31:i] &lt; 32
            dst[i+31:i] := ZeroExtend32(a[i+31:i] &gt;&gt; count[i+31:i])
        ELSE
            dst[i+31:i] := 0
        FI
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLVD - _mm256_maskz_srlv_epi32

| VPSRLVD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        IF count[i+31:i] &lt; 32
            dst[i+31:i] := ZeroExtend32(a[i+31:i] &gt;&gt; count[i+31:i])
        ELSE
            dst[i+31:i] := 0
        FI
    ELSE
        dst[i+31:i] := 0
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLVD - _mm_mask_srlv_epi32

| VPSRLVD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        IF count[i+31:i] &lt; 32
            dst[i+31:i] := ZeroExtend32(a[i+31:i] &gt;&gt; count[i+31:i])
        ELSE
            dst[i+31:i] := 0
        FI
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLVD - _mm_maskz_srlv_epi32

| VPSRLVD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        IF count[i+31:i] &lt; 32
            dst[i+31:i] := ZeroExtend32(a[i+31:i] &gt;&gt; count[i+31:i])
        ELSE
            dst[i+31:i] := 0
        FI
    ELSE
        dst[i+31:i] := 0
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLVQ - _mm256_mask_srlv_epi64

| VPSRLVQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        IF count[i+63:i] &lt; 64
            dst[i+63:i] := ZeroExtend64(a[i+63:i] &gt;&gt; count[i+63:i])
        ELSE
            dst[i+63:i] := 0
        FI
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLVQ - _mm256_maskz_srlv_epi64

| VPSRLVQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        IF count[i+63:i] &lt; 64
            dst[i+63:i] := ZeroExtend64(a[i+63:i] &gt;&gt; count[i+63:i])
        ELSE
            dst[i+63:i] := 0
        FI
    ELSE
        dst[i+63:i] := 0
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLVQ - _mm_mask_srlv_epi64

| VPSRLVQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        IF count[i+63:i] &lt; 64
            dst[i+63:i] := ZeroExtend64(a[i+63:i] &gt;&gt; count[i+63:i])
        ELSE
            dst[i+63:i] := 0
        FI
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLVQ - _mm_maskz_srlv_epi64

| VPSRLVQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        IF count[i+63:i] &lt; 64
            dst[i+63:i] := ZeroExtend64(a[i+63:i] &gt;&gt; count[i+63:i])
        ELSE
            dst[i+63:i] := 0
        FI
    ELSE
        dst[i+63:i] := 0
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBD - _mm256_mask_sub_epi32

| VPSUBD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed 32-bit integers in "b" from packed 32-bit integers in "a", and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] - b[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBD - _mm256_maskz_sub_epi32

| VPSUBD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed 32-bit integers in "b" from packed 32-bit integers in "a", and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] - b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBD - _mm_mask_sub_epi32

| VPSUBD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed 32-bit integers in "b" from packed 32-bit integers in "a", and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] - b[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBD - _mm_maskz_sub_epi32

| VPSUBD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed 32-bit integers in "b" from packed 32-bit integers in "a", and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] - b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBQ - _mm256_mask_sub_epi64

| VPSUBQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed 64-bit integers in "b" from packed 64-bit integers in "a", and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] - b[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBQ - _mm256_maskz_sub_epi64

| VPSUBQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed 64-bit integers in "b" from packed 64-bit integers in "a", and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] - b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBQ - _mm_mask_sub_epi64

| VPSUBQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed 64-bit integers in "b" from packed 64-bit integers in "a", and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] - b[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBQ - _mm_maskz_sub_epi64

| VPSUBQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed 64-bit integers in "b" from packed 64-bit integers in "a", and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] - b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPTERNLOGD - _mm256_mask_ternarylogic_epi32

| VPTERNLOGD_YMMu32_MASKmskw_YMMu32_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Bitwise ternary logic that provides the capability to implement any three-operand binary function; the
specific binary function is specified by value in "imm8". For each bit in each packed 32-bit integer, the
corresponding bit from "src", "a", and "b" are used to form a 3 bit index into "imm8", and the value at that
bit in "imm8" is written to the corresponding bit in "dst" using writemask "k" at 32-bit granularity (32-bit
elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        FOR h := 0 to 31
            index[2:0] := (src[i+h] &lt;&lt; 2) OR (a[i+h] &lt;&lt; 1) OR b[i+h]
            dst[i+h] := imm8[index[2:0]]
        ENDFOR
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPTERNLOGD - _mm256_maskz_ternarylogic_epi32

| VPTERNLOGD_YMMu32_MASKmskw_YMMu32_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Bitwise ternary logic that provides the capability to implement any three-operand binary function; the
specific binary function is specified by value in "imm8". For each bit in each packed 32-bit integer, the
corresponding bit from "a", "b", and "c" are used to form a 3 bit index into "imm8", and the value at that bit
in "imm8" is written to the corresponding bit in "dst" using zeromask "k" at 32-bit granularity (32-bit
elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        FOR h := 0 to 31
            index[2:0] := (a[i+h] &lt;&lt; 2) OR (b[i+h] &lt;&lt; 1) OR c[i+h]
            dst[i+h] := imm8[index[2:0]]
        ENDFOR
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPTERNLOGD - _mm256_ternarylogic_epi32

| VPTERNLOGD_YMMu32_MASKmskw_YMMu32_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Bitwise ternary logic that provides the capability to implement any three-operand binary function; the
specific binary function is specified by value in "imm8". For each bit in each packed 32-bit integer, the
corresponding bit from "a", "b", and "c" are used to form a 3 bit index into "imm8", and the value at that bit
in "imm8" is written to the corresponding bit in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*32
    FOR h := 0 to 31
        index[2:0] := (a[i+h] &lt;&lt; 2) OR (b[i+h] &lt;&lt; 1) OR c[i+h]
        dst[i+h] := imm8[index[2:0]]
    ENDFOR
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPTERNLOGD - _mm_mask_ternarylogic_epi32

| VPTERNLOGD_XMMu32_MASKmskw_XMMu32_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Bitwise ternary logic that provides the capability to implement any three-operand binary function; the
specific binary function is specified by value in "imm8". For each bit in each packed 32-bit integer, the
corresponding bit from "src", "a", and "b" are used to form a 3 bit index into "imm8", and the value at that
bit in "imm8" is written to the corresponding bit in "dst" using writemask "k" at 32-bit granularity (32-bit
elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        FOR h := 0 to 31
            index[2:0] := (src[i+h] &lt;&lt; 2) OR (a[i+h] &lt;&lt; 1) OR b[i+h]
            dst[i+h] := imm8[index[2:0]]
        ENDFOR
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPTERNLOGD - _mm_maskz_ternarylogic_epi32

| VPTERNLOGD_XMMu32_MASKmskw_XMMu32_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Bitwise ternary logic that provides the capability to implement any three-operand binary function; the
specific binary function is specified by value in "imm8". For each bit in each packed 32-bit integer, the
corresponding bit from "a", "b", and "c" are used to form a 3 bit index into "imm8", and the value at that bit
in "imm8" is written to the corresponding bit in "dst" using zeromask "k" at 32-bit granularity (32-bit
elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        FOR h := 0 to 31
            index[2:0] := (a[i+h] &lt;&lt; 2) OR (b[i+h] &lt;&lt; 1) OR c[i+h]
            dst[i+h] := imm8[index[2:0]]
        ENDFOR
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPTERNLOGD - _mm_ternarylogic_epi32

| VPTERNLOGD_XMMu32_MASKmskw_XMMu32_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Bitwise ternary logic that provides the capability to implement any three-operand binary function; the
specific binary function is specified by value in "imm8". For each bit in each packed 32-bit integer, the
corresponding bit from "a", "b", and "c" are used to form a 3 bit index into "imm8", and the value at that bit
in "imm8" is written to the corresponding bit in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*32
    FOR h := 0 to 31
        index[2:0] := (a[i+h] &lt;&lt; 2) OR (b[i+h] &lt;&lt; 1) OR c[i+h]
        dst[i+h] := imm8[index[2:0]]
    ENDFOR
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPTERNLOGQ - _mm256_mask_ternarylogic_epi64

| VPTERNLOGQ_YMMu64_MASKmskw_YMMu64_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Bitwise ternary logic that provides the capability to implement any three-operand binary function; the
specific binary function is specified by value in "imm8". For each bit in each packed 64-bit integer, the
corresponding bit from "src", "a", and "b" are used to form a 3 bit index into "imm8", and the value at that
bit in "imm8" is written to the corresponding bit in "dst" using writemask "k" at 64-bit granularity (64-bit
elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        FOR h := 0 to 63
            index[2:0] := (src[i+h] &lt;&lt; 2) OR (a[i+h] &lt;&lt; 1) OR b[i+h]
            dst[i+h] := imm8[index[2:0]]
        ENDFOR
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPTERNLOGQ - _mm256_maskz_ternarylogic_epi64

| VPTERNLOGQ_YMMu64_MASKmskw_YMMu64_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Bitwise ternary logic that provides the capability to implement any three-operand binary function; the
specific binary function is specified by value in "imm8". For each bit in each packed 64-bit integer, the
corresponding bit from "a", "b", and "c" are used to form a 3 bit index into "imm8", and the value at that bit
in "imm8" is written to the corresponding bit in "dst" using zeromask "k" at 64-bit granularity (64-bit
elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        FOR h := 0 to 63
            index[2:0] := (a[i+h] &lt;&lt; 2) OR (b[i+h] &lt;&lt; 1) OR c[i+h]
            dst[i+h] := imm8[index[2:0]]
        ENDFOR
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPTERNLOGQ - _mm256_ternarylogic_epi64

| VPTERNLOGQ_YMMu64_MASKmskw_YMMu64_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Bitwise ternary logic that provides the capability to implement any three-operand binary function; the
specific binary function is specified by value in "imm8". For each bit in each packed 64-bit integer, the
corresponding bit from "a", "b", and "c" are used to form a 3 bit index into "imm8", and the value at that bit
in "imm8" is written to the corresponding bit in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    FOR h := 0 to 63
        index[2:0] := (a[i+h] &lt;&lt; 2) OR (b[i+h] &lt;&lt; 1) OR c[i+h]
        dst[i+h] := imm8[index[2:0]]
    ENDFOR
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPTERNLOGQ - _mm_mask_ternarylogic_epi64

| VPTERNLOGQ_XMMu64_MASKmskw_XMMu64_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Bitwise ternary logic that provides the capability to implement any three-operand binary function; the
specific binary function is specified by value in "imm8". For each bit in each packed 64-bit integer, the
corresponding bit from "src", "a", and "b" are used to form a 3 bit index into "imm8", and the value at that
bit in "imm8" is written to the corresponding bit in "dst" using writemask "k" at 64-bit granularity (64-bit
elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        FOR h := 0 to 63
            index[2:0] := (src[i+h] &lt;&lt; 2) OR (a[i+h] &lt;&lt; 1) OR b[i+h]
            dst[i+h] := imm8[index[2:0]]
        ENDFOR
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPTERNLOGQ - _mm_maskz_ternarylogic_epi64

| VPTERNLOGQ_XMMu64_MASKmskw_XMMu64_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Bitwise ternary logic that provides the capability to implement any three-operand binary function; the
specific binary function is specified by value in "imm8". For each bit in each packed 64-bit integer, the
corresponding bit from "a", "b", and "c" are used to form a 3 bit index into "imm8", and the value at that bit
in "imm8" is written to the corresponding bit in "dst" using zeromask "k" at 64-bit granularity (64-bit
elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        FOR h := 0 to 63
            index[2:0] := (a[i+h] &lt;&lt; 2) OR (b[i+h] &lt;&lt; 1) OR c[i+h]
            dst[i+h] := imm8[index[2:0]]
        ENDFOR
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPTERNLOGQ - _mm_ternarylogic_epi64

| VPTERNLOGQ_XMMu64_MASKmskw_XMMu64_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Bitwise ternary logic that provides the capability to implement any three-operand binary function; the
specific binary function is specified by value in "imm8". For each bit in each packed 64-bit integer, the
corresponding bit from "a", "b", and "c" are used to form a 3 bit index into "imm8", and the value at that bit
in "imm8" is written to the corresponding bit in "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    FOR h := 0 to 63
        index[2:0] := (a[i+h] &lt;&lt; 2) OR (b[i+h] &lt;&lt; 1) OR c[i+h]
        dst[i+h] := imm8[index[2:0]]
    ENDFOR
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTMD - _mm256_mask_test_epi32_mask

| VPTESTMD_MASKmskw_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed 32-bit integers in "a" and "b", producing intermediate 32-bit values, and
set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is non-zero.

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k1[j]
        k[j] := ((a[i+31:i] AND b[i+31:i]) != 0) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTMD - _mm256_test_epi32_mask

| VPTESTMD_MASKmskw_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed 32-bit integers in "a" and "b", producing intermediate 32-bit values, and
set the corresponding bit in result mask "k" if the intermediate value is non-zero.

[algorithm]

FOR j := 0 to 7
    i := j*32
    k[j] := ((a[i+31:i] AND b[i+31:i]) != 0) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTMD - _mm_mask_test_epi32_mask

| VPTESTMD_MASKmskw_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed 32-bit integers in "a" and "b", producing intermediate 32-bit values, and
set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is non-zero.

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k1[j]
        k[j] := ((a[i+31:i] AND b[i+31:i]) != 0) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTMD - _mm_test_epi32_mask

| VPTESTMD_MASKmskw_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed 32-bit integers in "a" and "b", producing intermediate 32-bit values, and
set the corresponding bit in result mask "k" if the intermediate value is non-zero.

[algorithm]

FOR j := 0 to 3
    i := j*32
    k[j] := ((a[i+31:i] AND b[i+31:i]) != 0) ? 1 : 0
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTMQ - _mm256_mask_test_epi64_mask

| VPTESTMQ_MASKmskw_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed 64-bit integers in "a" and "b", producing intermediate 64-bit values, and
set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is non-zero.

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k1[j]
        k[j] := ((a[i+63:i] AND b[i+63:i]) != 0) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTMQ - _mm256_test_epi64_mask

| VPTESTMQ_MASKmskw_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed 64-bit integers in "a" and "b", producing intermediate 64-bit values, and
set the corresponding bit in result mask "k" if the intermediate value is non-zero.

[algorithm]

FOR j := 0 to 3
    i := j*64
    k[j] := ((a[i+63:i] AND b[i+63:i]) != 0) ? 1 : 0
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTMQ - _mm_mask_test_epi64_mask

| VPTESTMQ_MASKmskw_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed 64-bit integers in "a" and "b", producing intermediate 64-bit values, and
set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is non-zero.

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k1[j]
        k[j] := ((a[i+63:i] AND b[i+63:i]) != 0) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTMQ - _mm_test_epi64_mask

| VPTESTMQ_MASKmskw_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed 64-bit integers in "a" and "b", producing intermediate 64-bit values, and
set the corresponding bit in result mask "k" if the intermediate value is non-zero.

[algorithm]

FOR j := 0 to 1
    i := j*64
    k[j] := ((a[i+63:i] AND b[i+63:i]) != 0) ? 1 : 0
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTNMD - _mm256_mask_testn_epi32_mask

| VPTESTNMD_MASKmskw_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NAND of packed 32-bit integers in "a" and "b", producing intermediate 32-bit values, and
set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is zero.

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k1[j]
        k[j] := ((a[i+31:i] AND b[i+31:i]) == 0) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTNMD - _mm256_testn_epi32_mask

| VPTESTNMD_MASKmskw_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NAND of packed 32-bit integers in "a" and "b", producing intermediate 32-bit values, and
set the corresponding bit in result mask "k" if the intermediate value is zero.

[algorithm]

FOR j := 0 to 7
    i := j*32
    k[j] := ((a[i+31:i] AND b[i+31:i]) == 0) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTNMD - _mm_mask_testn_epi32_mask

| VPTESTNMD_MASKmskw_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NAND of packed 32-bit integers in "a" and "b", producing intermediate 32-bit values, and
set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is zero.

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k1[j]
        k[j] := ((a[i+31:i] AND b[i+31:i]) == 0) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTNMD - _mm_testn_epi32_mask

| VPTESTNMD_MASKmskw_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NAND of packed 32-bit integers in "a" and "b", producing intermediate 32-bit values, and
set the corresponding bit in result mask "k" if the intermediate value is zero.

[algorithm]

FOR j := 0 to 3
    i := j*32
    k[j] := ((a[i+31:i] AND b[i+31:i]) == 0) ? 1 : 0
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTNMQ - _mm256_mask_testn_epi64_mask

| VPTESTNMQ_MASKmskw_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NAND of packed 64-bit integers in "a" and "b", producing intermediate 64-bit values, and
set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is zero.

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k1[j]
        k[j] := ((a[i+63:i] AND b[i+63:i]) == 0) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTNMQ - _mm256_testn_epi64_mask

| VPTESTNMQ_MASKmskw_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NAND of packed 64-bit integers in "a" and "b", producing intermediate 64-bit values, and
set the corresponding bit in result mask "k" if the intermediate value is zero.

[algorithm]

FOR j := 0 to 3
    i := j*64
    k[j] := ((a[i+63:i] AND b[i+63:i]) == 0) ? 1 : 0
ENDFOR
k[MAX:4] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTNMQ - _mm_mask_testn_epi64_mask

| VPTESTNMQ_MASKmskw_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NAND of packed 64-bit integers in "a" and "b", producing intermediate 64-bit values, and
set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is zero.

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k1[j]
        k[j] := ((a[i+63:i] AND b[i+63:i]) == 0) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTNMQ - _mm_testn_epi64_mask

| VPTESTNMQ_MASKmskw_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NAND of packed 64-bit integers in "a" and "b", producing intermediate 64-bit values, and
set the corresponding bit in result mask "k" if the intermediate value is zero.

[algorithm]

FOR j := 0 to 1
    i := j*64
    k[j] := ((a[i+63:i] AND b[i+63:i]) == 0) ? 1 : 0
ENDFOR
k[MAX:2] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKHDQ - _mm256_mask_unpackhi_epi32

| VPUNPCKHDQ_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 32-bit integers from the high half of each 128-bit lane in "a" and "b", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

DEFINE INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]) {
    dst[31:0] := src1[95:64] 
    dst[63:32] := src2[95:64] 
    dst[95:64] := src1[127:96] 
    dst[127:96] := src2[127:96] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKHDQ - _mm256_maskz_unpackhi_epi32

| VPUNPCKHDQ_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 32-bit integers from the high half of each 128-bit lane in "a" and "b", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]) {
    dst[31:0] := src1[95:64] 
    dst[63:32] := src2[95:64] 
    dst[95:64] := src1[127:96] 
    dst[127:96] := src2[127:96] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKHDQ - _mm_mask_unpackhi_epi32

| VPUNPCKHDQ_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 32-bit integers from the high half of "a" and "b", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]) {
    dst[31:0] := src1[95:64] 
    dst[63:32] := src2[95:64] 
    dst[95:64] := src1[127:96] 
    dst[127:96] := src2[127:96] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKHDQ - _mm_maskz_unpackhi_epi32

| VPUNPCKHDQ_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 32-bit integers from the high half of "a" and "b", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]) {
    dst[31:0] := src1[95:64] 
    dst[63:32] := src2[95:64] 
    dst[95:64] := src1[127:96] 
    dst[127:96] := src2[127:96] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKHQDQ - _mm256_mask_unpackhi_epi64

| VPUNPCKHQDQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 64-bit integers from the high half of each 128-bit lane in "a" and "b", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

DEFINE INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]) {
    dst[63:0] := src1[127:64] 
    dst[127:64] := src2[127:64] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKHQDQ - _mm256_maskz_unpackhi_epi64

| VPUNPCKHQDQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 64-bit integers from the high half of each 128-bit lane in "a" and "b", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]) {
    dst[63:0] := src1[127:64] 
    dst[127:64] := src2[127:64] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKHQDQ - _mm_mask_unpackhi_epi64

| VPUNPCKHQDQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 64-bit integers from the high half of "a" and "b", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]) {
    dst[63:0] := src1[127:64] 
    dst[127:64] := src2[127:64] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKHQDQ - _mm_maskz_unpackhi_epi64

| VPUNPCKHQDQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 64-bit integers from the high half of "a" and "b", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]) {
    dst[63:0] := src1[127:64] 
    dst[127:64] := src2[127:64] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKLDQ - _mm256_mask_unpacklo_epi32

| VPUNPCKLDQ_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 32-bit integers from the low half of each 128-bit lane in "a" and "b", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

DEFINE INTERLEAVE_DWORDS(src1[127:0], src2[127:0]) {
    dst[31:0] := src1[31:0] 
    dst[63:32] := src2[31:0] 
    dst[95:64] := src1[63:32] 
    dst[127:96] := src2[63:32] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKLDQ - _mm256_maskz_unpacklo_epi32

| VPUNPCKLDQ_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 32-bit integers from the low half of each 128-bit lane in "a" and "b", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_DWORDS(src1[127:0], src2[127:0]) {
    dst[31:0] := src1[31:0] 
    dst[63:32] := src2[31:0] 
    dst[95:64] := src1[63:32] 
    dst[127:96] := src2[63:32] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKLDQ - _mm_mask_unpacklo_epi32

| VPUNPCKLDQ_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 32-bit integers from the low half of "a" and "b", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_DWORDS(src1[127:0], src2[127:0]) {
    dst[31:0] := src1[31:0] 
    dst[63:32] := src2[31:0] 
    dst[95:64] := src1[63:32] 
    dst[127:96] := src2[63:32] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKLDQ - _mm_maskz_unpacklo_epi32

| VPUNPCKLDQ_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 32-bit integers from the low half of "a" and "b", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_DWORDS(src1[127:0], src2[127:0]) {
    dst[31:0] := src1[31:0] 
    dst[63:32] := src2[31:0] 
    dst[95:64] := src1[63:32] 
    dst[127:96] := src2[63:32] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKLQDQ - _mm256_mask_unpacklo_epi64

| VPUNPCKLQDQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 64-bit integers from the low half of each 128-bit lane in "a" and "b", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

DEFINE INTERLEAVE_QWORDS(src1[127:0], src2[127:0]) {
    dst[63:0] := src1[63:0] 
    dst[127:64] := src2[63:0] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKLQDQ - _mm256_maskz_unpacklo_epi64

| VPUNPCKLQDQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 64-bit integers from the low half of each 128-bit lane in "a" and "b", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_QWORDS(src1[127:0], src2[127:0]) {
    dst[63:0] := src1[63:0] 
    dst[127:64] := src2[63:0] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKLQDQ - _mm_mask_unpacklo_epi64

| VPUNPCKLQDQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 64-bit integers from the low half of "a" and "b", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_QWORDS(src1[127:0], src2[127:0]) {
    dst[63:0] := src1[63:0] 
    dst[127:64] := src2[63:0] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKLQDQ - _mm_maskz_unpacklo_epi64

| VPUNPCKLQDQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 64-bit integers from the low half of "a" and "b", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_QWORDS(src1[127:0], src2[127:0]) {
    dst[63:0] := src1[63:0] 
    dst[127:64] := src2[63:0] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPXORD - _mm256_mask_xor_epi32

| VPXORD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XOR of packed 32-bit integers in "a" and "b", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPXORD - _mm256_maskz_xor_epi32

| VPXORD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XOR of packed 32-bit integers in "a" and "b", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPXORD - _mm_mask_xor_epi32

| VPXORD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XOR of packed 32-bit integers in "a" and "b", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPXORD - _mm_maskz_xor_epi32

| VPXORD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XOR of packed 32-bit integers in "a" and "b", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPXORQ - _mm256_mask_xor_epi64

| VPXORQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XOR of packed 64-bit integers in "a" and "b", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPXORQ - _mm256_maskz_xor_epi64

| VPXORQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XOR of packed 64-bit integers in "a" and "b", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPXORQ - _mm_mask_xor_epi64

| VPXORQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XOR of packed 64-bit integers in "a" and "b", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPXORQ - _mm_maskz_xor_epi64

| VPXORQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XOR of packed 64-bit integers in "a" and "b", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP14PD - _mm256_mask_rcp14_pd

| VRCP14PD_YMMf64_MASKmskw_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in "a", and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set). The maximum relative error for this approximation is less than 2^-14.

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := (1.0 / a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP14PD - _mm256_maskz_rcp14_pd

| VRCP14PD_YMMf64_MASKmskw_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in "a", and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set). The maximum relative error for this approximation is less than 2^-14.

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := (1.0 / a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP14PD - _mm256_rcp14_pd

| VRCP14PD_YMMf64_MASKmskw_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in "a", and
store the results in "dst". The maximum relative error for this approximation is less than 2^-14.

[algorithm]

FOR j := 0 to 3
    i := j*64
    dst[i+63:i] := (1.0 / a[i+63:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP14PD - _mm_mask_rcp14_pd

| VRCP14PD_XMMf64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in "a", and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set). The maximum relative error for this approximation is less than 2^-14.

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := (1.0 / a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP14PD - _mm_maskz_rcp14_pd

| VRCP14PD_XMMf64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in "a", and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set). The maximum relative error for this approximation is less than 2^-14.

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := (1.0 / a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP14PD - _mm_rcp14_pd

| VRCP14PD_XMMf64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in "a", and
store the results in "dst". The maximum relative error for this approximation is less than 2^-14.

[algorithm]

FOR j := 0 to 1
    i := j*64
    dst[i+63:i] := (1.0 / a[i+63:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP14PS - _mm256_mask_rcp14_ps

| VRCP14PS_YMMf32_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in "a", and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set). The maximum relative error for this approximation is less than 2^-14.

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := (1.0 / a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP14PS - _mm256_maskz_rcp14_ps

| VRCP14PS_YMMf32_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in "a", and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set). The maximum relative error for this approximation is less than 2^-14.

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := (1.0 / a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP14PS - _mm256_rcp14_ps

| VRCP14PS_YMMf32_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in "a", and
store the results in "dst". The maximum relative error for this approximation is less than 2^-14.

[algorithm]

FOR j := 0 to 7
    i := j*32
    dst[i+31:i] := (1.0 / a[i+31:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP14PS - _mm_mask_rcp14_ps

| VRCP14PS_XMMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in "a", and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set). The maximum relative error for this approximation is less than 2^-14.

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := (1.0 / a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP14PS - _mm_maskz_rcp14_ps

| VRCP14PS_XMMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in "a", and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set). The maximum relative error for this approximation is less than 2^-14.

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := (1.0 / a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP14PS - _mm_rcp14_ps

| VRCP14PS_XMMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in "a", and
store the results in "dst". The maximum relative error for this approximation is less than 2^-14.

[algorithm]

FOR j := 0 to 3
    i := j*32
    dst[i+31:i] := (1.0 / a[i+31:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRNDSCALEPD - _mm256_mask_roundscale_pd

| VRNDSCALEPD_YMMf64_MASKmskw_YMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Round packed double-precision (64-bit) floating-point elements in "a" to the number of fraction bits specified
by "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set). [round_imm_note]

[algorithm]

DEFINE RoundScaleFP64(src1[63:0], imm8[7:0]) {
    m[63:0] := FP64(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[63:0] := POW(2.0, -m) * ROUND(POW(2.0, m) * src1[63:0], imm8[3:0])
    IF IsInf(tmp[63:0])
        tmp[63:0] := src1[63:0]
    FI
    RETURN tmp[63:0]
}
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := RoundScaleFP64(a[i+63:i], imm8[7:0])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VRNDSCALEPD - _mm256_maskz_roundscale_pd

| VRNDSCALEPD_YMMf64_MASKmskw_YMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Round packed double-precision (64-bit) floating-point elements in "a" to the number of fraction bits specified
by "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set). [round_imm_note]

[algorithm]

DEFINE RoundScaleFP64(src1[63:0], imm8[7:0]) {
    m[63:0] := FP64(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[63:0] := POW(2.0, -m) * ROUND(POW(2.0, m) * src1[63:0], imm8[3:0])
    IF IsInf(tmp[63:0])
        tmp[63:0] := src1[63:0]
    FI
    RETURN tmp[63:0]
}
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := RoundScaleFP64(a[i+63:i], imm8[7:0])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VRNDSCALEPD - _mm256_roundscale_pd

| VRNDSCALEPD_YMMf64_MASKmskw_YMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Round packed double-precision (64-bit) floating-point elements in "a" to the number of fraction bits specified
by "imm8", and store the results in "dst". [round_imm_note]

[algorithm]

DEFINE RoundScaleFP64(src1[63:0], imm8[7:0]) {
    m[63:0] := FP64(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[63:0] := POW(2.0, -m) * ROUND(POW(2.0, m) * src1[63:0], imm8[3:0])
    IF IsInf(tmp[63:0])
        tmp[63:0] := src1[63:0]
    FI
    RETURN tmp[63:0]
}
FOR j := 0 to 3
    i := j*64
    dst[i+63:i] := RoundScaleFP64(a[i+63:i], imm8[7:0])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VRNDSCALEPD - _mm_mask_roundscale_pd

| VRNDSCALEPD_XMMf64_MASKmskw_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Round packed double-precision (64-bit) floating-point elements in "a" to the number of fraction bits specified
by "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set). [round_imm_note]

[algorithm]

DEFINE RoundScaleFP64(src1[63:0], imm8[7:0]) {
    m[63:0] := FP64(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[63:0] := POW(2.0, -m) * ROUND(POW(2.0, m) * src1[63:0], imm8[3:0])
    IF IsInf(tmp[63:0])
        tmp[63:0] := src1[63:0]
    FI
    RETURN tmp[63:0]
}
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := RoundScaleFP64(a[i+63:i], imm8[7:0])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRNDSCALEPD - _mm_maskz_roundscale_pd

| VRNDSCALEPD_XMMf64_MASKmskw_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Round packed double-precision (64-bit) floating-point elements in "a" to the number of fraction bits specified
by "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set). [round_imm_note]

[algorithm]

DEFINE RoundScaleFP64(src1[63:0], imm8[7:0]) {
    m[63:0] := FP64(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[63:0] := POW(2.0, -m) * ROUND(POW(2.0, m) * src1[63:0], imm8[3:0])
    IF IsInf(tmp[63:0])
        tmp[63:0] := src1[63:0]
    FI
    RETURN tmp[63:0]
}
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := RoundScaleFP64(a[i+63:i], imm8[7:0])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRNDSCALEPD - _mm_roundscale_pd

| VRNDSCALEPD_XMMf64_MASKmskw_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Round packed double-precision (64-bit) floating-point elements in "a" to the number of fraction bits specified
by "imm8", and store the results in "dst". [round_imm_note]

[algorithm]

DEFINE RoundScaleFP64(src1[63:0], imm8[7:0]) {
    m[63:0] := FP64(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[63:0] := POW(2.0, -m) * ROUND(POW(2.0, m) * src1[63:0], imm8[3:0])
    IF IsInf(tmp[63:0])
        tmp[63:0] := src1[63:0]
    FI
    RETURN tmp[63:0]
}
FOR j := 0 to 1
    i := j*64
    dst[i+63:i] := RoundScaleFP64(a[i+63:i], imm8[7:0])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRNDSCALEPS - _mm256_mask_roundscale_ps

| VRNDSCALEPS_YMMf32_MASKmskw_YMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Round packed single-precision (32-bit) floating-point elements in "a" to the number of fraction bits specified
by "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set). [round_imm_note]

[algorithm]

DEFINE RoundScaleFP32(src1[31:0], imm8[7:0]) {
    m[31:0] := FP32(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[31:0] := POW(FP32(2.0), -m) * ROUND(POW(FP32(2.0), m) * src1[31:0], imm8[3:0])
    IF IsInf(tmp[31:0])
        tmp[31:0] := src1[31:0]
    FI
    RETURN tmp[31:0]
}
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := RoundScaleFP32(a[i+31:i], imm8[7:0])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VRNDSCALEPS - _mm256_maskz_roundscale_ps

| VRNDSCALEPS_YMMf32_MASKmskw_YMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Round packed single-precision (32-bit) floating-point elements in "a" to the number of fraction bits specified
by "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set). [round_imm_note]

[algorithm]

DEFINE RoundScaleFP32(src1[31:0], imm8[7:0]) {
    m[31:0] := FP32(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[31:0] := POW(FP32(2.0), -m) * ROUND(POW(FP32(2.0), m) * src1[31:0], imm8[3:0])
    IF IsInf(tmp[31:0])
        tmp[31:0] := src1[31:0]
    FI
    RETURN tmp[31:0]
}
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := RoundScaleFP32(a[i+31:i], imm8[7:0])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VRNDSCALEPS - _mm256_roundscale_ps

| VRNDSCALEPS_YMMf32_MASKmskw_YMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Round packed single-precision (32-bit) floating-point elements in "a" to the number of fraction bits specified
by "imm8", and store the results in "dst". [round_imm_note]

[algorithm]

DEFINE RoundScaleFP32(src1[31:0], imm8[7:0]) {
    m[31:0] := FP32(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[31:0] := POW(FP32(2.0), -m) * ROUND(POW(FP32(2.0), m) * src1[31:0], imm8[3:0])
    IF IsInf(tmp[31:0])
        tmp[31:0] := src1[31:0]
    FI
    RETURN tmp[31:0]
}
FOR j := 0 to 7
    i := j*32
    dst[i+31:i] := RoundScaleFP32(a[i+31:i], imm8[7:0])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VRNDSCALEPS - _mm_mask_roundscale_ps

| VRNDSCALEPS_XMMf32_MASKmskw_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Round packed single-precision (32-bit) floating-point elements in "a" to the number of fraction bits specified
by "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set). [round_imm_note]

[algorithm]

DEFINE RoundScaleFP32(src1[31:0], imm8[7:0]) {
    m[31:0] := FP32(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[31:0] := POW(FP32(2.0), -m) * ROUND(POW(FP32(2.0), m) * src1[31:0], imm8[3:0])
    IF IsInf(tmp[31:0])
        tmp[31:0] := src1[31:0]
    FI
    RETURN tmp[31:0]
}
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := RoundScaleFP32(a[i+31:i], imm8[7:0])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRNDSCALEPS - _mm_maskz_roundscale_ps

| VRNDSCALEPS_XMMf32_MASKmskw_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Round packed single-precision (32-bit) floating-point elements in "a" to the number of fraction bits specified
by "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set). [round_imm_note]

[algorithm]

DEFINE RoundScaleFP32(src1[31:0], imm8[7:0]) {
    m[31:0] := FP32(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[31:0] := POW(FP32(2.0), -m) * ROUND(POW(FP32(2.0), m) * src1[31:0], imm8[3:0])
    IF IsInf(tmp[31:0])
        tmp[31:0] := src1[31:0]
    FI
    RETURN tmp[31:0]
}
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := RoundScaleFP32(a[i+31:i], imm8[7:0])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRNDSCALEPS - _mm_roundscale_ps

| VRNDSCALEPS_XMMf32_MASKmskw_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Round packed single-precision (32-bit) floating-point elements in "a" to the number of fraction bits specified
by "imm8", and store the results in "dst". [round_imm_note]

[algorithm]

DEFINE RoundScaleFP32(src1[31:0], imm8[7:0]) {
    m[31:0] := FP32(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[31:0] := POW(FP32(2.0), -m) * ROUND(POW(FP32(2.0), m) * src1[31:0], imm8[3:0])
    IF IsInf(tmp[31:0])
        tmp[31:0] := src1[31:0]
    FI
    RETURN tmp[31:0]
}
FOR j := 0 to 3
    i := j*32
    dst[i+31:i] := RoundScaleFP32(a[i+31:i], imm8[7:0])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRSQRT14PD - _mm256_mask_rsqrt14_pd

| VRSQRT14PD_YMMf64_MASKmskw_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of packed double-precision (64-bit) floating-point elements in
"a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding
mask bit is not set). The maximum relative error for this approximation is less than 2^-14.

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := (1.0 / SQRT(a[i+63:i]))
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VRSQRT14PD - _mm256_maskz_rsqrt14_pd

| VRSQRT14PD_YMMf64_MASKmskw_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of packed double-precision (64-bit) floating-point elements in
"a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit
is not set). The maximum relative error for this approximation is less than 2^-14.

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := (1.0 / SQRT(a[i+63:i]))
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VRSQRT14PD - _mm_mask_rsqrt14_pd

| VRSQRT14PD_XMMf64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of packed double-precision (64-bit) floating-point elements in
"a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding
mask bit is not set). The maximum relative error for this approximation is less than 2^-14.

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := (1.0 / SQRT(a[i+63:i]))
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRSQRT14PD - _mm_maskz_rsqrt14_pd

| VRSQRT14PD_XMMf64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of packed double-precision (64-bit) floating-point elements in
"a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit
is not set). The maximum relative error for this approximation is less than 2^-14.

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := (1.0 / SQRT(a[i+63:i]))
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRSQRT14PS - _mm256_mask_rsqrt14_ps

| VRSQRT14PS_YMMf32_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of packed single-precision (32-bit) floating-point elements in
"a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding
mask bit is not set). The maximum relative error for this approximation is less than 2^-14.

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := (1.0 / SQRT(a[i+31:i]))
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VRSQRT14PS - _mm256_maskz_rsqrt14_ps

| VRSQRT14PS_YMMf32_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of packed single-precision (32-bit) floating-point elements in
"a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit
is not set). The maximum relative error for this approximation is less than 2^-14.

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := (1.0 / SQRT(a[i+31:i]))
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VRSQRT14PS - _mm_mask_rsqrt14_ps

| VRSQRT14PS_XMMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of packed single-precision (32-bit) floating-point elements in
"a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding
mask bit is not set). The maximum relative error for this approximation is less than 2^-14.

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := (1.0 / SQRT(a[i+31:i]))
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRSQRT14PS - _mm_maskz_rsqrt14_ps

| VRSQRT14PS_XMMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of packed single-precision (32-bit) floating-point elements in
"a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit
is not set). The maximum relative error for this approximation is less than 2^-14.

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := (1.0 / SQRT(a[i+31:i]))
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSCALEFPD - _mm256_mask_scalef_pd

| VSCALEFPD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Scale the packed double-precision (64-bit) floating-point elements in "a" using values from "b", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

DEFINE SCALE(src1, src2) {
    IF (src2 == NaN)
        IF (src2 == SNaN)
            RETURN QNAN(src2)
        FI
    ELSE IF (src1 == NaN)
        IF (src1 == SNaN)
            RETURN QNAN(src1)
        FI
        IF (src2 != INF)
            RETURN QNAN(src1)
        FI
    ELSE
        tmp_src2 := src2
        tmp_src1 := src1
        IF (IS_DENORMAL(src2) AND MXCSR.DAZ)
            tmp_src2 := 0
        FI
        IF (IS_DENORMAL(src1) AND MXCSR.DAZ)
            tmp_src1 := 0
        FI
    FI
    dst[63:0] := tmp_src1[63:0] * POW(2.0, FLOOR(tmp_src2[63:0]))
    RETURN dst[63:0]
}
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VSCALEFPD - _mm256_maskz_scalef_pd

| VSCALEFPD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Scale the packed double-precision (64-bit) floating-point elements in "a" using values from "b", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE SCALE(src1, src2) {
    IF (src2 == NaN)
        IF (src2 == SNaN)
            RETURN QNAN(src2)
        FI
    ELSE IF (src1 == NaN)
        IF (src1 == SNaN)
            RETURN QNAN(src1)
        FI
        IF (src2 != INF)
            RETURN QNAN(src1)
        FI
    ELSE
        tmp_src2 := src2
        tmp_src1 := src1
        IF (IS_DENORMAL(src2) AND MXCSR.DAZ)
            tmp_src2 := 0
        FI
        IF (IS_DENORMAL(src1) AND MXCSR.DAZ)
            tmp_src1 := 0
        FI
    FI
    dst[63:0] := tmp_src1[63:0] * POW(2.0, FLOOR(tmp_src2[63:0]))
    RETURN dst[63:0]
}
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VSCALEFPD - _mm256_scalef_pd

| VSCALEFPD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Scale the packed double-precision (64-bit) floating-point elements in "a" using values from "b", and store the
results in "dst".

[algorithm]

DEFINE SCALE(src1, src2) {
    IF (src2 == NaN)
        IF (src2 == SNaN)
            RETURN QNAN(src2)
        FI
    ELSE IF (src1 == NaN)
        IF (src1 == SNaN)
            RETURN QNAN(src1)
        FI
        IF (src2 != INF)
            RETURN QNAN(src1)
        FI
    ELSE
        tmp_src2 := src2
        tmp_src1 := src1
        IF (IS_DENORMAL(src2) AND MXCSR.DAZ)
            tmp_src2 := 0
        FI
        IF (IS_DENORMAL(src1) AND MXCSR.DAZ)
            tmp_src1 := 0
        FI
    FI
    dst[63:0] := tmp_src1[63:0] * POW(2.0, FLOOR(tmp_src2[63:0]))
    RETURN dst[63:0]
}
FOR j := 0 to 3
    i := j*64
    dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VSCALEFPD - _mm_mask_scalef_pd

| VSCALEFPD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Scale the packed double-precision (64-bit) floating-point elements in "a" using values from "b", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

DEFINE SCALE(src1, src2) {
    IF (src2 == NaN)
        IF (src2 == SNaN)
            RETURN QNAN(src2)
        FI
    ELSE IF (src1 == NaN)
        IF (src1 == SNaN)
            RETURN QNAN(src1)
        FI
        IF (src2 != INF)
            RETURN QNAN(src1)
        FI
    ELSE
        tmp_src2 := src2
        tmp_src1 := src1
        IF (IS_DENORMAL(src2) AND MXCSR.DAZ)
            tmp_src2 := 0
        FI
        IF (IS_DENORMAL(src1) AND MXCSR.DAZ)
            tmp_src1 := 0
        FI
    FI
    dst[63:0] := tmp_src1[63:0] * POW(2.0, FLOOR(tmp_src2[63:0]))
    RETURN dst[63:0]
}
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSCALEFPD - _mm_maskz_scalef_pd

| VSCALEFPD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Scale the packed double-precision (64-bit) floating-point elements in "a" using values from "b", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE SCALE(src1, src2) {
    IF (src2 == NaN)
        IF (src2 == SNaN)
            RETURN QNAN(src2)
        FI
    ELSE IF (src1 == NaN)
        IF (src1 == SNaN)
            RETURN QNAN(src1)
        FI
        IF (src2 != INF)
            RETURN QNAN(src1)
        FI
    ELSE
        tmp_src2 := src2
        tmp_src1 := src1
        IF (IS_DENORMAL(src2) AND MXCSR.DAZ)
            tmp_src2 := 0
        FI
        IF (IS_DENORMAL(src1) AND MXCSR.DAZ)
            tmp_src1 := 0
        FI
    FI
    dst[63:0] := tmp_src1[63:0] * POW(2.0, FLOOR(tmp_src2[63:0]))
    RETURN dst[63:0]
}
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSCALEFPD - _mm_scalef_pd

| VSCALEFPD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Scale the packed double-precision (64-bit) floating-point elements in "a" using values from "b", and store the
results in "dst".

[algorithm]

DEFINE SCALE(src1, src2) {
    IF (src2 == NaN)
        IF (src2 == SNaN)
            RETURN QNAN(src2)
        FI
    ELSE IF (src1 == NaN)
        IF (src1 == SNaN)
            RETURN QNAN(src1)
        FI
        IF (src2 != INF)
            RETURN QNAN(src1)
        FI
    ELSE
        tmp_src2 := src2
        tmp_src1 := src1
        IF (IS_DENORMAL(src2) AND MXCSR.DAZ)
            tmp_src2 := 0
        FI
        IF (IS_DENORMAL(src1) AND MXCSR.DAZ)
            tmp_src1 := 0
        FI
    FI
    dst[63:0] := tmp_src1[63:0] * POW(2.0, FLOOR(tmp_src2[63:0]))
    RETURN dst[63:0]
}
FOR j := 0 to 1
    i := j*64
    dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSCALEFPS - _mm256_mask_scalef_ps

| VSCALEFPS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Scale the packed single-precision (32-bit) floating-point elements in "a" using values from "b", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

DEFINE SCALE(src1, src2) {
    IF (src2 == NaN)
        IF (src2 == SNaN)
            RETURN QNAN(src2)
        FI
    ELSE IF (src1 == NaN)
        IF (src1 == SNaN)
            RETURN QNAN(src1)
        FI
        IF (src2 != INF)
            RETURN QNAN(src1)
        FI
    ELSE
        tmp_src2 := src2
        tmp_src1 := src1
        IF (IS_DENORMAL(src2) AND MXCSR.DAZ)
            tmp_src2 := 0
        FI
        IF (IS_DENORMAL(src1) AND MXCSR.DAZ)
            tmp_src1 := 0
        FI
    FI
    dst[31:0] := tmp_src1[31:0] * POW(2.0, FLOOR(tmp_src2[31:0]))
    RETURN dst[31:0]
}
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VSCALEFPS - _mm256_maskz_scalef_ps

| VSCALEFPS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Scale the packed single-precision (32-bit) floating-point elements in "a" using values from "b", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE SCALE(src1, src2) {
    IF (src2 == NaN)
        IF (src2 == SNaN)
            RETURN QNAN(src2)
        FI
    ELSE IF (src1 == NaN)
        IF (src1 == SNaN)
            RETURN QNAN(src1)
        FI
        IF (src2 != INF)
            RETURN QNAN(src1)
        FI
    ELSE
        tmp_src2 := src2
        tmp_src1 := src1
        IF (IS_DENORMAL(src2) AND MXCSR.DAZ)
            tmp_src2 := 0
        FI
        IF (IS_DENORMAL(src1) AND MXCSR.DAZ)
            tmp_src1 := 0
        FI
    FI
    dst[31:0] := tmp_src1[31:0] * POW(2.0, FLOOR(tmp_src2[31:0]))
    RETURN dst[31:0]
}
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VSCALEFPS - _mm256_scalef_ps

| VSCALEFPS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Scale the packed single-precision (32-bit) floating-point elements in "a" using values from "b", and store the
results in "dst".

[algorithm]

DEFINE SCALE(src1, src2) {
    IF (src2 == NaN)
        IF (src2 == SNaN)
            RETURN QNAN(src2)
        FI
    ELSE IF (src1 == NaN)
        IF (src1 == SNaN)
            RETURN QNAN(src1)
        FI
        IF (src2 != INF)
            RETURN QNAN(src1)
        FI
    ELSE
        tmp_src2 := src2
        tmp_src1 := src1
        IF (IS_DENORMAL(src2) AND MXCSR.DAZ)
            tmp_src2 := 0
        FI
        IF (IS_DENORMAL(src1) AND MXCSR.DAZ)
            tmp_src1 := 0
        FI
    FI
    dst[31:0] := tmp_src1[31:0] * POW(2.0, FLOOR(tmp_src2[31:0]))
    RETURN dst[31:0]
}
FOR j := 0 to 7
    i := j*32
    dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VSCALEFPS - _mm_mask_scalef_ps

| VSCALEFPS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Scale the packed single-precision (32-bit) floating-point elements in "a" using values from "b", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

DEFINE SCALE(src1, src2) {
    IF (src2 == NaN)
        IF (src2 == SNaN)
            RETURN QNAN(src2)
        FI
    ELSE IF (src1 == NaN)
        IF (src1 == SNaN)
            RETURN QNAN(src1)
        FI
        IF (src2 != INF)
            RETURN QNAN(src1)
        FI
    ELSE
        tmp_src2 := src2
        tmp_src1 := src1
        IF (IS_DENORMAL(src2) AND MXCSR.DAZ)
            tmp_src2 := 0
        FI
        IF (IS_DENORMAL(src1) AND MXCSR.DAZ)
            tmp_src1 := 0
        FI
    FI
    dst[31:0] := tmp_src1[31:0] * POW(2.0, FLOOR(tmp_src2[31:0]))
    RETURN dst[31:0]
}
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSCALEFPS - _mm_maskz_scalef_ps

| VSCALEFPS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Scale the packed single-precision (32-bit) floating-point elements in "a" using values from "b", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE SCALE(src1, src2) {
    IF (src2 == NaN)
        IF (src2 == SNaN)
            RETURN QNAN(src2)
        FI
    ELSE IF (src1 == NaN)
        IF (src1 == SNaN)
            RETURN QNAN(src1)
        FI
        IF (src2 != INF)
            RETURN QNAN(src1)
        FI
    ELSE
        tmp_src2 := src2
        tmp_src1 := src1
        IF (IS_DENORMAL(src2) AND MXCSR.DAZ)
            tmp_src2 := 0
        FI
        IF (IS_DENORMAL(src1) AND MXCSR.DAZ)
            tmp_src1 := 0
        FI
    FI
    dst[31:0] := tmp_src1[31:0] * POW(2.0, FLOOR(tmp_src2[31:0]))
    RETURN dst[31:0]
}
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSCALEFPS - _mm_scalef_ps

| VSCALEFPS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Scale the packed single-precision (32-bit) floating-point elements in "a" using values from "b", and store the
results in "dst".

[algorithm]

DEFINE SCALE(src1, src2) {
    IF (src2 == NaN)
        IF (src2 == SNaN)
            RETURN QNAN(src2)
        FI
    ELSE IF (src1 == NaN)
        IF (src1 == SNaN)
            RETURN QNAN(src1)
        FI
        IF (src2 != INF)
            RETURN QNAN(src1)
        FI
    ELSE
        tmp_src2 := src2
        tmp_src1 := src1
        IF (IS_DENORMAL(src2) AND MXCSR.DAZ)
            tmp_src2 := 0
        FI
        IF (IS_DENORMAL(src1) AND MXCSR.DAZ)
            tmp_src1 := 0
        FI
    FI
    dst[31:0] := tmp_src1[31:0] * POW(2.0, FLOOR(tmp_src2[31:0]))
    RETURN dst[31:0]
}
FOR j := 0 to 3
    i := j*32
    dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSCATTERDPD - _mm256_i32scatter_pd

| VSCATTERDPD_MEMf64_MASKmskw_YMMf64_AVX512_VL256

--------------------------------------------------------------------------------------------------------------
Scatter double-precision (64-bit) floating-point elements from "a" into memory using 32-bit indices. 64-bit
elements are stored at addresses starting at "base_addr" and offset by each 32-bit element in "vindex" (each
index is scaled by the factor in "scale"). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 3
    i := j*64
    m := j*32
    addr := base_addr + SignExtend64(vindex[m+31:m]) * ZeroExtend64(scale) * 8
    MEM[addr+63:addr] := a[i+63:i]
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VSCATTERDPD - _mm256_mask_i32scatter_pd

| VSCATTERDPD_MEMf64_MASKmskw_YMMf64_AVX512_VL256

--------------------------------------------------------------------------------------------------------------
Scatter double-precision (64-bit) floating-point elements from "a" into memory using 32-bit indices. 64-bit
elements are stored at addresses starting at "base_addr" and offset by each 32-bit element in "vindex" (each
index is scaled by the factor in "scale") subject to mask "k" (elements are not stored when the corresponding
mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 3
    i := j*64
    m := j*32
    IF k[j]
        addr := base_addr + SignExtend64(vindex[m+31:m]) * ZeroExtend64(scale) * 8
        MEM[addr+63:addr] := a[i+63:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VSCATTERDPD - _mm_i32scatter_pd

| VSCATTERDPD_MEMf64_MASKmskw_XMMf64_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Scatter double-precision (64-bit) floating-point elements from "a" into memory using 32-bit indices. 64-bit
elements are stored at addresses starting at "base_addr" and offset by each 32-bit element in "vindex" (each
index is scaled by the factor in "scale"). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 1
    i := j*64
    m := j*32
    addr := base_addr + SignExtend64(vindex[m+31:m]) * ZeroExtend64(scale) * 8
    MEM[addr+63:addr] := a[i+63:i]
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VSCATTERDPD - _mm_mask_i32scatter_pd

| VSCATTERDPD_MEMf64_MASKmskw_XMMf64_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Scatter double-precision (64-bit) floating-point elements from "a" into memory using 32-bit indices. 64-bit
elements are stored at addresses starting at "base_addr" and offset by each 32-bit element in "vindex" (each
index is scaled by the factor in "scale") subject to mask "k" (elements are not stored when the corresponding
mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 1
    i := j*64
    m := j*32
    IF k[j]
        addr := base_addr + SignExtend64(vindex[m+31:m]) * ZeroExtend64(scale) * 8
        MEM[addr+63:addr] := a[i+63:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VSCATTERDPS - _mm256_i32scatter_ps

| VSCATTERDPS_MEMf32_MASKmskw_YMMf32_AVX512_VL256

--------------------------------------------------------------------------------------------------------------
Scatter single-precision (32-bit) floating-point elements from "a" into memory using 32-bit indices. 32-bit
elements are stored at addresses starting at "base_addr" and offset by each 32-bit element in "vindex" (each
index is scaled by the factor in "scale"). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 7
    i := j*32
    m := j*32
    addr := base_addr + SignExtend64(vindex[m+31:m]) * ZeroExtend64(scale) * 8
    MEM[addr+31:addr] := a[i+31:i]
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VSCATTERDPS - _mm256_mask_i32scatter_ps

| VSCATTERDPS_MEMf32_MASKmskw_YMMf32_AVX512_VL256

--------------------------------------------------------------------------------------------------------------
Scatter single-precision (32-bit) floating-point elements from "a" into memory using 32-bit indices. 32-bit
elements are stored at addresses starting at "base_addr" and offset by each 32-bit element in "vindex" (each
index is scaled by the factor in "scale") subject to mask "k" (elements are not stored when the corresponding
mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 7
    i := j*32
    m := j*32
    IF k[j]
        addr := base_addr + SignExtend64(vindex[m+31:m]) * ZeroExtend64(scale) * 8
        MEM[addr+31:addr] := a[i+31:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VSCATTERDPS - _mm_i32scatter_ps

| VSCATTERDPS_MEMf32_MASKmskw_XMMf32_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Scatter single-precision (32-bit) floating-point elements from "a" into memory using 32-bit indices. 32-bit
elements are stored at addresses starting at "base_addr" and offset by each 32-bit element in "vindex" (each
index is scaled by the factor in "scale"). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 3
    i := j*32
    m := j*32
    addr := base_addr + SignExtend64(vindex[m+31:m]) * ZeroExtend64(scale) * 8
    MEM[addr+31:addr] := a[i+31:i]
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VSCATTERDPS - _mm_mask_i32scatter_ps

| VSCATTERDPS_MEMf32_MASKmskw_XMMf32_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Scatter single-precision (32-bit) floating-point elements from "a" into memory using 32-bit indices. 32-bit
elements are stored at addresses starting at "base_addr" and offset by each 32-bit element in "vindex" (each
index is scaled by the factor in "scale") subject to mask "k" (elements are not stored when the corresponding
mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 3
    i := j*32
    m := j*32
    IF k[j]
        addr := base_addr + SignExtend64(vindex[m+31:m]) * ZeroExtend64(scale) * 8
        MEM[addr+31:addr] := a[i+31:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VSCATTERQPD - _mm256_i64scatter_pd

| VSCATTERQPD_MEMf64_MASKmskw_YMMf64_AVX512_VL256

--------------------------------------------------------------------------------------------------------------
Scatter double-precision (64-bit) floating-point elements from "a" into memory using 64-bit indices. 64-bit
elements are stored at addresses starting at "base_addr" and offset by each 64-bit element in "vindex" (each
index is scaled by the factor in "scale"). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 3
    i := j*64
    m := j*64
    addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
    MEM[addr+63:addr] := a[i+63:i]
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VSCATTERQPD - _mm256_mask_i64scatter_pd

| VSCATTERQPD_MEMf64_MASKmskw_YMMf64_AVX512_VL256

--------------------------------------------------------------------------------------------------------------
Scatter double-precision (64-bit) floating-point elements from "a" into memory using 64-bit indices. 64-bit
elements are stored at addresses starting at "base_addr" and offset by each 64-bit element in "vindex" (each
index is scaled by the factor in "scale") subject to mask "k" (elements are not stored when the corresponding
mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 3
    i := j*64
    m := j*64
    IF k[j]
        addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
        MEM[addr+63:addr] := a[i+63:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VSCATTERQPD - _mm_i64scatter_pd

| VSCATTERQPD_MEMf64_MASKmskw_XMMf64_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Scatter double-precision (64-bit) floating-point elements from "a" into memory using 64-bit indices. 64-bit
elements are stored at addresses starting at "base_addr" and offset by each 64-bit element in "vindex" (each
index is scaled by the factor in "scale"). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 1
    i := j*64
    m := j*64
    addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
    MEM[addr+63:addr] := a[i+63:i]
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VSCATTERQPD - _mm_mask_i64scatter_pd

| VSCATTERQPD_MEMf64_MASKmskw_XMMf64_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Scatter double-precision (64-bit) floating-point elements from "a" into memory using 64-bit indices. 64-bit
elements are stored at addresses starting at "base_addr" and offset by each 64-bit element in "vindex" (each
index is scaled by the factor in "scale") subject to mask "k" (elements are not stored when the corresponding
mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 1
    i := j*64
    m := j*64
    IF k[j]
        addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
        MEM[addr+63:addr] := a[i+63:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VSCATTERQPS - _mm256_i64scatter_ps

| VSCATTERQPS_MEMf32_MASKmskw_XMMf32_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Scatter single-precision (32-bit) floating-point elements from "a" into memory using 64-bit indices. 32-bit
elements are stored at addresses starting at "base_addr" and offset by each 64-bit element in "vindex" (each
index is scaled by the factor in "scale") subject to mask "k" (elements are not stored when the corresponding
mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 3
    i := j*32
    m := j*64
    addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
    MEM[addr+31:addr] := a[i+31:i]
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VSCATTERQPS - _mm256_mask_i64scatter_ps

| VSCATTERQPS_MEMf32_MASKmskw_XMMf32_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Scatter single-precision (32-bit) floating-point elements from "a" into memory using 64-bit indices. 32-bit
elements are stored at addresses starting at "base_addr" and offset by each 64-bit element in "vindex" (each
index is scaled by the factor in "scale") subject to mask "k" (elements are not stored when the corresponding
mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 3
    i := j*32
    m := j*64
    IF k[j]
        addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
        MEM[addr+31:addr] := a[i+31:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VSCATTERQPS - _mm_i64scatter_ps

| VSCATTERQPS_MEMf32_MASKmskw_XMMf32_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Scatter single-precision (32-bit) floating-point elements from "a" into memory using 64-bit indices. 32-bit
elements are stored at addresses starting at "base_addr" and offset by each 64-bit element in "vindex" (each
index is scaled by the factor in "scale") subject to mask "k" (elements are not stored when the corresponding
mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 1
    i := j*32
    m := j*64
    addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
    MEM[addr+31:addr] := a[i+31:i]
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VSCATTERQPS - _mm_mask_i64scatter_ps

| VSCATTERQPS_MEMf32_MASKmskw_XMMf32_AVX512_VL128

--------------------------------------------------------------------------------------------------------------
Scatter single-precision (32-bit) floating-point elements from "a" into memory using 64-bit indices. 32-bit
elements are stored at addresses starting at "base_addr" and offset by each 64-bit element in "vindex" (each
index is scaled by the factor in "scale") subject to mask "k" (elements are not stored when the corresponding
mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 1
    i := j*32
    m := j*64
    IF k[j]
        addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
        MEM[addr+31:addr] := a[i+31:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VSHUFF32X4 - _mm256_mask_shuffle_f32x4

| VSHUFF32X4_YMMf32_MASKmskw_YMMf32_YMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 128-bits (composed of 4 single-precision (32-bit) floating-point elements) selected by "imm8" from "a"
and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

tmp_dst.m128[0] := a.m128[imm8[0]]
tmp_dst.m128[1] := b.m128[imm8[1]]
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VSHUFF32X4 - _mm256_maskz_shuffle_f32x4

| VSHUFF32X4_YMMf32_MASKmskw_YMMf32_YMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 128-bits (composed of 4 single-precision (32-bit) floating-point elements) selected by "imm8" from "a"
and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask
bit is not set).

[algorithm]

tmp_dst.m128[0] := a.m128[imm8[0]]
tmp_dst.m128[1] := b.m128[imm8[1]]
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VSHUFF32X4 - _mm256_shuffle_f32x4

| VSHUFF32X4_YMMf32_MASKmskw_YMMf32_YMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 128-bits (composed of 4 single-precision (32-bit) floating-point elements) selected by "imm8" from "a"
and "b", and store the results in "dst".

[algorithm]

dst.m128[0] := a.m128[imm8[0]]
dst.m128[1] := b.m128[imm8[1]]
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VSHUFF64X2 - _mm256_mask_shuffle_f64x2

| VSHUFF64X2_YMMf64_MASKmskw_YMMf64_YMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 128-bits (composed of 2 double-precision (64-bit) floating-point elements) selected by "imm8" from "a"
and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

tmp_dst.m128[0] := a.m128[imm8[0]]
tmp_dst.m128[1] := b.m128[imm8[1]]
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VSHUFF64X2 - _mm256_maskz_shuffle_f64x2

| VSHUFF64X2_YMMf64_MASKmskw_YMMf64_YMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 128-bits (composed of 2 double-precision (64-bit) floating-point elements) selected by "imm8" from "a"
and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask
bit is not set).

[algorithm]

tmp_dst.m128[0] := a.m128[imm8[0]]
tmp_dst.m128[1] := b.m128[imm8[1]]
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VSHUFF64X2 - _mm256_shuffle_f64x2

| VSHUFF64X2_YMMf64_MASKmskw_YMMf64_YMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 128-bits (composed of 2 double-precision (64-bit) floating-point elements) selected by "imm8" from "a"
and "b", and store the results in "dst".

[algorithm]

dst.m128[0] := a.m128[imm8[0]]
dst.m128[1] := b.m128[imm8[1]]
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VSHUFI32X4 - _mm256_mask_shuffle_i32x4

| VSHUFI32X4_YMMu32_MASKmskw_YMMu32_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 128-bits (composed of 4 32-bit integers) selected by "imm8" from "a" and "b", and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

tmp_dst.m128[0] := a.m128[imm8[0]]
tmp_dst.m128[1] := b.m128[imm8[1]]
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VSHUFI32X4 - _mm256_maskz_shuffle_i32x4

| VSHUFI32X4_YMMu32_MASKmskw_YMMu32_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 128-bits (composed of 4 32-bit integers) selected by "imm8" from "a" and "b", and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

tmp_dst.m128[0] := a.m128[imm8[0]]
tmp_dst.m128[1] := b.m128[imm8[1]]
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VSHUFI32X4 - _mm256_shuffle_i32x4

| VSHUFI32X4_YMMu32_MASKmskw_YMMu32_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 128-bits (composed of 4 32-bit integers) selected by "imm8" from "a" and "b", and store the results in
"dst".

[algorithm]

dst.m128[0] := a.m128[imm8[0]]
dst.m128[1] := b.m128[imm8[1]]
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VSHUFI64X2 - _mm256_mask_shuffle_i64x2

| VSHUFI64X2_YMMu64_MASKmskw_YMMu64_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 128-bits (composed of 2 64-bit integers) selected by "imm8" from "a" and "b", and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

tmp_dst.m128[0] := a.m128[imm8[0]]
tmp_dst.m128[1] := b.m128[imm8[1]]
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VSHUFI64X2 - _mm256_maskz_shuffle_i64x2

| VSHUFI64X2_YMMu64_MASKmskw_YMMu64_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 128-bits (composed of 2 64-bit integers) selected by "imm8" from "a" and "b", and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

tmp_dst.m128[0] := a.m128[imm8[0]]
tmp_dst.m128[1] := b.m128[imm8[1]]
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VSHUFI64X2 - _mm256_shuffle_i64x2

| VSHUFI64X2_YMMu64_MASKmskw_YMMu64_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 128-bits (composed of 2 64-bit integers) selected by "imm8" from "a" and "b", and store the results in
"dst".

[algorithm]

dst.m128[0] := a.m128[imm8[0]]
dst.m128[1] := b.m128[imm8[1]]
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VSHUFPD - _mm256_mask_shuffle_pd

| VSHUFPD_YMMf64_MASKmskw_YMMf64_YMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements within 128-bit lanes using the control in "imm8",
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

tmp_dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
tmp_dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
tmp_dst[191:128] := (imm8[2] == 0) ? a[191:128] : a[255:192]
tmp_dst[255:192] := (imm8[3] == 0) ? b[191:128] : b[255:192]
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VSHUFPD - _mm256_maskz_shuffle_pd

| VSHUFPD_YMMf64_MASKmskw_YMMf64_YMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements within 128-bit lanes using the control in "imm8",
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

tmp_dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
tmp_dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
tmp_dst[191:128] := (imm8[2] == 0) ? a[191:128] : a[255:192]
tmp_dst[255:192] := (imm8[3] == 0) ? b[191:128] : b[255:192]
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VSHUFPD - _mm_mask_shuffle_pd

| VSHUFPD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements using the control in "imm8", and store the results
in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

tmp_dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
tmp_dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSHUFPD - _mm_maskz_shuffle_pd

| VSHUFPD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements using the control in "imm8", and store the results
in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

tmp_dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
tmp_dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSHUFPS - _mm256_mask_shuffle_ps

| VSHUFPS_YMMf32_MASKmskw_YMMf32_YMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" within 128-bit lanes using the control in
"imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[31:0] := src[31:0]
    1:    tmp[31:0] := src[63:32]
    2:    tmp[31:0] := src[95:64]
    3:    tmp[31:0] := src[127:96]
    ESAC
    RETURN tmp[31:0]
}
tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
tmp_dst[95:64] := SELECT4(b[127:0], imm8[5:4])
tmp_dst[127:96] := SELECT4(b[127:0], imm8[7:6])
tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
tmp_dst[223:192] := SELECT4(b[255:128], imm8[5:4])
tmp_dst[255:224] := SELECT4(b[255:128], imm8[7:6])
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VSHUFPS - _mm256_maskz_shuffle_ps

| VSHUFPS_YMMf32_MASKmskw_YMMf32_YMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" within 128-bit lanes using the control in
"imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask
bit is not set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[31:0] := src[31:0]
    1:    tmp[31:0] := src[63:32]
    2:    tmp[31:0] := src[95:64]
    3:    tmp[31:0] := src[127:96]
    ESAC
    RETURN tmp[31:0]
}
tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
tmp_dst[95:64] := SELECT4(b[127:0], imm8[5:4])
tmp_dst[127:96] := SELECT4(b[127:0], imm8[7:6])
tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
tmp_dst[223:192] := SELECT4(b[255:128], imm8[5:4])
tmp_dst[255:224] := SELECT4(b[255:128], imm8[7:6])
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VSHUFPS - _mm_mask_shuffle_ps

| VSHUFPS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" using the control in "imm8", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[31:0] := src[31:0]
    1:    tmp[31:0] := src[63:32]
    2:    tmp[31:0] := src[95:64]
    3:    tmp[31:0] := src[127:96]
    ESAC
    RETURN tmp[31:0]
}
tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
tmp_dst[95:64] := SELECT4(b[127:0], imm8[5:4])
tmp_dst[127:96] := SELECT4(b[127:0], imm8[7:6])
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSHUFPS - _mm_maskz_shuffle_ps

| VSHUFPS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" using the control in "imm8", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[31:0] := src[31:0]
    1:    tmp[31:0] := src[63:32]
    2:    tmp[31:0] := src[95:64]
    3:    tmp[31:0] := src[127:96]
    ESAC
    RETURN tmp[31:0]
}
tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
tmp_dst[95:64] := SELECT4(b[127:0], imm8[5:4])
tmp_dst[127:96] := SELECT4(b[127:0], imm8[7:6])
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSQRTPD - _mm256_mask_sqrt_pd

| VSQRTPD_YMMf64_MASKmskw_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the square root of packed double-precision (64-bit) floating-point elements in "a", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := SQRT(a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VSQRTPD - _mm256_maskz_sqrt_pd

| VSQRTPD_YMMf64_MASKmskw_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the square root of packed double-precision (64-bit) floating-point elements in "a", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := SQRT(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VSQRTPD - _mm_mask_sqrt_pd

| VSQRTPD_XMMf64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the square root of packed double-precision (64-bit) floating-point elements in "a", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := SQRT(a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSQRTPD - _mm_maskz_sqrt_pd

| VSQRTPD_XMMf64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the square root of packed double-precision (64-bit) floating-point elements in "a", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := SQRT(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSQRTPS - _mm256_mask_sqrt_ps

| VSQRTPS_YMMf32_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the square root of packed single-precision (32-bit) floating-point elements in "a", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := SQRT(a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VSQRTPS - _mm256_maskz_sqrt_ps

| VSQRTPS_YMMf32_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the square root of packed single-precision (32-bit) floating-point elements in "a", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := SQRT(a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VSQRTPS - _mm_mask_sqrt_ps

| VSQRTPS_XMMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the square root of packed single-precision (32-bit) floating-point elements in "a", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := SQRT(a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSQRTPS - _mm_maskz_sqrt_ps

| VSQRTPS_XMMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the square root of packed single-precision (32-bit) floating-point elements in "a", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := SQRT(a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSUBPD - _mm256_mask_sub_pd

| VSUBPD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed double-precision (64-bit) floating-point elements in "b" from packed double-precision (64-bit)
floating-point elements in "a", and store the results in "dst" using writemask "k" (elements are copied from
"src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] - b[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VSUBPD - _mm256_maskz_sub_pd

| VSUBPD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed double-precision (64-bit) floating-point elements in "b" from packed double-precision (64-bit)
floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] - b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VSUBPD - _mm_mask_sub_pd

| VSUBPD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed double-precision (64-bit) floating-point elements in "b" from packed double-precision (64-bit)
floating-point elements in "a", and store the results in "dst" using writemask "k" (elements are copied from
"src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] - b[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSUBPD - _mm_maskz_sub_pd

| VSUBPD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed double-precision (64-bit) floating-point elements in "b" from packed double-precision (64-bit)
floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] - b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSUBPS - _mm256_mask_sub_ps

| VSUBPS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed single-precision (32-bit) floating-point elements in "b" from packed single-precision (32-bit)
floating-point elements in "a", and store the results in "dst" using writemask "k" (elements are copied from
"src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] - b[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VSUBPS - _mm256_maskz_sub_ps

| VSUBPS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed single-precision (32-bit) floating-point elements in "b" from packed single-precision (32-bit)
floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] - b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI    
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VSUBPS - _mm_mask_sub_ps

| VSUBPS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed single-precision (32-bit) floating-point elements in "b" from packed single-precision (32-bit)
floating-point elements in "a", and store the results in "dst" using writemask "k" (elements are copied from
"src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] - b[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSUBPS - _mm_maskz_sub_ps

| VSUBPS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed single-precision (32-bit) floating-point elements in "b" from packed single-precision (32-bit)
floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] - b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI    
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VUNPCKHPD - _mm256_mask_unpackhi_pd

| VUNPCKHPD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave double-precision (64-bit) floating-point elements from the high half of each 128-bit
lane in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]) {
    dst[63:0] := src1[127:64] 
    dst[127:64] := src2[127:64] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VUNPCKHPD - _mm256_maskz_unpackhi_pd

| VUNPCKHPD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave double-precision (64-bit) floating-point elements from the high half of each 128-bit
lane in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]) {
    dst[63:0] := src1[127:64] 
    dst[127:64] := src2[127:64] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VUNPCKHPD - _mm_mask_unpackhi_pd

| VUNPCKHPD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave double-precision (64-bit) floating-point elements from the high half of "a" and "b", and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

DEFINE INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]) {
    dst[63:0] := src1[127:64] 
    dst[127:64] := src2[127:64] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VUNPCKHPD - _mm_maskz_unpackhi_pd

| VUNPCKHPD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave double-precision (64-bit) floating-point elements from the high half of "a" and "b", and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

DEFINE INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]) {
    dst[63:0] := src1[127:64] 
    dst[127:64] := src2[127:64] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VUNPCKHPS - _mm256_mask_unpackhi_ps

| VUNPCKHPS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave single-precision (32-bit) floating-point elements from the high half of each 128-bit
lane in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]) {
    dst[31:0] := src1[95:64] 
    dst[63:32] := src2[95:64] 
    dst[95:64] := src1[127:96] 
    dst[127:96] := src2[127:96] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VUNPCKHPS - _mm256_maskz_unpackhi_ps

| VUNPCKHPS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave single-precision (32-bit) floating-point elements from the high half of each 128-bit
lane in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]) {
    dst[31:0] := src1[95:64] 
    dst[63:32] := src2[95:64] 
    dst[95:64] := src1[127:96] 
    dst[127:96] := src2[127:96] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VUNPCKHPS - _mm_mask_unpackhi_ps

| VUNPCKHPS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave single-precision (32-bit) floating-point elements from the high half of "a" and "b", and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

DEFINE INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]) {
    dst[31:0] := src1[95:64] 
    dst[63:32] := src2[95:64] 
    dst[95:64] := src1[127:96] 
    dst[127:96] := src2[127:96] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VUNPCKHPS - _mm_maskz_unpackhi_ps

| VUNPCKHPS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave single-precision (32-bit) floating-point elements from the high half of "a" and "b", and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

DEFINE INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]) {
    dst[31:0] := src1[95:64] 
    dst[63:32] := src2[95:64] 
    dst[95:64] := src1[127:96] 
    dst[127:96] := src2[127:96] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VUNPCKLPD - _mm256_mask_unpacklo_pd

| VUNPCKLPD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave double-precision (64-bit) floating-point elements from the low half of each 128-bit lane
in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_QWORDS(src1[127:0], src2[127:0]) {
    dst[63:0] := src1[63:0] 
    dst[127:64] := src2[63:0] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VUNPCKLPD - _mm256_maskz_unpacklo_pd

| VUNPCKLPD_YMMf64_MASKmskw_YMMf64_YMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave double-precision (64-bit) floating-point elements from the low half of each 128-bit lane
in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_QWORDS(src1[127:0], src2[127:0]) {
    dst[63:0] := src1[63:0] 
    dst[127:64] := src2[63:0] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VUNPCKLPD - _mm_mask_unpacklo_pd

| VUNPCKLPD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave double-precision (64-bit) floating-point elements from the low half of "a" and "b", and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

DEFINE INTERLEAVE_QWORDS(src1[127:0], src2[127:0]) {
    dst[63:0] := src1[63:0] 
    dst[127:64] := src2[63:0] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VUNPCKLPD - _mm_maskz_unpacklo_pd

| VUNPCKLPD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave double-precision (64-bit) floating-point elements from the low half of "a" and "b", and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

DEFINE INTERLEAVE_QWORDS(src1[127:0], src2[127:0]) {
    dst[63:0] := src1[63:0] 
    dst[127:64] := src2[63:0] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VUNPCKLPS - _mm256_mask_unpacklo_ps

| VUNPCKLPS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave single-precision (32-bit) floating-point elements from the low half of each 128-bit lane
in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_DWORDS(src1[127:0], src2[127:0]) {
    dst[31:0] := src1[31:0] 
    dst[63:32] := src2[31:0] 
    dst[95:64] := src1[63:32] 
    dst[127:96] := src2[63:32] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VUNPCKLPS - _mm256_maskz_unpacklo_ps

| VUNPCKLPS_YMMf32_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave single-precision (32-bit) floating-point elements from the low half of each 128-bit lane
in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_DWORDS(src1[127:0], src2[127:0]) {
    dst[31:0] := src1[31:0] 
    dst[63:32] := src2[31:0] 
    dst[95:64] := src1[63:32] 
    dst[127:96] := src2[63:32] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VUNPCKLPS - _mm_mask_unpacklo_ps

| VUNPCKLPS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave single-precision (32-bit) floating-point elements from the low half of "a" and "b", and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

DEFINE INTERLEAVE_DWORDS(src1[127:0], src2[127:0]) {
    dst[31:0] := src1[31:0] 
    dst[63:32] := src2[31:0] 
    dst[95:64] := src1[63:32] 
    dst[127:96] := src2[63:32] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VUNPCKLPS - _mm_maskz_unpacklo_ps

| VUNPCKLPS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave single-precision (32-bit) floating-point elements from the low half of "a" and "b", and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

DEFINE INTERLEAVE_DWORDS(src1[127:0], src2[127:0]) {
    dst[31:0] := src1[31:0] 
    dst[63:32] := src2[31:0] 
    dst[95:64] := src1[63:32] 
    dst[127:96] := src2[63:32] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU64 - _mm512_storeu_epi64

| VMOVDQU64_MEMu64_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Store 512-bits (composed of 8 packed 64-bit integers) from "a" into memory.
		"mem_addr" does not need to be
aligned on any particular boundary.

[algorithm]

MEM[mem_addr+511:mem_addr] := a[511:0]

--------------------------------------------------------------------------------------------------------------

## VMOVDQU32 - _mm512_storeu_epi32

| VMOVDQU32_MEMu32_MASKmskw_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Store 512-bits (composed of 16 packed 32-bit integers) from "a" into memory.
		"mem_addr" does not need to be
aligned on any particular boundary.

[algorithm]

MEM[mem_addr+511:mem_addr] := a[511:0]

--------------------------------------------------------------------------------------------------------------

## VMOVDQU64 - _mm256_storeu_epi64

| VMOVDQU64_MEMu64_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Store 256-bits (composed of 4 packed 64-bit integers) from "a" into memory.
		"mem_addr" does not need to be
aligned on any particular boundary.

[algorithm]

MEM[mem_addr+255:mem_addr] := a[255:0]

--------------------------------------------------------------------------------------------------------------

## VMOVDQU32 - _mm256_storeu_epi32

| VMOVDQU32_MEMu32_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Store 256-bits (composed of 8 packed 32-bit integers) from "a" into memory.
		"mem_addr" does not need to be
aligned on any particular boundary.

[algorithm]

MEM[mem_addr+255:mem_addr] := a[255:0]

--------------------------------------------------------------------------------------------------------------

## VMOVDQU64 - _mm_storeu_epi64

| VMOVDQU64_MEMu64_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Store 128-bits (composed of 2 packed 64-bit integers) from "a" into memory.
		"mem_addr" does not need to be
aligned on any particular boundary.

[algorithm]

MEM[mem_addr+127:mem_addr] := a[127:0]

--------------------------------------------------------------------------------------------------------------

## VMOVDQU32 - _mm_storeu_epi32

| VMOVDQU32_MEMu32_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Store 128-bits (composed of 4 packed 32-bit integers) from "a" into memory.
		"mem_addr" does not need to be
aligned on any particular boundary.

[algorithm]

MEM[mem_addr+127:mem_addr] := a[127:0]

--------------------------------------------------------------------------------------------------------------

## VMOVDQA64 - _mm256_store_epi64

| VMOVDQA64_MEMu64_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Store 256-bits (composed of 4 packed 64-bit integers) from "a" into memory.
		"mem_addr" must be aligned on a
32-byte boundary or a general-protection exception may be generated.

[algorithm]

MEM[mem_addr+255:mem_addr] := a[255:0]

--------------------------------------------------------------------------------------------------------------

## VMOVDQA32 - _mm256_store_epi32

| VMOVDQA32_MEMu32_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Store 256-bits (composed of 8 packed 32-bit integers) from "a" into memory.
		"mem_addr" must be aligned on a
32-byte boundary or a general-protection exception may be generated.

[algorithm]

MEM[mem_addr+255:mem_addr] := a[255:0]

--------------------------------------------------------------------------------------------------------------

## VMOVDQA64 - _mm_store_epi64

| VMOVDQA64_MEMu64_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Store 128-bits (composed of 2 packed 64-bit integers) from "a" into memory.
		"mem_addr" must be aligned on a
16-byte boundary or a general-protection exception may be generated.

[algorithm]

MEM[mem_addr+127:mem_addr] := a[127:0]

--------------------------------------------------------------------------------------------------------------

## VMOVDQA32 - _mm_store_epi32

| VMOVDQA32_MEMu32_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Store 128-bits (composed of 4 packed 32-bit integers) from "a" into memory.
		"mem_addr" must be aligned on a
16-byte boundary or a general-protection exception may be generated.

[algorithm]

MEM[mem_addr+127:mem_addr] := a[127:0]

--------------------------------------------------------------------------------------------------------------

## VMOVDQU64 - _mm512_loadu_epi64

| VMOVDQU64_ZMMu64_MASKmskw_MEMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Load 512-bits (composed of 8 packed 64-bit integers) from memory into "dst".
		"mem_addr" does not need to be
aligned on any particular boundary.

[algorithm]

dst[511:0] := MEM[mem_addr+511:mem_addr]
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU32 - _mm512_loadu_epi32

| VMOVDQU32_ZMMu32_MASKmskw_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Load 512-bits (composed of 16 packed 32-bit integers) from memory into "dst".
		"mem_addr" does not need to be
aligned on any particular boundary.

[algorithm]

dst[511:0] := MEM[mem_addr+511:mem_addr]
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU64 - _mm256_loadu_epi64

| VMOVDQU64_YMMu64_MASKmskw_MEMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Load 256-bits (composed of 4 packed 64-bit integers) from memory into "dst".
		"mem_addr" does not need to be
aligned on any particular boundary.

[algorithm]

dst[255:0] := MEM[mem_addr+255:mem_addr]
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU32 - _mm256_loadu_epi32

| VMOVDQU32_YMMu32_MASKmskw_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Load 256-bits (composed of 8 packed 32-bit integers) from memory into "dst".
		"mem_addr" does not need to be
aligned on any particular boundary.

[algorithm]

dst[255:0] := MEM[mem_addr+255:mem_addr]
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU64 - _mm_loadu_epi64

| VMOVDQU64_XMMu64_MASKmskw_MEMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Load 128-bits (composed of 2 packed 64-bit integers) from memory into "dst".
		"mem_addr" does not need to be
aligned on any particular boundary.

[algorithm]

dst[127:0] := MEM[mem_addr+127:mem_addr]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU32 - _mm_loadu_epi32

| VMOVDQU32_XMMu32_MASKmskw_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Load 128-bits (composed of 4 packed 32-bit integers) from memory into "dst".
		"mem_addr" does not need to be
aligned on any particular boundary.

[algorithm]

dst[127:0] := MEM[mem_addr+127:mem_addr]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQA64 - _mm256_load_epi64

| VMOVDQA64_YMMu64_MASKmskw_MEMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Load 256-bits (composed of 4 packed 64-bit integers) from memory into "dst".
		"mem_addr" must be aligned on a
32-byte boundary or a general-protection exception may be generated.

[algorithm]

dst[255:0] := MEM[mem_addr+255:mem_addr]
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQA32 - _mm256_load_epi32

| VMOVDQA32_YMMu32_MASKmskw_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Load 256-bits (composed of 8 packed 32-bit integers) from memory into "dst".
		"mem_addr" must be aligned on a
32-byte boundary or a general-protection exception may be generated.

[algorithm]

dst[255:0] := MEM[mem_addr+255:mem_addr]
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQA64 - _mm_load_epi64

| VMOVDQA64_XMMu64_MASKmskw_MEMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Load 128-bits (composed of 2 packed 64-bit integers) from memory into "dst".
		"mem_addr" must be aligned on a
16-byte boundary or a general-protection exception may be generated.

[algorithm]

dst[127:0] := MEM[mem_addr+127:mem_addr]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQA32 - _mm_load_epi32

| VMOVDQA32_XMMu32_MASKmskw_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Load 128-bits (composed of 4 packed 32-bit integers) from memory into "dst".
		"mem_addr" must be aligned on a
16-byte boundary or a general-protection exception may be generated.

[algorithm]

dst[127:0] := MEM[mem_addr+127:mem_addr]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPXORQ - _mm256_xor_epi64

| VPXORQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XOR of packed 64-bit integers in "a" and "b", and store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPXORD - _mm256_xor_epi32

| VPXORD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XOR of packed 32-bit integers in "a" and "b", and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*32
    dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPXORQ - _mm_xor_epi64

| VPXORQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XOR of packed 64-bit integers in "a" and "b", and store the results in "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPXORD - _mm_xor_epi32

| VPXORD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XOR of packed 32-bit integers in "a" and "b", and store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*32
    dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPORQ - _mm256_or_epi64

| VPORQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of packed 64-bit integers in "a" and "b", and store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    dst[i+63:i] := a[i+63:i] OR b[i+63:i]
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPORD - _mm256_or_epi32

| VPORD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of packed 32-bit integers in "a" and "b", and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*32
    dst[i+31:i] := a[i+31:i] OR b[i+31:i]
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPORQ - _mm_or_epi64

| VPORQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of packed 64-bit integers in "a" and "b", and store the results in "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    dst[i+63:i] := a[i+63:i] OR b[i+63:i]
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPORD - _mm_or_epi32

| VPORD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of packed 32-bit integers in "a" and "b", and store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*32
    dst[i+31:i] := a[i+31:i] OR b[i+31:i]
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## KANDW - _kand_mask16

| KANDW_MASKmskw_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of 16-bit masks "a" and "b", and store the result in "k".

[algorithm]

k[15:0] := a[15:0] AND b[15:0]
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## KANDNW - _kandn_mask16

| KANDNW_MASKmskw_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of 16-bit masks "a" and then AND with "b", and store the result in "k".

[algorithm]

k[15:0] := (NOT a[15:0]) AND b[15:0]
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## KNOTW - _knot_mask16

| KNOTW_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of 16-bit mask "a", and store the result in "k".

[algorithm]

k[15:0] := NOT a[15:0]
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## KORW - _kor_mask16

| KORW_MASKmskw_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of 16-bit masks "a" and "b", and store the result in "k".

[algorithm]

k[15:0] := a[15:0] OR b[15:0]
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## KXNORW - _kxnor_mask16

| KXNORW_MASKmskw_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XNOR of 16-bit masks "a" and "b", and store the result in "k".

[algorithm]

k[15:0] := NOT (a[15:0] XOR b[15:0])
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## KXORW - _kxor_mask16

| KXORW_MASKmskw_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XOR of 16-bit masks "a" and "b", and store the result in "k".

[algorithm]

k[15:0] := a[15:0] XOR b[15:0]
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## KSHIFTLW - _kshiftli_mask16

| KSHIFTLW_MASKmskw_MASKmskw_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift the bits of 16-bit mask "a" left by "count" while shifting in zeros, and store the least significant 16
bits of the result in "k".

[algorithm]

k[MAX:0] := 0
IF count[7:0] &lt;= 15
    k[15:0] := a[15:0] &lt;&lt; count[7:0]
FI

--------------------------------------------------------------------------------------------------------------

## KSHIFTRW - _kshiftri_mask16

| KSHIFTRW_MASKmskw_MASKmskw_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift the bits of 16-bit mask "a" right by "count" while shifting in zeros, and store the least significant 16
bits of the result in "k".

[algorithm]

k[MAX:0] := 0
IF count[7:0] &lt;= 15
    k[15:0] := a[15:0] &gt;&gt; count[7:0]
FI

--------------------------------------------------------------------------------------------------------------

## KMOVW - _load_mask16

| KMOVW_MASKmskw_MEMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Load 16-bit mask from memory into "k".

[algorithm]

k[15:0] := MEM[mem_addr+15:mem_addr]

--------------------------------------------------------------------------------------------------------------

## KMOVW - _store_mask16

| KMOVW_MEMu16_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Store 16-bit mask from "a" into memory.

[algorithm]

MEM[mem_addr+15:mem_addr] := a[15:0]

--------------------------------------------------------------------------------------------------------------

## KORTESTW - _kortest_mask16_u8

| KORTESTW_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of 16-bit masks "a" and "b". If the result is all zeros, store 1 in "dst", otherwise
store 0 in "dst". If the result is all ones, store 1 in "all_ones", otherwise store 0 in "all_ones".

[algorithm]

tmp[15:0] := a[15:0] OR b[15:0]
IF tmp[15:0] == 0x0
    dst := 1
ELSE
    dst := 0
FI
IF tmp[15:0] == 0xFFFF
    MEM[all_ones+7:all_ones] := 1
ELSE
    MEM[all_ones+7:all_ones] := 0
FI

--------------------------------------------------------------------------------------------------------------

## KORTESTW - _kortestz_mask16_u8

| KORTESTW_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of 16-bit masks "a" and "b". If the result is all zeroes, store 1 in "dst", otherwise
store 0 in "dst".

[algorithm]

tmp[15:0] := a[15:0] OR b[15:0]
IF tmp[15:0] == 0x0
    dst := 1
ELSE
    dst := 0
FI

--------------------------------------------------------------------------------------------------------------

## KORTESTW - _kortestc_mask16_u8

| KORTESTW_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of 16-bit masks "a" and "b". If the result is all ones, store 1 in "dst", otherwise
store 0 in "dst".

[algorithm]

tmp[15:0] := a[15:0] OR b[15:0]
IF tmp[15:0] == 0xFFFF
    dst := 1
ELSE
    dst := 0
FI

--------------------------------------------------------------------------------------------------------------

## KMOVW - _cvtmask16_u32

| KMOVW_GPR32u32_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Convert 16-bit mask "a" into an integer value, and store the result in "dst".

[algorithm]

dst := ZeroExtend32(a[15:0])

--------------------------------------------------------------------------------------------------------------

## KMOVW - _cvtu32_mask16

| KMOVW_MASKmskw_GPR32u32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert integer value "a" into an 16-bit mask, and store the result in "k".

[algorithm]

k := ZeroExtend16(a[15:0])

--------------------------------------------------------------------------------------------------------------

## KANDNW - _mm512_kandn

| KANDNW_MASKmskw_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of 16-bit masks "a" and then AND with "b", and store the result in "k".

[algorithm]

k[15:0] := (NOT a[15:0]) AND b[15:0]
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## KANDW - _mm512_kand

| KANDW_MASKmskw_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of 16-bit masks "a" and "b", and store the result in "k".

[algorithm]

k[15:0] := a[15:0] AND b[15:0]
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## KMOVW - _mm512_kmov

| KMOVW_MASKmskw_MASKu16_AVX512

--------------------------------------------------------------------------------------------------------------
Copy 16-bit mask "a" to "k".

[algorithm]

k[15:0] := a[15:0]
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## KNOTW - _mm512_knot

| KNOTW_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of 16-bit mask "a", and store the result in "k".

[algorithm]

k[15:0] := NOT a[15:0]
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## KORW - _mm512_kor

| KORW_MASKmskw_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of 16-bit masks "a" and "b", and store the result in "k".

[algorithm]

k[15:0] := a[15:0] OR b[15:0]
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## KUNPCKBW - _mm512_kunpackb

| KUNPCKBW_MASKmskw_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 8 bits from masks "a" and "b", and store the 16-bit result in "k".

[algorithm]

k[7:0] := b[7:0]
k[15:8] := a[7:0]
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## KXNORW - _mm512_kxnor

| KXNORW_MASKmskw_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XNOR of 16-bit masks "a" and "b", and store the result in "k".

[algorithm]

k[15:0] := NOT (a[15:0] XOR b[15:0])
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## KXORW - _mm512_kxor

| KXORW_MASKmskw_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XOR of 16-bit masks "a" and "b", and store the result in "k".

[algorithm]

k[15:0] := a[15:0] XOR b[15:0]
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VADDPD - _mm512_maskz_add_pd

| VADDPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] + b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VADDPD - _mm512_maskz_add_round_pd

| VADDPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] + b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VADDPS - _mm512_maskz_add_ps

| VADDPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] + b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VADDPS - _mm512_maskz_add_round_ps

| VADDPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	[round_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] + b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VADDSD - _mm_add_round_sd

| VADDSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Add the lower double-precision (64-bit) floating-point element in "a" and "b", store the result in the lower
element of "dst", and copy the upper element from "a" to the upper element of "dst".
	[round_note]

[algorithm]

dst[63:0] := a[63:0] + b[63:0]
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VADDSD - _mm_mask_add_round_sd

| VADDSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Add the lower double-precision (64-bit) floating-point element in "a" and "b", store the result in the lower
element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy
the upper element from "a" to the upper element of "dst".
		[round_note]

[algorithm]

IF k[0]
    dst[63:0] := a[63:0] + b[63:0]
ELSE
    dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VADDSD - _mm_mask_add_sd

| VADDSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Add the lower double-precision (64-bit) floating-point element in "a" and "b", store the result in the lower
element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy
the upper element from "a" to the upper element of "dst".

[algorithm]

IF k[0]
    dst[63:0] := a[63:0] + b[63:0]
ELSE
    dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VADDSD - _mm_maskz_add_round_sd

| VADDSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Add the lower double-precision (64-bit) floating-point element in "a" and "b", store the result in the lower
element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper
element from "a" to the upper element of "dst".
		[round_note]

[algorithm]

IF k[0]
    dst[63:0] := a[63:0] + b[63:0]
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VADDSD - _mm_maskz_add_sd

| VADDSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Add the lower double-precision (64-bit) floating-point element in "a" and "b", store the result in the lower
element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper
element from "a" to the upper element of "dst".

[algorithm]

IF k[0]
    dst[63:0] := a[63:0] + b[63:0]
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VADDSS - _mm_add_round_ss

| VADDSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Add the lower single-precision (32-bit) floating-point element in "a" and "b", store the result in the lower
element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of "dst".
		[round_note]

[algorithm]

dst[31:0] := a[31:0] + b[31:0]
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VADDSS - _mm_mask_add_round_ss

| VADDSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Add the lower single-precision (32-bit) floating-point element in "a" and "b", store the result in the lower
element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy
the upper 3 packed elements from "a" to the upper elements of "dst". 
		[round_note]

[algorithm]

IF k[0]
    dst[31:0] := a[31:0] + b[31:0]
ELSE
    dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VADDSS - _mm_mask_add_ss

| VADDSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Add the lower single-precision (32-bit) floating-point element in "a" and "b", store the result in the lower
element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy
the upper 3 packed elements from "a" to the upper elements of "dst".

[algorithm]

IF k[0]
    dst[31:0] := a[31:0] + b[31:0]
ELSE
    dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VADDSS - _mm_maskz_add_round_ss

| VADDSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Add the lower single-precision (32-bit) floating-point element in "a" and "b", store the result in the lower
element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper
3 packed elements from "a" to the upper elements of "dst".
	[round_note]

[algorithm]

IF k[0]
    dst[31:0] := a[31:0] + b[31:0]
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VADDSS - _mm_maskz_add_ss

| VADDSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Add the lower single-precision (32-bit) floating-point element in "a" and "b", store the result in the lower
element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper
3 packed elements from "a" to the upper elements of "dst".

[algorithm]

IF k[0]
    dst[31:0] := a[31:0] + b[31:0]
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VALIGND - _mm512_maskz_alignr_epi32

| VALIGND_ZMMu32_MASKmskw_ZMMu32_ZMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate "a" and "b" into a 128-byte immediate result, shift the result right by "imm8" 32-bit elements,
and stores the low 64 bytes (16 elements) in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

temp[1023:512] := a[511:0]
temp[511:0] := b[511:0]
temp[1023:0] := temp[1023:0] &gt;&gt; (32*imm8[3:0])
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := temp[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VALIGNQ - _mm512_alignr_epi64

| VALIGNQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate "a" and "b" into a 128-byte immediate result, shift the result right by "imm8" 64-bit elements,
and store the low 64 bytes (8 elements) in "dst".

[algorithm]

temp[1023:512] := a[511:0]
temp[511:0] := b[511:0]
temp[1023:0] := temp[1023:0] &gt;&gt; (64*imm8[2:0])
dst[511:0] := temp[511:0]
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VALIGNQ - _mm512_mask_alignr_epi64

| VALIGNQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate "a" and "b" into a 128-byte immediate result, shift the result right by "imm8" 64-bit elements,
and store the low 64 bytes (8 elements) in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

temp[1023:512] := a[511:0]
temp[511:0] := b[511:0]
temp[1023:0] := temp[1023:0] &gt;&gt; (64*imm8[2:0])
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := temp[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VALIGNQ - _mm512_maskz_alignr_epi64

| VALIGNQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate "a" and "b" into a 128-byte immediate result, shift the result right by "imm8" 64-bit elements,
and stores the low 64 bytes (8 elements) in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

temp[1023:512] := a[511:0]
temp[511:0] := b[511:0]
temp[1023:0] := temp[1023:0] &gt;&gt; (64*imm8[2:0])
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := temp[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTF32X4 - _mm512_broadcast_f32x4

| VBROADCASTF32X4_ZMMf32_MASKmskw_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the 4 packed single-precision (32-bit) floating-point elements from "a" to all elements of "dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    n := (j % 4)*32
    dst[i+31:i] := a[n+31:n]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTF32X4 - _mm512_mask_broadcast_f32x4

| VBROADCASTF32X4_ZMMf32_MASKmskw_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the 4 packed single-precision (32-bit) floating-point elements from "a" to all elements of "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    n := (j % 4)*32
    IF k[j]
        dst[i+31:i] := a[n+31:n]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTF32X4 - _mm512_maskz_broadcast_f32x4

| VBROADCASTF32X4_ZMMf32_MASKmskw_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the 4 packed single-precision (32-bit) floating-point elements from "a" to all elements of "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    n := (j % 4)*32
    IF k[j]
        dst[i+31:i] := a[n+31:n]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTF64X4 - _mm512_broadcast_f64x4

| VBROADCASTF64X4_ZMMf64_MASKmskw_MEMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the 4 packed double-precision (64-bit) floating-point elements from "a" to all elements of "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    n := (j % 4)*64
    dst[i+63:i] := a[n+63:n]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTF64X4 - _mm512_mask_broadcast_f64x4

| VBROADCASTF64X4_ZMMf64_MASKmskw_MEMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the 4 packed double-precision (64-bit) floating-point elements from "a" to all elements of "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    n := (j % 4)*64
    IF k[j]
        dst[i+63:i] := a[n+63:n]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTF64X4 - _mm512_maskz_broadcast_f64x4

| VBROADCASTF64X4_ZMMf64_MASKmskw_MEMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the 4 packed double-precision (64-bit) floating-point elements from "a" to all elements of "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    n := (j % 4)*64
    IF k[j]
        dst[i+63:i] := a[n+63:n]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTI32X4 - _mm512_broadcast_i32x4

| VBROADCASTI32X4_ZMMu32_MASKmskw_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the 4 packed 32-bit integers from "a" to all elements of "dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    n := (j % 4)*32
    dst[i+31:i] := a[n+31:n]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTI32X4 - _mm512_mask_broadcast_i32x4

| VBROADCASTI32X4_ZMMu32_MASKmskw_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the 4 packed 32-bit integers from "a" to all elements of "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    n := (j % 4)*32
    IF k[j]
        dst[i+31:i] := a[n+31:n]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTI32X4 - _mm512_maskz_broadcast_i32x4

| VBROADCASTI32X4_ZMMu32_MASKmskw_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the 4 packed 32-bit integers from "a" to all elements of "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    n := (j % 4)*32
    IF k[j]
        dst[i+31:i] := a[n+31:n]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTI64X4 - _mm512_broadcast_i64x4

| VBROADCASTI64X4_ZMMu64_MASKmskw_MEMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the 4 packed 64-bit integers from "a" to all elements of "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    n := (j % 4)*64
    dst[i+63:i] := a[n+63:n]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTI64X4 - _mm512_mask_broadcast_i64x4

| VBROADCASTI64X4_ZMMu64_MASKmskw_MEMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the 4 packed 64-bit integers from "a" to all elements of "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    n := (j % 4)*64
    IF k[j]
        dst[i+63:i] := a[n+63:n]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTI64X4 - _mm512_maskz_broadcast_i64x4

| VBROADCASTI64X4_ZMMu64_MASKmskw_MEMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the 4 packed 64-bit integers from "a" to all elements of "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    n := (j % 4)*64
    IF k[j]
        dst[i+63:i] := a[n+63:n]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTSD - _mm512_broadcastsd_pd

| VBROADCASTSD_ZMMf64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low double-precision (64-bit) floating-point element from "a" to all elements of "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := a[63:0]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTSD - _mm512_mask_broadcastsd_pd

| VBROADCASTSD_ZMMf64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low double-precision (64-bit) floating-point element from "a" to all elements of "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := a[63:0]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTSD - _mm512_maskz_broadcastsd_pd

| VBROADCASTSD_ZMMf64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low double-precision (64-bit) floating-point element from "a" to all elements of "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := a[63:0]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTSS - _mm512_broadcastss_ps

| VBROADCASTSS_ZMMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low single-precision (32-bit) floating-point element from "a" to all elements of "dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := a[31:0]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTSS - _mm512_mask_broadcastss_ps

| VBROADCASTSS_ZMMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low single-precision (32-bit) floating-point element from "a" to all elements of "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := a[31:0]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VBROADCASTSS - _mm512_maskz_broadcastss_ps

| VBROADCASTSS_ZMMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low single-precision (32-bit) floating-point element from "a" to all elements of "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := a[31:0]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCMPSD - _mm_cmp_round_sd_mask

| VCMPSD_MASKmskw_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare the lower double-precision (64-bit) floating-point element in "a" and "b" based on the comparison
operand specified by "imm8", and store the result in mask vector "k". [sae_note]

[algorithm]

CASE (imm8[4:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
k[0] := ( a[63:0] OP b[63:0] ) ? 1 : 0
k[MAX:1] := 0

--------------------------------------------------------------------------------------------------------------

## VCMPSD - _mm_cmp_sd_mask

| VCMPSD_MASKmskw_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare the lower double-precision (64-bit) floating-point element in "a" and "b" based on the comparison
operand specified by "imm8", and store the result in mask vector "k".

[algorithm]

CASE (imm8[4:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
k[0] := ( a[63:0] OP b[63:0] ) ? 1 : 0
k[MAX:1] := 0

--------------------------------------------------------------------------------------------------------------

## VCMPSD - _mm_mask_cmp_round_sd_mask

| VCMPSD_MASKmskw_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare the lower double-precision (64-bit) floating-point element in "a" and "b" based on the comparison
operand specified by "imm8", and store the result in mask vector "k" using zeromask "k1" (the element is zeroed
out when mask bit 0 is not set). [sae_note]

[algorithm]

CASE (imm8[4:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
IF k1[0]
    k[0] := ( a[63:0] OP b[63:0] ) ? 1 : 0
ELSE
    k[0] := 0
FI
k[MAX:1] := 0

--------------------------------------------------------------------------------------------------------------

## VCMPSD - _mm_mask_cmp_sd_mask

| VCMPSD_MASKmskw_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare the lower double-precision (64-bit) floating-point element in "a" and "b" based on the comparison
operand specified by "imm8", and store the result in mask vector "k" using zeromask "k1" (the element is zeroed
out when mask bit 0 is not set).

[algorithm]

CASE (imm8[4:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
IF k1[0]
    k[0] := ( a[63:0] OP b[63:0] ) ? 1 : 0
ELSE
    k[0] := 0
FI
k[MAX:1] := 0

--------------------------------------------------------------------------------------------------------------

## VCMPSS - _mm_cmp_round_ss_mask

| VCMPSS_MASKmskw_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare the lower single-precision (32-bit) floating-point element in "a" and "b" based on the comparison
operand specified by "imm8", and store the result in mask vector "k". [sae_note]

[algorithm]

CASE (imm8[4:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
k[0] := ( a[31:0] OP b[31:0] ) ? 1 : 0
k[MAX:1] := 0

--------------------------------------------------------------------------------------------------------------

## VCMPSS - _mm_cmp_ss_mask

| VCMPSS_MASKmskw_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare the lower single-precision (32-bit) floating-point element in "a" and "b" based on the comparison
operand specified by "imm8", and store the result in mask vector "k".

[algorithm]

CASE (imm8[4:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
k[0] := ( a[31:0] OP b[31:0] ) ? 1 : 0
k[MAX:1] := 0

--------------------------------------------------------------------------------------------------------------

## VCMPSS - _mm_mask_cmp_round_ss_mask

| VCMPSS_MASKmskw_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare the lower single-precision (32-bit) floating-point element in "a" and "b" based on the comparison
operand specified by "imm8", and store the result in mask vector "k" using zeromask "k1" (the element is zeroed
out when mask bit 0 is not set). [sae_note]

[algorithm]

CASE (imm8[4:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
IF k1[0]
    k[0] := ( a[31:0] OP b[31:0] ) ? 1 : 0
ELSE
    k[0] := 0
FI
k[MAX:1] := 0

--------------------------------------------------------------------------------------------------------------

## VCMPSS - _mm_mask_cmp_ss_mask

| VCMPSS_MASKmskw_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare the lower single-precision (32-bit) floating-point element in "a" and "b" based on the comparison
operand specified by "imm8", and store the result in mask vector "k" using zeromask "k1" (the element is zeroed
out when mask bit 0 is not set).

[algorithm]

CASE (imm8[4:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
IF k1[0]
    k[0] := ( a[31:0] OP b[31:0] ) ? 1 : 0
ELSE
    k[0] := 0
FI
k[MAX:1] := 0

--------------------------------------------------------------------------------------------------------------

## VCOMISD - _mm_comi_round_sd

| VCOMISD_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare the lower double-precision (64-bit) floating-point element in "a" and "b" based on the comparison
operand specified by "imm8", and return the boolean result (0 or 1). [sae_note]

[algorithm]

CASE (imm8[4:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
RETURN ( a[63:0] OP b[63:0] ) ? 1 : 0

--------------------------------------------------------------------------------------------------------------

## VCOMISS - _mm_comi_round_ss

| VCOMISS_XMMf32_XMMf32_AVX512 | VUCOMISS_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare the lower single-precision (32-bit) floating-point element in "a" and "b" based on the comparison
operand specified by "imm8", and return the boolean result (0 or 1). [sae_note]

[algorithm]

CASE (imm8[4:0]) OF
0: OP := _CMP_EQ_OQ
1: OP := _CMP_LT_OS
2: OP := _CMP_LE_OS
3: OP := _CMP_UNORD_Q 
4: OP := _CMP_NEQ_UQ
5: OP := _CMP_NLT_US
6: OP := _CMP_NLE_US
7: OP := _CMP_ORD_Q
8: OP := _CMP_EQ_UQ
9: OP := _CMP_NGE_US
10: OP := _CMP_NGT_US
11: OP := _CMP_FALSE_OQ
12: OP := _CMP_NEQ_OQ
13: OP := _CMP_GE_OS
14: OP := _CMP_GT_OS
15: OP := _CMP_TRUE_UQ
16: OP := _CMP_EQ_OS
17: OP := _CMP_LT_OQ
18: OP := _CMP_LE_OQ
19: OP := _CMP_UNORD_S
20: OP := _CMP_NEQ_US
21: OP := _CMP_NLT_UQ
22: OP := _CMP_NLE_UQ
23: OP := _CMP_ORD_S
24: OP := _CMP_EQ_US
25: OP := _CMP_NGE_UQ 
26: OP := _CMP_NGT_UQ 
27: OP := _CMP_FALSE_OS 
28: OP := _CMP_NEQ_OS 
29: OP := _CMP_GE_OQ
30: OP := _CMP_GT_OQ
31: OP := _CMP_TRUE_US
ESAC
RETURN ( a[31:0] OP b[31:0] ) ? 1 : 0

--------------------------------------------------------------------------------------------------------------

## VCOMPRESSPD - _mm512_mask_compress_pd

| VCOMPRESSPD_ZMMf64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active double-precision (64-bit) floating-point elements in "a" (those with their
respective bit set in writemask "k") to "dst", and pass through the remaining elements from "src".

[algorithm]

size := 64
m := 0
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[m+size-1:m] := a[i+63:i]
        m := m + size
    FI
ENDFOR
dst[511:m] := src[511:m]
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCOMPRESSPD - _mm512_mask_compressstoreu_pd

| VCOMPRESSPD_MEMf64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active double-precision (64-bit) floating-point elements in "a" (those with their
respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

size := 64
m := base_addr
FOR j := 0 to 7
    i := j*64
    IF k[j]
        MEM[m+size-1:m] := a[i+63:i]
        m := m + size
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VCOMPRESSPD - _mm512_maskz_compress_pd

| VCOMPRESSPD_ZMMf64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active double-precision (64-bit) floating-point elements in "a" (those with their
respective bit set in zeromask "k") to "dst", and set the remaining elements to zero.

[algorithm]

size := 64
m := 0
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[m+size-1:m] := a[i+63:i]
        m := m + size
    FI
ENDFOR
dst[511:m] := 0
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCOMPRESSPS - _mm512_mask_compress_ps

| VCOMPRESSPS_ZMMf32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active single-precision (32-bit) floating-point elements in "a" (those with their
respective bit set in writemask "k") to "dst", and pass through the remaining elements from "src".

[algorithm]

size := 32
m := 0
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[m+size-1:m] := a[i+31:i]
        m := m + size
    FI
ENDFOR
dst[511:m] := src[511:m]
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCOMPRESSPS - _mm512_mask_compressstoreu_ps

| VCOMPRESSPS_MEMf32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active single-precision (32-bit) floating-point elements in "a" (those with their
respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

size := 32
m := base_addr
FOR j := 0 to 15
    i := j*32
    IF k[j]
        MEM[m+size-1:m] := a[i+31:i]
        m := m + size
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VCOMPRESSPS - _mm512_maskz_compress_ps

| VCOMPRESSPS_ZMMf32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active single-precision (32-bit) floating-point elements in "a" (those with their
respective bit set in zeromask "k") to "dst", and set the remaining elements to zero.

[algorithm]

size := 32
m := 0
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[m+size-1:m] := a[i+31:i]
        m := m + size
    FI
ENDFOR
dst[511:m] := 0
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTDQ2PD - _mm512_cvtepi32_pd

| VCVTDQ2PD_ZMMf64_MASKmskw_YMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and
store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*32
    m := j*64
    dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTDQ2PD - _mm512_mask_cvtepi32_pd

| VCVTDQ2PD_ZMMf64_MASKmskw_YMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    m := j*64
    IF k[j]
        dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
    ELSE
        dst[m+63:m] := src[m+63:m]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTDQ2PD - _mm512_maskz_cvtepi32_pd

| VCVTDQ2PD_ZMMf64_MASKmskw_YMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    m := j*64
    IF k[j]
        dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
    ELSE
        dst[m+63:m] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTDQ2PS - _mm512_cvt_roundepi32_ps

| VCVTDQ2PS_ZMMf32_MASKmskw_ZMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and
store the results in "dst".
	[round_note]

[algorithm]

FOR j := 0 to 15
    i := 32*j
    dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTDQ2PS - _mm512_cvtepi32_ps

| VCVTDQ2PS_ZMMf32_MASKmskw_ZMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and
store the results in "dst".

[algorithm]

FOR j := 0 to 15
    i := 32*j
    dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTDQ2PS - _mm512_mask_cvt_roundepi32_ps

| VCVTDQ2PS_ZMMf32_MASKmskw_ZMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).
	[round_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTDQ2PS - _mm512_mask_cvtepi32_ps

| VCVTDQ2PS_ZMMf32_MASKmskw_ZMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTDQ2PS - _mm512_maskz_cvt_roundepi32_ps

| VCVTDQ2PS_ZMMf32_MASKmskw_ZMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).
	[round_note]

[algorithm]

FOR j := 0 to 15
    i := 32*j
    IF k[j]
        dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTDQ2PS - _mm512_maskz_cvtepi32_ps

| VCVTDQ2PS_ZMMf32_MASKmskw_ZMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 15
    i := 32*j
    IF k[j]
        dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2DQ - _mm512_cvt_roundpd_epi32

| VCVTPD2DQ_YMMi32_MASKmskw_ZMMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers, and store
the results in "dst".
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := 32*j
    k := 64*j
    dst[i+31:i] := Convert_FP64_To_Int32(a[k+63:k])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2DQ - _mm512_cvtpd_epi32

| VCVTPD2DQ_YMMi32_MASKmskw_ZMMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers, and store
the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := 32*j
    k := 64*j
    dst[i+31:i] := Convert_FP64_To_Int32(a[k+63:k])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2DQ - _mm512_mask_cvt_roundpd_epi32

| VCVTPD2DQ_YMMi32_MASKmskw_ZMMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers, and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set). 
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*32
    l := j*64
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2DQ - _mm512_mask_cvtpd_epi32

| VCVTPD2DQ_YMMi32_MASKmskw_ZMMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers, and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    l := j*64
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2DQ - _mm512_maskz_cvt_roundpd_epi32

| VCVTPD2DQ_YMMi32_MASKmskw_ZMMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers, and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 64*j
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2DQ - _mm512_maskz_cvtpd_epi32

| VCVTPD2DQ_YMMi32_MASKmskw_ZMMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers, and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 64*j
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2PS - _mm512_cvt_roundpd_ps

| VCVTPD2PS_YMMf32_MASKmskw_ZMMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed single-precision (32-bit)
floating-point elements, and store the results in "dst".
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := 32*j
    k := 64*j
    dst[i+31:i] := Convert_FP64_To_FP32(a[k+63:k])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2PS - _mm512_cvtpd_ps

| VCVTPD2PS_YMMf32_MASKmskw_ZMMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed single-precision (32-bit)
floating-point elements, and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := 32*j
    k := 64*j
    dst[i+31:i] := Convert_FP64_To_FP32(a[k+63:k])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2PS - _mm512_mask_cvt_roundpd_ps

| VCVTPD2PS_YMMf32_MASKmskw_ZMMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed single-precision (32-bit)
floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set). 
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*32
    l := j*64
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2PS - _mm512_mask_cvtpd_ps

| VCVTPD2PS_YMMf32_MASKmskw_ZMMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed single-precision (32-bit)
floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 64*j
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2PS - _mm512_maskz_cvt_roundpd_ps

| VCVTPD2PS_YMMf32_MASKmskw_ZMMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed single-precision (32-bit)
floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set). 
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*32
    l := j*64
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2PS - _mm512_maskz_cvtpd_ps

| VCVTPD2PS_YMMf32_MASKmskw_ZMMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed single-precision (32-bit)
floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    l := j*64
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2UDQ - _mm512_cvt_roundpd_epu32

| VCVTPD2UDQ_YMMu32_MASKmskw_ZMMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers,
and store the results in "dst".
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := 32*j
    k := 64*j
    dst[i+31:i] := Convert_FP64_To_UInt32(a[k+63:k])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2UDQ - _mm512_cvtpd_epu32

| VCVTPD2UDQ_YMMu32_MASKmskw_ZMMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers,
and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := 32*j
    k := 64*j
    dst[i+31:i] := Convert_FP64_To_UInt32(a[k+63:k])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2UDQ - _mm512_mask_cvt_roundpd_epu32

| VCVTPD2UDQ_YMMu32_MASKmskw_ZMMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set). 
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*32
    l := j*64
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_UInt32(a[l+63:l])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2UDQ - _mm512_mask_cvtpd_epu32

| VCVTPD2UDQ_YMMu32_MASKmskw_ZMMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    l := j*64
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_UInt32(a[l+63:l])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2UDQ - _mm512_maskz_cvt_roundpd_epu32

| VCVTPD2UDQ_YMMu32_MASKmskw_ZMMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 64*j
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_UInt32(a[l+63:l])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPD2UDQ - _mm512_maskz_cvtpd_epu32

| VCVTPD2UDQ_YMMu32_MASKmskw_ZMMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 64*j
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_UInt32(a[l+63:l])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPH2PS - _mm512_cvt_roundph_ps

| VCVTPH2PS_ZMMf32_MASKmskw_YMMf16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed half-precision (16-bit) floating-point elements in "a" to packed single-precision (32-bit)
floating-point elements, and store the results in "dst". [sae_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    m := j*16
    dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPH2PS - _mm512_cvtph_ps

| VCVTPH2PS_ZMMf32_MASKmskw_YMMf16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed half-precision (16-bit) floating-point elements in "a" to packed single-precision (32-bit)
floating-point elements, and store the results in "dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    m := j*16
    dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPH2PS - _mm512_mask_cvt_roundph_ps

| VCVTPH2PS_ZMMf32_MASKmskw_YMMf16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed half-precision (16-bit) floating-point elements in "a" to packed single-precision (32-bit)
floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set). [sae_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    m := j*16
    IF k[j]
        dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPH2PS - _mm512_mask_cvtph_ps

| VCVTPH2PS_ZMMf32_MASKmskw_YMMf16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed half-precision (16-bit) floating-point elements in "a" to packed single-precision (32-bit)
floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    m := j*16
    IF k[j]
        dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPH2PS - _mm512_maskz_cvt_roundph_ps

| VCVTPH2PS_ZMMf32_MASKmskw_YMMf16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed half-precision (16-bit) floating-point elements in "a" to packed single-precision (32-bit)
floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set). [sae_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    m := j*16
    IF k[j]
        dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPH2PS - _mm512_maskz_cvtph_ps

| VCVTPH2PS_ZMMf32_MASKmskw_YMMf16_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed half-precision (16-bit) floating-point elements in "a" to packed single-precision (32-bit)
floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    m := j*16
    IF k[j]
        dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2DQ - _mm512_cvt_roundps_epi32

| VCVTPS2DQ_ZMMi32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers, and store
the results in "dst". 
	[round_note]

[algorithm]

FOR j := 0 to 15
    i := 32*j
    dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2DQ - _mm512_cvtps_epi32

| VCVTPS2DQ_ZMMi32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers, and store
the results in "dst".

[algorithm]

FOR j := 0 to 15
    i := 32*j
    dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2DQ - _mm512_mask_cvt_roundps_epi32

| VCVTPS2DQ_ZMMi32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers, and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set). 
	[round_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2DQ - _mm512_mask_cvtps_epi32

| VCVTPS2DQ_ZMMi32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers, and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2DQ - _mm512_maskz_cvt_roundps_epi32

| VCVTPS2DQ_ZMMi32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers, and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
[round_note]

[algorithm]

FOR j := 0 to 15
    i := 32*j
    IF k[j]
        dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2DQ - _mm512_maskz_cvtps_epi32

| VCVTPS2DQ_ZMMi32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers, and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := 32*j
    IF k[j]
        dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2PD - _mm512_cvt_roundps_pd

| VCVTPS2PD_ZMMf64_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed double-precision (64-bit)
floating-point elements, and store the results in "dst". [sae_note]

[algorithm]

FOR j := 0 to 7
    i := 64*j
    k := 32*j
    dst[i+63:i] := Convert_FP32_To_FP64(a[k+31:k])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2PD - _mm512_cvtps_pd

| VCVTPS2PD_ZMMf64_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed double-precision (64-bit)
floating-point elements, and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := 64*j
    k := 32*j
    dst[i+63:i] := Convert_FP32_To_FP64(a[k+31:k])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2PD - _mm512_mask_cvt_roundps_pd

| VCVTPS2PD_ZMMf64_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed double-precision (64-bit)
floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set). [sae_note]

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 32*j
    IF k[j]
        dst[i+63:i] := Convert_FP32_To_FP64(a[l+31:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2PD - _mm512_mask_cvtps_pd

| VCVTPS2PD_ZMMf64_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed double-precision (64-bit)
floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 32*j
    IF k[j]
        dst[i+63:i] := Convert_FP32_To_FP64(a[l+31:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2PD - _mm512_maskz_cvt_roundps_pd

| VCVTPS2PD_ZMMf64_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed double-precision (64-bit)
floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set). [sae_note]

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 32*j
    IF k[j]
        dst[i+63:i] := Convert_FP32_To_FP64(a[l+31:l])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2PD - _mm512_maskz_cvtps_pd

| VCVTPS2PD_ZMMf64_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed double-precision (64-bit)
floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 32*j
    IF k[j]
        dst[i+63:i] := Convert_FP32_To_FP64(a[l+31:l])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2PH - _mm512_cvt_roundps_ph

| VCVTPS2PH_YMMf16_MASKmskw_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed half-precision (16-bit)
floating-point elements, and store the results in "dst". [sae_note]

[algorithm]

FOR j := 0 to 15
    i := 16*j
    l := 32*j
    dst[i+15:i] := Convert_FP32_To_FP16(a[l+31:l])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2PH - _mm512_cvtps_ph

| VCVTPS2PH_YMMf16_MASKmskw_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed half-precision (16-bit)
floating-point elements, and store the results in "dst". [sae_note]

[algorithm]

FOR j := 0 to 15
    i := 16*j
    l := 32*j
    dst[i+15:i] := Convert_FP32_To_FP16(a[l+31:l])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2PH - _mm512_mask_cvt_roundps_ph

| VCVTPS2PH_YMMf16_MASKmskw_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed half-precision (16-bit)
floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set). [sae_note]

[algorithm]

FOR j := 0 to 15
    i := 16*j
    l := 32*j
    IF k[j]
        dst[i+15:i] := Convert_FP32_To_FP16(a[l+31:l])
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2PH - _mm512_mask_cvtps_ph

| VCVTPS2PH_YMMf16_MASKmskw_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed half-precision (16-bit)
floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set). [sae_note]

[algorithm]

FOR j := 0 to 15
    i := 16*j
    l := 32*j
    IF k[j]
        dst[i+15:i] := Convert_FP32_To_FP16(a[l+31:l])
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2PH - _mm512_maskz_cvt_roundps_ph

| VCVTPS2PH_YMMf16_MASKmskw_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed half-precision (16-bit)
floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set). [sae_note]

[algorithm]

FOR j := 0 to 15
    i := 16*j
    l := 32*j
    IF k[j]
        dst[i+15:i] := Convert_FP32_To_FP16(a[l+31:l])
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2PH - _mm512_maskz_cvtps_ph

| VCVTPS2PH_YMMf16_MASKmskw_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed half-precision (16-bit)
floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set). [sae_note]

[algorithm]

FOR j := 0 to 15
    i := 16*j
    l := 32*j
    IF k[j]
        dst[i+15:i] := Convert_FP32_To_FP16(a[l+31:l])
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2UDQ - _mm512_cvt_roundps_epu32

| VCVTPS2UDQ_ZMMu32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers,
and store the results in "dst". 
	[round_note]

[algorithm]

FOR j := 0 to 15
    i := 32*j
    dst[i+31:i] := Convert_FP32_To_UInt32(a[i+31:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2UDQ - _mm512_cvtps_epu32

| VCVTPS2UDQ_ZMMu32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers,
and store the results in "dst".

[algorithm]

FOR j := 0 to 15
    i := 32*j
    dst[i+31:i] := Convert_FP32_To_UInt32(a[i+31:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2UDQ - _mm512_mask_cvt_roundps_epu32

| VCVTPS2UDQ_ZMMu32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set). 
	[round_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := Convert_FP32_To_UInt32(a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2UDQ - _mm512_mask_cvtps_epu32

| VCVTPS2UDQ_ZMMu32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 15
    i := 32*j
    IF k[j]
        dst[i+31:i] := Convert_FP32_To_UInt32(a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2UDQ - _mm512_maskz_cvt_roundps_epu32

| VCVTPS2UDQ_ZMMu32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set). 
	[round_note]

[algorithm]

FOR j := 0 to 15
    i := 32*j
    IF k[j]
        dst[i+31:i] := Convert_FP32_To_UInt32(a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTPS2UDQ - _mm512_maskz_cvtps_epu32

| VCVTPS2UDQ_ZMMu32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 15
    i := 32*j
    IF k[j]
        dst[i+31:i] := Convert_FP32_To_UInt32(a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTSD2SI - _mm_cvt_roundsd_i32

| VCVTSD2SI_GPR32i32_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower double-precision (64-bit) floating-point element in "a" to a 32-bit integer, and store the
result in "dst".
	[round_note]

[algorithm]

dst[31:0] := Convert_FP64_To_Int32(a[63:0])

--------------------------------------------------------------------------------------------------------------

## VCVTSD2SI - _mm_cvt_roundsd_i64

| VCVTSD2SI_GPR64i64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower double-precision (64-bit) floating-point element in "a" to a 64-bit integer, and store the
result in "dst".
	[round_note]

[algorithm]

dst[63:0] := Convert_FP64_To_Int64(a[63:0])

--------------------------------------------------------------------------------------------------------------

## VCVTSD2SI - _mm_cvt_roundsd_si32

| VCVTSD2SI_GPR32i32_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower double-precision (64-bit) floating-point element in "a" to a 32-bit integer, and store the
result in "dst".
	[round_note]

[algorithm]

dst[31:0] := Convert_FP64_To_Int32(a[63:0])

--------------------------------------------------------------------------------------------------------------

## VCVTSD2SI - _mm_cvt_roundsd_si64

| VCVTSD2SI_GPR64i64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower double-precision (64-bit) floating-point element in "a" to a 64-bit integer, and store the
result in "dst".
	[round_note]

[algorithm]

dst[63:0] := Convert_FP64_To_Int64(a[63:0])

--------------------------------------------------------------------------------------------------------------

## VCVTSD2SI - _mm_cvtsd_i32

| VCVTSD2SI_GPR32i32_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower double-precision (64-bit) floating-point element in "a" to a 32-bit integer, and store the
result in "dst".

[algorithm]

dst[31:0] := Convert_FP64_To_Int32(a[63:0])

--------------------------------------------------------------------------------------------------------------

## VCVTSD2SI - _mm_cvtsd_i64

| VCVTSD2SI_GPR64i64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower double-precision (64-bit) floating-point element in "a" to a 64-bit integer, and store the
result in "dst".

[algorithm]

dst[63:0] := Convert_FP64_To_Int64(a[63:0])

--------------------------------------------------------------------------------------------------------------

## VCVTSD2SS - _mm_cvt_roundsd_ss

| VCVTSD2SS_XMMf32_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower double-precision (64-bit) floating-point element in "b" to a single-precision (32-bit)
floating-point element, store the result in the lower element of "dst", and copy the upper 3 packed elements
from "a" to the upper elements of "dst".
	[round_note]

[algorithm]

dst[31:0] := Convert_FP64_To_FP32(b[63:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTSD2SS - _mm_mask_cvt_roundsd_ss

| VCVTSD2SS_XMMf32_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower double-precision (64-bit) floating-point element in "b" to a single-precision (32-bit)
floating-point element, store the result in the lower element of "dst" using writemask "k" (the element is
copied from "src" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper
elements of "dst".
	[round_note]

[algorithm]

IF k[0]
    dst[31:0] := Convert_FP64_To_FP32(b[63:0])
ELSE
    dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTSD2SS - _mm_mask_cvtsd_ss

| VCVTSD2SS_XMMf32_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower double-precision (64-bit) floating-point element in "b" to a single-precision (32-bit)
floating-point element, store the result in the lower element of "dst" using writemask "k" (the element is
copied from "src" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper
elements of "dst".

[algorithm]

IF k[0]
    dst[31:0] := Convert_FP64_To_FP32(b[63:0])
ELSE
    dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTSD2SS - _mm_maskz_cvt_roundsd_ss

| VCVTSD2SS_XMMf32_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower double-precision (64-bit) floating-point element in "b" to a single-precision (32-bit)
floating-point element, store the result in the lower element of "dst" using zeromask "k" (the element is
zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of
"dst". 
	[round_note]

[algorithm]

IF k[0]
    dst[31:0] := Convert_FP64_To_FP32(b[63:0])
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTSD2SS - _mm_maskz_cvtsd_ss

| VCVTSD2SS_XMMf32_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower double-precision (64-bit) floating-point element in "b" to a single-precision (32-bit)
floating-point element, store the result in the lower element of "dst" using zeromask "k" (the element is
zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of
"dst".

[algorithm]

IF k[0]
    dst[31:0] := Convert_FP64_To_FP32(b[63:0])
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTSD2USI - _mm_cvt_roundsd_u32

| VCVTSD2USI_GPR32u32_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower double-precision (64-bit) floating-point element in "a" to an unsigned 32-bit integer, and
store the result in "dst".
	[round_note]

[algorithm]

dst[31:0] := Convert_FP64_To_UInt32(a[63:0])

--------------------------------------------------------------------------------------------------------------

## VCVTSD2USI - _mm_cvt_roundsd_u64

| VCVTSD2USI_GPR64u64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower double-precision (64-bit) floating-point element in "a" to an unsigned 64-bit integer, and
store the result in "dst".
	[round_note]

[algorithm]

dst[63:0] := Convert_FP64_To_UInt64(a[63:0])

--------------------------------------------------------------------------------------------------------------

## VCVTSD2USI - _mm_cvtsd_u32

| VCVTSD2USI_GPR32u32_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower double-precision (64-bit) floating-point element in "a" to an unsigned 32-bit integer, and
store the result in "dst".

[algorithm]

dst[31:0] := Convert_FP64_To_UInt32(a[63:0])

--------------------------------------------------------------------------------------------------------------

## VCVTSD2USI - _mm_cvtsd_u64

| VCVTSD2USI_GPR64u64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower double-precision (64-bit) floating-point element in "a" to an unsigned 64-bit integer, and
store the result in "dst".

[algorithm]

dst[63:0] := Convert_FP64_To_UInt64(a[63:0])

--------------------------------------------------------------------------------------------------------------

## VCVTSI2SD - _mm_cvt_roundi64_sd

| VCVTSI2SD_XMMf64_XMMf64_GPR64i64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the signed 64-bit integer "b" to a double-precision (64-bit) floating-point element, store the result
in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst".
	[round_note]

[algorithm]

dst[63:0] := Convert_Int64_To_FP64(b[63:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTSI2SD - _mm_cvt_roundsi64_sd

| VCVTSI2SD_XMMf64_XMMf64_GPR64i64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the signed 64-bit integer "b" to a double-precision (64-bit) floating-point element, store the result
in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst".
[round_note]

[algorithm]

dst[63:0] := Convert_Int64_To_FP64(b[63:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTSI2SD - _mm_cvti32_sd

| VCVTSI2SD_XMMf64_XMMf64_GPR32i32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the signed 32-bit integer "b" to a double-precision (64-bit) floating-point element, store the result
in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst".

[algorithm]

dst[63:0] := Convert_Int32_To_FP64(b[31:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTSI2SD - _mm_cvti64_sd

| VCVTSI2SD_XMMf64_XMMf64_GPR64i64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the signed 64-bit integer "b" to a double-precision (64-bit) floating-point element, store the result
in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst".

[algorithm]

dst[63:0] := Convert_Int64_To_FP64(b[63:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTSI2SS - _mm_cvt_roundi32_ss

| VCVTSI2SS_XMMf32_XMMf32_GPR32i32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the signed 32-bit integer "b" to a single-precision (32-bit) floating-point element, store the result
in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of
"dst".
	[round_note]

[algorithm]

dst[31:0] := Convert_Int32_To_FP32(b[31:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTSI2SS - _mm_cvt_roundi64_ss

| VCVTSI2SS_XMMf32_XMMf32_GPR64i64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the signed 64-bit integer "b" to a single-precision (32-bit) floating-point element, store the result
in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of
"dst".
	[round_note]

[algorithm]

dst[31:0] := Convert_Int64_To_FP32(b[63:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTSI2SS - _mm_cvt_roundsi32_ss

| VCVTSI2SS_XMMf32_XMMf32_GPR32i32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the signed 32-bit integer "b" to a single-precision (32-bit) floating-point element, store the result
in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of
"dst".
	[round_note]

[algorithm]

dst[31:0] := Convert_Int32_To_FP32(b[31:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTSI2SS - _mm_cvt_roundsi64_ss

| VCVTSI2SS_XMMf32_XMMf32_GPR64i64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the signed 64-bit integer "b" to a single-precision (32-bit) floating-point element, store the result
in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of
"dst".
	[round_note]

[algorithm]

dst[31:0] := Convert_Int64_To_FP32(b[63:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTSI2SS - _mm_cvti32_ss

| VCVTSI2SS_XMMf32_XMMf32_GPR32i32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the signed 32-bit integer "b" to a single-precision (32-bit) floating-point element, store the result
in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of "dst".

[algorithm]

dst[31:0] := Convert_Int32_To_FP32(b[31:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTSI2SS - _mm_cvti64_ss

| VCVTSI2SS_XMMf32_XMMf32_GPR64i64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the signed 64-bit integer "b" to a single-precision (32-bit) floating-point element, store the result
in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of "dst".

[algorithm]

dst[31:0] := Convert_Int64_To_FP32(b[63:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTSS2SD - _mm_cvt_roundss_sd

| VCVTSS2SD_XMMf64_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower single-precision (32-bit) floating-point element in "b" to a double-precision (64-bit)
floating-point element, store the result in the lower element of "dst", and copy the upper element from "a" to
the upper element of "dst". 
	[sae_note]

[algorithm]

dst[63:0] := Convert_FP32_To_FP64(b[31:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTSS2SD - _mm_mask_cvt_roundss_sd

| VCVTSS2SD_XMMf64_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower single-precision (32-bit) floating-point element in "b" to a double-precision (64-bit)
floating-point element, store the result in the lower element of "dst" using writemask "k" (the element is
copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of
"dst".
	[sae_note]

[algorithm]

IF k[0]
    dst[63:0] := Convert_FP32_To_FP64(b[31:0])
ELSE
    dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTSS2SD - _mm_mask_cvtss_sd

| VCVTSS2SD_XMMf64_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower single-precision (32-bit) floating-point element in "b" to a double-precision (64-bit)
floating-point element, store the result in the lower element of "dst" using writemask "k" (the element is
copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of
"dst".

[algorithm]

IF k[0]
    dst[63:0] := Convert_FP32_To_FP64(b[31:0])
ELSE
    dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTSS2SD - _mm_maskz_cvt_roundss_sd

| VCVTSS2SD_XMMf64_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower single-precision (32-bit) floating-point element in "b" to a double-precision (64-bit)
floating-point element, store the result in the lower element of "dst" using zeromask "k" (the element is
zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
[sae_note]

[algorithm]

IF k[0]
    dst[63:0] := Convert_FP32_To_FP64(b[31:0])
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTSS2SD - _mm_maskz_cvtss_sd

| VCVTSS2SD_XMMf64_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower single-precision (32-bit) floating-point element in "b" to a double-precision (64-bit)
floating-point element, store the result in the lower element of "dst" using zeromask "k" (the element is
zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".

[algorithm]

IF k[0]
    dst[63:0] := Convert_FP32_To_FP64(b[31:0])
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTSS2SI - _mm_cvt_roundss_i32

| VCVTSS2SI_GPR32i32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower single-precision (32-bit) floating-point element in "a" to a 32-bit integer, and store the
result in "dst".
	[round_note]

[algorithm]

dst[31:0] := Convert_FP32_To_Int32(a[31:0])

--------------------------------------------------------------------------------------------------------------

## VCVTSS2SI - _mm_cvt_roundss_i64

| VCVTSS2SI_GPR64i64_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower single-precision (32-bit) floating-point element in "a" to a 64-bit integer, and store the
result in "dst".
	[round_note]

[algorithm]

dst[63:0] := Convert_FP32_To_Int64(a[31:0])

--------------------------------------------------------------------------------------------------------------

## VCVTSS2SI - _mm_cvt_roundss_si32

| VCVTSS2SI_GPR32i32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower single-precision (32-bit) floating-point element in "a" to a 32-bit integer, and store the
result in "dst".
	[round_note]

[algorithm]

dst[31:0] := Convert_FP32_To_Int32(a[31:0])

--------------------------------------------------------------------------------------------------------------

## VCVTSS2SI - _mm_cvt_roundss_si64

| VCVTSS2SI_GPR64i64_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower single-precision (32-bit) floating-point element in "a" to a 64-bit integer, and store the
result in "dst".
	[round_note]

[algorithm]

dst[63:0] := Convert_FP32_To_Int64(a[31:0])

--------------------------------------------------------------------------------------------------------------

## VCVTSS2SI - _mm_cvtss_i32

| VCVTSS2SI_GPR32i32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower single-precision (32-bit) floating-point element in "a" to a 32-bit integer, and store the
result in "dst".

[algorithm]

dst[31:0] := Convert_FP32_To_Int32(a[31:0])

--------------------------------------------------------------------------------------------------------------

## VCVTSS2SI - _mm_cvtss_i64

| VCVTSS2SI_GPR64i64_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower single-precision (32-bit) floating-point element in "a" to a 64-bit integer, and store the
result in "dst".

[algorithm]

dst[63:0] := Convert_FP32_To_Int64(a[31:0])

--------------------------------------------------------------------------------------------------------------

## VCVTSS2USI - _mm_cvt_roundss_u32

| VCVTSS2USI_GPR32u32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower single-precision (32-bit) floating-point element in "a" to an unsigned 32-bit integer, and
store the result in "dst".
	[round_note]

[algorithm]

dst[31:0] := Convert_FP32_To_UInt32(a[31:0])

--------------------------------------------------------------------------------------------------------------

## VCVTSS2USI - _mm_cvt_roundss_u64

| VCVTSS2USI_GPR64u64_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower single-precision (32-bit) floating-point element in "a" to an unsigned 64-bit integer, and
store the result in "dst".
	[round_note]

[algorithm]

dst[63:0] := Convert_FP32_To_UInt64(a[31:0])

--------------------------------------------------------------------------------------------------------------

## VCVTSS2USI - _mm_cvtss_u32

| VCVTSS2USI_GPR32u32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower single-precision (32-bit) floating-point element in "a" to an unsigned 32-bit integer, and
store the result in "dst".

[algorithm]

dst[31:0] := Convert_FP32_To_UInt32(a[31:0])

--------------------------------------------------------------------------------------------------------------

## VCVTSS2USI - _mm_cvtss_u64

| VCVTSS2USI_GPR64u64_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower single-precision (32-bit) floating-point element in "a" to an unsigned 64-bit integer, and
store the result in "dst".

[algorithm]

dst[63:0] := Convert_FP32_To_UInt64(a[31:0])

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2DQ - _mm512_cvtt_roundpd_epi32

| VCVTTPD2DQ_YMMi32_MASKmskw_ZMMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers with
truncation, and store the results in "dst". [sae_note]

[algorithm]

FOR j := 0 to 7
    i := 32*j
    k := 64*j
    dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[k+63:k])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2DQ - _mm512_cvttpd_epi32

| VCVTTPD2DQ_YMMi32_MASKmskw_ZMMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers with
truncation, and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := 32*j
    k := 64*j
    dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[k+63:k])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2DQ - _mm512_mask_cvtt_roundpd_epi32

| VCVTTPD2DQ_YMMi32_MASKmskw_ZMMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers with
truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set). [sae_note]

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 64*j
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[l+63:l])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2DQ - _mm512_mask_cvttpd_epi32

| VCVTTPD2DQ_YMMi32_MASKmskw_ZMMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers with
truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 64*j
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[l+63:l])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2DQ - _mm512_maskz_cvtt_roundpd_epi32

| VCVTTPD2DQ_YMMi32_MASKmskw_ZMMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers with
truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set). [sae_note]

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 64*j
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[l+63:l])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2DQ - _mm512_maskz_cvttpd_epi32

| VCVTTPD2DQ_YMMi32_MASKmskw_ZMMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers with
truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 64*j
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[l+63:l])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2UDQ - _mm512_cvtt_roundpd_epu32

| VCVTTPD2UDQ_YMMu32_MASKmskw_ZMMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers
with truncation, and store the results in "dst". [sae_note]

[algorithm]

FOR j := 0 to 7
    i := 32*j
    k := 64*j
    dst[i+31:i] := Convert_FP64_To_UInt32_Truncate(a[k+63:k])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2UDQ - _mm512_cvttpd_epu32

| VCVTTPD2UDQ_YMMu32_MASKmskw_ZMMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers
with truncation, and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := 32*j
    k := 64*j
    dst[i+31:i] := Convert_FP64_To_UInt32_Truncate(a[k+63:k])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2UDQ - _mm512_mask_cvtt_roundpd_epu32

| VCVTTPD2UDQ_YMMu32_MASKmskw_ZMMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers
with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set). [sae_note]

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 64*j
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_UInt32_Truncate(a[l+63:l])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2UDQ - _mm512_mask_cvttpd_epu32

| VCVTTPD2UDQ_YMMu32_MASKmskw_ZMMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers
with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 64*j
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_UInt32_Truncate(a[l+63:l])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2UDQ - _mm512_maskz_cvtt_roundpd_epu32

| VCVTTPD2UDQ_YMMu32_MASKmskw_ZMMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers
with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set). [sae_note]

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 64*j
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_UInt32_Truncate(a[l+63:l])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPD2UDQ - _mm512_maskz_cvttpd_epu32

| VCVTTPD2UDQ_YMMu32_MASKmskw_ZMMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers
with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 32*j
    l := 64*j
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_UInt32_Truncate(a[l+63:l])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2DQ - _mm512_cvtt_roundps_epi32

| VCVTTPS2DQ_ZMMi32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers with
truncation, and store the results in "dst". [sae_note]

[algorithm]

FOR j := 0 to 15
    i := 32*j
    dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2DQ - _mm512_cvttps_epi32

| VCVTTPS2DQ_ZMMi32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers with
truncation, and store the results in "dst".

[algorithm]

FOR j := 0 to 15
    i := 32*j
    dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2DQ - _mm512_mask_cvtt_roundps_epi32

| VCVTTPS2DQ_ZMMi32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers with
truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set). [sae_note]

[algorithm]

FOR j := 0 to 15
    i := 32*j
    IF k[j]
        dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2DQ - _mm512_mask_cvttps_epi32

| VCVTTPS2DQ_ZMMi32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers with
truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := 32*j
    IF k[j]
        dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2DQ - _mm512_maskz_cvtt_roundps_epi32

| VCVTTPS2DQ_ZMMi32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers with
truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set). [sae_note]

[algorithm]

FOR j := 0 to 15
    i := 32*j
    IF k[j]
        dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2DQ - _mm512_maskz_cvttps_epi32

| VCVTTPS2DQ_ZMMi32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers with
truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := 32*j
    IF k[j]
        dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2UDQ - _mm512_cvtt_roundps_epu32

| VCVTTPS2UDQ_ZMMu32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers
with truncation, and store the results in "dst". [sae_note]

[algorithm]

FOR j := 0 to 15
    i := 32*j
    dst[i+31:i] := Convert_FP32_To_UInt32_Truncate(a[i+31:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2UDQ - _mm512_cvttps_epu32

| VCVTTPS2UDQ_ZMMu32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers
with truncation, and store the results in "dst".

[algorithm]

FOR j := 0 to 15
    i := 32*j
    dst[i+31:i] := Convert_FP32_To_UInt32_Truncate(a[i+31:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2UDQ - _mm512_mask_cvtt_roundps_epu32

| VCVTTPS2UDQ_ZMMu32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers
with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set). [sae_note]

[algorithm]

FOR j := 0 to 15
    i := 32*j
    IF k[j]
        dst[i+31:i] := Convert_FP32_To_UInt32_Truncate(a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2UDQ - _mm512_mask_cvttps_epu32

| VCVTTPS2UDQ_ZMMu32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers
with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := 32*j
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_UInt32_Truncate(a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2UDQ - _mm512_maskz_cvtt_roundps_epu32

| VCVTTPS2UDQ_ZMMu32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers
with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set). [sae_note]

[algorithm]

FOR j := 0 to 15
    i := 32*j
    IF k[j]
        dst[i+31:i] := Convert_FP32_To_UInt32_Truncate(a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTPS2UDQ - _mm512_maskz_cvttps_epu32

| VCVTTPS2UDQ_ZMMu32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed double-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers
with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := 32*j
    IF k[j]
        dst[i+31:i] := Convert_FP64_To_UInt32_Truncate(a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTTSD2SI - _mm_cvtt_roundsd_i32

| VCVTTSD2SI_GPR32i32_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower double-precision (64-bit) floating-point element in "a" to a 32-bit integer with truncation,
and store the result in "dst".
	[sae_note]

[algorithm]

dst[31:0] := Convert_FP64_To_Int32_Truncate(a[63:0])

--------------------------------------------------------------------------------------------------------------

## VCVTTSD2SI - _mm_cvtt_roundsd_i64

| VCVTTSD2SI_GPR64i64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower double-precision (64-bit) floating-point element in "a" to a 64-bit integer with truncation,
and store the result in "dst".
	[sae_note]

[algorithm]

dst[63:0] := Convert_FP64_To_Int64_Truncate(a[63:0])

--------------------------------------------------------------------------------------------------------------

## VCVTTSD2SI - _mm_cvtt_roundsd_si32

| VCVTTSD2SI_GPR32i32_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower double-precision (64-bit) floating-point element in "a" to a 32-bit integer with truncation,
and store the result in "dst".
	[sae_note]

[algorithm]

dst[31:0] := Convert_FP64_To_Int32_Truncate(a[63:0])

--------------------------------------------------------------------------------------------------------------

## VCVTTSD2SI - _mm_cvtt_roundsd_si64

| VCVTTSD2SI_GPR64i64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower double-precision (64-bit) floating-point element in "a" to a 64-bit integer with truncation,
and store the result in "dst".
	[sae_note]

[algorithm]

dst[63:0] := Convert_FP64_To_Int64_Truncate(a[63:0])

--------------------------------------------------------------------------------------------------------------

## VCVTTSD2SI - _mm_cvttsd_i32

| VCVTTSD2SI_GPR32i32_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower double-precision (64-bit) floating-point element in "a" to a 32-bit integer with truncation,
and store the result in "dst".

[algorithm]

dst[31:0] := Convert_FP64_To_Int32_Truncate(a[63:0])

--------------------------------------------------------------------------------------------------------------

## VCVTTSD2SI - _mm_cvttsd_i64

| VCVTTSD2SI_GPR64i64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower double-precision (64-bit) floating-point element in "a" to a 64-bit integer with truncation,
and store the result in "dst".

[algorithm]

dst[63:0] := Convert_FP64_To_Int64_Truncate(a[63:0])

--------------------------------------------------------------------------------------------------------------

## VCVTTSD2USI - _mm_cvtt_roundsd_u32

| VCVTTSD2USI_GPR32u32_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower double-precision (64-bit) floating-point element in "a" to an unsigned 32-bit integer with
truncation, and store the result in "dst".
	[sae_note]

[algorithm]

dst[31:0] := Convert_FP64_To_UInt32_Truncate(a[63:0])

--------------------------------------------------------------------------------------------------------------

## VCVTTSD2USI - _mm_cvtt_roundsd_u64

| VCVTTSD2USI_GPR64u64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower double-precision (64-bit) floating-point element in "a" to an unsigned 64-bit integer with
truncation, and store the result in "dst".
	[sae_note]

[algorithm]

dst[63:0] := Convert_FP64_To_UInt64_Truncate(a[63:0])

--------------------------------------------------------------------------------------------------------------

## VCVTTSD2USI - _mm_cvttsd_u32

| VCVTTSD2USI_GPR32u32_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower double-precision (64-bit) floating-point element in "a" to an unsigned 32-bit integer with
truncation, and store the result in "dst".

[algorithm]

dst[31:0] := Convert_FP64_To_UInt32_Truncate(a[63:0])

--------------------------------------------------------------------------------------------------------------

## VCVTTSD2USI - _mm_cvttsd_u64

| VCVTTSD2USI_GPR64u64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower double-precision (64-bit) floating-point element in "a" to an unsigned 64-bit integer with
truncation, and store the result in "dst".

[algorithm]

dst[63:0] := Convert_FP64_To_UInt64_Truncate(a[63:0])

--------------------------------------------------------------------------------------------------------------

## VCVTTSS2SI - _mm_cvtt_roundss_i32

| VCVTTSS2SI_GPR32i32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower single-precision (32-bit) floating-point element in "a" to a 32-bit integer with truncation,
and store the result in "dst".
	[sae_note]

[algorithm]

dst[31:0] := Convert_FP32_To_Int32_Truncate(a[31:0])

--------------------------------------------------------------------------------------------------------------

## VCVTTSS2SI - _mm_cvtt_roundss_i64

| VCVTTSS2SI_GPR64i64_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower single-precision (32-bit) floating-point element in "a" to a 64-bit integer with truncation,
and store the result in "dst".
	[sae_note]

[algorithm]

dst[63:0] := Convert_FP32_To_Int64_Truncate(a[31:0])

--------------------------------------------------------------------------------------------------------------

## VCVTTSS2SI - _mm_cvtt_roundss_si32

| VCVTTSS2SI_GPR32i32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower single-precision (32-bit) floating-point element in "a" to a 32-bit integer with truncation,
and store the result in "dst".
	[sae_note]

[algorithm]

dst[31:0] := Convert_FP32_To_Int32_Truncate(a[31:0])

--------------------------------------------------------------------------------------------------------------

## VCVTTSS2SI - _mm_cvtt_roundss_si64

| VCVTTSS2SI_GPR64i64_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower single-precision (32-bit) floating-point element in "a" to a 64-bit integer with truncation,
and store the result in "dst".
	[sae_note]

[algorithm]

dst[63:0] := Convert_FP32_To_Int64_Truncate(a[31:0])

--------------------------------------------------------------------------------------------------------------

## VCVTTSS2SI - _mm_cvttss_i32

| VCVTTSS2SI_GPR32i32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower single-precision (32-bit) floating-point element in "a" to a 32-bit integer with truncation,
and store the result in "dst".

[algorithm]

dst[31:0] := Convert_FP32_To_Int32_Truncate(a[31:0])

--------------------------------------------------------------------------------------------------------------

## VCVTTSS2SI - _mm_cvttss_i64

| VCVTTSS2SI_GPR64i64_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower single-precision (32-bit) floating-point element in "a" to a 64-bit integer with truncation,
and store the result in "dst".

[algorithm]

dst[63:0] := Convert_FP32_To_Int64_Truncate(a[31:0])

--------------------------------------------------------------------------------------------------------------

## VCVTTSS2USI - _mm_cvtt_roundss_u32

| VCVTTSS2USI_GPR32u32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower single-precision (32-bit) floating-point element in "a" to an unsigned 32-bit integer with
truncation, and store the result in "dst".
	[sae_note]

[algorithm]

dst[31:0] := Convert_FP32_To_UInt32_Truncate(a[31:0])

--------------------------------------------------------------------------------------------------------------

## VCVTTSS2USI - _mm_cvtt_roundss_u64

| VCVTTSS2USI_GPR64u64_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower single-precision (32-bit) floating-point element in "a" to an unsigned 64-bit integer with
truncation, and store the result in "dst".
	[sae_note]

[algorithm]

dst[63:0] := Convert_FP32_To_UInt64_Truncate(a[31:0])

--------------------------------------------------------------------------------------------------------------

## VCVTTSS2USI - _mm_cvttss_u32

| VCVTTSS2USI_GPR32u32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower single-precision (32-bit) floating-point element in "a" to an unsigned 32-bit integer with
truncation, and store the result in "dst".

[algorithm]

dst[31:0] := Convert_FP32_To_UInt32_Truncate(a[31:0])

--------------------------------------------------------------------------------------------------------------

## VCVTTSS2USI - _mm_cvttss_u64

| VCVTTSS2USI_GPR64u64_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the lower single-precision (32-bit) floating-point element in "a" to an unsigned 64-bit integer with
truncation, and store the result in "dst".

[algorithm]

dst[63:0] := Convert_FP32_To_UInt64_Truncate(a[31:0])

--------------------------------------------------------------------------------------------------------------

## VCVTUDQ2PD - _mm512_cvtepu32_pd

| VCVTUDQ2PD_ZMMf64_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed double-precision (64-bit) floating-point elements,
and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    dst[i+63:i] := Convert_Int64_To_FP64(a[l+31:l])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUDQ2PD - _mm512_mask_cvtepu32_pd

| VCVTUDQ2PD_ZMMf64_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed double-precision (64-bit) floating-point elements,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_Int64_To_FP64(a[l+31:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUDQ2PD - _mm512_maskz_cvtepu32_pd

| VCVTUDQ2PD_ZMMf64_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed double-precision (64-bit) floating-point elements,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    l := j*32
    IF k[j]
        dst[i+63:i] := Convert_Int64_To_FP64(a[l+31:l])
    ELSE
        dst[i+63:i] := 0
    FI    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUDQ2PS - _mm512_cvt_roundepu32_ps

| VCVTUDQ2PS_ZMMf32_MASKmskw_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed single-precision (32-bit) floating-point elements,
and store the results in "dst".
	[round_note]

[algorithm]

FOR j := 0 to 15
    i := 32*j
    dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUDQ2PS - _mm512_cvtepu32_ps

| VCVTUDQ2PS_ZMMf32_MASKmskw_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed single-precision (32-bit) floating-point elements,
and store the results in "dst".

[algorithm]

FOR j := 0 to 15
    i := 32*j
    dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUDQ2PS - _mm512_mask_cvt_roundepu32_ps

| VCVTUDQ2PS_ZMMf32_MASKmskw_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed single-precision (32-bit) floating-point elements,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set). 
	[round_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUDQ2PS - _mm512_mask_cvtepu32_ps

| VCVTUDQ2PS_ZMMf32_MASKmskw_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed single-precision (32-bit) floating-point elements,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUDQ2PS - _mm512_maskz_cvt_roundepu32_ps

| VCVTUDQ2PS_ZMMf32_MASKmskw_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed single-precision (32-bit) floating-point elements,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).
	[round_note]

[algorithm]

FOR j := 0 to 15
    i := 32*j
    IF k[j]
        dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUDQ2PS - _mm512_maskz_cvtepu32_ps

| VCVTUDQ2PS_ZMMf32_MASKmskw_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed single-precision (32-bit) floating-point elements,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 15
    i := 32*j
    IF k[j]
        dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUSI2SD - _mm_cvt_roundu64_sd

| VCVTUSI2SD_XMMf64_XMMf64_GPR64u64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the unsigned 64-bit integer "b" to a double-precision (64-bit) floating-point element, store the
result in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst".
[round_note]

[algorithm]

dst[63:0] := Convert_Int64_To_FP64(b[63:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUSI2SD - _mm_cvtu32_sd

| VCVTUSI2SD_XMMf64_XMMf64_GPR32u32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the unsigned 32-bit integer "b" to a double-precision (64-bit) floating-point element, store the
result in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst".

[algorithm]

dst[63:0] := Convert_Int32_To_FP64(b[31:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUSI2SD - _mm_cvtu64_sd

| VCVTUSI2SD_XMMf64_XMMf64_GPR64u64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the unsigned 64-bit integer "b" to a double-precision (64-bit) floating-point element, store the
result in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst".

[algorithm]

dst[63:0] := Convert_Int64_To_FP64(b[63:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUSI2SS - _mm_cvt_roundu32_ss

| VCVTUSI2SS_XMMf32_XMMf32_GPR32u32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the unsigned 32-bit integer "b" to a single-precision (32-bit) floating-point element, store the
result in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of
"dst". 
	[round_note]

[algorithm]

dst[31:0] := Convert_Int32_To_FP32(b[31:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUSI2SS - _mm_cvt_roundu64_ss

| VCVTUSI2SS_XMMf32_XMMf32_GPR64u64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the unsigned 64-bit integer "b" to a single-precision (32-bit) floating-point element, store the
result in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of
"dst". 
	[round_note]

[algorithm]

dst[31:0] := Convert_Int64_To_FP32(b[63:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUSI2SS - _mm_cvtu32_ss

| VCVTUSI2SS_XMMf32_XMMf32_GPR32u32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the unsigned 32-bit integer "b" to a single-precision (32-bit) floating-point element, store the
result in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of
"dst".

[algorithm]

dst[31:0] := Convert_Int32_To_FP32(b[31:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTUSI2SS - _mm_cvtu64_ss

| VCVTUSI2SS_XMMf32_XMMf32_GPR64u64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the unsigned 64-bit integer "b" to a single-precision (32-bit) floating-point element, store the
result in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of
"dst".

[algorithm]

dst[31:0] := Convert_Int64_To_FP32(b[63:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VDIVPD - _mm512_div_pd

| VDIVPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Divide packed double-precision (64-bit) floating-point elements in "a" by packed elements in "b", and store
the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := 64*j
    dst[i+63:i] := a[i+63:i] / b[i+63:i]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VDIVPD - _mm512_div_round_pd

| VDIVPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Divide packed double-precision (64-bit) floating-point elements in "a" by packed elements in "b", =and store
the results in "dst".
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := 64*j
    dst[i+63:i] := a[i+63:i] / b[i+63:i]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VDIVPD - _mm512_mask_div_pd

| VDIVPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Divide packed double-precision (64-bit) floating-point elements in "a" by packed elements in "b", and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := 64*j
    IF k[j]
        dst[i+63:i] := a[i+63:i] / b[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VDIVPD - _mm512_mask_div_round_pd

| VDIVPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Divide packed double-precision (64-bit) floating-point elements in "a" by packed elements in "b", and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := 64*j
    IF k[j]
        dst[i+63:i] := a[i+63:i] / b[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VDIVPD - _mm512_maskz_div_pd

| VDIVPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Divide packed double-precision (64-bit) floating-point elements in "a" by packed elements in "b", and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 64*j
    IF k[j]
        dst[i+63:i] := a[i+63:i] / b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VDIVPD - _mm512_maskz_div_round_pd

| VDIVPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Divide packed double-precision (64-bit) floating-point elements in "a" by packed elements in "b", and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := 64*j
    IF k[j]
        dst[i+63:i] := a[i+63:i] / b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VDIVPS - _mm512_div_ps

| VDIVPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Divide packed single-precision (32-bit) floating-point elements in "a" by packed elements in "b", and store
the results in "dst".

[algorithm]

FOR j := 0 to 15
    i := 32*j
    dst[i+31:i] := a[i+31:i] / b[i+31:i]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VDIVPS - _mm512_div_round_ps

| VDIVPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Divide packed single-precision (32-bit) floating-point elements in "a" by packed elements in "b", and store
the results in "dst".
	[round_note]

[algorithm]

FOR j := 0 to 15
    i := 32*j
    dst[i+31:i] := a[i+31:i] / b[i+31:i]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VDIVPS - _mm512_mask_div_ps

| VDIVPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Divide packed single-precision (32-bit) floating-point elements in "a" by packed elements in "b", and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 15
    i := 32*j
    IF k[j]
        dst[i+31:i] := a[i+31:i] / b[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VDIVPS - _mm512_mask_div_round_ps

| VDIVPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Divide packed single-precision (32-bit) floating-point elements in "a" by packed elements in "b", and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).
	[round_note]

[algorithm]

FOR j := 0 to 15
    i := 32*j
    IF k[j]
        dst[i+31:i] := a[i+31:i] / b[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VDIVPS - _mm512_maskz_div_ps

| VDIVPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Divide packed single-precision (32-bit) floating-point elements in "a" by packed elements in "b", and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := 32*j
    IF k[j]
        dst[i+31:i] := a[i+31:i] / b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VDIVPS - _mm512_maskz_div_round_ps

| VDIVPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Divide packed single-precision (32-bit) floating-point elements in "a" by packed elements in "b", and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).
	[round_note]

[algorithm]

FOR j := 0 to 15
    i := 32*j
    IF k[j]
        dst[i+31:i] := a[i+31:i] / b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VDIVSD - _mm_div_round_sd

| VDIVSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Divide the lower double-precision (64-bit) floating-point element in "a" by the lower double-precision
(64-bit) floating-point element in "b", store the result in the lower element of "dst", and copy the upper
element from "a" to the upper element of "dst".
		[round_note]

[algorithm]

dst[63:0] := a[63:0] / b[63:0]
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VDIVSD - _mm_mask_div_round_sd

| VDIVSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Divide the lower double-precision (64-bit) floating-point element in "a" by the lower double-precision
(64-bit) floating-point element in "b", store the result in the lower element of "dst" using writemask "k" (the
element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper
element of "dst". 
		[round_note]

[algorithm]

IF k[0]
    dst[63:0] := a[63:0] / b[63:0]
ELSE
    dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VDIVSD - _mm_mask_div_sd

| VDIVSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Divide the lower double-precision (64-bit) floating-point element in "a" by the lower double-precision
(64-bit) floating-point element in "b", store the result in the lower element of "dst" using writemask "k" (the
element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper
element of "dst".

[algorithm]

IF k[0]
    dst[63:0] := a[63:0] / b[63:0]
ELSE
    dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VDIVSD - _mm_maskz_div_round_sd

| VDIVSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Divide the lower double-precision (64-bit) floating-point element in "a" by the lower double-precision
(64-bit) floating-point element in "b", store the result in the lower element of "dst" using zeromask "k" (the
element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of
"dst".
		[round_note]

[algorithm]

IF k[0]
    dst[63:0] := a[63:0] / b[63:0]
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VDIVSD - _mm_maskz_div_sd

| VDIVSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Divide the lower double-precision (64-bit) floating-point element in "a" by the lower double-precision
(64-bit) floating-point element in "b", store the result in the lower element of "dst" using zeromask "k" (the
element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of
"dst".

[algorithm]

IF k[0]
    dst[63:0] := a[63:0] / b[63:0]
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VDIVSS - _mm_div_round_ss

| VDIVSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Divide the lower single-precision (32-bit) floating-point element in "a" by the lower single-precision
(32-bit) floating-point element in "b", store the result in the lower element of "dst", and copy the upper 3
packed elements from "a" to the upper elements of "dst".
		[round_note]

[algorithm]

dst[31:0] := a[31:0] / b[31:0]
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VDIVSS - _mm_mask_div_round_ss

| VDIVSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Divide the lower single-precision (32-bit) floating-point element in "a" by the lower single-precision
(32-bit) floating-point element in "b", store the result in the lower element of "dst" using writemask "k" (the
element is copied from "src" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the
upper elements of "dst". 
		[round_note]

[algorithm]

IF k[0]
    dst[31:0] := a[31:0] / b[31:0]
ELSE
    dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VDIVSS - _mm_mask_div_ss

| VDIVSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Divide the lower single-precision (32-bit) floating-point element in "a" by the lower single-precision
(32-bit) floating-point element in "b", store the result in the lower element of "dst" using writemask "k" (the
element is copied from "src" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the
upper elements of "dst".

[algorithm]

IF k[0]
    dst[31:0] := a[31:0] / b[31:0]
ELSE
    dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VDIVSS - _mm_maskz_div_round_ss

| VDIVSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Divide the lower single-precision (32-bit) floating-point element in "a" by the lower single-precision
(32-bit) floating-point element in "b", store the result in the lower element of "dst" using zeromask "k" (the
element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper
elements of "dst".
		[round_note]

[algorithm]

IF k[0]
    dst[31:0] := a[31:0] / b[31:0]
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VDIVSS - _mm_maskz_div_ss

| VDIVSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Divide the lower single-precision (32-bit) floating-point element in "a" by the lower single-precision
(32-bit) floating-point element in "b", store the result in the lower element of "dst" using zeromask "k" (the
element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper
elements of "dst".

[algorithm]

IF k[0]
    dst[31:0] := a[31:0] / b[31:0]
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VEXPANDPD - _mm512_mask_expand_pd

| VEXPANDPD_ZMMf64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active double-precision (64-bit) floating-point elements from "a" (those with their respective
bit set in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := a[m+63:m]
        m := m + 64
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VEXPANDPD - _mm512_mask_expandloadu_pd

| VEXPANDPD_ZMMf64_MASKmskw_MEMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active double-precision (64-bit) floating-point elements from unaligned memory at "mem_addr"
(those with their respective bit set in mask "k"), and store the results in "dst" using writemask "k" (elements
are copied from "src" when the corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
        m := m + 64
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VEXPANDPD - _mm512_maskz_expand_pd

| VEXPANDPD_ZMMf64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active double-precision (64-bit) floating-point elements from "a" (those with their respective
bit set in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := a[m+63:m]
        m := m + 64
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VEXPANDPD - _mm512_maskz_expandloadu_pd

| VEXPANDPD_ZMMf64_MASKmskw_MEMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active double-precision (64-bit) floating-point elements from unaligned memory at "mem_addr"
(those with their respective bit set in mask "k"), and store the results in "dst" using zeromask "k" (elements
are zeroed out when the corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
        m := m + 64
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VEXPANDPS - _mm512_mask_expand_ps

| VEXPANDPS_ZMMf32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active single-precision (32-bit) floating-point elements from "a" (those with their respective
bit set in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := a[m+31:m]
        m := m + 32
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VEXPANDPS - _mm512_mask_expandloadu_ps

| VEXPANDPS_ZMMf32_MASKmskw_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active single-precision (32-bit) floating-point elements from unaligned memory at "mem_addr"
(those with their respective bit set in mask "k"), and store the results in "dst" using writemask "k" (elements
are copied from "src" when the corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
        m := m + 32
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VEXPANDPS - _mm512_maskz_expand_ps

| VEXPANDPS_ZMMf32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active single-precision (32-bit) floating-point elements from "a" (those with their respective
bit set in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := a[m+31:m]
        m := m + 32
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VEXPANDPS - _mm512_maskz_expandloadu_ps

| VEXPANDPS_ZMMf32_MASKmskw_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active single-precision (32-bit) floating-point elements from unaligned memory at "mem_addr"
(those with their respective bit set in mask "k"), and store the results in "dst" using zeromask "k" (elements
are zeroed out when the corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
        m := m + 32
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VEXTRACTF32X4 - _mm512_extractf32x4_ps

| VEXTRACTF32X4_XMMf32_MASKmskw_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract 128 bits (composed of 4 packed single-precision (32-bit) floating-point elements) from "a", selected
with "imm8", and store the result in "dst".

[algorithm]

CASE imm8[1:0] OF
0: dst[127:0] := a[127:0]
1: dst[127:0] := a[255:128]
2: dst[127:0] := a[383:256]
3: dst[127:0] := a[511:384]
ESAC
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VEXTRACTF32X4 - _mm512_mask_extractf32x4_ps

| VEXTRACTF32X4_XMMf32_MASKmskw_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract 128 bits (composed of 4 packed single-precision (32-bit) floating-point elements) from "a", selected
with "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

CASE imm8[1:0] OF
0: tmp[127:0] := a[127:0]
1: tmp[127:0] := a[255:128]
2: tmp[127:0] := a[383:256]
3: tmp[127:0] := a[511:384]
ESAC
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VEXTRACTF32X4 - _mm512_maskz_extractf32x4_ps

| VEXTRACTF32X4_XMMf32_MASKmskw_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract 128 bits (composed of 4 packed single-precision (32-bit) floating-point elements) from "a", selected
with "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set).

[algorithm]

CASE imm8[1:0] OF
0: tmp[127:0] := a[127:0]
1: tmp[127:0] := a[255:128]
2: tmp[127:0] := a[383:256]
3: tmp[127:0] := a[511:384]
ESAC
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VEXTRACTF64X4 - _mm512_extractf64x4_pd

| VEXTRACTF64X4_YMMf64_MASKmskw_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract 256 bits (composed of 4 packed double-precision (64-bit) floating-point elements) from "a", selected
with "imm8", and store the result in "dst".

[algorithm]

CASE imm8[0] OF
0: dst[255:0] := a[255:0]
1: dst[255:0] := a[511:256]
ESAC
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VEXTRACTF64X4 - _mm512_mask_extractf64x4_pd

| VEXTRACTF64X4_YMMf64_MASKmskw_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract 256 bits (composed of 4 packed double-precision (64-bit) floating-point elements) from "a", selected
with "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

CASE imm8[0] OF
0: tmp[255:0] := a[255:0]
1: tmp[255:0] := a[511:256]
ESAC
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VEXTRACTF64X4 - _mm512_maskz_extractf64x4_pd

| VEXTRACTF64X4_YMMf64_MASKmskw_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract 256 bits (composed of 4 packed double-precision (64-bit) floating-point elements) from "a", selected
with "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set).

[algorithm]

CASE imm8[0] OF
0: tmp[255:0] := a[255:0]
1: tmp[255:0] := a[511:256]
ESAC
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VEXTRACTI32X4 - _mm512_extracti32x4_epi32

| VEXTRACTI32X4_XMMu32_MASKmskw_ZMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract 128 bits (composed of 4 packed 32-bit integers) from "a", selected with "imm8", and store the result
in "dst".

[algorithm]

CASE imm8[1:0] OF
0: dst[127:0] := a[127:0]
1: dst[127:0] := a[255:128]
2: dst[127:0] := a[383:256]
3: dst[127:0] := a[511:384]
ESAC
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VEXTRACTI32X4 - _mm512_mask_extracti32x4_epi32

| VEXTRACTI32X4_XMMu32_MASKmskw_ZMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract 128 bits (composed of 4 packed 32-bit integers) from "a", selected with "imm8", and store the results
in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

CASE imm8[1:0] OF
0: tmp[127:0] := a[127:0]
1: tmp[127:0] := a[255:128]
2: tmp[127:0] := a[383:256]
3: tmp[127:0] := a[511:384]
ESAC
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VEXTRACTI32X4 - _mm512_maskz_extracti32x4_epi32

| VEXTRACTI32X4_XMMu32_MASKmskw_ZMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract 128 bits (composed of 4 packed 32-bit integers) from "a", selected with "imm8", and store the results
in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

CASE imm8[1:0] OF
0: tmp[127:0] := a[127:0]
1: tmp[127:0] := a[255:128]
2: tmp[127:0] := a[383:256]
3: tmp[127:0] := a[511:384]
ESAC
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VEXTRACTI64X4 - _mm512_extracti64x4_epi64

| VEXTRACTI64X4_YMMu64_MASKmskw_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract 256 bits (composed of 4 packed 64-bit integers) from "a", selected with "imm8", and store the result
in "dst".

[algorithm]

CASE imm8[0] OF
0: dst[255:0] := a[255:0]
1: dst[255:0] := a[511:256]
ESAC
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VEXTRACTI64X4 - _mm512_mask_extracti64x4_epi64

| VEXTRACTI64X4_YMMu64_MASKmskw_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract 256 bits (composed of 4 packed 64-bit integers) from "a", selected with "imm8", and store the results
in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

CASE imm8[0] OF
0: tmp[255:0] := a[255:0]
1: tmp[255:0] := a[511:256]
ESAC
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VEXTRACTI64X4 - _mm512_maskz_extracti64x4_epi64

| VEXTRACTI64X4_YMMu64_MASKmskw_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Extract 256 bits (composed of 4 packed 64-bit integers) from "a", selected with "imm8", and store the results
in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

CASE imm8[0] OF
0: tmp[255:0] := a[255:0]
1: tmp[255:0] := a[511:256]
ESAC
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VFIXUPIMMPD - _mm512_fixupimm_pd

| VFIXUPIMMPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Fix up packed double-precision (64-bit) floating-point elements in "a" and "b" using packed 64-bit integers in
"c", and store the results in "dst". "imm8" is used to set the required flags reporting.

[algorithm]

enum TOKEN_TYPE {
    QNAN_TOKEN := 0, \
    SNAN_TOKEN := 1, \
    ZERO_VALUE_TOKEN := 2, \
    ONE_VALUE_TOKEN := 3, \
    NEG_INF_TOKEN := 4, \
    POS_INF_TOKEN := 5, \
    NEG_VALUE_TOKEN := 6, \
    POS_VALUE_TOKEN := 7
}
DEFINE FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]) {
    tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
    CASE(tsrc[63:0]) OF
    QNAN_TOKEN:j := 0
    SNAN_TOKEN:j := 1
    ZERO_VALUE_TOKEN: j := 2
    ONE_VALUE_TOKEN: j := 3
    NEG_INF_TOKEN: j := 4
    POS_INF_TOKEN: j := 5
    NEG_VALUE_TOKEN: j := 6
    POS_VALUE_TOKEN: j := 7
    ESAC
    
    token_response[3:0] := src3[3+4*j:4*j]
    
    CASE(token_response[3:0]) OF
    0 : dest[63:0] := src1[63:0]
    1 : dest[63:0] := tsrc[63:0]
    2 : dest[63:0] := QNaN(tsrc[63:0])
    3 : dest[63:0] := QNAN_Indefinite
    4 : dest[63:0] := -INF
    5 : dest[63:0] := +INF
    6 : dest[63:0] := tsrc.sign? -INF : +INF
    7 : dest[63:0] := -0
    8 : dest[63:0] := +0
    9 : dest[63:0] := -1
    10: dest[63:0] := +1
    11: dest[63:0] := 1/2
    12: dest[63:0] := 90.0
    13: dest[63:0] := PI/2
    14: dest[63:0] := MAX_FLOAT
    15: dest[63:0] := -MAX_FLOAT
    ESAC
    
    CASE(tsrc[31:0]) OF
    ZERO_VALUE_TOKEN:
        IF (imm8[0]) #ZE; FI
    ZERO_VALUE_TOKEN:
        IF (imm8[1]) #IE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[2]) #ZE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[3]) #IE; FI
    SNAN_TOKEN:
        IF (imm8[4]) #IE; FI
    NEG_INF_TOKEN:
        IF (imm8[5]) #IE; FI
    NEG_VALUE_TOKEN:
        IF (imm8[6]) #IE; FI
    POS_INF_TOKEN:
        IF (imm8[7]) #IE; FI
    ESAC
    RETURN dest[63:0]
}
FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFIXUPIMMPD - _mm512_fixupimm_round_pd

| VFIXUPIMMPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Fix up packed double-precision (64-bit) floating-point elements in "a" and "b" using packed 64-bit integers in
"c", and store the results in "dst". "imm8" is used to set the required flags reporting.
	[sae_note]

[algorithm]

enum TOKEN_TYPE {
    QNAN_TOKEN := 0, \
    SNAN_TOKEN := 1, \
    ZERO_VALUE_TOKEN := 2, \
    ONE_VALUE_TOKEN := 3, \
    NEG_INF_TOKEN := 4, \
    POS_INF_TOKEN := 5, \
    NEG_VALUE_TOKEN := 6, \
    POS_VALUE_TOKEN := 7
}
DEFINE FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]) {
    tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
    CASE(tsrc[63:0]) OF
    QNAN_TOKEN:j := 0
    SNAN_TOKEN:j := 1
    ZERO_VALUE_TOKEN: j := 2
    ONE_VALUE_TOKEN: j := 3
    NEG_INF_TOKEN: j := 4
    POS_INF_TOKEN: j := 5
    NEG_VALUE_TOKEN: j := 6
    POS_VALUE_TOKEN: j := 7
    ESAC
    
    token_response[3:0] := src3[3+4*j:4*j]
    
    CASE(token_response[3:0]) OF
    0 : dest[63:0] := src1[63:0]
    1 : dest[63:0] := tsrc[63:0]
    2 : dest[63:0] := QNaN(tsrc[63:0])
    3 : dest[63:0] := QNAN_Indefinite
    4 : dest[63:0] := -INF
    5 : dest[63:0] := +INF
    6 : dest[63:0] := tsrc.sign? -INF : +INF
    7 : dest[63:0] := -0
    8 : dest[63:0] := +0
    9 : dest[63:0] := -1
    10: dest[63:0] := +1
    11: dest[63:0] := 1/2
    12: dest[63:0] := 90.0
    13: dest[63:0] := PI/2
    14: dest[63:0] := MAX_FLOAT
    15: dest[63:0] := -MAX_FLOAT
    ESAC
    
    CASE(tsrc[31:0]) OF
    ZERO_VALUE_TOKEN:
        IF (imm8[0]) #ZE; FI
    ZERO_VALUE_TOKEN:
        IF (imm8[1]) #IE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[2]) #ZE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[3]) #IE; FI
    SNAN_TOKEN:
        IF (imm8[4]) #IE; FI
    NEG_INF_TOKEN:
        IF (imm8[5]) #IE; FI
    NEG_VALUE_TOKEN:
        IF (imm8[6]) #IE; FI
    POS_INF_TOKEN:
        IF (imm8[7]) #IE; FI
    ESAC
    RETURN dest[63:0]
}
FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFIXUPIMMPD - _mm512_mask_fixupimm_pd

| VFIXUPIMMPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Fix up packed double-precision (64-bit) floating-point elements in "a" and "b" using packed 64-bit integers in
"c", and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding
mask bit is not set). "imm8" is used to set the required flags reporting.

[algorithm]

enum TOKEN_TYPE {
    QNAN_TOKEN := 0, \
    SNAN_TOKEN := 1, \
    ZERO_VALUE_TOKEN := 2, \
    ONE_VALUE_TOKEN := 3, \
    NEG_INF_TOKEN := 4, \
    POS_INF_TOKEN := 5, \
    NEG_VALUE_TOKEN := 6, \
    POS_VALUE_TOKEN := 7
}
DEFINE FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]) {
    tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
    CASE(tsrc[63:0]) OF
    QNAN_TOKEN:j := 0
    SNAN_TOKEN:j := 1
    ZERO_VALUE_TOKEN: j := 2
    ONE_VALUE_TOKEN: j := 3
    NEG_INF_TOKEN: j := 4
    POS_INF_TOKEN: j := 5
    NEG_VALUE_TOKEN: j := 6
    POS_VALUE_TOKEN: j := 7
    ESAC
    
    token_response[3:0] := src3[3+4*j:4*j]
    
    CASE(token_response[3:0]) OF
    0 : dest[63:0] := src1[63:0]
    1 : dest[63:0] := tsrc[63:0]
    2 : dest[63:0] := QNaN(tsrc[63:0])
    3 : dest[63:0] := QNAN_Indefinite
    4 : dest[63:0] := -INF
    5 : dest[63:0] := +INF
    6 : dest[63:0] := tsrc.sign? -INF : +INF
    7 : dest[63:0] := -0
    8 : dest[63:0] := +0
    9 : dest[63:0] := -1
    10: dest[63:0] := +1
    11: dest[63:0] := 1/2
    12: dest[63:0] := 90.0
    13: dest[63:0] := PI/2
    14: dest[63:0] := MAX_FLOAT
    15: dest[63:0] := -MAX_FLOAT
    ESAC
    
    CASE(tsrc[31:0]) OF
    ZERO_VALUE_TOKEN:
        IF (imm8[0]) #ZE; FI
    ZERO_VALUE_TOKEN:
        IF (imm8[1]) #IE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[2]) #ZE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[3]) #IE; FI
    SNAN_TOKEN:
        IF (imm8[4]) #IE; FI
    NEG_INF_TOKEN:
        IF (imm8[5]) #IE; FI
    NEG_VALUE_TOKEN:
        IF (imm8[6]) #IE; FI
    POS_INF_TOKEN:
        IF (imm8[7]) #IE; FI
    ESAC
    RETURN dest[63:0]
}
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFIXUPIMMPD - _mm512_mask_fixupimm_round_pd

| VFIXUPIMMPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Fix up packed double-precision (64-bit) floating-point elements in "a" and "b" using packed 64-bit integers in
"c", and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding
mask bit is not set). "imm8" is used to set the required flags reporting.
	[sae_note]

[algorithm]

enum TOKEN_TYPE {
    QNAN_TOKEN := 0, \
    SNAN_TOKEN := 1, \
    ZERO_VALUE_TOKEN := 2, \
    ONE_VALUE_TOKEN := 3, \
    NEG_INF_TOKEN := 4, \
    POS_INF_TOKEN := 5, \
    NEG_VALUE_TOKEN := 6, \
    POS_VALUE_TOKEN := 7
}
DEFINE FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]) {
    tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
    CASE(tsrc[63:0]) OF
    QNAN_TOKEN:j := 0
    SNAN_TOKEN:j := 1
    ZERO_VALUE_TOKEN: j := 2
    ONE_VALUE_TOKEN: j := 3
    NEG_INF_TOKEN: j := 4
    POS_INF_TOKEN: j := 5
    NEG_VALUE_TOKEN: j := 6
    POS_VALUE_TOKEN: j := 7
    ESAC
    
    token_response[3:0] := src3[3+4*j:4*j]
    
    CASE(token_response[3:0]) OF
    0 : dest[63:0] := src1[63:0]
    1 : dest[63:0] := tsrc[63:0]
    2 : dest[63:0] := QNaN(tsrc[63:0])
    3 : dest[63:0] := QNAN_Indefinite
    4 : dest[63:0] := -INF
    5 : dest[63:0] := +INF
    6 : dest[63:0] := tsrc.sign? -INF : +INF
    7 : dest[63:0] := -0
    8 : dest[63:0] := +0
    9 : dest[63:0] := -1
    10: dest[63:0] := +1
    11: dest[63:0] := 1/2
    12: dest[63:0] := 90.0
    13: dest[63:0] := PI/2
    14: dest[63:0] := MAX_FLOAT
    15: dest[63:0] := -MAX_FLOAT
    ESAC
    
    CASE(tsrc[31:0]) OF
    ZERO_VALUE_TOKEN:
        IF (imm8[0]) #ZE; FI
    ZERO_VALUE_TOKEN:
        IF (imm8[1]) #IE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[2]) #ZE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[3]) #IE; FI
    SNAN_TOKEN:
        IF (imm8[4]) #IE; FI
    NEG_INF_TOKEN:
        IF (imm8[5]) #IE; FI
    NEG_VALUE_TOKEN:
        IF (imm8[6]) #IE; FI
    POS_INF_TOKEN:
        IF (imm8[7]) #IE; FI
    ESAC
    RETURN dest[63:0]
}
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFIXUPIMMPD - _mm512_maskz_fixupimm_pd

| VFIXUPIMMPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Fix up packed double-precision (64-bit) floating-point elements in "a" and "b" using packed 64-bit integers in
"c", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit
is not set). "imm8" is used to set the required flags reporting.

[algorithm]

enum TOKEN_TYPE {
    QNAN_TOKEN := 0, \
    SNAN_TOKEN := 1, \
    ZERO_VALUE_TOKEN := 2, \
    ONE_VALUE_TOKEN := 3, \
    NEG_INF_TOKEN := 4, \
    POS_INF_TOKEN := 5, \
    NEG_VALUE_TOKEN := 6, \
    POS_VALUE_TOKEN := 7
}
DEFINE FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]) {
    tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
    CASE(tsrc[63:0]) OF
    QNAN_TOKEN:j := 0
    SNAN_TOKEN:j := 1
    ZERO_VALUE_TOKEN: j := 2
    ONE_VALUE_TOKEN: j := 3
    NEG_INF_TOKEN: j := 4
    POS_INF_TOKEN: j := 5
    NEG_VALUE_TOKEN: j := 6
    POS_VALUE_TOKEN: j := 7
    ESAC
    
    token_response[3:0] := src3[3+4*j:4*j]
    
    CASE(token_response[3:0]) OF
    0 : dest[63:0] := src1[63:0]
    1 : dest[63:0] := tsrc[63:0]
    2 : dest[63:0] := QNaN(tsrc[63:0])
    3 : dest[63:0] := QNAN_Indefinite
    4 : dest[63:0] := -INF
    5 : dest[63:0] := +INF
    6 : dest[63:0] := tsrc.sign? -INF : +INF
    7 : dest[63:0] := -0
    8 : dest[63:0] := +0
    9 : dest[63:0] := -1
    10: dest[63:0] := +1
    11: dest[63:0] := 1/2
    12: dest[63:0] := 90.0
    13: dest[63:0] := PI/2
    14: dest[63:0] := MAX_FLOAT
    15: dest[63:0] := -MAX_FLOAT
    ESAC
    
    CASE(tsrc[31:0]) OF
    ZERO_VALUE_TOKEN:
        IF (imm8[0]) #ZE; FI
    ZERO_VALUE_TOKEN:
        IF (imm8[1]) #IE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[2]) #ZE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[3]) #IE; FI
    SNAN_TOKEN:
        IF (imm8[4]) #IE; FI
    NEG_INF_TOKEN:
        IF (imm8[5]) #IE; FI
    NEG_VALUE_TOKEN:
        IF (imm8[6]) #IE; FI
    POS_INF_TOKEN:
        IF (imm8[7]) #IE; FI
    ESAC
    RETURN dest[63:0]
}
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFIXUPIMMPD - _mm512_maskz_fixupimm_round_pd

| VFIXUPIMMPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Fix up packed double-precision (64-bit) floating-point elements in "a" and "b" using packed 64-bit integers in
"c", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit
is not set). "imm8" is used to set the required flags reporting.
	[sae_note]

[algorithm]

enum TOKEN_TYPE {
    QNAN_TOKEN := 0, \
    SNAN_TOKEN := 1, \
    ZERO_VALUE_TOKEN := 2, \
    ONE_VALUE_TOKEN := 3, \
    NEG_INF_TOKEN := 4, \
    POS_INF_TOKEN := 5, \
    NEG_VALUE_TOKEN := 6, \
    POS_VALUE_TOKEN := 7
}
DEFINE FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]) {
    tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
    CASE(tsrc[63:0]) OF
    QNAN_TOKEN:j := 0
    SNAN_TOKEN:j := 1
    ZERO_VALUE_TOKEN: j := 2
    ONE_VALUE_TOKEN: j := 3
    NEG_INF_TOKEN: j := 4
    POS_INF_TOKEN: j := 5
    NEG_VALUE_TOKEN: j := 6
    POS_VALUE_TOKEN: j := 7
    ESAC
    
    token_response[3:0] := src3[3+4*j:4*j]
    
    CASE(token_response[3:0]) OF
    0 : dest[63:0] := src1[63:0]
    1 : dest[63:0] := tsrc[63:0]
    2 : dest[63:0] := QNaN(tsrc[63:0])
    3 : dest[63:0] := QNAN_Indefinite
    4 : dest[63:0] := -INF
    5 : dest[63:0] := +INF
    6 : dest[63:0] := tsrc.sign? -INF : +INF
    7 : dest[63:0] := -0
    8 : dest[63:0] := +0
    9 : dest[63:0] := -1
    10: dest[63:0] := +1
    11: dest[63:0] := 1/2
    12: dest[63:0] := 90.0
    13: dest[63:0] := PI/2
    14: dest[63:0] := MAX_FLOAT
    15: dest[63:0] := -MAX_FLOAT
    ESAC
    
    CASE(tsrc[31:0]) OF
    ZERO_VALUE_TOKEN:
        IF (imm8[0]) #ZE; FI
    ZERO_VALUE_TOKEN:
        IF (imm8[1]) #IE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[2]) #ZE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[3]) #IE; FI
    SNAN_TOKEN:
        IF (imm8[4]) #IE; FI
    NEG_INF_TOKEN:
        IF (imm8[5]) #IE; FI
    NEG_VALUE_TOKEN:
        IF (imm8[6]) #IE; FI
    POS_INF_TOKEN:
        IF (imm8[7]) #IE; FI
    ESAC
    RETURN dest[63:0]
}
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFIXUPIMMPS - _mm512_fixupimm_ps

| VFIXUPIMMPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Fix up packed single-precision (32-bit) floating-point elements in "a" and "b" using packed 32-bit integers in
"c", and store the results in "dst". "imm8" is used to set the required flags reporting.

[algorithm]

enum TOKEN_TYPE {
    QNAN_TOKEN := 0, \
    SNAN_TOKEN := 1, \
    ZERO_VALUE_TOKEN := 2, \
    ONE_VALUE_TOKEN := 3, \
    NEG_INF_TOKEN := 4, \
    POS_INF_TOKEN := 5, \
    NEG_VALUE_TOKEN := 6, \
    POS_VALUE_TOKEN := 7
}
DEFINE FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]) {
    tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
    CASE(tsrc[31:0]) OF
    QNAN_TOKEN:j := 0
    SNAN_TOKEN:j := 1
    ZERO_VALUE_TOKEN: j := 2
    ONE_VALUE_TOKEN: j := 3
    NEG_INF_TOKEN: j := 4
    POS_INF_TOKEN: j := 5
    NEG_VALUE_TOKEN: j := 6
    POS_VALUE_TOKEN: j := 7
    ESAC
    
    token_response[3:0] := src3[3+4*j:4*j]
    
    CASE(token_response[3:0]) OF
    0 : dest[31:0] := src1[31:0]
    1 : dest[31:0] := tsrc[31:0]
    2 : dest[31:0] := QNaN(tsrc[31:0])
    3 : dest[31:0] := QNAN_Indefinite
    4 : dest[31:0] := -INF
    5 : dest[31:0] := +INF
    6 : dest[31:0] := tsrc.sign? -INF : +INF
    7 : dest[31:0] := -0
    8 : dest[31:0] := +0
    9 : dest[31:0] := -1
    10: dest[31:0] := +1
    11: dest[31:0] := 1/2
    12: dest[31:0] := 90.0
    13: dest[31:0] := PI/2
    14: dest[31:0] := MAX_FLOAT
    15: dest[31:0] := -MAX_FLOAT
    ESAC
    
    CASE(tsrc[31:0]) OF
    ZERO_VALUE_TOKEN:
        IF (imm8[0]) #ZE; FI
    ZERO_VALUE_TOKEN:
        IF (imm8[1]) #IE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[2]) #ZE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[3]) #IE; FI
    SNAN_TOKEN:
        IF (imm8[4]) #IE; FI
    NEG_INF_TOKEN:
        IF (imm8[5]) #IE; FI
    NEG_VALUE_TOKEN:
        IF (imm8[6]) #IE; FI
    POS_INF_TOKEN:
        IF (imm8[7]) #IE; FI
    ESAC
    RETURN dest[31:0]
}
FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFIXUPIMMPS - _mm512_fixupimm_round_ps

| VFIXUPIMMPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Fix up packed single-precision (32-bit) floating-point elements in "a" and "b" using packed 32-bit integers in
"c", and store the results in "dst". "imm8" is used to set the required flags reporting.
	[sae_note]

[algorithm]

enum TOKEN_TYPE {
    QNAN_TOKEN := 0, \
    SNAN_TOKEN := 1, \
    ZERO_VALUE_TOKEN := 2, \
    ONE_VALUE_TOKEN := 3, \
    NEG_INF_TOKEN := 4, \
    POS_INF_TOKEN := 5, \
    NEG_VALUE_TOKEN := 6, \
    POS_VALUE_TOKEN := 7
}
DEFINE FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]) {
    tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
    CASE(tsrc[31:0]) OF
    QNAN_TOKEN:j := 0
    SNAN_TOKEN:j := 1
    ZERO_VALUE_TOKEN: j := 2
    ONE_VALUE_TOKEN: j := 3
    NEG_INF_TOKEN: j := 4
    POS_INF_TOKEN: j := 5
    NEG_VALUE_TOKEN: j := 6
    POS_VALUE_TOKEN: j := 7
    ESAC
    
    token_response[3:0] := src3[3+4*j:4*j]
    
    CASE(token_response[3:0]) OF
    0 : dest[31:0] := src1[31:0]
    1 : dest[31:0] := tsrc[31:0]
    2 : dest[31:0] := QNaN(tsrc[31:0])
    3 : dest[31:0] := QNAN_Indefinite
    4 : dest[31:0] := -INF
    5 : dest[31:0] := +INF
    6 : dest[31:0] := tsrc.sign? -INF : +INF
    7 : dest[31:0] := -0
    8 : dest[31:0] := +0
    9 : dest[31:0] := -1
    10: dest[31:0] := +1
    11: dest[31:0] := 1/2
    12: dest[31:0] := 90.0
    13: dest[31:0] := PI/2
    14: dest[31:0] := MAX_FLOAT
    15: dest[31:0] := -MAX_FLOAT
    ESAC
    
    CASE(tsrc[31:0]) OF
    ZERO_VALUE_TOKEN:
        IF (imm8[0]) #ZE; FI
    ZERO_VALUE_TOKEN:
        IF (imm8[1]) #IE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[2]) #ZE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[3]) #IE; FI
    SNAN_TOKEN:
        IF (imm8[4]) #IE; FI
    NEG_INF_TOKEN:
        IF (imm8[5]) #IE; FI
    NEG_VALUE_TOKEN:
        IF (imm8[6]) #IE; FI
    POS_INF_TOKEN:
        IF (imm8[7]) #IE; FI
    ESAC
    RETURN dest[31:0]
}
FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFIXUPIMMPS - _mm512_mask_fixupimm_ps

| VFIXUPIMMPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Fix up packed single-precision (32-bit) floating-point elements in "a" and "b" using packed 32-bit integers in
"c", and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding
mask bit is not set). "imm8" is used to set the required flags reporting.

[algorithm]

enum TOKEN_TYPE {
    QNAN_TOKEN := 0, \
    SNAN_TOKEN := 1, \
    ZERO_VALUE_TOKEN := 2, \
    ONE_VALUE_TOKEN := 3, \
    NEG_INF_TOKEN := 4, \
    POS_INF_TOKEN := 5, \
    NEG_VALUE_TOKEN := 6, \
    POS_VALUE_TOKEN := 7
}
DEFINE FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]) {
    tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
    CASE(tsrc[31:0]) OF
    QNAN_TOKEN:j := 0
    SNAN_TOKEN:j := 1
    ZERO_VALUE_TOKEN: j := 2
    ONE_VALUE_TOKEN: j := 3
    NEG_INF_TOKEN: j := 4
    POS_INF_TOKEN: j := 5
    NEG_VALUE_TOKEN: j := 6
    POS_VALUE_TOKEN: j := 7
    ESAC
    
    token_response[3:0] := src3[3+4*j:4*j]
    
    CASE(token_response[3:0]) OF
    0 : dest[31:0] := src1[31:0]
    1 : dest[31:0] := tsrc[31:0]
    2 : dest[31:0] := QNaN(tsrc[31:0])
    3 : dest[31:0] := QNAN_Indefinite
    4 : dest[31:0] := -INF
    5 : dest[31:0] := +INF
    6 : dest[31:0] := tsrc.sign? -INF : +INF
    7 : dest[31:0] := -0
    8 : dest[31:0] := +0
    9 : dest[31:0] := -1
    10: dest[31:0] := +1
    11: dest[31:0] := 1/2
    12: dest[31:0] := 90.0
    13: dest[31:0] := PI/2
    14: dest[31:0] := MAX_FLOAT
    15: dest[31:0] := -MAX_FLOAT
    ESAC
    
    CASE(tsrc[31:0]) OF
    ZERO_VALUE_TOKEN:
        IF (imm8[0]) #ZE; FI
    ZERO_VALUE_TOKEN:
        IF (imm8[1]) #IE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[2]) #ZE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[3]) #IE; FI
    SNAN_TOKEN:
        IF (imm8[4]) #IE; FI
    NEG_INF_TOKEN:
        IF (imm8[5]) #IE; FI
    NEG_VALUE_TOKEN:
        IF (imm8[6]) #IE; FI
    POS_INF_TOKEN:
        IF (imm8[7]) #IE; FI
    ESAC
    RETURN dest[31:0]
}
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
    ELSE
        dst[i+31:i] := a[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFIXUPIMMPS - _mm512_mask_fixupimm_round_ps

| VFIXUPIMMPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Fix up packed single-precision (32-bit) floating-point elements in "a" and "b" using packed 32-bit integers in
"c", and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding
mask bit is not set). "imm8" is used to set the required flags reporting.
	[sae_note]

[algorithm]

enum TOKEN_TYPE {
    QNAN_TOKEN := 0, \
    SNAN_TOKEN := 1, \
    ZERO_VALUE_TOKEN := 2, \
    ONE_VALUE_TOKEN := 3, \
    NEG_INF_TOKEN := 4, \
    POS_INF_TOKEN := 5, \
    NEG_VALUE_TOKEN := 6, \
    POS_VALUE_TOKEN := 7
}
DEFINE FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]) {
    tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
    CASE(tsrc[31:0]) OF
    QNAN_TOKEN:j := 0
    SNAN_TOKEN:j := 1
    ZERO_VALUE_TOKEN: j := 2
    ONE_VALUE_TOKEN: j := 3
    NEG_INF_TOKEN: j := 4
    POS_INF_TOKEN: j := 5
    NEG_VALUE_TOKEN: j := 6
    POS_VALUE_TOKEN: j := 7
    ESAC
    
    token_response[3:0] := src3[3+4*j:4*j]
    
    CASE(token_response[3:0]) OF
    0 : dest[31:0] := src1[31:0]
    1 : dest[31:0] := tsrc[31:0]
    2 : dest[31:0] := QNaN(tsrc[31:0])
    3 : dest[31:0] := QNAN_Indefinite
    4 : dest[31:0] := -INF
    5 : dest[31:0] := +INF
    6 : dest[31:0] := tsrc.sign? -INF : +INF
    7 : dest[31:0] := -0
    8 : dest[31:0] := +0
    9 : dest[31:0] := -1
    10: dest[31:0] := +1
    11: dest[31:0] := 1/2
    12: dest[31:0] := 90.0
    13: dest[31:0] := PI/2
    14: dest[31:0] := MAX_FLOAT
    15: dest[31:0] := -MAX_FLOAT
    ESAC
    
    CASE(tsrc[31:0]) OF
    ZERO_VALUE_TOKEN:
        IF (imm8[0]) #ZE; FI
    ZERO_VALUE_TOKEN:
        IF (imm8[1]) #IE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[2]) #ZE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[3]) #IE; FI
    SNAN_TOKEN:
        IF (imm8[4]) #IE; FI
    NEG_INF_TOKEN:
        IF (imm8[5]) #IE; FI
    NEG_VALUE_TOKEN:
        IF (imm8[6]) #IE; FI
    POS_INF_TOKEN:
        IF (imm8[7]) #IE; FI
    ESAC
    RETURN dest[31:0]
}
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
    ELSE
        dst[i+31:i] := a[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFIXUPIMMPS - _mm512_maskz_fixupimm_ps

| VFIXUPIMMPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Fix up packed single-precision (32-bit) floating-point elements in "a" and "b" using packed 32-bit integers in
"c", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit
is not set). "imm8" is used to set the required flags reporting.

[algorithm]

enum TOKEN_TYPE {
    QNAN_TOKEN := 0, \
    SNAN_TOKEN := 1, \
    ZERO_VALUE_TOKEN := 2, \
    ONE_VALUE_TOKEN := 3, \
    NEG_INF_TOKEN := 4, \
    POS_INF_TOKEN := 5, \
    NEG_VALUE_TOKEN := 6, \
    POS_VALUE_TOKEN := 7
}
DEFINE FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]) {
    tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
    CASE(tsrc[31:0]) OF
    QNAN_TOKEN:j := 0
    SNAN_TOKEN:j := 1
    ZERO_VALUE_TOKEN: j := 2
    ONE_VALUE_TOKEN: j := 3
    NEG_INF_TOKEN: j := 4
    POS_INF_TOKEN: j := 5
    NEG_VALUE_TOKEN: j := 6
    POS_VALUE_TOKEN: j := 7
    ESAC
    
    token_response[3:0] := src3[3+4*j:4*j]
    
    CASE(token_response[3:0]) OF
    0 : dest[31:0] := src1[31:0]
    1 : dest[31:0] := tsrc[31:0]
    2 : dest[31:0] := QNaN(tsrc[31:0])
    3 : dest[31:0] := QNAN_Indefinite
    4 : dest[31:0] := -INF
    5 : dest[31:0] := +INF
    6 : dest[31:0] := tsrc.sign? -INF : +INF
    7 : dest[31:0] := -0
    8 : dest[31:0] := +0
    9 : dest[31:0] := -1
    10: dest[31:0] := +1
    11: dest[31:0] := 1/2
    12: dest[31:0] := 90.0
    13: dest[31:0] := PI/2
    14: dest[31:0] := MAX_FLOAT
    15: dest[31:0] := -MAX_FLOAT
    ESAC
    
    CASE(tsrc[31:0]) OF
    ZERO_VALUE_TOKEN:
        IF (imm8[0]) #ZE; FI
    ZERO_VALUE_TOKEN:
        IF (imm8[1]) #IE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[2]) #ZE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[3]) #IE; FI
    SNAN_TOKEN:
        IF (imm8[4]) #IE; FI
    NEG_INF_TOKEN:
        IF (imm8[5]) #IE; FI
    NEG_VALUE_TOKEN:
        IF (imm8[6]) #IE; FI
    POS_INF_TOKEN:
        IF (imm8[7]) #IE; FI
    ESAC
    RETURN dest[31:0]
}
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFIXUPIMMPS - _mm512_maskz_fixupimm_round_ps

| VFIXUPIMMPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Fix up packed single-precision (32-bit) floating-point elements in "a" and "b" using packed 32-bit integers in
"c", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit
is not set). "imm8" is used to set the required flags reporting.
	[sae_note]

[algorithm]

enum TOKEN_TYPE {
    QNAN_TOKEN := 0, \
    SNAN_TOKEN := 1, \
    ZERO_VALUE_TOKEN := 2, \
    ONE_VALUE_TOKEN := 3, \
    NEG_INF_TOKEN := 4, \
    POS_INF_TOKEN := 5, \
    NEG_VALUE_TOKEN := 6, \
    POS_VALUE_TOKEN := 7
}
DEFINE FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]) {
    tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
    CASE(tsrc[31:0]) OF
    QNAN_TOKEN:j := 0
    SNAN_TOKEN:j := 1
    ZERO_VALUE_TOKEN: j := 2
    ONE_VALUE_TOKEN: j := 3
    NEG_INF_TOKEN: j := 4
    POS_INF_TOKEN: j := 5
    NEG_VALUE_TOKEN: j := 6
    POS_VALUE_TOKEN: j := 7
    ESAC
    
    token_response[3:0] := src3[3+4*j:4*j]
    
    CASE(token_response[3:0]) OF
    0 : dest[31:0] := src1[31:0]
    1 : dest[31:0] := tsrc[31:0]
    2 : dest[31:0] := QNaN(tsrc[31:0])
    3 : dest[31:0] := QNAN_Indefinite
    4 : dest[31:0] := -INF
    5 : dest[31:0] := +INF
    6 : dest[31:0] := tsrc.sign? -INF : +INF
    7 : dest[31:0] := -0
    8 : dest[31:0] := +0
    9 : dest[31:0] := -1
    10: dest[31:0] := +1
    11: dest[31:0] := 1/2
    12: dest[31:0] := 90.0
    13: dest[31:0] := PI/2
    14: dest[31:0] := MAX_FLOAT
    15: dest[31:0] := -MAX_FLOAT
    ESAC
    
    CASE(tsrc[31:0]) OF
    ZERO_VALUE_TOKEN:
        IF (imm8[0]) #ZE; FI
    ZERO_VALUE_TOKEN:
        IF (imm8[1]) #IE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[2]) #ZE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[3]) #IE; FI
    SNAN_TOKEN:
        IF (imm8[4]) #IE; FI
    NEG_INF_TOKEN:
        IF (imm8[5]) #IE; FI
    NEG_VALUE_TOKEN:
        IF (imm8[6]) #IE; FI
    POS_INF_TOKEN:
        IF (imm8[7]) #IE; FI
    ESAC
    RETURN dest[31:0]
}
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFIXUPIMMSD - _mm_fixupimm_round_sd

| VFIXUPIMMSD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Fix up the lower double-precision (64-bit) floating-point elements in "a" and "b" using the lower 64-bit
integer in "c", store the result in the lower element of "dst", and copy the upper element from "a" to the
upper element of "dst". "imm8" is used to set the required flags reporting.
	[sae_note]

[algorithm]

enum TOKEN_TYPE {
    QNAN_TOKEN := 0, \
    SNAN_TOKEN := 1, \
    ZERO_VALUE_TOKEN := 2, \
    ONE_VALUE_TOKEN := 3, \
    NEG_INF_TOKEN := 4, \
    POS_INF_TOKEN := 5, \
    NEG_VALUE_TOKEN := 6, \
    POS_VALUE_TOKEN := 7
}
DEFINE FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]) {
    tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
    CASE(tsrc[63:0]) OF
    QNAN_TOKEN:j := 0
    SNAN_TOKEN:j := 1
    ZERO_VALUE_TOKEN: j := 2
    ONE_VALUE_TOKEN: j := 3
    NEG_INF_TOKEN: j := 4
    POS_INF_TOKEN: j := 5
    NEG_VALUE_TOKEN: j := 6
    POS_VALUE_TOKEN: j := 7
    ESAC
    
    token_response[3:0] := src3[3+4*j:4*j]
    
    CASE(token_response[3:0]) OF
    0 : dest[63:0] := src1[63:0]
    1 : dest[63:0] := tsrc[63:0]
    2 : dest[63:0] := QNaN(tsrc[63:0])
    3 : dest[63:0] := QNAN_Indefinite
    4 : dest[63:0] := -INF
    5 : dest[63:0] := +INF
    6 : dest[63:0] := tsrc.sign? -INF : +INF
    7 : dest[63:0] := -0
    8 : dest[63:0] := +0
    9 : dest[63:0] := -1
    10: dest[63:0] := +1
    11: dest[63:0] := 1/2
    12: dest[63:0] := 90.0
    13: dest[63:0] := PI/2
    14: dest[63:0] := MAX_FLOAT
    15: dest[63:0] := -MAX_FLOAT
    ESAC
    
    CASE(tsrc[31:0]) OF
    ZERO_VALUE_TOKEN:
        IF (imm8[0]) #ZE; FI
    ZERO_VALUE_TOKEN:
        IF (imm8[1]) #IE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[2]) #ZE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[3]) #IE; FI
    SNAN_TOKEN:
        IF (imm8[4]) #IE; FI
    NEG_INF_TOKEN:
        IF (imm8[5]) #IE; FI
    NEG_VALUE_TOKEN:
        IF (imm8[6]) #IE; FI
    POS_INF_TOKEN:
        IF (imm8[7]) #IE; FI
    ESAC
    RETURN dest[63:0]
}
dst[63:0] := FIXUPIMMPD(a[63:0], b[63:0], c[63:0], imm8[7:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFIXUPIMMSD - _mm_fixupimm_sd

| VFIXUPIMMSD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Fix up the lower double-precision (64-bit) floating-point elements in "a" and "b" using the lower 64-bit
integer in "c", store the result in the lower element of "dst", and copy the upper element from "a" to the
upper element of "dst". "imm8" is used to set the required flags reporting.

[algorithm]

enum TOKEN_TYPE {
    QNAN_TOKEN := 0, \
    SNAN_TOKEN := 1, \
    ZERO_VALUE_TOKEN := 2, \
    ONE_VALUE_TOKEN := 3, \
    NEG_INF_TOKEN := 4, \
    POS_INF_TOKEN := 5, \
    NEG_VALUE_TOKEN := 6, \
    POS_VALUE_TOKEN := 7
}
DEFINE FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]) {
    tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
    CASE(tsrc[63:0]) OF
    QNAN_TOKEN:j := 0
    SNAN_TOKEN:j := 1
    ZERO_VALUE_TOKEN: j := 2
    ONE_VALUE_TOKEN: j := 3
    NEG_INF_TOKEN: j := 4
    POS_INF_TOKEN: j := 5
    NEG_VALUE_TOKEN: j := 6
    POS_VALUE_TOKEN: j := 7
    ESAC
    
    token_response[3:0] := src3[3+4*j:4*j]
    
    CASE(token_response[3:0]) OF
    0 : dest[63:0] := src1[63:0]
    1 : dest[63:0] := tsrc[63:0]
    2 : dest[63:0] := QNaN(tsrc[63:0])
    3 : dest[63:0] := QNAN_Indefinite
    4 : dest[63:0] := -INF
    5 : dest[63:0] := +INF
    6 : dest[63:0] := tsrc.sign? -INF : +INF
    7 : dest[63:0] := -0
    8 : dest[63:0] := +0
    9 : dest[63:0] := -1
    10: dest[63:0] := +1
    11: dest[63:0] := 1/2
    12: dest[63:0] := 90.0
    13: dest[63:0] := PI/2
    14: dest[63:0] := MAX_FLOAT
    15: dest[63:0] := -MAX_FLOAT
    ESAC
    
    CASE(tsrc[31:0]) OF
    ZERO_VALUE_TOKEN:
        IF (imm8[0]) #ZE; FI
    ZERO_VALUE_TOKEN:
        IF (imm8[1]) #IE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[2]) #ZE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[3]) #IE; FI
    SNAN_TOKEN:
        IF (imm8[4]) #IE; FI
    NEG_INF_TOKEN:
        IF (imm8[5]) #IE; FI
    NEG_VALUE_TOKEN:
        IF (imm8[6]) #IE; FI
    POS_INF_TOKEN:
        IF (imm8[7]) #IE; FI
    ESAC
    RETURN dest[63:0]
}
dst[63:0] := FIXUPIMMPD(a[63:0], b[63:0], c[63:0], imm8[7:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFIXUPIMMSD - _mm_mask_fixupimm_round_sd

| VFIXUPIMMSD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Fix up the lower double-precision (64-bit) floating-point elements in "a" and "b" using the lower 64-bit
integer in "c", store the result in the lower element of "dst" using writemask "k" (the element is copied from
"a" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst". "imm8" is
used to set the required flags reporting.
	[sae_note]

[algorithm]

enum TOKEN_TYPE {
    QNAN_TOKEN := 0, \
    SNAN_TOKEN := 1, \
    ZERO_VALUE_TOKEN := 2, \
    ONE_VALUE_TOKEN := 3, \
    NEG_INF_TOKEN := 4, \
    POS_INF_TOKEN := 5, \
    NEG_VALUE_TOKEN := 6, \
    POS_VALUE_TOKEN := 7
}
DEFINE FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]) {
    tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
    CASE(tsrc[63:0]) OF
    QNAN_TOKEN:j := 0
    SNAN_TOKEN:j := 1
    ZERO_VALUE_TOKEN: j := 2
    ONE_VALUE_TOKEN: j := 3
    NEG_INF_TOKEN: j := 4
    POS_INF_TOKEN: j := 5
    NEG_VALUE_TOKEN: j := 6
    POS_VALUE_TOKEN: j := 7
    ESAC
    
    token_response[3:0] := src3[3+4*j:4*j]
    
    CASE(token_response[3:0]) OF
    0 : dest[63:0] := src1[63:0]
    1 : dest[63:0] := tsrc[63:0]
    2 : dest[63:0] := QNaN(tsrc[63:0])
    3 : dest[63:0] := QNAN_Indefinite
    4 : dest[63:0] := -INF
    5 : dest[63:0] := +INF
    6 : dest[63:0] := tsrc.sign? -INF : +INF
    7 : dest[63:0] := -0
    8 : dest[63:0] := +0
    9 : dest[63:0] := -1
    10: dest[63:0] := +1
    11: dest[63:0] := 1/2
    12: dest[63:0] := 90.0
    13: dest[63:0] := PI/2
    14: dest[63:0] := MAX_FLOAT
    15: dest[63:0] := -MAX_FLOAT
    ESAC
    
    CASE(tsrc[31:0]) OF
    ZERO_VALUE_TOKEN:
        IF (imm8[0]) #ZE; FI
    ZERO_VALUE_TOKEN:
        IF (imm8[1]) #IE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[2]) #ZE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[3]) #IE; FI
    SNAN_TOKEN:
        IF (imm8[4]) #IE; FI
    NEG_INF_TOKEN:
        IF (imm8[5]) #IE; FI
    NEG_VALUE_TOKEN:
        IF (imm8[6]) #IE; FI
    POS_INF_TOKEN:
        IF (imm8[7]) #IE; FI
    ESAC
    RETURN dest[63:0]
}
IF k[0]
    dst[63:0] := FIXUPIMMPD(a[63:0], b[63:0], c[63:0], imm8[7:0])
ELSE
    dst[63:0] := a[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFIXUPIMMSD - _mm_mask_fixupimm_sd

| VFIXUPIMMSD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Fix up the lower double-precision (64-bit) floating-point elements in "a" and "b" using the lower 64-bit
integer in "c", store the result in the lower element of "dst" using writemask "k" (the element is copied from
"a" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst". "imm8" is
used to set the required flags reporting.

[algorithm]

enum TOKEN_TYPE {
    QNAN_TOKEN := 0, \
    SNAN_TOKEN := 1, \
    ZERO_VALUE_TOKEN := 2, \
    ONE_VALUE_TOKEN := 3, \
    NEG_INF_TOKEN := 4, \
    POS_INF_TOKEN := 5, \
    NEG_VALUE_TOKEN := 6, \
    POS_VALUE_TOKEN := 7
}
DEFINE FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]) {
    tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
    CASE(tsrc[63:0]) OF
    QNAN_TOKEN:j := 0
    SNAN_TOKEN:j := 1
    ZERO_VALUE_TOKEN: j := 2
    ONE_VALUE_TOKEN: j := 3
    NEG_INF_TOKEN: j := 4
    POS_INF_TOKEN: j := 5
    NEG_VALUE_TOKEN: j := 6
    POS_VALUE_TOKEN: j := 7
    ESAC
    
    token_response[3:0] := src3[3+4*j:4*j]
    
    CASE(token_response[3:0]) OF
    0 : dest[63:0] := src1[63:0]
    1 : dest[63:0] := tsrc[63:0]
    2 : dest[63:0] := QNaN(tsrc[63:0])
    3 : dest[63:0] := QNAN_Indefinite
    4 : dest[63:0] := -INF
    5 : dest[63:0] := +INF
    6 : dest[63:0] := tsrc.sign? -INF : +INF
    7 : dest[63:0] := -0
    8 : dest[63:0] := +0
    9 : dest[63:0] := -1
    10: dest[63:0] := +1
    11: dest[63:0] := 1/2
    12: dest[63:0] := 90.0
    13: dest[63:0] := PI/2
    14: dest[63:0] := MAX_FLOAT
    15: dest[63:0] := -MAX_FLOAT
    ESAC
    
    CASE(tsrc[31:0]) OF
    ZERO_VALUE_TOKEN:
        IF (imm8[0]) #ZE; FI
    ZERO_VALUE_TOKEN:
        IF (imm8[1]) #IE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[2]) #ZE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[3]) #IE; FI
    SNAN_TOKEN:
        IF (imm8[4]) #IE; FI
    NEG_INF_TOKEN:
        IF (imm8[5]) #IE; FI
    NEG_VALUE_TOKEN:
        IF (imm8[6]) #IE; FI
    POS_INF_TOKEN:
        IF (imm8[7]) #IE; FI
    ESAC
    RETURN dest[63:0]
}
IF k[0]
    dst[63:0] := FIXUPIMMPD(a[63:0], b[63:0], c[63:0], imm8[7:0])
ELSE
    dst[63:0] := a[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFIXUPIMMSD - _mm_maskz_fixupimm_round_sd

| VFIXUPIMMSD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Fix up the lower double-precision (64-bit) floating-point elements in "a" and "b" using the lower 64-bit
integer in "c", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out
when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst". "imm8" is used
to set the required flags reporting.
	[sae_note]

[algorithm]

enum TOKEN_TYPE {
    QNAN_TOKEN := 0, \
    SNAN_TOKEN := 1, \
    ZERO_VALUE_TOKEN := 2, \
    ONE_VALUE_TOKEN := 3, \
    NEG_INF_TOKEN := 4, \
    POS_INF_TOKEN := 5, \
    NEG_VALUE_TOKEN := 6, \
    POS_VALUE_TOKEN := 7
}
DEFINE FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]) {
    tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
    CASE(tsrc[63:0]) OF
    QNAN_TOKEN:j := 0
    SNAN_TOKEN:j := 1
    ZERO_VALUE_TOKEN: j := 2
    ONE_VALUE_TOKEN: j := 3
    NEG_INF_TOKEN: j := 4
    POS_INF_TOKEN: j := 5
    NEG_VALUE_TOKEN: j := 6
    POS_VALUE_TOKEN: j := 7
    ESAC
    
    token_response[3:0] := src3[3+4*j:4*j]
    
    CASE(token_response[3:0]) OF
    0 : dest[63:0] := src1[63:0]
    1 : dest[63:0] := tsrc[63:0]
    2 : dest[63:0] := QNaN(tsrc[63:0])
    3 : dest[63:0] := QNAN_Indefinite
    4 : dest[63:0] := -INF
    5 : dest[63:0] := +INF
    6 : dest[63:0] := tsrc.sign? -INF : +INF
    7 : dest[63:0] := -0
    8 : dest[63:0] := +0
    9 : dest[63:0] := -1
    10: dest[63:0] := +1
    11: dest[63:0] := 1/2
    12: dest[63:0] := 90.0
    13: dest[63:0] := PI/2
    14: dest[63:0] := MAX_FLOAT
    15: dest[63:0] := -MAX_FLOAT
    ESAC
    
    CASE(tsrc[31:0]) OF
    ZERO_VALUE_TOKEN:
        IF (imm8[0]) #ZE; FI
    ZERO_VALUE_TOKEN:
        IF (imm8[1]) #IE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[2]) #ZE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[3]) #IE; FI
    SNAN_TOKEN:
        IF (imm8[4]) #IE; FI
    NEG_INF_TOKEN:
        IF (imm8[5]) #IE; FI
    NEG_VALUE_TOKEN:
        IF (imm8[6]) #IE; FI
    POS_INF_TOKEN:
        IF (imm8[7]) #IE; FI
    ESAC
    RETURN dest[63:0]
}
IF k[0]
    dst[63:0] := FIXUPIMMPD(a[63:0], b[63:0], c[63:0], imm8[7:0])
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFIXUPIMMSD - _mm_maskz_fixupimm_sd

| VFIXUPIMMSD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Fix up the lower double-precision (64-bit) floating-point elements in "a" and "b" using the lower 64-bit
integer in "c", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out
when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst". "imm8" is used
to set the required flags reporting.

[algorithm]

enum TOKEN_TYPE {
    QNAN_TOKEN := 0, \
    SNAN_TOKEN := 1, \
    ZERO_VALUE_TOKEN := 2, \
    ONE_VALUE_TOKEN := 3, \
    NEG_INF_TOKEN := 4, \
    POS_INF_TOKEN := 5, \
    NEG_VALUE_TOKEN := 6, \
    POS_VALUE_TOKEN := 7
}
DEFINE FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]) {
    tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
    CASE(tsrc[63:0]) OF
    QNAN_TOKEN:j := 0
    SNAN_TOKEN:j := 1
    ZERO_VALUE_TOKEN: j := 2
    ONE_VALUE_TOKEN: j := 3
    NEG_INF_TOKEN: j := 4
    POS_INF_TOKEN: j := 5
    NEG_VALUE_TOKEN: j := 6
    POS_VALUE_TOKEN: j := 7
    ESAC
    
    token_response[3:0] := src3[3+4*j:4*j]
    
    CASE(token_response[3:0]) OF
    0 : dest[63:0] := src1[63:0]
    1 : dest[63:0] := tsrc[63:0]
    2 : dest[63:0] := QNaN(tsrc[63:0])
    3 : dest[63:0] := QNAN_Indefinite
    4 : dest[63:0] := -INF
    5 : dest[63:0] := +INF
    6 : dest[63:0] := tsrc.sign? -INF : +INF
    7 : dest[63:0] := -0
    8 : dest[63:0] := +0
    9 : dest[63:0] := -1
    10: dest[63:0] := +1
    11: dest[63:0] := 1/2
    12: dest[63:0] := 90.0
    13: dest[63:0] := PI/2
    14: dest[63:0] := MAX_FLOAT
    15: dest[63:0] := -MAX_FLOAT
    ESAC
    
    CASE(tsrc[31:0]) OF
    ZERO_VALUE_TOKEN:
        IF (imm8[0]) #ZE; FI
    ZERO_VALUE_TOKEN:
        IF (imm8[1]) #IE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[2]) #ZE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[3]) #IE; FI
    SNAN_TOKEN:
        IF (imm8[4]) #IE; FI
    NEG_INF_TOKEN:
        IF (imm8[5]) #IE; FI
    NEG_VALUE_TOKEN:
        IF (imm8[6]) #IE; FI
    POS_INF_TOKEN:
        IF (imm8[7]) #IE; FI
    ESAC
    RETURN dest[63:0]
}
IF k[0]
    dst[63:0] := FIXUPIMMPD(a[63:0], b[63:0], c[63:0], imm8[7:0])
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFIXUPIMMSS - _mm_fixupimm_round_ss

| VFIXUPIMMSS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Fix up the lower single-precision (32-bit) floating-point elements in "a" and "b" using the lower 32-bit
integer in "c", store the result in the lower element of "dst", and copy the upper 3 packed elements from "a"
to the upper elements of "dst". "imm8" is used to set the required flags reporting.
	[sae_note]

[algorithm]

enum TOKEN_TYPE {
    QNAN_TOKEN := 0, \
    SNAN_TOKEN := 1, \
    ZERO_VALUE_TOKEN := 2, \
    ONE_VALUE_TOKEN := 3, \
    NEG_INF_TOKEN := 4, \
    POS_INF_TOKEN := 5, \
    NEG_VALUE_TOKEN := 6, \
    POS_VALUE_TOKEN := 7
}
DEFINE FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]) {
    tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
    CASE(tsrc[31:0]) OF
    QNAN_TOKEN:j := 0
    SNAN_TOKEN:j := 1
    ZERO_VALUE_TOKEN: j := 2
    ONE_VALUE_TOKEN: j := 3
    NEG_INF_TOKEN: j := 4
    POS_INF_TOKEN: j := 5
    NEG_VALUE_TOKEN: j := 6
    POS_VALUE_TOKEN: j := 7
    ESAC
    
    token_response[3:0] := src3[3+4*j:4*j]
    
    CASE(token_response[3:0]) OF
    0 : dest[31:0] := src1[31:0]
    1 : dest[31:0] := tsrc[31:0]
    2 : dest[31:0] := QNaN(tsrc[31:0])
    3 : dest[31:0] := QNAN_Indefinite
    4 : dest[31:0] := -INF
    5 : dest[31:0] := +INF
    6 : dest[31:0] := tsrc.sign? -INF : +INF
    7 : dest[31:0] := -0
    8 : dest[31:0] := +0
    9 : dest[31:0] := -1
    10: dest[31:0] := +1
    11: dest[31:0] := 1/2
    12: dest[31:0] := 90.0
    13: dest[31:0] := PI/2
    14: dest[31:0] := MAX_FLOAT
    15: dest[31:0] := -MAX_FLOAT
    ESAC
    
    CASE(tsrc[31:0]) OF
    ZERO_VALUE_TOKEN:
        IF (imm8[0]) #ZE; FI
    ZERO_VALUE_TOKEN:
        IF (imm8[1]) #IE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[2]) #ZE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[3]) #IE; FI
    SNAN_TOKEN:
        IF (imm8[4]) #IE; FI
    NEG_INF_TOKEN:
        IF (imm8[5]) #IE; FI
    NEG_VALUE_TOKEN:
        IF (imm8[6]) #IE; FI
    POS_INF_TOKEN:
        IF (imm8[7]) #IE; FI
    ESAC
    RETURN dest[31:0]
}
dst[31:0] := FIXUPIMMPD(a[31:0], b[31:0], c[31:0], imm8[7:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFIXUPIMMSS - _mm_fixupimm_ss

| VFIXUPIMMSS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Fix up the lower single-precision (32-bit) floating-point elements in "a" and "b" using the lower 32-bit
integer in "c", store the result in the lower element of "dst", and copy the upper 3 packed elements from "a"
to the upper elements of "dst". "imm8" is used to set the required flags reporting.

[algorithm]

enum TOKEN_TYPE {
    QNAN_TOKEN := 0, \
    SNAN_TOKEN := 1, \
    ZERO_VALUE_TOKEN := 2, \
    ONE_VALUE_TOKEN := 3, \
    NEG_INF_TOKEN := 4, \
    POS_INF_TOKEN := 5, \
    NEG_VALUE_TOKEN := 6, \
    POS_VALUE_TOKEN := 7
}
DEFINE FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]) {
    tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
    CASE(tsrc[31:0]) OF
    QNAN_TOKEN:j := 0
    SNAN_TOKEN:j := 1
    ZERO_VALUE_TOKEN: j := 2
    ONE_VALUE_TOKEN: j := 3
    NEG_INF_TOKEN: j := 4
    POS_INF_TOKEN: j := 5
    NEG_VALUE_TOKEN: j := 6
    POS_VALUE_TOKEN: j := 7
    ESAC
    
    token_response[3:0] := src3[3+4*j:4*j]
    
    CASE(token_response[3:0]) OF
    0 : dest[31:0] := src1[31:0]
    1 : dest[31:0] := tsrc[31:0]
    2 : dest[31:0] := QNaN(tsrc[31:0])
    3 : dest[31:0] := QNAN_Indefinite
    4 : dest[31:0] := -INF
    5 : dest[31:0] := +INF
    6 : dest[31:0] := tsrc.sign? -INF : +INF
    7 : dest[31:0] := -0
    8 : dest[31:0] := +0
    9 : dest[31:0] := -1
    10: dest[31:0] := +1
    11: dest[31:0] := 1/2
    12: dest[31:0] := 90.0
    13: dest[31:0] := PI/2
    14: dest[31:0] := MAX_FLOAT
    15: dest[31:0] := -MAX_FLOAT
    ESAC
    
    CASE(tsrc[31:0]) OF
    ZERO_VALUE_TOKEN:
        IF (imm8[0]) #ZE; FI
    ZERO_VALUE_TOKEN:
        IF (imm8[1]) #IE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[2]) #ZE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[3]) #IE; FI
    SNAN_TOKEN:
        IF (imm8[4]) #IE; FI
    NEG_INF_TOKEN:
        IF (imm8[5]) #IE; FI
    NEG_VALUE_TOKEN:
        IF (imm8[6]) #IE; FI
    POS_INF_TOKEN:
        IF (imm8[7]) #IE; FI
    ESAC
    RETURN dest[31:0]
}
dst[31:0] := FIXUPIMMPD(a[31:0], b[31:0], c[31:0], imm8[7:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFIXUPIMMSS - _mm_mask_fixupimm_round_ss

| VFIXUPIMMSS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Fix up the lower single-precision (32-bit) floating-point elements in "a" and "b" using the lower 32-bit
integer in "c", store the result in the lower element of "dst" using writemask "k" (the element is copied from
"a" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
"imm8" is used to set the required flags reporting.
	[sae_note]

[algorithm]

enum TOKEN_TYPE {
    QNAN_TOKEN := 0, \
    SNAN_TOKEN := 1, \
    ZERO_VALUE_TOKEN := 2, \
    ONE_VALUE_TOKEN := 3, \
    NEG_INF_TOKEN := 4, \
    POS_INF_TOKEN := 5, \
    NEG_VALUE_TOKEN := 6, \
    POS_VALUE_TOKEN := 7
}
DEFINE FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]) {
    tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
    CASE(tsrc[31:0]) OF
    QNAN_TOKEN:j := 0
    SNAN_TOKEN:j := 1
    ZERO_VALUE_TOKEN: j := 2
    ONE_VALUE_TOKEN: j := 3
    NEG_INF_TOKEN: j := 4
    POS_INF_TOKEN: j := 5
    NEG_VALUE_TOKEN: j := 6
    POS_VALUE_TOKEN: j := 7
    ESAC
    
    token_response[3:0] := src3[3+4*j:4*j]
    
    CASE(token_response[3:0]) OF
    0 : dest[31:0] := src1[31:0]
    1 : dest[31:0] := tsrc[31:0]
    2 : dest[31:0] := QNaN(tsrc[31:0])
    3 : dest[31:0] := QNAN_Indefinite
    4 : dest[31:0] := -INF
    5 : dest[31:0] := +INF
    6 : dest[31:0] := tsrc.sign? -INF : +INF
    7 : dest[31:0] := -0
    8 : dest[31:0] := +0
    9 : dest[31:0] := -1
    10: dest[31:0] := +1
    11: dest[31:0] := 1/2
    12: dest[31:0] := 90.0
    13: dest[31:0] := PI/2
    14: dest[31:0] := MAX_FLOAT
    15: dest[31:0] := -MAX_FLOAT
    ESAC
    
    CASE(tsrc[31:0]) OF
    ZERO_VALUE_TOKEN:
        IF (imm8[0]) #ZE; FI
    ZERO_VALUE_TOKEN:
        IF (imm8[1]) #IE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[2]) #ZE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[3]) #IE; FI
    SNAN_TOKEN:
        IF (imm8[4]) #IE; FI
    NEG_INF_TOKEN:
        IF (imm8[5]) #IE; FI
    NEG_VALUE_TOKEN:
        IF (imm8[6]) #IE; FI
    POS_INF_TOKEN:
        IF (imm8[7]) #IE; FI
    ESAC
    RETURN dest[31:0]
}
IF k[0]
    dst[31:0] := FIXUPIMMPD(a[31:0], b[31:0], c[31:0], imm8[7:0])
ELSE
    dst[31:0] := a[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFIXUPIMMSS - _mm_mask_fixupimm_ss

| VFIXUPIMMSS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Fix up the lower single-precision (32-bit) floating-point elements in "a" and "b" using the lower 32-bit
integer in "c", store the result in the lower element of "dst" using writemask "k" (the element is copied from
"a" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
"imm8" is used to set the required flags reporting.

[algorithm]

enum TOKEN_TYPE {
    QNAN_TOKEN := 0, \
    SNAN_TOKEN := 1, \
    ZERO_VALUE_TOKEN := 2, \
    ONE_VALUE_TOKEN := 3, \
    NEG_INF_TOKEN := 4, \
    POS_INF_TOKEN := 5, \
    NEG_VALUE_TOKEN := 6, \
    POS_VALUE_TOKEN := 7
}
DEFINE FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]) {
    tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
    CASE(tsrc[31:0]) OF
    QNAN_TOKEN:j := 0
    SNAN_TOKEN:j := 1
    ZERO_VALUE_TOKEN: j := 2
    ONE_VALUE_TOKEN: j := 3
    NEG_INF_TOKEN: j := 4
    POS_INF_TOKEN: j := 5
    NEG_VALUE_TOKEN: j := 6
    POS_VALUE_TOKEN: j := 7
    ESAC
    
    token_response[3:0] := src3[3+4*j:4*j]
    
    CASE(token_response[3:0]) OF
    0 : dest[31:0] := src1[31:0]
    1 : dest[31:0] := tsrc[31:0]
    2 : dest[31:0] := QNaN(tsrc[31:0])
    3 : dest[31:0] := QNAN_Indefinite
    4 : dest[31:0] := -INF
    5 : dest[31:0] := +INF
    6 : dest[31:0] := tsrc.sign? -INF : +INF
    7 : dest[31:0] := -0
    8 : dest[31:0] := +0
    9 : dest[31:0] := -1
    10: dest[31:0] := +1
    11: dest[31:0] := 1/2
    12: dest[31:0] := 90.0
    13: dest[31:0] := PI/2
    14: dest[31:0] := MAX_FLOAT
    15: dest[31:0] := -MAX_FLOAT
    ESAC
    
    CASE(tsrc[31:0]) OF
    ZERO_VALUE_TOKEN:
        IF (imm8[0]) #ZE; FI
    ZERO_VALUE_TOKEN:
        IF (imm8[1]) #IE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[2]) #ZE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[3]) #IE; FI
    SNAN_TOKEN:
        IF (imm8[4]) #IE; FI
    NEG_INF_TOKEN:
        IF (imm8[5]) #IE; FI
    NEG_VALUE_TOKEN:
        IF (imm8[6]) #IE; FI
    POS_INF_TOKEN:
        IF (imm8[7]) #IE; FI
    ESAC
    RETURN dest[31:0]
}
IF k[0]
    dst[31:0] := FIXUPIMMPD(a[31:0], b[31:0], c[31:0], imm8[7:0])
ELSE
    dst[31:0] := a[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFIXUPIMMSS - _mm_maskz_fixupimm_round_ss

| VFIXUPIMMSS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Fix up the lower single-precision (32-bit) floating-point elements in "a" and "b" using the lower 32-bit
integer in "c", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out
when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
"imm8" is used to set the required flags reporting.
	[sae_note]

[algorithm]

enum TOKEN_TYPE {
    QNAN_TOKEN := 0, \
    SNAN_TOKEN := 1, \
    ZERO_VALUE_TOKEN := 2, \
    ONE_VALUE_TOKEN := 3, \
    NEG_INF_TOKEN := 4, \
    POS_INF_TOKEN := 5, \
    NEG_VALUE_TOKEN := 6, \
    POS_VALUE_TOKEN := 7
}
DEFINE FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]) {
    tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
    CASE(tsrc[31:0]) OF
    QNAN_TOKEN:j := 0
    SNAN_TOKEN:j := 1
    ZERO_VALUE_TOKEN: j := 2
    ONE_VALUE_TOKEN: j := 3
    NEG_INF_TOKEN: j := 4
    POS_INF_TOKEN: j := 5
    NEG_VALUE_TOKEN: j := 6
    POS_VALUE_TOKEN: j := 7
    ESAC
    
    token_response[3:0] := src3[3+4*j:4*j]
    
    CASE(token_response[3:0]) OF
    0 : dest[31:0] := src1[31:0]
    1 : dest[31:0] := tsrc[31:0]
    2 : dest[31:0] := QNaN(tsrc[31:0])
    3 : dest[31:0] := QNAN_Indefinite
    4 : dest[31:0] := -INF
    5 : dest[31:0] := +INF
    6 : dest[31:0] := tsrc.sign? -INF : +INF
    7 : dest[31:0] := -0
    8 : dest[31:0] := +0
    9 : dest[31:0] := -1
    10: dest[31:0] := +1
    11: dest[31:0] := 1/2
    12: dest[31:0] := 90.0
    13: dest[31:0] := PI/2
    14: dest[31:0] := MAX_FLOAT
    15: dest[31:0] := -MAX_FLOAT
    ESAC
    
    CASE(tsrc[31:0]) OF
    ZERO_VALUE_TOKEN:
        IF (imm8[0]) #ZE; FI
    ZERO_VALUE_TOKEN:
        IF (imm8[1]) #IE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[2]) #ZE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[3]) #IE; FI
    SNAN_TOKEN:
        IF (imm8[4]) #IE; FI
    NEG_INF_TOKEN:
        IF (imm8[5]) #IE; FI
    NEG_VALUE_TOKEN:
        IF (imm8[6]) #IE; FI
    POS_INF_TOKEN:
        IF (imm8[7]) #IE; FI
    ESAC
    RETURN dest[31:0]
}
IF k[0]
    dst[31:0] := FIXUPIMMPD(a[31:0], b[31:0], c[31:0], imm8[7:0])
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFIXUPIMMSS - _mm_maskz_fixupimm_ss

| VFIXUPIMMSS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Fix up the lower single-precision (32-bit) floating-point elements in "a" and "b" using the lower 32-bit
integer in "c", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out
when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
"imm8" is used to set the required flags reporting.

[algorithm]

enum TOKEN_TYPE {
    QNAN_TOKEN := 0, \
    SNAN_TOKEN := 1, \
    ZERO_VALUE_TOKEN := 2, \
    ONE_VALUE_TOKEN := 3, \
    NEG_INF_TOKEN := 4, \
    POS_INF_TOKEN := 5, \
    NEG_VALUE_TOKEN := 6, \
    POS_VALUE_TOKEN := 7
}
DEFINE FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]) {
    tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
    CASE(tsrc[31:0]) OF
    QNAN_TOKEN:j := 0
    SNAN_TOKEN:j := 1
    ZERO_VALUE_TOKEN: j := 2
    ONE_VALUE_TOKEN: j := 3
    NEG_INF_TOKEN: j := 4
    POS_INF_TOKEN: j := 5
    NEG_VALUE_TOKEN: j := 6
    POS_VALUE_TOKEN: j := 7
    ESAC
    
    token_response[3:0] := src3[3+4*j:4*j]
    
    CASE(token_response[3:0]) OF
    0 : dest[31:0] := src1[31:0]
    1 : dest[31:0] := tsrc[31:0]
    2 : dest[31:0] := QNaN(tsrc[31:0])
    3 : dest[31:0] := QNAN_Indefinite
    4 : dest[31:0] := -INF
    5 : dest[31:0] := +INF
    6 : dest[31:0] := tsrc.sign? -INF : +INF
    7 : dest[31:0] := -0
    8 : dest[31:0] := +0
    9 : dest[31:0] := -1
    10: dest[31:0] := +1
    11: dest[31:0] := 1/2
    12: dest[31:0] := 90.0
    13: dest[31:0] := PI/2
    14: dest[31:0] := MAX_FLOAT
    15: dest[31:0] := -MAX_FLOAT
    ESAC
    
    CASE(tsrc[31:0]) OF
    ZERO_VALUE_TOKEN:
        IF (imm8[0]) #ZE; FI
    ZERO_VALUE_TOKEN:
        IF (imm8[1]) #IE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[2]) #ZE; FI
    ONE_VALUE_TOKEN:
        IF (imm8[3]) #IE; FI
    SNAN_TOKEN:
        IF (imm8[4]) #IE; FI
    NEG_INF_TOKEN:
        IF (imm8[5]) #IE; FI
    NEG_VALUE_TOKEN:
        IF (imm8[6]) #IE; FI
    POS_INF_TOKEN:
        IF (imm8[7]) #IE; FI
    ESAC
    RETURN dest[31:0]
}
IF k[0]
    dst[31:0] := FIXUPIMMPD(a[31:0], b[31:0], c[31:0], imm8[7:0])
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADD132PD - _mm512_maskz_fmadd_pd

| VFMADD132PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMADD213PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMADD231PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", add the intermediate result
to packed elements in "c", and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADD132PD - _mm512_maskz_fmadd_round_pd

| VFMADD132PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMADD213PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMADD231PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", add the intermediate result
to packed elements in "c", and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set). 
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADD132PS - _mm512_maskz_fmadd_ps

| VFMADD132PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMADD213PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMADD231PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", add the intermediate result
to packed elements in "c", and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADD132PS - _mm512_maskz_fmadd_round_ps

| VFMADD132PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMADD213PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMADD231PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", add the intermediate result
to packed elements in "c", and store the results in "a" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set). 
	[round_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADD132SD - _mm_fmadd_round_sd

| VFMADD132SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMADD213SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMADD231SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and add the intermediate
result to the lower element in "c". Store the result in the lower element of "dst", and copy the upper element
from "a" to the upper element of "dst".
	[round_note]

[algorithm]

dst[63:0] := (a[63:0] * b[63:0]) + c[63:0]
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADD132SD - _mm_mask3_fmadd_round_sd

| VFMADD132SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMADD213SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMADD231SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and add the intermediate
result to the lower element in "c". Store the result in the lower element of "dst" using writemask "k" (the
element is copied from "c" when mask bit 0 is not set), and copy the upper element from "c" to the upper
element of "dst".
	[round_note]

[algorithm]

IF k[0]
    dst[63:0] := (a[63:0] * b[63:0]) + c[63:0]
ELSE
    dst[63:0] := c[63:0]
FI
dst[127:64] := c[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADD132SD - _mm_mask3_fmadd_sd

| VFMADD132SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMADD213SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMADD231SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and add the intermediate
result to the lower element in "c". Store the result in the lower element of "dst" using writemask "k" (the
element is copied from "c" when mask bit 0 is not set), and copy the upper element from "c" to the upper
element of "dst".

[algorithm]

IF k[0]
    dst[63:0] := (a[63:0] * b[63:0]) + c[63:0]
ELSE
    dst[63:0] := c[63:0]
FI
dst[127:64] := c[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADD132SD - _mm_mask_fmadd_round_sd

| VFMADD132SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMADD213SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMADD231SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and add the intermediate
result to the lower element in "c". Store the result in the lower element of "dst" using writemask "k" (the
element is copied from "a" when mask bit 0 is not set), and copy the upper element from "a" to the upper
element of "dst".
	[round_note]

[algorithm]

IF k[0]
    dst[63:0] := (a[63:0] * b[63:0]) + c[63:0]
ELSE
    dst[63:0] := a[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADD132SD - _mm_mask_fmadd_sd

| VFMADD132SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMADD213SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMADD231SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and add the intermediate
result to the lower element in "c". Store the result in the lower element of "dst" using writemask "k" (the
element is copied from "a" when mask bit 0 is not set), and copy the upper element from "a" to the upper
element of "dst".

[algorithm]

IF k[0]
    dst[63:0] := (a[63:0] * b[63:0]) + c[63:0]
ELSE
    dst[63:0] := a[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADD132SD - _mm_maskz_fmadd_round_sd

| VFMADD132SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMADD213SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMADD231SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and add the intermediate
result to the lower element in "c". Store the result in the lower element of "dst" using zeromask "k" (the
element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of
"dst".
	[round_note]

[algorithm]

IF k[0]
    dst[63:0] := (a[63:0] * b[63:0]) + c[63:0]
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADD132SD - _mm_maskz_fmadd_sd

| VFMADD132SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMADD213SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMADD231SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and add the intermediate
result to the lower element in "c". Store the result in the lower element of "dst" using zeromask "k" (the
element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of
"dst".

[algorithm]

IF k[0]
    dst[63:0] := (a[63:0] * b[63:0]) + c[63:0]
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADD132SS - _mm_mask3_fmadd_round_ss

| VFMADD132SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMADD213SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMADD231SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and add the intermediate
result to the lower element in "c". Store the result in the lower element of "dst" using writemask "k" (the
element is copied from "c" when mask bit 0 is not set), and copy the upper 3 packed elements from "c" to the
upper elements of "dst".
	[round_note]

[algorithm]

IF k[0]
    dst[31:0] := (a[31:0] * b[31:0]) + c[31:0]
ELSE
    dst[31:0] := c[31:0]
FI
dst[127:32] := c[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADD132SS - _mm_mask3_fmadd_ss

| VFMADD132SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMADD213SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMADD231SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and add the intermediate
result to the lower element in "c". Store the result in the lower element of "dst" using writemask "k" (the
element is copied from "c" when mask bit 0 is not set), and copy the upper 3 packed elements from "c" to the
upper elements of "dst".

[algorithm]

IF k[0]
    dst[31:0] := (a[31:0] * b[31:0]) + c[31:0]
ELSE
    dst[31:0] := c[31:0]
FI
dst[127:32] := c[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADD132SS - _mm_fmadd_round_ss

| VFMADD132SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMADD213SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMADD231SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and add the intermediate
result to the lower element in "c". Store the result in the lower element of "dst", and copy the upper 3 packed
elements from "a" to the upper elements of "dst".
	[round_note]

[algorithm]

dst[31:0] := (a[31:0] * b[31:0]) + c[31:0]
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADD132SS - _mm_mask_fmadd_round_ss

| VFMADD132SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMADD213SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMADD231SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and add the intermediate
result to the lower element in "c". Store the result in the lower element of "dst" using writemask "k" (the
element is copied from "a" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the
upper elements of "dst".
	[round_note]

[algorithm]

IF k[0]
    dst[31:0] := (a[31:0] * b[31:0]) + c[31:0]
ELSE
    dst[31:0] := a[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADD132SS - _mm_mask_fmadd_ss

| VFMADD132SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMADD213SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMADD231SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and add the intermediate
result to the lower element in "c". Store the result in the lower element of "dst" using writemask "k" (the
element is copied from "a" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the
upper elements of "dst".

[algorithm]

IF k[0]
    dst[31:0] := (a[31:0] * b[31:0]) + c[31:0]
ELSE
    dst[31:0] := a[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADD132SS - _mm_maskz_fmadd_round_ss

| VFMADD132SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMADD213SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMADD231SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and add the intermediate
result to the lower element in "c". Store the result in the lower element of "dst" using zeromask "k" (the
element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper
elements of "dst".
	[round_note]

[algorithm]

IF k[0]
    dst[31:0] := (a[31:0] * b[31:0]) + c[31:0]
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADD132SS - _mm_maskz_fmadd_ss

| VFMADD132SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMADD213SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMADD231SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and add the intermediate
result to the lower element in "c". Store the result in the lower element of "dst" using zeromask "k" (the
element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper
elements of "dst".

[algorithm]

IF k[0]
    dst[31:0] := (a[31:0] * b[31:0]) + c[31:0]
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADDSUB132PD - _mm512_fmaddsub_pd

| VFMADDSUB132PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMADDSUB213PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMADDSUB231PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively add and
subtract packed elements in "c" to/from the intermediate result, and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF ((j &amp; 1) == 0)
        dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
    ELSE
        dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADDSUB132PD - _mm512_fmaddsub_round_pd

| VFMADDSUB132PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMADDSUB213PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMADDSUB231PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively add and
subtract packed elements in "c" to/from the intermediate result, and store the results in "dst". 
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF ((j &amp; 1) == 0)
        dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
    ELSE
        dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADDSUB132PD - _mm512_mask3_fmaddsub_pd

| VFMADDSUB132PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMADDSUB213PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMADDSUB231PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively add and
subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using writemask
"k" (elements are copied from "c" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        IF ((j &amp; 1) == 0)
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
        ELSE
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
        FI
    ELSE
        dst[i+63:i] := c[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADDSUB132PD - _mm512_mask3_fmaddsub_round_pd

| VFMADDSUB132PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMADDSUB213PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMADDSUB231PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively add and
subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using writemask
"k" (elements are copied from "c" when the corresponding mask bit is not set). [round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        IF ((j &amp; 1) == 0)
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
        ELSE
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
        FI
    ELSE 
        dst[i+63:i] := c[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADDSUB132PD - _mm512_mask_fmaddsub_pd

| VFMADDSUB132PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMADDSUB213PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMADDSUB231PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively add and
subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using writemask
"k" (elements are copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        IF ((j &amp; 1) == 0)
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
        ELSE
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
        FI
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADDSUB132PD - _mm512_mask_fmaddsub_round_pd

| VFMADDSUB132PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMADDSUB213PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMADDSUB231PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively add and
subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using writemask
"k" (elements are copied from "a" when the corresponding mask bit is not set). [round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        IF ((j &amp; 1) == 0)
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
        ELSE
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
        FI
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADDSUB132PD - _mm512_maskz_fmaddsub_pd

| VFMADDSUB132PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMADDSUB213PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMADDSUB231PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively add and
subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        IF ((j &amp; 1) == 0)
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
        ELSE
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
        FI
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADDSUB132PD - _mm512_maskz_fmaddsub_round_pd

| VFMADDSUB132PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMADDSUB213PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMADDSUB231PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively add and
subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set). 
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        IF ((j &amp; 1) == 0)
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
        ELSE
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
        FI
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADDSUB132PS - _mm512_fmaddsub_ps

| VFMADDSUB132PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMADDSUB213PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMADDSUB231PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively add and
subtract packed elements in "c" to/from the intermediate result, and store the results in "dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF ((j &amp; 1) == 0)
        dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
    ELSE
        dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADDSUB132PS - _mm512_fmaddsub_round_ps

| VFMADDSUB132PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMADDSUB213PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMADDSUB231PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively add and
subtract packed elements in "c" to/from the intermediate result, and store the results in "dst". 
	[round_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF ((j &amp; 1) == 0)
        dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
    ELSE
        dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADDSUB132PS - _mm512_mask3_fmaddsub_ps

| VFMADDSUB132PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMADDSUB213PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMADDSUB231PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively add and
subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using writemask
"k" (elements are copied from "c" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        IF ((j &amp; 1) == 0)
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
        ELSE
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
        FI
    ELSE
        dst[i+31:i] := c[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADDSUB132PS - _mm512_mask3_fmaddsub_round_ps

| VFMADDSUB132PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMADDSUB213PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMADDSUB231PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively add and
subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using writemask
"k" (elements are copied from "c" when the corresponding mask bit is not set). [round_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        IF ((j &amp; 1) == 0)
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
        ELSE
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
        FI
    ELSE
        dst[i+31:i] := c[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADDSUB132PS - _mm512_mask_fmaddsub_ps

| VFMADDSUB132PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMADDSUB213PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMADDSUB231PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively add and
subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using writemask
"k" (elements are copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        IF ((j &amp; 1) == 0)
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
        ELSE
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
        FI
    ELSE
        dst[i+31:i] := a[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADDSUB132PS - _mm512_mask_fmaddsub_round_ps

| VFMADDSUB132PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMADDSUB213PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMADDSUB231PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively add and
subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using writemask
"k" (elements are copied from "a" when the corresponding mask bit is not set). [round_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        IF ((j &amp; 1) == 0)
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
        ELSE
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
        FI
    ELSE
        dst[i+31:i] := a[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADDSUB132PS - _mm512_maskz_fmaddsub_ps

| VFMADDSUB132PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMADDSUB213PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMADDSUB231PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively add and
subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        IF ((j &amp; 1) == 0)
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
        ELSE
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
        FI
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMADDSUB132PS - _mm512_maskz_fmaddsub_round_ps

| VFMADDSUB132PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMADDSUB213PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMADDSUB231PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively add and
subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set). 
	[round_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        IF ((j &amp; 1) == 0)
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
        ELSE
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
        FI
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUB132PD - _mm512_maskz_fmsub_pd

| VFMSUB132PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMSUB213PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMSUB231PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", subtract packed elements in
"c" from the intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUB132PD - _mm512_maskz_fmsub_round_pd

| VFMSUB132PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMSUB213PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMSUB231PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", subtract packed elements in
"c" from the intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out
when the corresponding mask bit is not set). 
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUB132PS - _mm512_maskz_fmsub_ps

| VFMSUB132PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMSUB213PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMSUB231PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", subtract packed elements in
"c" from the intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUB132PS - _mm512_maskz_fmsub_round_ps

| VFMSUB132PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMSUB213PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMSUB231PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", subtract packed elements in
"c" from the intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out
when the corresponding mask bit is not set). 
	[round_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUB132SD - _mm_fmsub_round_sd

| VFMSUB132SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMSUB213SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMSUB231SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and subtract the lower
element in "c" from the intermediate result. Store the result in the lower element of "dst", and copy the upper
element from "a" to the upper element of "dst".
	[round_note]

[algorithm]

dst[63:0] := (a[63:0] * b[63:0]) - c[63:0]
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUB132SD - _mm_mask3_fmsub_round_sd

| VFMSUB132SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMSUB213SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMSUB231SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and subtract the lower
element in "c" from the intermediate result. Store the result in the lower element of "dst" using writemask "k"
(the element is copied from "c" when mask bit 0 is not set), and copy the upper element from "c" to the upper
element of "dst".
	[round_note]

[algorithm]

IF k[0]
    dst[63:0] := (a[63:0] * b[63:0]) - c[63:0]
ELSE
    dst[63:0] := c[63:0]
FI
dst[127:64] := c[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUB132SD - _mm_mask3_fmsub_sd

| VFMSUB132SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMSUB213SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMSUB231SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and subtract the lower
element in "c" from the intermediate result. Store the result in the lower element of "dst" using writemask "k"
(the element is copied from "c" when mask bit 0 is not set), and copy the upper element from "c" to the upper
element of "dst".

[algorithm]

IF k[0]
    dst[63:0] := (a[63:0] * b[63:0]) - c[63:0]
ELSE
    dst[63:0] := c[63:0]
FI
dst[127:64] := c[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUB132SD - _mm_mask_fmsub_round_sd

| VFMSUB132SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMSUB213SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMSUB231SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and subtract the lower
element in "c" from the intermediate result. Store the result in the lower element of "dst" using writemask "k"
(the element is copied from "a" when mask bit 0 is not set), and copy the upper element from "a" to the upper
element of "dst".
	[round_note]

[algorithm]

IF k[0]
    dst[63:0] := (a[63:0] * b[63:0]) - c[63:0]
ELSE
    dst[63:0] := a[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUB132SD - _mm_mask_fmsub_sd

| VFMSUB132SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMSUB213SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMSUB231SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and subtract the lower
element in "c" from the intermediate result. Store the result in the lower element of "dst" using writemask "k"
(the element is copied from "a" when mask bit 0 is not set), and copy the upper element from "a" to the upper
element of "dst".

[algorithm]

IF k[0]
    dst[63:0] := (a[63:0] * b[63:0]) - c[63:0]
ELSE
    dst[63:0] := a[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUB132SD - _mm_maskz_fmsub_round_sd

| VFMSUB132SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMSUB213SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMSUB231SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and subtract the lower
element in "c" from the intermediate result. Store the result in the lower element of "dst" using zeromask "k"
(the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper
element of "dst". 
	[round_note]

[algorithm]

IF k[0]
    dst[63:0] := (a[63:0] * b[63:0]) - c[63:0]
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUB132SD - _mm_maskz_fmsub_sd

| VFMSUB132SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMSUB213SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFMSUB231SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and subtract the lower
element in "c" from the intermediate result. Store the result in the lower element of "dst" using zeromask "k"
(the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper
element of "dst".

[algorithm]

IF k[0]
    dst[63:0] := (a[63:0] * b[63:0]) - c[63:0]
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUB132SS - _mm_fmsub_round_ss

| VFMSUB132SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMSUB213SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMSUB231SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and subtract the lower
element in "c" from the intermediate result. Store the result in the lower element of "dst", and copy the upper
3 packed elements from "a" to the upper elements of "dst".
	[round_note]

[algorithm]

dst[31:0] := (a[31:0] * b[31:0]) - c[31:0]
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUB132SS - _mm_mask3_fmsub_round_ss

| VFMSUB132SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMSUB213SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMSUB231SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and subtract the lower
element in "c" from the intermediate result. Store the result in the lower element of "dst" using writemask "k"
(the element is copied from "c" when mask bit 0 is not set), and copy the upper 3 packed elements from "c" to
the upper elements of "dst".
	[round_note]

[algorithm]

IF k[0]
    dst[31:0] := (a[31:0] * b[31:0]) - c[31:0]
ELSE
    dst[31:0] := c[31:0]
FI
dst[127:32] := c[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUB132SS - _mm_mask3_fmsub_ss

| VFMSUB132SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMSUB213SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMSUB231SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and subtract the lower
element in "c" from the intermediate result. Store the result in the lower element of "dst" using writemask "k"
(the element is copied from "c" when mask bit 0 is not set), and copy the upper 3 packed elements from "c" to
the upper elements of "dst".

[algorithm]

IF k[0]
    dst[31:0] := (a[31:0] * b[31:0]) - c[31:0]
ELSE
    dst[31:0] := c[31:0]
FI
dst[127:32] := c[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUB132SS - _mm_mask_fmsub_round_ss

| VFMSUB132SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMSUB213SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMSUB231SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and subtract the lower
element in "c" from the intermediate result. Store the result in the lower element of "dst" using writemask "k"
(the element is copied from "a" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to
the upper elements of "dst". 
	[round_note]

[algorithm]

IF k[0]
    dst[31:0] := (a[31:0] * b[31:0]) - c[31:0]
ELSE
    dst[31:0] := a[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUB132SS - _mm_mask_fmsub_ss

| VFMSUB132SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMSUB213SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMSUB231SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and subtract the lower
element in "c" from the intermediate result. Store the result in the lower element of "dst" using writemask "k"
(the element is copied from "a" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to
the upper elements of "dst".

[algorithm]

IF k[0]
    dst[31:0] := (a[31:0] * b[31:0]) - c[31:0]
ELSE
    dst[31:0] := a[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUB132SS - _mm_maskz_fmsub_round_ss

| VFMSUB132SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMSUB213SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMSUB231SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and subtract the lower
element in "c" from the intermediate result. Store the result in the lower element of "dst" using zeromask "k"
(the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the
upper elements of "dst". 
	[round_note]

[algorithm]

IF k[0]
    dst[31:0] := (a[31:0] * b[31:0]) - c[31:0]
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUB132SS - _mm_maskz_fmsub_ss

| VFMSUB132SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMSUB213SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFMSUB231SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and subtract the lower
element in "c" from the intermediate result. Store the result in the lower element of "dst" using zeromask "k"
(the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the
upper elements of "dst".

[algorithm]

IF k[0]
    dst[31:0] := (a[31:0] * b[31:0]) - c[31:0]
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUBADD132PD - _mm512_fmsubadd_pd

| VFMSUBADD132PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMSUBADD213PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMSUBADD231PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively subtract and
add packed elements in "c" from/to the intermediate result, and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF ((j &amp; 1) == 0)
        dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
    ELSE
        dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUBADD132PD - _mm512_fmsubadd_round_pd

| VFMSUBADD132PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMSUBADD213PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMSUBADD231PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively subtract and
add packed elements in "c" from/to the intermediate result, and store the results in "dst". 
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF ((j &amp; 1) == 0)
        dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
    ELSE
        dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUBADD132PD - _mm512_mask3_fmsubadd_pd

| VFMSUBADD132PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMSUBADD213PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMSUBADD231PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively subtract and
add packed elements in "c" from/to the intermediate result, and store the results in "dst" using writemask "k"
(elements are copied from "c" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        IF ((j &amp; 1) == 0)
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
        ELSE
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
        FI
    ELSE
        dst[i+63:i] := c[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUBADD132PD - _mm512_mask3_fmsubadd_round_pd

| VFMSUBADD132PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMSUBADD213PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMSUBADD231PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively subtract and
add packed elements in "c" from/to the intermediate result, and store the results in "dst" using writemask "k"
(elements are copied from "c" when the corresponding mask bit is not set). [round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        IF ((j &amp; 1) == 0)
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
        ELSE
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
        FI
    ELSE
        dst[i+63:i] := c[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUBADD132PD - _mm512_mask_fmsubadd_pd

| VFMSUBADD132PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMSUBADD213PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMSUBADD231PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively subtract and
add packed elements in "c" from/to the intermediate result, and store the results in "dst" using writemask "k"
(elements are copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        IF ((j &amp; 1) == 0)
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
        ELSE
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
        FI
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUBADD132PD - _mm512_mask_fmsubadd_round_pd

| VFMSUBADD132PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMSUBADD213PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMSUBADD231PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively subtract and
add packed elements in "c" from/to the intermediate result, and store the results in "dst" using writemask "k"
(elements are copied from "a" when the corresponding mask bit is not set). [round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        IF ((j &amp; 1) == 0)
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
        ELSE
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
        FI
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUBADD132PD - _mm512_maskz_fmsubadd_pd

| VFMSUBADD132PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMSUBADD213PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMSUBADD231PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively subtract and
add packed elements in "c" from/to the intermediate result, and store the results in "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        IF ((j &amp; 1) == 0)
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
        ELSE
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
        FI
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUBADD132PD - _mm512_maskz_fmsubadd_round_pd

| VFMSUBADD132PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMSUBADD213PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFMSUBADD231PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively subtract and
add packed elements in "c" from/to the intermediate result, and store the results in "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set). 
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        IF ((j &amp; 1) == 0)
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
        ELSE
            dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
        FI
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUBADD132PS - _mm512_fmsubadd_ps

| VFMSUBADD132PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMSUBADD213PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMSUBADD231PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively subtract and
add packed elements in "c" from/to the intermediate result, and store the results in "dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF ((j &amp; 1) == 0)
        dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
    ELSE
        dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUBADD132PS - _mm512_fmsubadd_round_ps

| VFMSUBADD132PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMSUBADD213PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMSUBADD231PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively subtract and
add packed elements in "c" from/to the intermediate result, and store the results in "dst". 
	[round_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF ((j &amp; 1) == 0)
        dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
    ELSE
        dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUBADD132PS - _mm512_mask3_fmsubadd_ps

| VFMSUBADD132PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMSUBADD213PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMSUBADD231PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively subtract and
add packed elements in "c" from/to the intermediate result, and store the results in "dst" using writemask "k"
(elements are copied from "c" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        IF ((j &amp; 1) == 0)
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
        ELSE
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
        FI
    ELSE
        dst[i+31:i] := c[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUBADD132PS - _mm512_mask3_fmsubadd_round_ps

| VFMSUBADD132PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMSUBADD213PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMSUBADD231PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively subtract and
add packed elements in "c" from/to the intermediate result, and store the results in "dst" using writemask "k"
(elements are copied from "c" when the corresponding mask bit is not set). [round_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        IF ((j &amp; 1) == 0)
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
        ELSE
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
        FI
    ELSE
        dst[i+31:i] := c[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUBADD132PS - _mm512_mask_fmsubadd_ps

| VFMSUBADD132PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMSUBADD213PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMSUBADD231PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively subtract and
add packed elements in "c" from/to the intermediate result, and store the results in "dst" using writemask "k"
(elements are copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        IF ((j &amp; 1) == 0)
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
        ELSE
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
        FI
    ELSE
        dst[i+31:i] := a[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUBADD132PS - _mm512_mask_fmsubadd_round_ps

| VFMSUBADD132PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMSUBADD213PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMSUBADD231PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively subtract and
add packed elements in "c" from/to the intermediate result, and store the results in "dst" using writemask "k"
(elements are copied from "a" when the corresponding mask bit is not set).
	[round_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        IF ((j &amp; 1) == 0)
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
        ELSE
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
        FI
    ELSE
        dst[i+31:i] := a[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUBADD132PS - _mm512_maskz_fmsubadd_ps

| VFMSUBADD132PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMSUBADD213PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMSUBADD231PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively subtract and
add packed elements in "c" from/to the intermediate result, and store the results in "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        IF ((j &amp; 1) == 0)
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
        ELSE
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
        FI
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFMSUBADD132PS - _mm512_maskz_fmsubadd_round_ps

| VFMSUBADD132PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMSUBADD213PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFMSUBADD231PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively subtract and
add packed elements in "c" from/to the intermediate result, and store the results in "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set). 
	[round_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        IF ((j &amp; 1) == 0)
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
        ELSE
            dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
        FI
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMADD132PD - _mm512_maskz_fnmadd_pd

| VFNMADD132PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFNMADD213PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFNMADD231PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", add the negated intermediate
result to packed elements in "c", and store the results in "dst" using zeromask "k" (elements are zeroed out
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR    
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMADD132PD - _mm512_maskz_fnmadd_round_pd

| VFNMADD132PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFNMADD213PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFNMADD231PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", add the negated intermediate
result to packed elements in "c", and store the results in "dst" using zeromask "k" (elements are zeroed out
when the corresponding mask bit is not set). [round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR    
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMADD132PS - _mm512_maskz_fnmadd_ps

| VFNMADD132PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFNMADD213PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFNMADD231PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", add the negated intermediate
result to packed elements in "c", and store the results in "dst" using zeromask "k" (elements are zeroed out
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR    
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMADD132PS - _mm512_maskz_fnmadd_round_ps

| VFNMADD132PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFNMADD213PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFNMADD231PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", add the negated intermediate
result to packed elements in "c", and store the results in "dst" using zeromask "k" (elements are zeroed out
when the corresponding mask bit is not set). [round_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR    
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMADD132SD - _mm_fnmadd_round_sd

| VFNMADD132SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMADD213SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMADD231SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and add the negated
intermediate result to the lower element in "c". Store the result in the lower element of "dst", and copy the
upper element from "a" to the upper element of "dst".
	[round_note]

[algorithm]

dst[63:0] := -(a[63:0] * b[63:0]) + c[63:0]
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMADD132SD - _mm_mask3_fnmadd_round_sd

| VFNMADD132SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMADD213SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMADD231SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and add the negated
intermediate result to the lower element in "c". Store the result in the lower element of "dst" using writemask
"k" (the element is copied from "c" when mask bit 0 is not set), and copy the upper element from "c" to the
upper element of "dst".
	[round_note]

[algorithm]

IF k[0]
    dst[63:0] := -(a[63:0] * b[63:0]) + c[63:0]
ELSE
    dst[63:0] := c[63:0]
FI
dst[127:64] := c[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMADD132SD - _mm_mask3_fnmadd_sd

| VFNMADD132SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMADD213SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMADD231SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and add the negated
intermediate result to the lower element in "c". Store the result in the lower element of "dst" using writemask
"k" (the element is copied from "c" when mask bit 0 is not set), and copy the upper element from "c" to the
upper element of "dst".

[algorithm]

IF k[0]
    dst[63:0] := -(a[63:0] * b[63:0]) + c[63:0]
ELSE
    dst[63:0] := c[63:0]
FI
dst[127:64] := c[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMADD132SD - _mm_mask_fnmadd_round_sd

| VFNMADD132SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMADD213SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMADD231SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and add the negated
intermediate result to the lower element in "c". Store the result in the lower element of "dst" using writemask
"k" (the element is copied from "a" when mask bit 0 is not set), and copy the upper element from "a" to the
upper element of "dst".
	[round_note]

[algorithm]

IF k[0]
    dst[63:0] := -(a[63:0] * b[63:0]) + c[63:0]
ELSE
    dst[63:0] := a[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMADD132SD - _mm_mask_fnmadd_sd

| VFNMADD132SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMADD213SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMADD231SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and add the negated
intermediate result to the lower element in "c". Store the result in the lower element of "dst" using writemask
"k" (the element is copied from "a" when mask bit 0 is not set), and copy the upper element from "a" to the
upper element of "dst".

[algorithm]

IF k[0]
    dst[63:0] := -(a[63:0] * b[63:0]) + c[63:0]
ELSE
    dst[63:0] := a[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMADD132SD - _mm_maskz_fnmadd_round_sd

| VFNMADD132SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMADD213SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMADD231SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and add the negated
intermediate result to the lower element in "c". Store the result in the lower element of "dst" using zeromask
"k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper
element of "dst".
	[round_note]

[algorithm]

IF k[0]
    dst[63:0] := -(a[63:0] * b[63:0]) + c[63:0]
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMADD213SD - _mm_maskz_fnmadd_sd

| VFNMADD213SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMADD231SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMADD132SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and add the negated
intermediate result to the lower element in "c". Store the result in the lower element of "dst" using zeromask
"k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper
element of "dst".

[algorithm]

IF k[0]
    dst[63:0] := -(a[63:0] * b[63:0]) + c[63:0]
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMADD132SS - _mm_fnmadd_round_ss

| VFNMADD132SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMADD213SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMADD231SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and add the negated
intermediate result to the lower element in "c". Store the result in the lower element of "dst", and copy the
upper 3 packed elements from "a" to the upper elements of "dst".
	[round_note]

[algorithm]

dst[31:0] := -(a[31:0] * b[31:0]) + c[31:0]
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMADD132SS - _mm_mask3_fnmadd_round_ss

| VFNMADD132SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMADD213SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMADD231SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and add the negated
intermediate result to the lower element in "c". Store the result in the lower element of "dst" using writemask
"k" (the element is copied from "c" when mask bit 0 is not set), and copy the upper 3 packed elements from "c"
to the upper elements of "dst".
	[round_note]

[algorithm]

IF k[0]
    dst[31:0] := -(a[31:0] * b[31:0]) + c[31:0]
ELSE
    dst[31:0] := c[31:0]
FI
dst[127:32] := c[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMADD132SS - _mm_mask3_fnmadd_ss

| VFNMADD132SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMADD213SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMADD231SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and add the negated
intermediate result to the lower element in "c". Store the result in the lower element of "dst" using writemask
"k" (the element is copied from "c" when mask bit 0 is not set), and copy the upper 3 packed elements from "c"
to the upper elements of "dst".

[algorithm]

IF k[0]
    dst[31:0] := -(a[31:0] * b[31:0]) + c[31:0]
ELSE
    dst[31:0] := c[31:0]
FI
dst[127:32] := c[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMADD132SS - _mm_mask_fnmadd_round_ss

| VFNMADD132SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMADD213SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMADD231SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and add the negated
intermediate result to the lower element in "c". Store the result in the lower element of "dst" using writemask
"k" (the element is copied from "a" when mask bit 0 is not set), and copy the upper 3 packed elements from "a"
to the upper elements of "dst". 
	[round_note]

[algorithm]

IF k[0]
    dst[31:0] := -(a[31:0] * b[31:0]) + c[31:0]
ELSE
    dst[31:0] := a[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMADD132SS - _mm_mask_fnmadd_ss

| VFNMADD132SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMADD213SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMADD231SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and add the negated
intermediate result to the lower element in "c". Store the result in the lower element of "dst" using writemask
"k" (the element is copied from "a" when mask bit 0 is not set), and copy the upper 3 packed elements from "a"
to the upper elements of "dst".

[algorithm]

IF k[0]
    dst[31:0] := -(a[31:0] * b[31:0]) + c[31:0]
ELSE
    dst[31:0] := a[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMADD132SS - _mm_maskz_fnmadd_round_ss

| VFNMADD132SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMADD213SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMADD231SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and add the negated
intermediate result to the lower element in "c". Store the result in the lower element of "dst" using zeromask
"k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to
the upper elements of "dst". 
	[round_note]

[algorithm]

IF k[0]
    dst[31:0] := -(a[31:0] * b[31:0]) + c[31:0]
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMADD132SS - _mm_maskz_fnmadd_ss

| VFNMADD132SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMADD213SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMADD231SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and add the negated
intermediate result to the lower element in "c". Store the result in the lower element of "dst" using zeromask
"k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to
the upper elements of "dst".

[algorithm]

IF k[0]
    dst[31:0] := -(a[31:0] * b[31:0]) + c[31:0]
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMSUB132PD - _mm512_maskz_fnmsub_pd

| VFNMSUB132PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFNMSUB213PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFNMSUB231PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", subtract packed elements in
"c" from the negated intermediate result, and store the results in "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR    
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMSUB132PD - _mm512_maskz_fnmsub_round_pd

| VFNMSUB132PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFNMSUB213PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VFNMSUB231PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", subtract packed elements in
"c" from the negated intermediate result, and store the results in "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set). [round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR    
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMSUB132PS - _mm512_maskz_fnmsub_ps

| VFNMSUB132PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFNMSUB213PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFNMSUB231PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", subtract packed elements in
"c" from the negated intermediate result, and store the results in "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR    
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMSUB132PS - _mm512_maskz_fnmsub_round_ps

| VFNMSUB132PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFNMSUB213PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VFNMSUB231PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", subtract packed elements in
"c" from the negated intermediate result, and store the results in "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set). 
	[round_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR    
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMSUB132SD - _mm_fnmsub_round_sd

| VFNMSUB132SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMSUB213SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMSUB231SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and subtract the lower
element in "c" from the negated intermediate result. Store the result in the lower element of "dst", and copy
the upper element from "a" to the upper element of "dst".
	[round_note]

[algorithm]

dst[63:0] := -(a[63:0] * b[63:0]) - c[63:0]
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMSUB132SD - _mm_mask3_fnmsub_round_sd

| VFNMSUB132SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMSUB213SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMSUB231SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and subtract the lower
element in "c" from the negated intermediate result. Store the result in the lower element of "dst" using
writemask "k" (the element is copied from "c" when mask bit 0 is not set), and copy the upper element from "c"
to the upper element of "dst".
	[round_note]

[algorithm]

IF k[0]
    dst[63:0] := -(a[63:0] * b[63:0]) - c[63:0]
ELSE
    dst[63:0] := c[63:0]
FI
dst[127:64] := c[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMSUB132SD - _mm_mask3_fnmsub_sd

| VFNMSUB132SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMSUB213SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMSUB231SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and subtract the lower
element in "c" from the negated intermediate result. Store the result in the lower element of "dst" using
writemask "k" (the element is copied from "c" when mask bit 0 is not set), and copy the upper element from "c"
to the upper element of "dst".

[algorithm]

IF k[0]
    dst[63:0] := -(a[63:0] * b[63:0]) - c[63:0]
ELSE
    dst[63:0] := c[63:0]
FI
dst[127:64] := c[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMSUB132SD - _mm_mask_fnmsub_round_sd

| VFNMSUB132SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMSUB213SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMSUB231SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and subtract the lower
element in "c" from the negated intermediate result. Store the result in the lower element of "dst" using
writemask "k" (the element is copied from "c" when mask bit 0 is not set), and copy the upper element from "a"
to the upper element of "dst".
	[round_note]

[algorithm]

IF k[0]
    dst[63:0] := -(a[63:0] * b[63:0]) - c[63:0]
ELSE
    dst[63:0] := a[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMSUB132SD - _mm_mask_fnmsub_sd

| VFNMSUB132SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMSUB213SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMSUB231SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and subtract the lower
element in "c" from the negated intermediate result. Store the result in the lower element of "dst" using
writemask "k" (the element is copied from "c" when mask bit 0 is not set), and copy the upper element from "a"
to the upper element of "dst".

[algorithm]

IF k[0]
    dst[63:0] := -(a[63:0] * b[63:0]) - c[63:0]
ELSE
    dst[63:0] := a[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMSUB132SD - _mm_maskz_fnmsub_round_sd

| VFNMSUB132SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMSUB213SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMSUB231SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and subtract the lower
element in "c" from the negated intermediate result. Store the result in "dst" using zeromask "k" (the element
is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of
"dst".
	[round_note]

[algorithm]

IF k[0]
    dst[63:0] := -(a[63:0] * b[63:0]) - c[63:0]
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMSUB132SD - _mm_maskz_fnmsub_sd

| VFNMSUB132SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMSUB213SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512 | VFNMSUB231SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and subtract the lower
element in "c" from the negated intermediate result. Store the result in "dst" using zeromask "k" (the element
is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".

[algorithm]

IF k[0]
    dst[63:0] := -(a[63:0] * b[63:0]) - c[63:0]
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMSUB132SS - _mm_fnmsub_round_ss

| VFNMSUB132SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMSUB213SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMSUB231SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", subtract the lower
element in "c" from the negated intermediate result, store the result in the lower element of "dst", and copy
the upper 3 packed elements from "a" to the upper elements of "dst".
	[round_note]

[algorithm]

dst[31:0] := -(a[31:0] * b[31:0]) - c[31:0]
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMSUB132SS - _mm_mask3_fnmsub_round_ss

| VFNMSUB132SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMSUB213SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMSUB231SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", subtract the lower
element in "c" from the negated intermediate result. Store the result in the lower element of "dst" using
writemask "k" (the element is copied from "c" when mask bit 0 is not set), and copy the upper 3 packed elements
from "c" to the upper elements of "dst".
	[round_note]

[algorithm]

IF k[0]
    dst[31:0] := -(a[31:0] * b[31:0]) - c[31:0]
ELSE
    dst[31:0] := c[31:0]
FI
dst[127:32] := c[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMSUB132SS - _mm_mask3_fnmsub_ss

| VFNMSUB132SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMSUB213SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMSUB231SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and subtract the lower
element in "c" from the negated intermediate result. Store the result in the lower element of "dst" using
writemask "k" (the element is copied from "c" when mask bit 0 is not set), and copy the upper 3 packed elements
from "c" to the upper elements of "dst".

[algorithm]

IF k[0]
    dst[31:0] := -(a[31:0] * b[31:0]) - c[31:0]
ELSE
    dst[31:0] := c[31:0]
FI
dst[127:32] := c[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMSUB132SS - _mm_mask_fnmsub_round_ss

| VFNMSUB132SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMSUB213SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMSUB231SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and subtract the lower
element in "c" from the negated intermediate result. Store the result in the lower element of "dst" using
writemask "k" (the element is copied from "c" when mask bit 0 is not set), and copy the upper 3 packed elements
from "a" to the upper elements of "dst".
	[round_note]

[algorithm]

IF k[0]
    dst[31:0] := -(a[31:0] * b[31:0]) - c[31:0]
ELSE
    dst[31:0] := a[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMSUB132SS - _mm_mask_fnmsub_ss

| VFNMSUB132SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMSUB213SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMSUB231SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and subtract the lower
element in "c" from the negated intermediate result. Store the result in the lower element of "dst" using
writemask "k" (the element is copied from "c" when mask bit 0 is not set), and copy the upper 3 packed elements
from "a" to the upper elements of "dst".

[algorithm]

IF k[0]
    dst[31:0] := -(a[31:0] * b[31:0]) - c[31:0]
ELSE
    dst[31:0] := a[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMSUB132SS - _mm_maskz_fnmsub_round_ss

| VFNMSUB132SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMSUB213SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMSUB231SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and subtract the lower
element in "c" from the negated intermediate result. Store the result in the lower element of "dst" using
zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from
"a" to the upper elements of "dst".
	[round_note]

[algorithm]

IF k[0]
    dst[31:0] := -(a[31:0] * b[31:0]) - c[31:0]
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VFNMSUB132SS - _mm_maskz_fnmsub_ss

| VFNMSUB132SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMSUB213SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512 | VFNMSUB231SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and subtract the lower
element in "c" from the negated intermediate result. Store the result in the lower element of "dst" using
zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from
"a" to the upper elements of "dst".

[algorithm]

IF k[0]
    dst[31:0] := -(a[31:0] * b[31:0]) - c[31:0]
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGATHERDPD - _mm512_i32gather_pd

| VGATHERDPD_ZMMf64_MASKmskw_MEMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Gather double-precision (64-bit) floating-point elements from memory using 32-bit indices. 64-bit elements are
loaded from addresses starting at "base_addr" and offset by each 32-bit element in "vindex" (each index is
scaled by the factor in "scale"). Gathered elements are merged into "dst". "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 7
    i := j*64
    m := j*32
    addr := base_addr + SignExtend64(vindex[m+31:m]) * ZeroExtend64(scale) * 8
    dst[i+63:i] := MEM[addr+63:addr]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VGATHERDPD - _mm512_mask_i32gather_pd

| VGATHERDPD_ZMMf64_MASKmskw_MEMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Gather double-precision (64-bit) floating-point elements from memory using 32-bit indices. 64-bit elements are
loaded from addresses starting at "base_addr" and offset by each 32-bit element in "vindex" (each index is
scaled by the factor in "scale"). Gathered elements are merged into "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 7
    i := j*64
    m := j*32
    IF k[j]
        addr := base_addr + SignExtend64(vindex[m+31:m]) * ZeroExtend64(scale) * 8
        dst[i+63:i] := MEM[addr+63:addr]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VGATHERQPD - _mm512_i64gather_pd

| VGATHERQPD_ZMMf64_MASKmskw_MEMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Gather double-precision (64-bit) floating-point elements from memory using 64-bit indices. 64-bit elements are
loaded from addresses starting at "base_addr" and offset by each 64-bit element in "vindex" (each index is
scaled by the factor in "scale"). Gathered elements are merged into "dst". "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 7
    i := j*64
    m := j*64
    addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
    dst[i+63:i] := MEM[addr+63:addr]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VGATHERQPD - _mm512_mask_i64gather_pd

| VGATHERQPD_ZMMf64_MASKmskw_MEMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Gather double-precision (64-bit) floating-point elements from memory using 64-bit indices. 64-bit elements are
loaded from addresses starting at "base_addr" and offset by each 64-bit element in "vindex" (each index is
scaled by the factor in "scale"). Gathered elements are merged into "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 7
    i := j*64
    m := j*64
    IF k[j]
        addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
        dst[i+63:i] := MEM[addr+63:addr]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VGATHERQPS - _mm512_i64gather_ps

| VGATHERQPS_YMMf32_MASKmskw_MEMf32_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Gather single-precision (32-bit) floating-point elements from memory using 64-bit indices. 32-bit elements are
loaded from addresses starting at "base_addr" and offset by each 64-bit element in "vindex" (each index is
scaled by the factor in "scale"). Gathered elements are merged into "dst". "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 7
    i := j*32
    m := j*64
    addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
    dst[i+31:i] := MEM[addr+31:addr]
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VGATHERQPS - _mm512_mask_i64gather_ps

| VGATHERQPS_YMMf32_MASKmskw_MEMf32_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Gather single-precision (32-bit) floating-point elements from memory using 64-bit indices. 32-bit elements are
loaded from addresses starting at "base_addr" and offset by each 64-bit element in "vindex" (each index is
scaled by the factor in "scale"). Gathered elements are merged into "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 7
    i := j*32
    m := j*64
    IF k[j]
        addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
        dst[i+31:i] := MEM[addr+31:addr]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VGETEXPPD - _mm512_maskz_getexp_pd

| VGETEXPPD_ZMMf64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the exponent of each packed double-precision (64-bit) floating-point element in "a" to a
double-precision (64-bit) floating-point number representing the integer exponent, and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). This intrinsic
essentially calculates "floor(log2(x))" for each element.

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := ConvertExpFP64(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VGETEXPPD - _mm512_maskz_getexp_round_pd

| VGETEXPPD_ZMMf64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the exponent of each packed double-precision (64-bit) floating-point element in "a" to a
double-precision (64-bit) floating-point number representing the integer exponent, and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). This intrinsic
essentially calculates "floor(log2(x))" for each element.
	[sae_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := ConvertExpFP64(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VGETEXPPS - _mm512_maskz_getexp_ps

| VGETEXPPS_ZMMf32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the exponent of each packed single-precision (32-bit) floating-point element in "a" to a
single-precision (32-bit) floating-point number representing the integer exponent, and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). This intrinsic
essentially calculates "floor(log2(x))" for each element.

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := ConvertExpFP32(a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VGETEXPPS - _mm512_maskz_getexp_round_ps

| VGETEXPPS_ZMMf32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the exponent of each packed single-precision (32-bit) floating-point element in "a" to a
single-precision (32-bit) floating-point number representing the integer exponent, and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). This intrinsic
essentially calculates "floor(log2(x))" for each element.
	[sae_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := ConvertExpFP32(a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VGETEXPSD - _mm_getexp_round_sd

| VGETEXPSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the exponent of the lower double-precision (64-bit) floating-point element in "b" to a
double-precision (64-bit) floating-point number representing the integer exponent, store the result in the
lower element of "dst", and copy the upper element from "a" to the upper element of "dst". This intrinsic
essentially calculates "floor(log2(x))" for the lower element.
	[sae_note]

[algorithm]

dst[63:0] := ConvertExpFP64(b[63:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGETEXPSD - _mm_getexp_sd

| VGETEXPSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the exponent of the lower double-precision (64-bit) floating-point element in "b" to a
double-precision (64-bit) floating-point number representing the integer exponent, store the result in the
lower element of "dst", and copy the upper element from "a" to the upper element of "dst". This intrinsic
essentially calculates "floor(log2(x))" for the lower element.

[algorithm]

dst[63:0] := ConvertExpFP64(b[63:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGETEXPSD - _mm_mask_getexp_round_sd

| VGETEXPSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the exponent of the lower double-precision (64-bit) floating-point element in "b" to a
double-precision (64-bit) floating-point number representing the integer exponent, store the result in the
lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and
copy the upper element from "a" to the upper element of "dst". This intrinsic essentially calculates
"floor(log2(x))" for the lower element.
	[sae_note]

[algorithm]

IF k[0]
    dst[63:0] := ConvertExpFP64(b[63:0])
ELSE
    dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGETEXPSD - _mm_mask_getexp_sd

| VGETEXPSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the exponent of the lower double-precision (64-bit) floating-point element in "b" to a
double-precision (64-bit) floating-point number representing the integer exponent, store the result in the
lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and
copy the upper element from "a" to the upper element of "dst". This intrinsic essentially calculates
"floor(log2(x))" for the lower element.

[algorithm]

IF k[0]
    dst[63:0] := ConvertExpFP64(b[63:0])
ELSE
    dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGETEXPSD - _mm_maskz_getexp_round_sd

| VGETEXPSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the exponent of the lower double-precision (64-bit) floating-point element in "b" to a
double-precision (64-bit) floating-point number representing the integer exponent, store the result in the
lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the
upper element from "a" to the upper element of "dst". This intrinsic essentially calculates "floor(log2(x))"
for the lower element.
	[sae_note]

[algorithm]

IF k[0]
    dst[63:0] := ConvertExpFP64(b[63:0])
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGETEXPSD - _mm_maskz_getexp_sd

| VGETEXPSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the exponent of the lower double-precision (64-bit) floating-point element in "b" to a
double-precision (64-bit) floating-point number representing the integer exponent, store the result in the
lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the
upper element from "a" to the upper element of "dst". This intrinsic essentially calculates "floor(log2(x))"
for the lower element.

[algorithm]

IF k[0]
    dst[63:0] := ConvertExpFP64(b[63:0])
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGETEXPSS - _mm_getexp_round_ss

| VGETEXPSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the exponent of the lower single-precision (32-bit) floating-point element in "b" to a
single-precision (32-bit) floating-point number representing the integer exponent, store the result in the
lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of "dst". This
intrinsic essentially calculates "floor(log2(x))" for the lower element.
	[sae_note]

[algorithm]

dst[31:0] := ConvertExpFP32(b[31:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGETEXPSS - _mm_getexp_ss

| VGETEXPSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the exponent of the lower single-precision (32-bit) floating-point element in "b" to a
single-precision (32-bit) floating-point number representing the integer exponent, store the result in the
lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of "dst". This
intrinsic essentially calculates "floor(log2(x))" for the lower element.

[algorithm]

dst[31:0] := ConvertExpFP32(b[31:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGETEXPSS - _mm_mask_getexp_round_ss

| VGETEXPSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the exponent of the lower single-precision (32-bit) floating-point element in "b" to a
single-precision (32-bit) floating-point number representing the integer exponent, store the result in the
lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and
copy the upper 3 packed elements from "a" to the upper elements of "dst". This intrinsic essentially calculates
"floor(log2(x))" for the lower element.
	[sae_note]

[algorithm]

IF k[0]
    dst[31:0] := ConvertExpFP32(b[31:0])
ELSE
    dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGETEXPSS - _mm_mask_getexp_ss

| VGETEXPSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the exponent of the lower single-precision (32-bit) floating-point element in "b" to a
single-precision (32-bit) floating-point number representing the integer exponent, store the result in the
lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and
copy the upper 3 packed elements from "a" to the upper elements of "dst". This intrinsic essentially calculates
"floor(log2(x))" for the lower element.

[algorithm]

IF k[0]
    dst[31:0] := ConvertExpFP32(b[31:0])
ELSE
    dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGETEXPSS - _mm_maskz_getexp_round_ss

| VGETEXPSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the exponent of the lower single-precision (32-bit) floating-point element in "b" to a
single-precision (32-bit) floating-point number representing the integer exponent, store the result in the
lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the
upper 3 packed elements from "a" to the upper elements of "dst". This intrinsic essentially calculates
"floor(log2(x))" for the lower element.
	[sae_note]

[algorithm]

IF k[0]
    dst[31:0] := ConvertExpFP32(b[31:0])
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGETEXPSS - _mm_maskz_getexp_ss

| VGETEXPSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert the exponent of the lower single-precision (32-bit) floating-point element in "b" to a
single-precision (32-bit) floating-point number representing the integer exponent, store the result in the
lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the
upper 3 packed elements from "a" to the upper elements of "dst". This intrinsic essentially calculates
"floor(log2(x))" for the lower element.

[algorithm]

IF k[0]
    dst[31:0] := ConvertExpFP32(b[31:0])
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGETMANTPD - _mm512_maskz_getmant_pd

| VGETMANTPD_ZMMf64_MASKmskw_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Normalize the mantissas of packed double-precision (64-bit) floating-point elements in "a", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). This
intrinsic essentially calculates "(2^k)*|x.significand|", where "k" depends on the interval range defined by
"interv" and the sign depends on "sc" and the source sign.
	[getmant_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VGETMANTPD - _mm512_maskz_getmant_round_pd

| VGETMANTPD_ZMMf64_MASKmskw_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Normalize the mantissas of packed double-precision (64-bit) floating-point elements in "a", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). This
intrinsic essentially calculates "(2^k)*|x.significand|", where "k" depends on the interval range defined by
"interv" and the sign depends on "sc" and the source sign.
	[getmant_note][sae_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VGETMANTPS - _mm512_maskz_getmant_ps

| VGETMANTPS_ZMMf32_MASKmskw_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Normalize the mantissas of packed single-precision (32-bit) floating-point elements in "a", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). This
intrinsic essentially calculates "(2^k)*|x.significand|", where "k" depends on the interval range defined by
"interv" and the sign depends on "sc" and the source sign.
	[getmant_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VGETMANTPS - _mm512_maskz_getmant_round_ps

| VGETMANTPS_ZMMf32_MASKmskw_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Normalize the mantissas of packed single-precision (32-bit) floating-point elements in "a", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). This
intrinsic essentially calculates "(2^k)*|x.significand|", where "k" depends on the interval range defined by
"interv" and the sign depends on "sc" and the source sign.
	[getmant_note][sae_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VGETMANTSD - _mm_getmant_round_sd

| VGETMANTSD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Normalize the mantissas of the lower double-precision (64-bit) floating-point element in "b", store the result
in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst". This
intrinsic essentially calculates "(2^k)*|x.significand|", where "k" depends on the interval range defined by
"interv" and the sign depends on "sc" and the source sign.
	[getmant_note][sae_note]

[algorithm]

dst[63:0] := GetNormalizedMantissa(b[63:0], sc, interv)
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGETMANTSD - _mm_getmant_sd

| VGETMANTSD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Normalize the mantissas of the lower double-precision (64-bit) floating-point element in "b", store the result
in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst". This
intrinsic essentially calculates "(2^k)*|x.significand|", where "k" depends on the interval range defined by
"interv" and the sign depends on "sc" and the source sign.
	[getmant_note]

[algorithm]

dst[63:0] := GetNormalizedMantissa(b[63:0], sc, interv)
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGETMANTSD - _mm_mask_getmant_round_sd

| VGETMANTSD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Normalize the mantissas of the lower double-precision (64-bit) floating-point element in "b", store the result
in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not
set), and copy the upper element from "a" to the upper element of "dst". This intrinsic essentially calculates
"(2^k)*|x.significand|", where "k" depends on the interval range defined by "interv" and the sign depends on
"sc" and the source sign.
	[getmant_note][sae_note]

[algorithm]

IF k[0]
    dst[63:0] := GetNormalizedMantissa(b[63:0], sc, interv)
ELSE
    dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGETMANTSD - _mm_mask_getmant_sd

| VGETMANTSD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Normalize the mantissas of the lower double-precision (64-bit) floating-point element in "b", store the result
in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not
set), and copy the upper element from "a" to the upper element of "dst". This intrinsic essentially calculates
"(2^k)*|x.significand|", where "k" depends on the interval range defined by "interv" and the sign depends on
"sc" and the source sign.
	[getmant_note]

[algorithm]

IF k[0]
    dst[63:0] := GetNormalizedMantissa(b[63:0], sc, interv)
ELSE
    dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGETMANTSD - _mm_maskz_getmant_round_sd

| VGETMANTSD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Normalize the mantissas of the lower double-precision (64-bit) floating-point element in "b", store the result
in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and
copy the upper element from "a" to the upper element of "dst". This intrinsic essentially calculates
"(2^k)*|x.significand|", where "k" depends on the interval range defined by "interv" and the sign depends on
"sc" and the source sign.
	[getmant_note][sae_note]

[algorithm]

IF k[0]
    dst[63:0] := GetNormalizedMantissa(b[63:0], sc, interv)
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGETMANTSD - _mm_maskz_getmant_sd

| VGETMANTSD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Normalize the mantissas of the lower double-precision (64-bit) floating-point element in "b", store the result
in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and
copy the upper element from "a" to the upper element of "dst". This intrinsic essentially calculates
"(2^k)*|x.significand|", where "k" depends on the interval range defined by "interv" and the sign depends on
"sc" and the source sign.
	[getmant_note]

[algorithm]

IF k[0]
    dst[63:0] := GetNormalizedMantissa(b[63:0], sc, interv)
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGETMANTSS - _mm_getmant_round_ss

| VGETMANTSS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Normalize the mantissas of the lower single-precision (32-bit) floating-point element in "b", store the result
in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of "dst".
This intrinsic essentially calculates "(2^k)*|x.significand|", where "k" depends on the interval range defined
by "interv" and the sign depends on "sc" and the source sign.
	[getmant_note][sae_note]

[algorithm]

dst[31:0] := GetNormalizedMantissa(b[31:0], sc, interv)
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGETMANTSS - _mm_getmant_ss

| VGETMANTSS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Normalize the mantissas of the lower single-precision (32-bit) floating-point element in "b", store the result
in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of "dst".
This intrinsic essentially calculates "(2^k)*|x.significand|", where "k" depends on the interval range defined
by "interv" and the sign depends on "sc" and the source sign.
	[getmant_note]

[algorithm]

dst[31:0] := GetNormalizedMantissa(b[31:0], sc, interv)
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGETMANTSS - _mm_mask_getmant_round_ss

| VGETMANTSS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Normalize the mantissas of the lower single-precision (32-bit) floating-point element in "b", store the result
in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not
set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". This intrinsic essentially
calculates "(2^k)*|x.significand|", where "k" depends on the interval range defined by "interv" and the sign
depends on "sc" and the source sign.
	[getmant_note][sae_note]

[algorithm]

IF k[0]
    dst[31:0] := GetNormalizedMantissa(b[31:0], sc, interv)
ELSE
    dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGETMANTSS - _mm_mask_getmant_ss

| VGETMANTSS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Normalize the mantissas of the lower single-precision (32-bit) floating-point element in "b", store the result
in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not
set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". This intrinsic essentially
calculates "(2^k)*|x.significand|", where "k" depends on the interval range defined by "interv" and the sign
depends on "sc" and the source sign.
	[getmant_note]

[algorithm]

IF k[0]
    dst[31:0] := GetNormalizedMantissa(b[31:0], sc, interv)
ELSE
    dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGETMANTSS - _mm_maskz_getmant_round_ss

| VGETMANTSS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Normalize the mantissas of the lower single-precision (32-bit) floating-point element in "b", store the result
in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and
copy the upper 3 packed elements from "a" to the upper elements of "dst". This intrinsic essentially calculates
"(2^k)*|x.significand|", where "k" depends on the interval range defined by "interv" and the sign depends on
"sc" and the source sign.
	[getmant_note][sae_note]

[algorithm]

IF k[0]
    dst[31:0] := GetNormalizedMantissa(b[31:0], sc, interv)
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGETMANTSS - _mm_maskz_getmant_ss

| VGETMANTSS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Normalize the mantissas of the lower single-precision (32-bit) floating-point element in "b", store the result
in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and
copy the upper 3 packed elements from "a" to the upper elements of "dst". This intrinsic essentially calculates
"(2^k)*|x.significand|", where "k" depends on the interval range defined by "interv" and the sign depends on
"sc" and the source sign.
	[getmant_note]

[algorithm]

IF k[0]
    dst[31:0] := GetNormalizedMantissa(b[31:0], sc, interv)
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VINSERTF32X4 - _mm512_insertf32x4

| VINSERTF32X4_ZMMf32_MASKmskw_ZMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Copy "a" to "dst", then insert 128 bits (composed of 4 packed single-precision (32-bit) floating-point
elements) from "b" into "dst" at the location specified by "imm8".

[algorithm]

dst[511:0] := a[511:0]
CASE (imm8[1:0]) OF
0: dst[127:0] := b[127:0]
1: dst[255:128] := b[127:0]
2: dst[383:256] := b[127:0]
3: dst[511:384] := b[127:0]
ESAC
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VINSERTF32X4 - _mm512_mask_insertf32x4

| VINSERTF32X4_ZMMf32_MASKmskw_ZMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Copy "a" to "tmp", then insert 128 bits (composed of 4 packed single-precision (32-bit) floating-point
elements) from "b" into "tmp" at the location specified by "imm8". Store "tmp" to "dst" using writemask "k"
(elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

tmp[511:0] := a[511:0]
CASE (imm8[1:0]) OF
0: tmp[127:0] := b[127:0]
1: tmp[255:128] := b[127:0]
2: tmp[383:256] := b[127:0]
3: tmp[511:384] := b[127:0]
ESAC
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VINSERTF32X4 - _mm512_maskz_insertf32x4

| VINSERTF32X4_ZMMf32_MASKmskw_ZMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Copy "a" to "tmp", then insert 128 bits (composed of 4 packed single-precision (32-bit) floating-point
elements) from "b" into "tmp" at the location specified by "imm8". Store "tmp" to "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

tmp[511:0] := a[511:0]
CASE (imm8[1:0]) OF
0: tmp[127:0] := b[127:0]
1: tmp[255:128] := b[127:0]
2: tmp[383:256] := b[127:0]
3: tmp[511:384] := b[127:0]
ESAC
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VINSERTF64X4 - _mm512_insertf64x4

| VINSERTF64X4_ZMMf64_MASKmskw_ZMMf64_YMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Copy "a" to "dst", then insert 256 bits (composed of 4 packed double-precision (64-bit) floating-point
elements) from "b" into "dst" at the location specified by "imm8".

[algorithm]

dst[511:0] := a[511:0]
CASE (imm8[0]) OF
0: dst[255:0] := b[255:0]
1: dst[511:256] := b[255:0]
ESAC
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VINSERTF64X4 - _mm512_mask_insertf64x4

| VINSERTF64X4_ZMMf64_MASKmskw_ZMMf64_YMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Copy "a" to "tmp", then insert 256 bits (composed of 4 packed double-precision (64-bit) floating-point
elements) from "b" into "tmp" at the location specified by "imm8". Store "tmp" to "dst" using writemask "k"
(elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

tmp[511:0] := a[511:0]
CASE (imm8[0]) OF
0: tmp[255:0] := b[255:0]
1: tmp[511:256] := b[255:0]
ESAC
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VINSERTF64X4 - _mm512_maskz_insertf64x4

| VINSERTF64X4_ZMMf64_MASKmskw_ZMMf64_YMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Copy "a" to "tmp", then insert 256 bits (composed of 4 packed double-precision (64-bit) floating-point
elements) from "b" into "tmp" at the location specified by "imm8". Store "tmp" to "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

tmp[511:0] := a[511:0]
CASE (imm8[0]) OF
0: tmp[255:0] := b[255:0]
1: tmp[511:256] := b[255:0]
ESAC
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VINSERTI32X4 - _mm512_inserti32x4

| VINSERTI32X4_ZMMu32_MASKmskw_ZMMu32_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Copy "a" to "dst", then insert 128 bits (composed of 4 packed 32-bit integers) from "b" into "dst" at the
location specified by "imm8".

[algorithm]

dst[511:0] := a[511:0]
CASE (imm8[1:0]) OF
0: dst[127:0] := b[127:0]
1: dst[255:128] := b[127:0]
2: dst[383:256] := b[127:0]
3: dst[511:384] := b[127:0]
ESAC
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VINSERTI32X4 - _mm512_mask_inserti32x4

| VINSERTI32X4_ZMMu32_MASKmskw_ZMMu32_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Copy "a" to "tmp", then insert 128 bits (composed of 4 packed 32-bit integers) from "b" into "tmp" at the
location specified by "imm8". Store "tmp" to "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

tmp[511:0] := a[511:0]
CASE (imm8[1:0]) OF
0: tmp[127:0] := b[127:0]
1: tmp[255:128] := b[127:0]
2: tmp[383:256] := b[127:0]
3: tmp[511:384] := b[127:0]
ESAC
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VINSERTI32X4 - _mm512_maskz_inserti32x4

| VINSERTI32X4_ZMMu32_MASKmskw_ZMMu32_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Copy "a" to "tmp", then insert 128 bits (composed of 4 packed 32-bit integers) from "b" into "tmp" at the
location specified by "imm8". Store "tmp" to "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

tmp[511:0] := a[511:0]
CASE (imm8[1:0]) OF
0: tmp[127:0] := b[127:0]
1: tmp[255:128] := b[127:0]
2: tmp[383:256] := b[127:0]
3: tmp[511:384] := b[127:0]
ESAC
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VINSERTI64X4 - _mm512_inserti64x4

| VINSERTI64X4_ZMMu64_MASKmskw_ZMMu64_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Copy "a" to "dst", then insert 256 bits (composed of 4 packed 64-bit integers) from "b" into "dst" at the
location specified by "imm8".

[algorithm]

dst[511:0] := a[511:0]
CASE (imm8[0]) OF
0: dst[255:0] := b[255:0]
1: dst[511:256] := b[255:0]
ESAC
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VINSERTI64X4 - _mm512_mask_inserti64x4

| VINSERTI64X4_ZMMu64_MASKmskw_ZMMu64_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Copy "a" to "tmp", then insert 256 bits (composed of 4 packed 64-bit integers) from "b" into "tmp" at the
location specified by "imm8". Store "tmp" to "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

tmp[511:0] := a[511:0]
CASE (imm8[0]) OF
0: tmp[255:0] := b[255:0]
1: tmp[511:256] := b[255:0]
ESAC
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VINSERTI64X4 - _mm512_maskz_inserti64x4

| VINSERTI64X4_ZMMu64_MASKmskw_ZMMu64_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Copy "a" to "tmp", then insert 256 bits (composed of 4 packed 64-bit integers) from "b" into "tmp" at the
location specified by "imm8". Store "tmp" to "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

tmp[511:0] := a[511:0]
CASE (imm8[0]) OF
0: tmp[255:0] := b[255:0]
1: tmp[511:256] := b[255:0]
ESAC
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMAXPD - _mm512_mask_max_pd

| VMAXPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed maximum
values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMAXPD - _mm512_mask_max_round_pd

| VMAXPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed maximum
values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set). [sae_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMAXPD - _mm512_maskz_max_pd

| VMAXPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed maximum
values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMAXPD - _mm512_maskz_max_round_pd

| VMAXPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed maximum
values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
[sae_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMAXPD - _mm512_max_pd

| VMAXPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed maximum
values in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMAXPD - _mm512_max_round_pd

| VMAXPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed maximum
values in "dst". [sae_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMAXPS - _mm512_mask_max_ps

| VMAXPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed maximum
values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMAXPS - _mm512_mask_max_round_ps

| VMAXPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed maximum
values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set). [sae_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMAXPS - _mm512_maskz_max_ps

| VMAXPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed maximum
values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMAXPS - _mm512_maskz_max_round_ps

| VMAXPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed maximum
values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
[sae_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMAXPS - _mm512_max_ps

| VMAXPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed maximum
values in "dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMAXPS - _mm512_max_round_ps

| VMAXPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed maximum
values in "dst". [sae_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMAXSD - _mm_mask_max_round_sd

| VMAXSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare the lower double-precision (64-bit) floating-point elements in "a" and "b", store the maximum value in
the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set),
and copy the upper element from "a" to the upper element of "dst". [sae_note]

[algorithm]

IF k[0]
    dst[63:0] := MAX(a[63:0], b[63:0])
ELSE
    dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMAXSD - _mm_mask_max_sd

| VMAXSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare the lower double-precision (64-bit) floating-point elements in "a" and "b", store the maximum value in
the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set),
and copy the upper element from "a" to the upper element of "dst".

[algorithm]

IF k[0]
    dst[63:0] := MAX(a[63:0], b[63:0])
ELSE
    dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMAXSD - _mm_maskz_max_round_sd

| VMAXSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare the lower double-precision (64-bit) floating-point elements in "a" and "b", store the maximum value in
the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy
the upper element from "a" to the upper element of "dst". [sae_note]

[algorithm]

IF k[0]
    dst[63:0] := MAX(a[63:0], b[63:0])
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMAXSD - _mm_maskz_max_sd

| VMAXSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare the lower double-precision (64-bit) floating-point elements in "a" and "b", store the maximum value in
the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy
the upper element from "a" to the upper element of "dst".

[algorithm]

IF k[0]
    dst[63:0] := MAX(a[63:0], b[63:0])
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMAXSD - _mm_max_round_sd

| VMAXSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare the lower double-precision (64-bit) floating-point elements in "a" and "b", store the maximum value in
the lower element of "dst", and copy the upper element from "a" to the upper element of "dst". [sae_note]

[algorithm]

dst[63:0] := MAX(a[63:0], b[63:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMAXSS - _mm_mask_max_round_ss

| VMAXSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare the lower single-precision (32-bit) floating-point elements in "a" and "b", store the maximum value in
the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set),
and copy the upper 3 packed elements from "a" to the upper elements of "dst". [sae_note]

[algorithm]

IF k[0]
    dst[31:0] := MAX(a[31:0], b[31:0])
ELSE
    dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMAXSS - _mm_mask_max_ss

| VMAXSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare the lower single-precision (32-bit) floating-point elements in "a" and "b", store the maximum value in
the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set),
and copy the upper 3 packed elements from "a" to the upper elements of "dst".

[algorithm]

IF k[0]
    dst[31:0] := MAX(a[31:0], b[31:0])
ELSE
    dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMAXSS - _mm_maskz_max_round_ss

| VMAXSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare the lower single-precision (32-bit) floating-point elements in "a" and "b", store the maximum value in
the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy
the upper 3 packed elements from "a" to the upper elements of "dst". [sae_note]

[algorithm]

IF k[0]
    dst[31:0] := MAX(a[31:0], b[31:0])
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMAXSS - _mm_maskz_max_ss

| VMAXSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare the lower single-precision (32-bit) floating-point elements in "a" and "b", store the maximum value in
the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy
the upper 3 packed elements from "a" to the upper elements of "dst".

[algorithm]

IF k[0]
    dst[31:0] := MAX(a[31:0], b[31:0])
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMAXSS - _mm_max_round_ss

| VMAXSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare the lower single-precision (32-bit) floating-point elements in "a" and "b", store the maximum value in
the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of "dst".
[sae_note]

[algorithm]

dst[31:0] := MAX(a[31:0], b[31:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMINPD - _mm512_mask_min_pd

| VMINPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed minimum
values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMINPD - _mm512_mask_min_round_pd

| VMINPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed minimum
values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set). [sae_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMINPD - _mm512_maskz_min_pd

| VMINPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed minimum
values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMINPD - _mm512_maskz_min_round_pd

| VMINPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed minimum
values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
[sae_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMINPD - _mm512_min_pd

| VMINPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed minimum
values in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMINPD - _mm512_min_round_pd

| VMINPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed minimum
values in "dst". [sae_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMINPS - _mm512_mask_min_ps

| VMINPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed minimum
values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMINPS - _mm512_mask_min_round_ps

| VMINPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed minimum
values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set). [sae_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMINPS - _mm512_maskz_min_ps

| VMINPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed minimum
values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMINPS - _mm512_maskz_min_round_ps

| VMINPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed minimum
values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
[sae_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMINPS - _mm512_min_ps

| VMINPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed minimum
values in "dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMINPS - _mm512_min_round_ps

| VMINPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed minimum
values in "dst". [sae_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMINSD - _mm_mask_min_round_sd

| VMINSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare the lower double-precision (64-bit) floating-point elements in "a" and "b", store the minimum value in
the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set),
and copy the upper element from "a" to the upper element of "dst". [sae_note]

[algorithm]

IF k[0]
    dst[63:0] := MIN(a[63:0], b[63:0])
ELSE
    dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMINSD - _mm_mask_min_sd

| VMINSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare the lower double-precision (64-bit) floating-point elements in "a" and "b", store the minimum value in
the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set),
and copy the upper element from "a" to the upper element of "dst".

[algorithm]

IF k[0]
    dst[63:0] := MIN(a[63:0], b[63:0])
ELSE
    dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMINSD - _mm_maskz_min_round_sd

| VMINSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare the lower double-precision (64-bit) floating-point elements in "a" and "b", store the minimum value in
the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy
the upper element from "a" to the upper element of "dst". [sae_note]

[algorithm]

IF k[0]
    dst[63:0] := MIN(a[63:0], b[63:0])
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMINSD - _mm_maskz_min_sd

| VMINSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare the lower double-precision (64-bit) floating-point elements in "a" and "b", store the minimum value in
the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy
the upper element from "a" to the upper element of "dst".

[algorithm]

IF k[0]
    dst[63:0] := MIN(a[63:0], b[63:0])
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMINSD - _mm_min_round_sd

| VMINSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare the lower double-precision (64-bit) floating-point elements in "a" and "b", store the minimum value in
the lower element of "dst" , and copy the upper element from "a" to the upper element of "dst". [sae_note]

[algorithm]

dst[63:0] := MIN(a[63:0], b[63:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMINSS - _mm_mask_min_round_ss

| VMINSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare the lower single-precision (32-bit) floating-point elements in "a" and "b", store the minimum value in
the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set),
and copy the upper 3 packed elements from "a" to the upper elements of "dst". [sae_note]

[algorithm]

IF k[0]
    dst[31:0] := MIN(a[31:0], b[31:0])
ELSE
    dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMINSS - _mm_mask_min_ss

| VMINSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare the lower single-precision (32-bit) floating-point elements in "a" and "b", store the minimum value in
the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set),
and copy the upper 3 packed elements from "a" to the upper elements of "dst".

[algorithm]

IF k[0]
    dst[31:0] := MIN(a[31:0], b[31:0])
ELSE
    dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMINSS - _mm_maskz_min_round_ss

| VMINSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare the lower single-precision (32-bit) floating-point elements in "a" and "b", store the minimum value in
the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy
the upper 3 packed elements from "a" to the upper elements of "dst". [sae_note]

[algorithm]

IF k[0]
    dst[31:0] := MIN(a[31:0], b[31:0])
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMINSS - _mm_maskz_min_ss

| VMINSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare the lower single-precision (32-bit) floating-point elements in "a" and "b", store the minimum value in
the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy
the upper 3 packed elements from "a" to the upper elements of "dst".

[algorithm]

IF k[0]
    dst[31:0] := MIN(a[31:0], b[31:0])
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMINSS - _mm_min_round_ss

| VMINSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare the lower single-precision (32-bit) floating-point elements in "a" and "b", store the minimum value in
the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of "dst".
[sae_note]

[algorithm]

dst[31:0] := MIN(a[31:0], b[31:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVAPD - _mm512_maskz_load_pd

| VMOVAPD_ZMMf64_MASKmskw_MEMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed double-precision (64-bit) floating-point elements from memory into "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set). "mem_addr" must be aligned on a 64-byte
boundary or a general-protection exception may be generated.

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVAPD - _mm512_maskz_mov_pd

| VMOVAPD_ZMMf64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Move packed double-precision (64-bit) floating-point elements from "a" into "dst" using zeromask "k" (elements
are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVAPS - _mm512_maskz_load_ps

| VMOVAPS_ZMMf32_MASKmskw_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed single-precision (32-bit) floating-point elements from memory into "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set). "mem_addr" must be aligned on a 64-byte
boundary or a general-protection exception may be generated.

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVAPS - _mm512_maskz_mov_ps

| VMOVAPS_ZMMf32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Move packed single-precision (32-bit) floating-point elements from "a" into "dst" using zeromask "k" (elements
are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDDUP - _mm512_mask_movedup_pd

| VMOVDDUP_ZMMf64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Duplicate even-indexed double-precision (64-bit) floating-point elements from "a", and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

tmp[63:0] := a[63:0]
tmp[127:64] := a[63:0]
tmp[191:128] := a[191:128]
tmp[255:192] := a[191:128]
tmp[319:256] := a[319:256] 
tmp[383:320] := a[319:256] 
tmp[447:384] := a[447:384]
tmp[511:448] := a[447:384]
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDDUP - _mm512_maskz_movedup_pd

| VMOVDDUP_ZMMf64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Duplicate even-indexed double-precision (64-bit) floating-point elements from "a", and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

tmp[63:0] := a[63:0]
tmp[127:64] := a[63:0]
tmp[191:128] := a[191:128]
tmp[255:192] := a[191:128]
tmp[319:256] := a[319:256] 
tmp[383:320] := a[319:256] 
tmp[447:384] := a[447:384]
tmp[511:448] := a[447:384]
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDDUP - _mm512_movedup_pd

| VMOVDDUP_ZMMf64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Duplicate even-indexed double-precision (64-bit) floating-point elements from "a", and store the results in
"dst".

[algorithm]

dst[63:0] := a[63:0]
dst[127:64] := a[63:0]
dst[191:128] := a[191:128]
dst[255:192] := a[191:128]
dst[319:256] := a[319:256]
dst[383:320] := a[319:256]
dst[447:384] := a[447:384]
dst[511:448] := a[447:384]
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQA32 - _mm512_maskz_load_epi32

| VMOVDQA32_ZMMu32_MASKmskw_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed 32-bit integers from memory into "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set). 
	"mem_addr" must be aligned on a 64-byte boundary or a general-protection
exception may be generated.

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQA32 - _mm512_maskz_mov_epi32

| VMOVDQA32_ZMMu32_MASKmskw_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Move packed 32-bit integers from "a" into "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQA64 - _mm512_maskz_load_epi64

| VMOVDQA64_ZMMu64_MASKmskw_MEMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed 64-bit integers from memory into "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set). 
	"mem_addr" must be aligned on a 64-byte boundary or a general-protection
exception may be generated.

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQA64 - _mm512_maskz_mov_epi64

| VMOVDQA64_ZMMu64_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Move packed 64-bit integers from "a" into "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU32 - _mm512_loadu_si512

| VMOVDQU32_ZMMu32_MASKmskw_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Load 512-bits of integer data from memory into "dst".
	"mem_addr" does not need to be aligned on any
particular boundary.

[algorithm]

dst[511:0] := MEM[mem_addr+511:mem_addr]
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU32 - _mm512_mask_loadu_epi32

| VMOVDQU32_ZMMu32_MASKmskw_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed 32-bit integers from memory into "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set). 
	"mem_addr" does not need to be aligned on any particular boundary.

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU32 - _mm512_mask_storeu_epi32

| VMOVDQU32_MEMu32_MASKmskw_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Store packed 32-bit integers from "a" into memory using writemask "k".
	"mem_addr" does not need to be aligned
on any particular boundary.

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VMOVDQU32 - _mm512_maskz_loadu_epi32

| VMOVDQU32_ZMMu32_MASKmskw_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed 32-bit integers from memory into "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).
	"mem_addr" does not need to be aligned on any particular boundary.

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU32 - _mm512_storeu_si512

| VMOVDQU32_MEMu32_MASKmskw_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Store 512-bits of integer data from "a" into memory.
	"mem_addr" does not need to be aligned on any particular
boundary.

[algorithm]

MEM[mem_addr+511:mem_addr] := a[511:0]

--------------------------------------------------------------------------------------------------------------

## VMOVDQU64 - _mm512_mask_loadu_epi64

| VMOVDQU64_ZMMu64_MASKmskw_MEMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed 64-bit integers from memory into "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set). 
	"mem_addr" does not need to be aligned on any particular boundary.

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVDQU64 - _mm512_mask_storeu_epi64

| VMOVDQU64_MEMu64_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Store packed 64-bit integers from "a" into memory using writemask "k".
	"mem_addr" does not need to be aligned
on any particular boundary.

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VMOVDQU64 - _mm512_maskz_loadu_epi64

| VMOVDQU64_ZMMu64_MASKmskw_MEMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed 64-bit integers from memory into "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).
	"mem_addr" does not need to be aligned on any particular boundary.

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVNTDQA - _mm512_stream_load_si512

| VMOVNTDQA_ZMMu32_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Load 512-bits of integer data from memory into "dst" using a non-temporal memory hint. 
	"mem_addr" must be
aligned on a 64-byte boundary or a general-protection exception may be generated.

[algorithm]

dst[511:0] := MEM[mem_addr+511:mem_addr]
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVNTDQ - _mm512_stream_si512

| VMOVNTDQ_MEMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Store 512-bits of integer data from "a" into memory using a non-temporal memory hint. 
	"mem_addr" must be
aligned on a 64-byte boundary or a general-protection exception may be generated.

[algorithm]

MEM[mem_addr+511:mem_addr] := a[511:0]

--------------------------------------------------------------------------------------------------------------

## VMOVNTPD - _mm512_stream_pd

| VMOVNTPD_MEMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Store 512-bits (composed of 8 packed double-precision (64-bit) floating-point elements) from "a" into memory
using a non-temporal memory hint. 
	"mem_addr" must be aligned on a 64-byte boundary or a general-protection
exception may be generated.

[algorithm]

MEM[mem_addr+511:mem_addr] := a[511:0]

--------------------------------------------------------------------------------------------------------------

## VMOVNTPS - _mm512_stream_ps

| VMOVNTPS_MEMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Store 512-bits (composed of 16 packed single-precision (32-bit) floating-point elements) from "a" into memory
using a non-temporal memory hint. 
	"mem_addr" must be aligned on a 64-byte boundary or a general-protection
exception may be generated.

[algorithm]

MEM[mem_addr+511:mem_addr] := a[511:0]

--------------------------------------------------------------------------------------------------------------

## VMOVSD - _mm_mask_load_sd

| VMOVSD_XMMf64_MASKmskw_MEMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Load a double-precision (64-bit) floating-point element from memory into the lower element of "dst" using
writemask "k" (the element is copied from "src" when mask bit 0 is not set), and set the upper element of "dst"
to zero. "mem_addr" must be aligned on a 16-byte boundary or a general-protection exception may be generated.

[algorithm]

IF k[0]
    dst[63:0] := MEM[mem_addr+63:mem_addr]
ELSE
    dst[63:0] := src[63:0]
FI
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVSD - _mm_mask_move_sd

| VMOVSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Move the lower double-precision (64-bit) floating-point element from "b" to the lower element of "dst" using
writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper element from
"a" to the upper element of "dst".

[algorithm]

IF k[0]
    dst[63:0] := b[63:0]
ELSE
    dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVSD - _mm_mask_store_sd

| VMOVSD_MEMf64_MASKmskw_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Store the lower double-precision (64-bit) floating-point element from "a" into memory using writemask
"k".
	"mem_addr" must be aligned on a 16-byte boundary or a general-protection exception may be generated.

[algorithm]

IF k[0]
    MEM[mem_addr+63:mem_addr] := a[63:0]
FI

--------------------------------------------------------------------------------------------------------------

## VMOVSD - _mm_maskz_load_sd

| VMOVSD_XMMf64_MASKmskw_MEMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Load a double-precision (64-bit) floating-point element from memory into the lower element of "dst" using
zeromask "k" (the element is zeroed out when mask bit 0 is not set), and set the upper element of "dst" to
zero. "mem_addr" must be aligned on a 16-byte boundary or a general-protection exception may be generated.

[algorithm]

IF k[0]
    dst[63:0] := MEM[mem_addr+63:mem_addr]
ELSE
    dst[63:0] := 0
FI
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVSD - _mm_maskz_move_sd

| VMOVSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Move the lower double-precision (64-bit) floating-point element from "b" to the lower element of "dst" using
zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the
upper element of "dst".

[algorithm]

IF k[0]
    dst[63:0] := b[63:0]
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVSHDUP - _mm512_mask_movehdup_ps

| VMOVSHDUP_ZMMf32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Duplicate odd-indexed single-precision (32-bit) floating-point elements from "a", and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

tmp[31:0] := a[63:32] 
tmp[63:32] := a[63:32] 
tmp[95:64] := a[127:96] 
tmp[127:96] := a[127:96]
tmp[159:128] := a[191:160] 
tmp[191:160] := a[191:160] 
tmp[223:192] := a[255:224] 
tmp[255:224] := a[255:224]
tmp[287:256] := a[319:288] 
tmp[319:288] := a[319:288] 
tmp[351:320] := a[383:352] 
tmp[383:352] := a[383:352] 
tmp[415:384] := a[447:416] 
tmp[447:416] := a[447:416] 
tmp[479:448] := a[511:480]
tmp[511:480] := a[511:480]
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVSHDUP - _mm512_maskz_movehdup_ps

| VMOVSHDUP_ZMMf32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Duplicate odd-indexed single-precision (32-bit) floating-point elements from "a", and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

tmp[31:0] := a[63:32] 
tmp[63:32] := a[63:32] 
tmp[95:64] := a[127:96] 
tmp[127:96] := a[127:96]
tmp[159:128] := a[191:160] 
tmp[191:160] := a[191:160] 
tmp[223:192] := a[255:224] 
tmp[255:224] := a[255:224]
tmp[287:256] := a[319:288] 
tmp[319:288] := a[319:288] 
tmp[351:320] := a[383:352] 
tmp[383:352] := a[383:352] 
tmp[415:384] := a[447:416] 
tmp[447:416] := a[447:416] 
tmp[479:448] := a[511:480]
tmp[511:480] := a[511:480]
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVSHDUP - _mm512_movehdup_ps

| VMOVSHDUP_ZMMf32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Duplicate odd-indexed single-precision (32-bit) floating-point elements from "a", and store the results in
"dst".

[algorithm]

dst[31:0] := a[63:32] 
dst[63:32] := a[63:32] 
dst[95:64] := a[127:96] 
dst[127:96] := a[127:96]
dst[159:128] := a[191:160] 
dst[191:160] := a[191:160] 
dst[223:192] := a[255:224] 
dst[255:224] := a[255:224]
dst[287:256] := a[319:288] 
dst[319:288] := a[319:288] 
dst[351:320] := a[383:352] 
dst[383:352] := a[383:352] 
dst[415:384] := a[447:416] 
dst[447:416] := a[447:416] 
dst[479:448] := a[511:480]
dst[511:480] := a[511:480]
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVSLDUP - _mm512_mask_moveldup_ps

| VMOVSLDUP_ZMMf32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Duplicate even-indexed single-precision (32-bit) floating-point elements from "a", and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

tmp[31:0] := a[31:0] 
tmp[63:32] := a[31:0] 
tmp[95:64] := a[95:64] 
tmp[127:96] := a[95:64]
tmp[159:128] := a[159:128] 
tmp[191:160] := a[159:128] 
tmp[223:192] := a[223:192] 
tmp[255:224] := a[223:192]
tmp[287:256] := a[287:256] 
tmp[319:288] := a[287:256] 
tmp[351:320] := a[351:320] 
tmp[383:352] := a[351:320] 
tmp[415:384] := a[415:384] 
tmp[447:416] := a[415:384] 
tmp[479:448] := a[479:448]
tmp[511:480] := a[479:448]
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR    
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVSLDUP - _mm512_maskz_moveldup_ps

| VMOVSLDUP_ZMMf32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Duplicate even-indexed single-precision (32-bit) floating-point elements from "a", and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

tmp[31:0] := a[31:0] 
tmp[63:32] := a[31:0] 
tmp[95:64] := a[95:64] 
tmp[127:96] := a[95:64]
tmp[159:128] := a[159:128] 
tmp[191:160] := a[159:128] 
tmp[223:192] := a[223:192] 
tmp[255:224] := a[223:192]
tmp[287:256] := a[287:256] 
tmp[319:288] := a[287:256] 
tmp[351:320] := a[351:320] 
tmp[383:352] := a[351:320] 
tmp[415:384] := a[415:384] 
tmp[447:416] := a[415:384] 
tmp[479:448] := a[479:448]
tmp[511:480] := a[479:448]
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVSLDUP - _mm512_moveldup_ps

| VMOVSLDUP_ZMMf32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Duplicate even-indexed single-precision (32-bit) floating-point elements from "a", and store the results in
"dst".

[algorithm]

dst[31:0] := a[31:0] 
dst[63:32] := a[31:0] 
dst[95:64] := a[95:64] 
dst[127:96] := a[95:64]
dst[159:128] := a[159:128] 
dst[191:160] := a[159:128] 
dst[223:192] := a[223:192] 
dst[255:224] := a[223:192]
dst[287:256] := a[287:256] 
dst[319:288] := a[287:256] 
dst[351:320] := a[351:320] 
dst[383:352] := a[351:320] 
dst[415:384] := a[415:384] 
dst[447:416] := a[415:384] 
dst[479:448] := a[479:448]
dst[511:480] := a[479:448]
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVSS - _mm_mask_load_ss

| VMOVSS_XMMf32_MASKmskw_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Load a single-precision (32-bit) floating-point element from memory into the lower element of "dst" using
writemask "k" (the element is copied from "src" when mask bit 0 is not set), and set the upper elements of
"dst" to zero. "mem_addr" must be aligned on a 16-byte boundary or a general-protection exception may be
generated.

[algorithm]

IF k[0]
    dst[31:0] := MEM[mem_addr+31:mem_addr]
ELSE
    dst[31:0] := src[31:0]
FI
dst[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVSS - _mm_mask_move_ss

| VMOVSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Move the lower single-precision (32-bit) floating-point element from "b" to the lower element of "dst" using
writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper 3 packed
elements from "a" to the upper elements of "dst".

[algorithm]

IF k[0]
    dst[31:0] := b[31:0]
ELSE
    dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVSS - _mm_mask_store_ss

| VMOVSS_MEMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Store the lower single-precision (32-bit) floating-point element from "a" into memory using writemask
"k".
	"mem_addr" must be aligned on a 16-byte boundary or a general-protection exception may be generated.

[algorithm]

IF k[0]
    MEM[mem_addr+31:mem_addr] := a[31:0]
FI

--------------------------------------------------------------------------------------------------------------

## VMOVSS - _mm_maskz_load_ss

| VMOVSS_XMMf32_MASKmskw_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Load a single-precision (32-bit) floating-point element from memory into the lower element of "dst" using
zeromask "k" (the element is zeroed out when mask bit 0 is not set), and set the upper elements of "dst" to
zero. "mem_addr" must be aligned on a 16-byte boundary or a general-protection exception may be generated.

[algorithm]

IF k[0]
    dst[31:0] := MEM[mem_addr+31:mem_addr]
ELSE
    dst[31:0] := 0
FI
dst[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVSS - _mm_maskz_move_ss

| VMOVSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Move the lower single-precision (32-bit) floating-point element from "b" to the lower element of "dst" using
zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from
"a" to the upper elements of "dst".

[algorithm]

IF k[0]
    dst[31:0] := b[31:0]
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVUPD - _mm512_loadu_pd

| VMOVUPD_ZMMf64_MASKmskw_MEMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Load 512-bits (composed of 8 packed double-precision (64-bit) floating-point elements) from memory into "dst".
"mem_addr" does not need to be aligned on any particular boundary.

[algorithm]

dst[511:0] := MEM[mem_addr+511:mem_addr]
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVUPD - _mm512_mask_loadu_pd

| VMOVUPD_ZMMf64_MASKmskw_MEMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed double-precision (64-bit) floating-point elements from memoy into "dst" using writemask "k"
(elements are copied from "src" when the corresponding mask bit is not set).
	"mem_addr" does not need to be
aligned on any particular boundary.

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVUPD - _mm512_mask_storeu_pd

| VMOVUPD_MEMf64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Store packed double-precision (64-bit) floating-point elements from "a" into memory using writemask
"k".
	"mem_addr" does not need to be aligned on any particular boundary.

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VMOVUPD - _mm512_maskz_loadu_pd

| VMOVUPD_ZMMf64_MASKmskw_MEMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed double-precision (64-bit) floating-point elements from memoy into "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set).
	"mem_addr" does not need to be aligned
on any particular boundary.

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVUPD - _mm512_storeu_pd

| VMOVUPD_MEMf64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Store 512-bits (composed of 8 packed double-precision (64-bit) floating-point elements) from "a" into memory.
"mem_addr" does not need to be aligned on any particular boundary.

[algorithm]

MEM[mem_addr+511:mem_addr] := a[511:0]

--------------------------------------------------------------------------------------------------------------

## VMOVUPS - _mm512_loadu_ps

| VMOVUPS_ZMMf32_MASKmskw_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Load 512-bits (composed of 16 packed single-precision (32-bit) floating-point elements) from memory into
"dst". 
	"mem_addr" does not need to be aligned on any particular boundary.

[algorithm]

dst[511:0] := MEM[mem_addr+511:mem_addr]
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVUPS - _mm512_mask_loadu_ps

| VMOVUPS_ZMMf32_MASKmskw_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed single-precision (32-bit) floating-point elements from memory into "dst" using writemask "k"
(elements are copied from "src" when the corresponding mask bit is not set).
	"mem_addr" does not need to be
aligned on any particular boundary.

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVUPS - _mm512_mask_storeu_ps

| VMOVUPS_MEMf32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Store packed single-precision (32-bit) floating-point elements from "a" into memory using writemask
"k".
	"mem_addr" does not need to be aligned on any particular boundary.

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VMOVUPS - _mm512_maskz_loadu_ps

| VMOVUPS_ZMMf32_MASKmskw_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Load packed single-precision (32-bit) floating-point elements from memory into "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set).
	"mem_addr" does not need to be aligned
on any particular boundary.

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVUPS - _mm512_storeu_ps

| VMOVUPS_MEMf32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Store 512-bits (composed of 16 packed single-precision (32-bit) floating-point elements) from "a" into memory.
"mem_addr" does not need to be aligned on any particular boundary.

[algorithm]

MEM[mem_addr+511:mem_addr] := a[511:0]

--------------------------------------------------------------------------------------------------------------

## VMULPD - _mm512_maskz_mul_pd

| VMULPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] * b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMULPD - _mm512_maskz_mul_round_pd

| VMULPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] * b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMULPS - _mm512_maskz_mul_ps

| VMULPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] * b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMULPS - _mm512_maskz_mul_round_ps

| VMULPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	[round_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] * b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMULSD - _mm_mask_mul_round_sd

| VMULSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower double-precision (64-bit) floating-point element in "a" and "b", store the result in the
lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and
copy the upper element from "a" to the upper element of "dst".
		[round_note]

[algorithm]

IF k[0]
    dst[63:0] := a[63:0] * b[63:0]
ELSE
    dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMULSD - _mm_mask_mul_sd

| VMULSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower double-precision (64-bit) floating-point element in "a" and "b", store the result in the
lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and
copy the upper element from "a" to the upper element of "dst".

[algorithm]

IF k[0]
    dst[63:0] := a[63:0] * b[63:0]
ELSE
    dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMULSD - _mm_maskz_mul_round_sd

| VMULSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower double-precision (64-bit) floating-point element in "a" and "b", store the result in the
lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the
upper element from "a" to the upper element of "dst".
		[round_note]

[algorithm]

IF k[0]
    dst[63:0] := a[63:0] * b[63:0]
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMULSD - _mm_maskz_mul_sd

| VMULSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower double-precision (64-bit) floating-point element in "a" and "b", store the result in the
lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the
upper element from "a" to the upper element of "dst".

[algorithm]

IF k[0]
    dst[63:0] := a[63:0] * b[63:0]
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMULSD - _mm_mul_round_sd

| VMULSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower double-precision (64-bit) floating-point element in "a" and "b", store the result in the
lower element of "dst", and copy the upper element from "a" to the upper element of "dst".
		[round_note]

[algorithm]

dst[63:0] := a[63:0] * b[63:0]
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMULSS - _mm_mask_mul_round_ss

| VMULSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point element in "a" and "b", store the result in the
lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and
copy the upper 3 packed elements from "a" to the upper elements of "dst".
		[round_note]

[algorithm]

IF k[0]
    dst[31:0] := a[31:0] * b[31:0]
ELSE
    dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMULSS - _mm_mask_mul_ss

| VMULSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point element in "a" and "b", store the result in the
lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and
copy the upper 3 packed elements from "a" to the upper elements of "dst".

[algorithm]

IF k[0]
    dst[31:0] := a[31:0] * b[31:0]
ELSE
    dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMULSS - _mm_maskz_mul_round_ss

| VMULSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point element in "a" and "b", store the result in the
lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the
upper 3 packed elements from "a" to the upper elements of "dst".
		[round_note]

[algorithm]

IF k[0]
    dst[31:0] := a[31:0] * b[31:0]
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMULSS - _mm_maskz_mul_ss

| VMULSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point element in "a" and "b", store the result in the
lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the
upper 3 packed elements from "a" to the upper elements of "dst".

[algorithm]

IF k[0]
    dst[31:0] := a[31:0] * b[31:0]
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VMULSS - _mm_mul_round_ss

| VMULSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point element in "a" and "b", store the result in the
lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of
"dst".
		[round_note]

[algorithm]

dst[31:0] := a[31:0] * b[31:0]
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPABSD - _mm512_abs_epi32

| VPABSD_ZMMi32_MASKmskw_ZMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the absolute value of packed signed 32-bit integers in "a", and store the unsigned results in "dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := ABS(a[i+31:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPABSD - _mm512_mask_abs_epi32

| VPABSD_ZMMi32_MASKmskw_ZMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the absolute value of packed signed 32-bit integers in "a", and store the unsigned results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := ABS(a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPABSD - _mm512_maskz_abs_epi32

| VPABSD_ZMMi32_MASKmskw_ZMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the absolute value of packed signed 32-bit integers in "a", and store the unsigned results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := ABS(a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPABSQ - _mm512_abs_epi64

| VPABSQ_ZMMi64_MASKmskw_ZMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the absolute value of packed signed 64-bit integers in "a", and store the unsigned results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := ABS(a[i+63:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPABSQ - _mm512_mask_abs_epi64

| VPABSQ_ZMMi64_MASKmskw_ZMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the absolute value of packed signed 64-bit integers in "a", and store the unsigned results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := ABS(a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPABSQ - _mm512_maskz_abs_epi64

| VPABSQ_ZMMi64_MASKmskw_ZMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the absolute value of packed signed 64-bit integers in "a", and store the unsigned results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := ABS(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDD - _mm512_maskz_add_epi32

| VPADDD_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed 32-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] + b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDQ - _mm512_add_epi64

| VPADDQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed 64-bit integers in "a" and "b", and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := a[i+63:i] + b[i+63:i]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDQ - _mm512_mask_add_epi64

| VPADDQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed 64-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] + b[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPADDQ - _mm512_maskz_add_epi64

| VPADDQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Add packed 64-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] + b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPANDD - _mm512_maskz_and_epi32

| VPANDD_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed 32-bit integers in "a" and "b", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] AND b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPANDND - _mm512_maskz_andnot_epi32

| VPANDND_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of packed 32-bit integers in "a" and then AND with "b", and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := (NOT a[i+31:i]) AND b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPANDNQ - _mm512_maskz_andnot_epi64

| VPANDNQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NOT of packed 64-bit integers in "a" and then AND with "b", and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := (NOT a[i+63:i]) AND b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPANDQ - _mm512_maskz_and_epi64

| VPANDQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed 64-bit integers in "a" and "b", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] AND b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTB - _mm512_set1_epi8

| VPBROADCASTB_ZMMu8_MASKmskw_GPR32u8_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast 8-bit integer "a" to all elements of "dst".

[algorithm]

FOR j := 0 to 63
    i := j*8
    dst[i+7:i] := a[7:0]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTD - _mm512_broadcastd_epi32

| VPBROADCASTD_ZMMu32_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low packed 32-bit integer from "a" to all elements of "dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := a[31:0]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTD - _mm512_mask_broadcastd_epi32

| VPBROADCASTD_ZMMu32_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low packed 32-bit integer from "a" to all elements of "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := a[31:0]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTD - _mm512_mask_set1_epi32

| VPBROADCASTD_ZMMu32_MASKmskw_GPR32u32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast 32-bit integer "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := a[31:0]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTD - _mm512_maskz_broadcastd_epi32

| VPBROADCASTD_ZMMu32_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low packed 32-bit integer from "a" to all elements of "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := a[31:0]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTD - _mm512_maskz_set1_epi32

| VPBROADCASTD_ZMMu32_MASKmskw_GPR32u32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast 32-bit integer "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := a[31:0]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTD - _mm512_set1_epi32

| VPBROADCASTD_ZMMu32_MASKmskw_GPR32u32_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast 32-bit integer "a" to all elements of "dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := a[31:0]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTQ - _mm512_broadcastq_epi64

| VPBROADCASTQ_ZMMu64_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low packed 64-bit integer from "a" to all elements of "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := a[63:0]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTQ - _mm512_mask_broadcastq_epi64

| VPBROADCASTQ_ZMMu64_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low packed 64-bit integer from "a" to all elements of "dst" using writemask "k" (elements are
copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := a[63:0]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTQ - _mm512_mask_set1_epi64

| VPBROADCASTQ_ZMMu64_MASKmskw_GPR64u64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast 64-bit integer "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := a[63:0]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTQ - _mm512_maskz_broadcastq_epi64

| VPBROADCASTQ_ZMMu64_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low packed 64-bit integer from "a" to all elements of "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := a[63:0]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTQ - _mm512_maskz_set1_epi64

| VPBROADCASTQ_ZMMu64_MASKmskw_GPR64u64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast 64-bit integer "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := a[63:0]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTQ - _mm512_set1_epi64

| VPBROADCASTQ_ZMMu64_MASKmskw_GPR64u64_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast 64-bit integer "a" to all elements of "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := a[63:0]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPBROADCASTW - _mm512_set1_epi16

| VPBROADCASTW_ZMMu16_MASKmskw_GPR32u16_AVX512

--------------------------------------------------------------------------------------------------------------
Broadcast the low packed 16-bit integer from "a" to all all elements of "dst".

[algorithm]

FOR j := 0 to 31
    i := j*16
    dst[i+15:i] := a[15:0]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPD - _mm512_cmplt_epi32_mask

| VPCMPD_MASKmskw_MASKmskw_ZMMi32_ZMMi32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 15
    i := j*32
    k[j] := ( a[i+31:i] &lt; b[i+31:i] ) ? 1 : 0
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPD - _mm512_mask_cmplt_epi32_mask

| VPCMPD_MASKmskw_MASKmskw_ZMMi32_ZMMi32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k1[j]
        k[j] := ( a[i+31:i] &lt; b[i+31:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm512_cmp_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_ZMMi64_ZMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" based on the comparison operand specified by "imm8", and
store the results in mask vector "k".

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 7
    i := j*64
    k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPEQQ - _mm512_cmpeq_epi64_mask

| VPCMPEQQ_MASKmskw_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed 64-bit integers in "a" and "b" for equality, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 7
    i := j*64
    k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm512_cmpge_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_ZMMi64_ZMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask
vector "k".

[algorithm]

FOR j := 0 to 7
    i := j*64
    k[j] := ( a[i+63:i] &gt;= b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPGTQ - _mm512_cmpgt_epi64_mask

| VPCMPGTQ_MASKmskw_MASKmskw_ZMMi64_ZMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 7
    i := j*64
    k[j] := ( a[i+63:i] &gt; b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm512_cmple_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_ZMMi64_ZMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k".

[algorithm]

FOR j := 0 to 7
    i := j*64
    k[j] := ( a[i+63:i] &lt;= b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm512_cmplt_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_ZMMi64_ZMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 7
    i := j*64
    k[j] := ( a[i+63:i] &lt; b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm512_cmpneq_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_ZMMi64_ZMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 7
    i := j*64
    k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm512_mask_cmp_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_ZMMi64_ZMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" based on the comparison operand specified by "imm8", and
store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask
bit is not set).

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 7
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPEQQ - _mm512_mask_cmpeq_epi64_mask

| VPCMPEQQ_MASKmskw_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed 64-bit integers in "a" and "b" for equality, and store the results in mask vector "k" using
zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm512_mask_cmpge_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_ZMMi64_ZMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] &gt;= b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPGTQ - _mm512_mask_cmpgt_epi64_mask

| VPCMPGTQ_MASKmskw_MASKmskw_ZMMi64_ZMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] &gt; b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm512_mask_cmple_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_ZMMi64_ZMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] &lt;= b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm512_mask_cmplt_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_ZMMi64_ZMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" for less-than, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] &lt; b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPQ - _mm512_mask_cmpneq_epi64_mask

| VPCMPQ_MASKmskw_MASKmskw_ZMMi64_ZMMi64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm512_cmp_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_ZMMu64_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" based on the comparison operand specified by "imm8",
and store the results in mask vector "k".

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 7
    i := j*64
    k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm512_cmpeq_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_ZMMu64_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" for equality, and store the results in mask vector "k".

[algorithm]

FOR j := 0 to 7
    i := j*64
    k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm512_cmpge_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_ZMMu64_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" for greater-than-or-equal, and store the results in
mask vector "k".

[algorithm]

FOR j := 0 to 7
    i := j*64
    k[j] := ( a[i+63:i] &gt;= b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm512_cmpgt_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_ZMMu64_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 7
    i := j*64
    k[j] := ( a[i+63:i] &gt; b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm512_cmple_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_ZMMu64_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k".

[algorithm]

FOR j := 0 to 7
    i := j*64
    k[j] := ( a[i+63:i] &lt;= b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm512_cmplt_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_ZMMu64_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" for less-than, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 7
    i := j*64
    k[j] := ( a[i+63:i] &lt; b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm512_cmpneq_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_ZMMu64_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" for not-equal, and store the results in mask vector
"k".

[algorithm]

FOR j := 0 to 7
    i := j*64
    k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm512_mask_cmp_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_ZMMu64_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" based on the comparison operand specified by "imm8",
and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding
mask bit is not set).

[algorithm]

CASE (imm8[2:0]) OF
0: OP := _MM_CMPINT_EQ
1: OP := _MM_CMPINT_LT
2: OP := _MM_CMPINT_LE
3: OP := _MM_CMPINT_FALSE
4: OP := _MM_CMPINT_NE
5: OP := _MM_CMPINT_NLT
6: OP := _MM_CMPINT_NLE
7: OP := _MM_CMPINT_TRUE
ESAC
FOR j := 0 to 7
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm512_mask_cmpeq_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_ZMMu64_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" for equality, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm512_mask_cmpge_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_ZMMu64_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" for greater-than-or-equal, and store the results in
mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] &gt;= b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm512_mask_cmpgt_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_ZMMu64_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" for greater-than, and store the results in mask vector
"k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] &gt; b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm512_mask_cmple_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_ZMMu64_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask
vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] &lt;= b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm512_mask_cmplt_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_ZMMu64_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" for less-than, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] &lt; b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCMPUQ - _mm512_mask_cmpneq_epu64_mask

| VPCMPUQ_MASKmskw_MASKmskw_ZMMu64_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k"
using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k1[j]
        k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
    ELSE 
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPCOMPRESSD - _mm512_mask_compress_epi32

| VPCOMPRESSD_ZMMu32_MASKmskw_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active 32-bit integers in "a" (those with their respective bit set in writemask "k") to
"dst", and pass through the remaining elements from "src".

[algorithm]

size := 32
m := 0
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[m+size-1:m] := a[i+31:i]
        m := m + size
    FI
ENDFOR
dst[511:m] := src[511:m]
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPCOMPRESSD - _mm512_mask_compressstoreu_epi32

| VPCOMPRESSD_MEMu32_MASKmskw_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active 32-bit integers in "a" (those with their respective bit set in writemask "k") to
unaligned memory at "base_addr".

[algorithm]

size := 32
m := base_addr
FOR j := 0 to 15
    i := j*32
    IF k[j]
        MEM[m+size-1:m] := a[i+31:i]
        m := m + size
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPCOMPRESSD - _mm512_maskz_compress_epi32

| VPCOMPRESSD_ZMMu32_MASKmskw_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active 32-bit integers in "a" (those with their respective bit set in zeromask "k") to
"dst", and set the remaining elements to zero.

[algorithm]

size := 32
m := 0
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[m+size-1:m] := a[i+31:i]
        m := m + size
    FI
ENDFOR
dst[511:m] := 0
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPCOMPRESSQ - _mm512_mask_compress_epi64

| VPCOMPRESSQ_ZMMu64_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active 64-bit integers in "a" (those with their respective bit set in writemask "k") to
"dst", and pass through the remaining elements from "src".

[algorithm]

size := 64
m := 0
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[m+size-1:m] := a[i+63:i]
        m := m + size
    FI
ENDFOR
dst[511:m] := src[511:m]
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPCOMPRESSQ - _mm512_mask_compressstoreu_epi64

| VPCOMPRESSQ_MEMu64_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active 64-bit integers in "a" (those with their respective bit set in writemask "k") to
unaligned memory at "base_addr".

[algorithm]

size := 64
m := base_addr
FOR j := 0 to 7
    i := j*64
    IF k[j]
        MEM[m+size-1:m] := a[i+63:i]
        m := m + size
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPCOMPRESSQ - _mm512_maskz_compress_epi64

| VPCOMPRESSQ_ZMMu64_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active 64-bit integers in "a" (those with their respective bit set in zeromask "k") to
"dst", and set the remaining elements to zero.

[algorithm]

size := 64
m := 0
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[m+size-1:m] := a[i+63:i]
        m := m + size
    FI
ENDFOR
dst[511:m] := 0
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMD - _mm512_mask_permutexvar_epi32

| VPERMD_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 32-bit integers in "a" across lanes using the corresponding index in "idx", and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    id := idx[i+3:i]*32
    IF k[j]
        dst[i+31:i] := a[id+31:id]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMD - _mm512_maskz_permutexvar_epi32

| VPERMD_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 32-bit integers in "a" across lanes using the corresponding index in "idx", and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    id := idx[i+3:i]*32
    IF k[j]
        dst[i+31:i] := a[id+31:id]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMD - _mm512_permutexvar_epi32

| VPERMD_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 32-bit integers in "a" across lanes using the corresponding index in "idx", and store the results in
"dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    id := idx[i+3:i]*32
    dst[i+31:i] := a[id+31:id]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2D - _mm512_mask2_permutex2var_epi32

| VPERMI2D_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 32-bit integers in "a" and "b" across lanes using the corresponding selector and index in "idx", and
store the results in "dst" using writemask "k" (elements are copied from "idx" when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    off := idx[i+3:i]*32
    IF k[j]
        dst[i+31:i] := idx[i+4] ? b[off+31:off] : a[off+31:off]
    ELSE
        dst[i+31:i] := idx[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMT2D - _mm512_mask_permutex2var_epi32

| VPERMT2D_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 32-bit integers in "a" and "b" across lanes using the corresponding selector and index in "idx", and
store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    off := idx[i+3:i]*32
    IF k[j]
        dst[i+31:i] := idx[i+4] ? b[off+31:off] : a[off+31:off]
    ELSE
        dst[i+31:i] := a[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2D - _mm512_maskz_permutex2var_epi32

| VPERMI2D_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512 | VPERMT2D_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 32-bit integers in "a" and "b" across lanes using the corresponding selector and index in "idx", and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    off := idx[i+3:i]*32
    IF k[j]
        dst[i+31:i] := (idx[i+4]) ? b[off+31:off] : a[off+31:off]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2D - _mm512_permutex2var_epi32

| VPERMI2D_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512 | VPERMT2D_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 32-bit integers in "a" and "b" across lanes using the corresponding selector and index in "idx", and
store the results in "dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    off := idx[i+3:i]*32
    dst[i+31:i] := idx[i+4] ? b[off+31:off] : a[off+31:off]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2PD - _mm512_mask2_permutex2var_pd

| VPERMI2PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" and "b" across lanes using the corresponding
selector and index in "idx", and store the results in "dst" using writemask "k" (elements are copied from "idx"
when the corresponding mask bit is not set)

[algorithm]

FOR j := 0 to 7
    i := j*64
    off := idx[i+2:i]*64
    IF k[j]
        dst[i+63:i] := idx[i+3] ? b[off+63:off] : a[off+63:off]
    ELSE
        dst[i+63:i] := idx[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMT2PD - _mm512_mask_permutex2var_pd

| VPERMT2PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" and "b" across lanes using the corresponding
selector and index in "idx", and store the results in "dst" using writemask "k" (elements are copied from "a"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    off := idx[i+2:i]*64
    IF k[j]
        dst[i+63:i] := idx[i+3] ? b[off+63:off] : a[off+63:off]
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2PD - _mm512_maskz_permutex2var_pd

| VPERMI2PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VPERMT2PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" and "b" across lanes using the corresponding
selector and index in "idx", and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    off := idx[i+2:i]*64
    IF k[j]
        dst[i+63:i] := (idx[i+3]) ? b[off+63:off] : a[off+63:off]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2PD - _mm512_permutex2var_pd

| VPERMI2PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512 | VPERMT2PD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" and "b" across lanes using the corresponding
selector and index in "idx", and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    off := idx[i+2:i]*64
    dst[i+63:i] := idx[i+3] ? b[off+63:off] : a[off+63:off]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2PS - _mm512_mask2_permutex2var_ps

| VPERMI2PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" and "b" across lanes using the corresponding
selector and index in "idx", and store the results in "dst" using writemask "k" (elements are copied from "idx"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    off := idx[i+3:i]*32
    IF k[j]
        dst[i+31:i] := idx[i+4] ? b[off+31:off] : a[off+31:off]
    ELSE
        dst[i+31:i] := idx[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMT2PS - _mm512_mask_permutex2var_ps

| VPERMT2PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" and "b" across lanes using the corresponding
selector and index in "idx", and store the results in "dst" using writemask "k" (elements are copied from "a"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    off := idx[i+3:i]*32
    IF k[j]
        dst[i+31:i] := idx[i+4] ? b[off+31:off] : a[off+31:off]
    ELSE
        dst[i+31:i] := a[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2PS - _mm512_maskz_permutex2var_ps

| VPERMI2PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VPERMT2PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" and "b" across lanes using the corresponding
selector and index in "idx", and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    off := idx[i+3:i]*32
    IF k[j]
        dst[i+31:i] := (idx[i+4]) ? b[off+31:off] : a[off+31:off]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2PS - _mm512_permutex2var_ps

| VPERMI2PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512 | VPERMT2PS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" and "b" across lanes using the corresponding
selector and index in "idx", and store the results in "dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    off := idx[i+3:i]*32
    dst[i+31:i] := idx[i+4] ? b[off+31:off] : a[off+31:off]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2Q - _mm512_mask2_permutex2var_epi64

| VPERMI2Q_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 64-bit integers in "a" and "b" across lanes using the corresponding selector and index in "idx", and
store the results in "dst" using writemask "k" (elements are copied from "idx" when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    off := idx[i+2:i]*64
    IF k[j]
        dst[i+63:i] := idx[i+3] ? b[off+63:off] : a[off+63:off]
    ELSE
        dst[i+63:i] := idx[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMT2Q - _mm512_mask_permutex2var_epi64

| VPERMT2Q_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 64-bit integers in "a" and "b" across lanes using the corresponding selector and index in "idx", and
store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    off := idx[i+2:i]*64
    IF k[j]
        dst[i+63:i] := idx[i+3] ? b[off+63:off] : a[off+63:off]
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2Q - _mm512_maskz_permutex2var_epi64

| VPERMI2Q_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512 | VPERMT2Q_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 64-bit integers in "a" and "b" across lanes using the corresponding selector and index in "idx", and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    off := idx[i+2:i]*64
    IF k[j]
        dst[i+63:i] := (idx[i+3]) ? b[off+63:off] : a[off+63:off]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2Q - _mm512_permutex2var_epi64

| VPERMI2Q_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512 | VPERMT2Q_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 64-bit integers in "a" and "b" across lanes using the corresponding selector and index in "idx", and
store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    off := idx[i+2:i]*64
    dst[i+63:i] := idx[i+3] ? b[off+63:off] : a[off+63:off]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMILPD - _mm512_mask_permute_pd

| VPERMILPD_ZMMf64_MASKmskw_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" within 128-bit lanes using the control in
"imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

IF (imm8[0] == 0) tmp_dst[63:0] := a[63:0]; FI
IF (imm8[0] == 1) tmp_dst[63:0] := a[127:64]; FI
IF (imm8[1] == 0) tmp_dst[127:64] := a[63:0]; FI
IF (imm8[1] == 1) tmp_dst[127:64] := a[127:64]; FI
IF (imm8[2] == 0) tmp_dst[191:128] := a[191:128]; FI
IF (imm8[2] == 1) tmp_dst[191:128] := a[255:192]; FI
IF (imm8[3] == 0) tmp_dst[255:192] := a[191:128]; FI
IF (imm8[3] == 1) tmp_dst[255:192] := a[255:192]; FI
IF (imm8[4] == 0) tmp_dst[319:256] := a[319:256]; FI
IF (imm8[4] == 1) tmp_dst[319:256] := a[383:320]; FI
IF (imm8[5] == 0) tmp_dst[383:320] := a[319:256]; FI
IF (imm8[5] == 1) tmp_dst[383:320] := a[383:320]; FI
IF (imm8[6] == 0) tmp_dst[447:384] := a[447:384]; FI
IF (imm8[6] == 1) tmp_dst[447:384] := a[511:448]; FI
IF (imm8[7] == 0) tmp_dst[511:448] := a[447:384]; FI
IF (imm8[7] == 1) tmp_dst[511:448] := a[511:448]; FI
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMILPD - _mm512_mask_permutevar_pd

| VPERMILPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" within 128-bit lanes using the control in
"b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding
mask bit is not set).

[algorithm]

IF (b[1] == 0) tmp_dst[63:0] := a[63:0]; FI
IF (b[1] == 1) tmp_dst[63:0] := a[127:64]; FI
IF (b[65] == 0) tmp_dst[127:64] := a[63:0]; FI
IF (b[65] == 1) tmp_dst[127:64] := a[127:64]; FI
IF (b[129] == 0) tmp_dst[191:128] := a[191:128]; FI
IF (b[129] == 1) tmp_dst[191:128] := a[255:192]; FI
IF (b[193] == 0) tmp_dst[255:192] := a[191:128]; FI
IF (b[193] == 1) tmp_dst[255:192] := a[255:192]; FI
IF (b[257] == 0) tmp_dst[319:256] := a[319:256]; FI
IF (b[257] == 1) tmp_dst[319:256] := a[383:320]; FI
IF (b[321] == 0) tmp_dst[383:320] := a[319:256]; FI
IF (b[321] == 1) tmp_dst[383:320] := a[383:320]; FI
IF (b[385] == 0) tmp_dst[447:384] := a[447:384]; FI
IF (b[385] == 1) tmp_dst[447:384] := a[511:448]; FI
IF (b[449] == 0) tmp_dst[511:448] := a[447:384]; FI
IF (b[449] == 1) tmp_dst[511:448] := a[511:448]; FI
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMILPD - _mm512_maskz_permute_pd

| VPERMILPD_ZMMf64_MASKmskw_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" within 128-bit lanes using the control in
"imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask
bit is not set).

[algorithm]

IF (imm8[0] == 0) tmp_dst[63:0] := a[63:0]; FI
IF (imm8[0] == 1) tmp_dst[63:0] := a[127:64]; FI
IF (imm8[1] == 0) tmp_dst[127:64] := a[63:0]; FI
IF (imm8[1] == 1) tmp_dst[127:64] := a[127:64]; FI
IF (imm8[2] == 0) tmp_dst[191:128] := a[191:128]; FI
IF (imm8[2] == 1) tmp_dst[191:128] := a[255:192]; FI
IF (imm8[3] == 0) tmp_dst[255:192] := a[191:128]; FI
IF (imm8[3] == 1) tmp_dst[255:192] := a[255:192]; FI
IF (imm8[4] == 0) tmp_dst[319:256] := a[319:256]; FI
IF (imm8[4] == 1) tmp_dst[319:256] := a[383:320]; FI
IF (imm8[5] == 0) tmp_dst[383:320] := a[319:256]; FI
IF (imm8[5] == 1) tmp_dst[383:320] := a[383:320]; FI
IF (imm8[6] == 0) tmp_dst[447:384] := a[447:384]; FI
IF (imm8[6] == 1) tmp_dst[447:384] := a[511:448]; FI
IF (imm8[7] == 0) tmp_dst[511:448] := a[447:384]; FI
IF (imm8[7] == 1) tmp_dst[511:448] := a[511:448]; FI
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMILPD - _mm512_maskz_permutevar_pd

| VPERMILPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" within 128-bit lanes using the control in
"b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit
is not set).

[algorithm]

IF (b[1] == 0) tmp_dst[63:0] := a[63:0]; FI
IF (b[1] == 1) tmp_dst[63:0] := a[127:64]; FI
IF (b[65] == 0) tmp_dst[127:64] := a[63:0]; FI
IF (b[65] == 1) tmp_dst[127:64] := a[127:64]; FI
IF (b[129] == 0) tmp_dst[191:128] := a[191:128]; FI
IF (b[129] == 1) tmp_dst[191:128] := a[255:192]; FI
IF (b[193] == 0) tmp_dst[255:192] := a[191:128]; FI
IF (b[193] == 1) tmp_dst[255:192] := a[255:192]; FI
IF (b[257] == 0) tmp_dst[319:256] := a[319:256]; FI
IF (b[257] == 1) tmp_dst[319:256] := a[383:320]; FI
IF (b[321] == 0) tmp_dst[383:320] := a[319:256]; FI
IF (b[321] == 1) tmp_dst[383:320] := a[383:320]; FI
IF (b[385] == 0) tmp_dst[447:384] := a[447:384]; FI
IF (b[385] == 1) tmp_dst[447:384] := a[511:448]; FI
IF (b[449] == 0) tmp_dst[511:448] := a[447:384]; FI
IF (b[449] == 1) tmp_dst[511:448] := a[511:448]; FI
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMILPD - _mm512_permute_pd

| VPERMILPD_ZMMf64_MASKmskw_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" within 128-bit lanes using the control in
"imm8", and store the results in "dst".

[algorithm]

IF (imm8[0] == 0) dst[63:0] := a[63:0]; FI
IF (imm8[0] == 1) dst[63:0] := a[127:64]; FI
IF (imm8[1] == 0) dst[127:64] := a[63:0]; FI
IF (imm8[1] == 1) dst[127:64] := a[127:64]; FI
IF (imm8[2] == 0) dst[191:128] := a[191:128]; FI
IF (imm8[2] == 1) dst[191:128] := a[255:192]; FI
IF (imm8[3] == 0) dst[255:192] := a[191:128]; FI
IF (imm8[3] == 1) dst[255:192] := a[255:192]; FI
IF (imm8[4] == 0) dst[319:256] := a[319:256]; FI
IF (imm8[4] == 1) dst[319:256] := a[383:320]; FI
IF (imm8[5] == 0) dst[383:320] := a[319:256]; FI
IF (imm8[5] == 1) dst[383:320] := a[383:320]; FI
IF (imm8[6] == 0) dst[447:384] := a[447:384]; FI
IF (imm8[6] == 1) dst[447:384] := a[511:448]; FI
IF (imm8[7] == 0) dst[511:448] := a[447:384]; FI
IF (imm8[7] == 1) dst[511:448] := a[511:448]; FI
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMILPD - _mm512_permutevar_pd

| VPERMILPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" within 128-bit lanes using the control in
"b", and store the results in "dst".

[algorithm]

IF (b[1] == 0) dst[63:0] := a[63:0]; FI
IF (b[1] == 1) dst[63:0] := a[127:64]; FI
IF (b[65] == 0) dst[127:64] := a[63:0]; FI
IF (b[65] == 1) dst[127:64] := a[127:64]; FI
IF (b[129] == 0) dst[191:128] := a[191:128]; FI
IF (b[129] == 1) dst[191:128] := a[255:192]; FI
IF (b[193] == 0) dst[255:192] := a[191:128]; FI
IF (b[193] == 1) dst[255:192] := a[255:192]; FI
IF (b[257] == 0) dst[319:256] := a[319:256]; FI
IF (b[257] == 1) dst[319:256] := a[383:320]; FI
IF (b[321] == 0) dst[383:320] := a[319:256]; FI
IF (b[321] == 1) dst[383:320] := a[383:320]; FI
IF (b[385] == 0) dst[447:384] := a[447:384]; FI
IF (b[385] == 1) dst[447:384] := a[511:448]; FI
IF (b[449] == 0) dst[511:448] := a[447:384]; FI
IF (b[449] == 1) dst[511:448] := a[511:448]; FI
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMILPS - _mm512_mask_permute_ps

| VPERMILPS_ZMMf32_MASKmskw_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" within 128-bit lanes using the control in
"imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[31:0] := src[31:0]
    1:    tmp[31:0] := src[63:32]
    2:    tmp[31:0] := src[95:64]
    3:    tmp[31:0] := src[127:96]
    ESAC
    RETURN tmp[31:0]
}
tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
tmp_dst[287:256] := SELECT4(a[383:256], imm8[1:0])
tmp_dst[319:288] := SELECT4(a[383:256], imm8[3:2])
tmp_dst[351:320] := SELECT4(a[383:256], imm8[5:4])
tmp_dst[383:352] := SELECT4(a[383:256], imm8[7:6])
tmp_dst[415:384] := SELECT4(a[511:384], imm8[1:0])
tmp_dst[447:416] := SELECT4(a[511:384], imm8[3:2])
tmp_dst[479:448] := SELECT4(a[511:384], imm8[5:4])
tmp_dst[511:480] := SELECT4(a[511:384], imm8[7:6])
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMILPS - _mm512_mask_permutevar_ps

| VPERMILPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" within 128-bit lanes using the control in
"b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding
mask bit is not set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[31:0] := src[31:0]
    1:    tmp[31:0] := src[63:32]
    2:    tmp[31:0] := src[95:64]
    3:    tmp[31:0] := src[127:96]
    ESAC
    RETURN tmp[31:0]
}
tmp_dst[31:0] := SELECT4(a[127:0], b[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], b[33:32])
tmp_dst[95:64] := SELECT4(a[127:0], b[65:64])
tmp_dst[127:96] := SELECT4(a[127:0], b[97:96])
tmp_dst[159:128] := SELECT4(a[255:128], b[129:128])
tmp_dst[191:160] := SELECT4(a[255:128], b[161:160])
tmp_dst[223:192] := SELECT4(a[255:128], b[193:192])
tmp_dst[255:224] := SELECT4(a[255:128], b[225:224])
tmp_dst[287:256] := SELECT4(a[383:256], b[257:256])
tmp_dst[319:288] := SELECT4(a[383:256], b[289:288])
tmp_dst[351:320] := SELECT4(a[383:256], b[321:320])
tmp_dst[383:352] := SELECT4(a[383:256], b[353:352])
tmp_dst[415:384] := SELECT4(a[511:384], b[385:384])
tmp_dst[447:416] := SELECT4(a[511:384], b[417:416])
tmp_dst[479:448] := SELECT4(a[511:384], b[449:448])
tmp_dst[511:480] := SELECT4(a[511:384], b[481:480])
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMILPS - _mm512_maskz_permute_ps

| VPERMILPS_ZMMf32_MASKmskw_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" within 128-bit lanes using the control in
"imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask
bit is not set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[31:0] := src[31:0]
    1:    tmp[31:0] := src[63:32]
    2:    tmp[31:0] := src[95:64]
    3:    tmp[31:0] := src[127:96]
    ESAC
    RETURN tmp[31:0]
}
tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
tmp_dst[287:256] := SELECT4(a[383:256], imm8[1:0])
tmp_dst[319:288] := SELECT4(a[383:256], imm8[3:2])
tmp_dst[351:320] := SELECT4(a[383:256], imm8[5:4])
tmp_dst[383:352] := SELECT4(a[383:256], imm8[7:6])
tmp_dst[415:384] := SELECT4(a[511:384], imm8[1:0])
tmp_dst[447:416] := SELECT4(a[511:384], imm8[3:2])
tmp_dst[479:448] := SELECT4(a[511:384], imm8[5:4])
tmp_dst[511:480] := SELECT4(a[511:384], imm8[7:6])
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMILPS - _mm512_maskz_permutevar_ps

| VPERMILPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" within 128-bit lanes using the control in
"b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit
is not set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[31:0] := src[31:0]
    1:    tmp[31:0] := src[63:32]
    2:    tmp[31:0] := src[95:64]
    3:    tmp[31:0] := src[127:96]
    ESAC
    RETURN tmp[31:0]
}
tmp_dst[31:0] := SELECT4(a[127:0], b[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], b[33:32])
tmp_dst[95:64] := SELECT4(a[127:0], b[65:64])
tmp_dst[127:96] := SELECT4(a[127:0], b[97:96])
tmp_dst[159:128] := SELECT4(a[255:128], b[129:128])
tmp_dst[191:160] := SELECT4(a[255:128], b[161:160])
tmp_dst[223:192] := SELECT4(a[255:128], b[193:192])
tmp_dst[255:224] := SELECT4(a[255:128], b[225:224])
tmp_dst[287:256] := SELECT4(a[383:256], b[257:256])
tmp_dst[319:288] := SELECT4(a[383:256], b[289:288])
tmp_dst[351:320] := SELECT4(a[383:256], b[321:320])
tmp_dst[383:352] := SELECT4(a[383:256], b[353:352])
tmp_dst[415:384] := SELECT4(a[511:384], b[385:384])
tmp_dst[447:416] := SELECT4(a[511:384], b[417:416])
tmp_dst[479:448] := SELECT4(a[511:384], b[449:448])
tmp_dst[511:480] := SELECT4(a[511:384], b[481:480])
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMILPS - _mm512_permute_ps

| VPERMILPS_ZMMf32_MASKmskw_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" within 128-bit lanes using the control in
"imm8", and store the results in "dst".

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[31:0] := src[31:0]
    1:    tmp[31:0] := src[63:32]
    2:    tmp[31:0] := src[95:64]
    3:    tmp[31:0] := src[127:96]
    ESAC
    RETURN tmp[31:0]
}
dst[31:0] := SELECT4(a[127:0], imm8[1:0])
dst[63:32] := SELECT4(a[127:0], imm8[3:2])
dst[95:64] := SELECT4(a[127:0], imm8[5:4])
dst[127:96] := SELECT4(a[127:0], imm8[7:6])
dst[159:128] := SELECT4(a[255:128], imm8[1:0])
dst[191:160] := SELECT4(a[255:128], imm8[3:2])
dst[223:192] := SELECT4(a[255:128], imm8[5:4])
dst[255:224] := SELECT4(a[255:128], imm8[7:6])
dst[287:256] := SELECT4(a[383:256], imm8[1:0])
dst[319:288] := SELECT4(a[383:256], imm8[3:2])
dst[351:320] := SELECT4(a[383:256], imm8[5:4])
dst[383:352] := SELECT4(a[383:256], imm8[7:6])
dst[415:384] := SELECT4(a[511:384], imm8[1:0])
dst[447:416] := SELECT4(a[511:384], imm8[3:2])
dst[479:448] := SELECT4(a[511:384], imm8[5:4])
dst[511:480] := SELECT4(a[511:384], imm8[7:6])
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMILPS - _mm512_permutevar_ps

| VPERMILPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" within 128-bit lanes using the control in
"b", and store the results in "dst".

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[31:0] := src[31:0]
    1:    tmp[31:0] := src[63:32]
    2:    tmp[31:0] := src[95:64]
    3:    tmp[31:0] := src[127:96]
    ESAC
    RETURN tmp[31:0]
}
dst[31:0] := SELECT4(a[127:0], b[1:0])
dst[63:32] := SELECT4(a[127:0], b[33:32])
dst[95:64] := SELECT4(a[127:0], b[65:64])
dst[127:96] := SELECT4(a[127:0], b[97:96])
dst[159:128] := SELECT4(a[255:128], b[129:128])
dst[191:160] := SELECT4(a[255:128], b[161:160])
dst[223:192] := SELECT4(a[255:128], b[193:192])
dst[255:224] := SELECT4(a[255:128], b[225:224])
dst[287:256] := SELECT4(a[383:256], b[257:256])
dst[319:288] := SELECT4(a[383:256], b[289:288])
dst[351:320] := SELECT4(a[383:256], b[321:320])
dst[383:352] := SELECT4(a[383:256], b[353:352])
dst[415:384] := SELECT4(a[511:384], b[385:384])
dst[447:416] := SELECT4(a[511:384], b[417:416])
dst[479:448] := SELECT4(a[511:384], b[449:448])
dst[511:480] := SELECT4(a[511:384], b[481:480])
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMPD - _mm512_mask_permutex_pd

| VPERMPD_ZMMf64_MASKmskw_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" within 256-bit lanes using the control in
"imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[63:0] := src[63:0]
    1:    tmp[63:0] := src[127:64]
    2:    tmp[63:0] := src[191:128]
    3:    tmp[63:0] := src[255:192]
    ESAC
    RETURN tmp[63:0]
}
tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
tmp_dst[319:256] := SELECT4(a[511:256], imm8[1:0])
tmp_dst[383:320] := SELECT4(a[511:256], imm8[3:2])
tmp_dst[447:384] := SELECT4(a[511:256], imm8[5:4])
tmp_dst[511:448] := SELECT4(a[511:256], imm8[7:6])
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMPD - _mm512_mask_permutexvar_pd

| VPERMPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" across lanes using the corresponding index in
"idx", and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    id := idx[i+2:i]*64
    IF k[j]
        dst[i+63:i] := a[id+63:id]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMPD - _mm512_maskz_permutex_pd

| VPERMPD_ZMMf64_MASKmskw_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" within 256-bit lanes using the control in
"imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask
bit is not set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[63:0] := src[63:0]
    1:    tmp[63:0] := src[127:64]
    2:    tmp[63:0] := src[191:128]
    3:    tmp[63:0] := src[255:192]
    ESAC
    RETURN tmp[63:0]
}
tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
tmp_dst[319:256] := SELECT4(a[511:256], imm8[1:0])
tmp_dst[383:320] := SELECT4(a[511:256], imm8[3:2])
tmp_dst[447:384] := SELECT4(a[511:256], imm8[5:4])
tmp_dst[511:448] := SELECT4(a[511:256], imm8[7:6])
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMPD - _mm512_maskz_permutexvar_pd

| VPERMPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" across lanes using the corresponding index in
"idx", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    id := idx[i+2:i]*64
    IF k[j]
        dst[i+63:i] := a[id+63:id]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMPD - _mm512_permutex_pd

| VPERMPD_ZMMf64_MASKmskw_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" within 256-bit lanes using the control in
"imm8", and store the results in "dst".

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[63:0] := src[63:0]
    1:    tmp[63:0] := src[127:64]
    2:    tmp[63:0] := src[191:128]
    3:    tmp[63:0] := src[255:192]
    ESAC
    RETURN tmp[63:0]
}
dst[63:0] := SELECT4(a[255:0], imm8[1:0])
dst[127:64] := SELECT4(a[255:0], imm8[3:2])
dst[191:128] := SELECT4(a[255:0], imm8[5:4])
dst[255:192] := SELECT4(a[255:0], imm8[7:6])
dst[319:256] := SELECT4(a[511:256], imm8[1:0])
dst[383:320] := SELECT4(a[511:256], imm8[3:2])
dst[447:384] := SELECT4(a[511:256], imm8[5:4])
dst[511:448] := SELECT4(a[511:256], imm8[7:6])
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMPD - _mm512_permutexvar_pd

| VPERMPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements in "a" across lanes using the corresponding index in
"idx", and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    id := idx[i+2:i]*64
    dst[i+63:i] := a[id+63:id]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMPS - _mm512_mask_permutexvar_ps

| VPERMPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" across lanes using the corresponding index in
"idx", and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    id := idx[i+3:i]*32
    IF k[j]
        dst[i+31:i] := a[id+31:id]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMPS - _mm512_maskz_permutexvar_ps

| VPERMPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" across lanes using the corresponding index in
"idx", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    id := idx[i+3:i]*32
    IF k[j]
        dst[i+31:i] := a[id+31:id]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMPS - _mm512_permutexvar_ps

| VPERMPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" across lanes using the corresponding index in
"idx".

[algorithm]

FOR j := 0 to 15
    i := j*32
    id := idx[i+3:i]*32
    dst[i+31:i] := a[id+31:id]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMQ - _mm512_mask_permutex_epi64

| VPERMQ_ZMMu64_MASKmskw_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 64-bit integers in "a" within 256-bit lanes using the control in "imm8", and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[63:0] := src[63:0]
    1:    tmp[63:0] := src[127:64]
    2:    tmp[63:0] := src[191:128]
    3:    tmp[63:0] := src[255:192]
    ESAC
    RETURN tmp[63:0]
}
tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
tmp_dst[319:256] := SELECT4(a[511:256], imm8[1:0])
tmp_dst[383:320] := SELECT4(a[511:256], imm8[3:2])
tmp_dst[447:384] := SELECT4(a[511:256], imm8[5:4])
tmp_dst[511:448] := SELECT4(a[511:256], imm8[7:6])
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMQ - _mm512_mask_permutexvar_epi64

| VPERMQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 64-bit integers in "a" across lanes using the corresponding index in "idx", and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    id := idx[i+2:i]*64
    IF k[j]
        dst[i+63:i] := a[id+63:id]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMQ - _mm512_maskz_permutex_epi64

| VPERMQ_ZMMu64_MASKmskw_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 64-bit integers in "a" within 256-bit lanes using the control in "imm8", and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[63:0] := src[63:0]
    1:    tmp[63:0] := src[127:64]
    2:    tmp[63:0] := src[191:128]
    3:    tmp[63:0] := src[255:192]
    ESAC
    RETURN tmp[63:0]
}
tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
tmp_dst[319:256] := SELECT4(a[511:256], imm8[1:0])
tmp_dst[383:320] := SELECT4(a[511:256], imm8[3:2])
tmp_dst[447:384] := SELECT4(a[511:256], imm8[5:4])
tmp_dst[511:448] := SELECT4(a[511:256], imm8[7:6])
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMQ - _mm512_maskz_permutexvar_epi64

| VPERMQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 64-bit integers in "a" across lanes using the corresponding index in "idx", and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    id := idx[i+2:i]*64
    IF k[j]
        dst[i+63:i] := a[id+63:id]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMQ - _mm512_permutex_epi64

| VPERMQ_ZMMu64_MASKmskw_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 64-bit integers in "a" within 256-bit lanes using the control in "imm8", and store the results in
"dst".

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[63:0] := src[63:0]
    1:    tmp[63:0] := src[127:64]
    2:    tmp[63:0] := src[191:128]
    3:    tmp[63:0] := src[255:192]
    ESAC
    RETURN tmp[63:0]
}
dst[63:0] := SELECT4(a[255:0], imm8[1:0])
dst[127:64] := SELECT4(a[255:0], imm8[3:2])
dst[191:128] := SELECT4(a[255:0], imm8[5:4])
dst[255:192] := SELECT4(a[255:0], imm8[7:6])
dst[319:256] := SELECT4(a[511:256], imm8[1:0])
dst[383:320] := SELECT4(a[511:256], imm8[3:2])
dst[447:384] := SELECT4(a[511:256], imm8[5:4])
dst[511:448] := SELECT4(a[511:256], imm8[7:6])
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMQ - _mm512_permutexvar_epi64

| VPERMQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 64-bit integers in "a" across lanes using the corresponding index in "idx", and store the results in
"dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    id := idx[i+2:i]*64
    dst[i+63:i] := a[id+63:id]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDD - _mm512_mask_expand_epi32

| VPEXPANDD_ZMMu32_MASKmskw_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 32-bit integers from "a" (those with their respective bit set in mask "k"), and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

m := 0
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := a[m+31:m]
        m := m + 32
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDD - _mm512_mask_expandloadu_epi32

| VPEXPANDD_ZMMu32_MASKmskw_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 32-bit integers from unaligned memory at "mem_addr" (those with their respective bit
set in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
        m := m + 32
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDD - _mm512_maskz_expand_epi32

| VPEXPANDD_ZMMu32_MASKmskw_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 32-bit integers from "a" (those with their respective bit set in mask "k"), and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := a[m+31:m]
        m := m + 32
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDD - _mm512_maskz_expandloadu_epi32

| VPEXPANDD_ZMMu32_MASKmskw_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 32-bit integers from unaligned memory at "mem_addr" (those with their respective bit
set in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
        m := m + 32
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDQ - _mm512_mask_expand_epi64

| VPEXPANDQ_ZMMu64_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 64-bit integers from "a" (those with their respective bit set in mask "k"), and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

m := 0
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := a[m+63:m]
        m := m + 64
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDQ - _mm512_mask_expandloadu_epi64

| VPEXPANDQ_ZMMu64_MASKmskw_MEMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 64-bit integers from unaligned memory at "mem_addr" (those with their respective bit
set in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
        m := m + 64
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDQ - _mm512_maskz_expand_epi64

| VPEXPANDQ_ZMMu64_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 64-bit integers from "a" (those with their respective bit set in mask "k"), and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := a[m+63:m]
        m := m + 64
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDQ - _mm512_maskz_expandloadu_epi64

| VPEXPANDQ_ZMMu64_MASKmskw_MEMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 64-bit integers from unaligned memory at "mem_addr" (those with their respective bit
set in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
        m := m + 64
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPGATHERDQ - _mm512_i32gather_epi64

| VPGATHERDQ_ZMMu64_MASKmskw_MEMu64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Gather 64-bit integers from memory using 32-bit indices. 64-bit elements are loaded from addresses starting at
"base_addr" and offset by each 32-bit element in "vindex" (each index is scaled by the factor in "scale").
Gathered elements are merged into "dst". "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 7
    i := j*64
    m := j*32
    addr := base_addr + SignExtend64(vindex[m+31:m]) * ZeroExtend64(scale) * 8
    dst[i+63:i] := MEM[addr+63:addr]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPGATHERDQ - _mm512_mask_i32gather_epi64

| VPGATHERDQ_ZMMu64_MASKmskw_MEMu64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Gather 64-bit integers from memory using 32-bit indices. 64-bit elements are loaded from addresses starting at
"base_addr" and offset by each 32-bit element in "vindex" (each index is scaled by the factor in "scale").
Gathered elements are merged into "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 7
    i := j*64
    m := j*32
    IF k[j]
        addr := base_addr + SignExtend64(vindex[m+31:m]) * ZeroExtend64(scale) * 8
        dst[i+63:i] := MEM[addr+63:addr]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPGATHERQD - _mm512_i64gather_epi32

| VPGATHERQD_YMMu32_MASKmskw_MEMu32_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Gather 32-bit integers from memory using 64-bit indices. 32-bit elements are loaded from addresses starting at
"base_addr" and offset by each 64-bit element in "vindex" (each index is scaled by the factor in "scale").
Gathered elements are merged into "dst". "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 7
    i := j*32
    m := j*64
    addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
    dst[i+31:i] := MEM[addr+31:addr]
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPGATHERQD - _mm512_mask_i64gather_epi32

| VPGATHERQD_YMMu32_MASKmskw_MEMu32_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Gather 32-bit integers from memory using 64-bit indices. 32-bit elements are loaded from addresses starting at
"base_addr" and offset by each 64-bit element in "vindex" (each index is scaled by the factor in "scale").
Gathered elements are merged into "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 7
    i := j*32
    m := j*64
    IF k[j]
        addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
        dst[i+31:i] := MEM[addr+31:addr]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPGATHERQQ - _mm512_i64gather_epi64

| VPGATHERQQ_ZMMu64_MASKmskw_MEMu64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Gather 64-bit integers from memory using 64-bit indices. 64-bit elements are loaded from addresses starting at
"base_addr" and offset by each 64-bit element in "vindex" (each index is scaled by the factor in "scale").
Gathered elements are merged into "dst". "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 7
    i := j*64
    m := j*64
    addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
    dst[i+63:i] := MEM[addr+63:addr]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPGATHERQQ - _mm512_mask_i64gather_epi64

| VPGATHERQQ_ZMMu64_MASKmskw_MEMu64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Gather 64-bit integers from memory using 64-bit indices. 64-bit elements are loaded from addresses starting at
"base_addr" and offset by each 64-bit element in "vindex" (each index is scaled by the factor in "scale").
Gathered elements are merged into "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 7
    i := j*64
    m := j*64
    IF k[j]
        addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
        dst[i+63:i] := MEM[addr+63:addr]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXSD - _mm512_maskz_max_epi32

| VPMAXSD_ZMMi32_MASKmskw_ZMMi32_ZMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := 0 
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXSQ - _mm512_mask_max_epi64

| VPMAXSQ_ZMMi64_MASKmskw_ZMMi64_ZMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b", and store packed maximum values in "dst" using writemask
"k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXSQ - _mm512_maskz_max_epi64

| VPMAXSQ_ZMMi64_MASKmskw_ZMMi64_ZMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXSQ - _mm512_max_epi64

| VPMAXSQ_ZMMi64_MASKmskw_ZMMi64_ZMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b", and store packed maximum values in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXUD - _mm512_maskz_max_epu32

| VPMAXUD_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b", and store packed maximum values in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXUQ - _mm512_mask_max_epu64

| VPMAXUQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b", and store packed maximum values in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXUQ - _mm512_maskz_max_epu64

| VPMAXUQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b", and store packed maximum values in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMAXUQ - _mm512_max_epu64

| VPMAXUQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b", and store packed maximum values in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINSD - _mm512_maskz_min_epi32

| VPMINSD_ZMMi32_MASKmskw_ZMMi32_ZMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 32-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINSQ - _mm512_mask_min_epi64

| VPMINSQ_ZMMi64_MASKmskw_ZMMi64_ZMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b", and store packed minimum values in "dst" using writemask
"k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINSQ - _mm512_maskz_min_epi64

| VPMINSQ_ZMMi64_MASKmskw_ZMMi64_ZMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINSQ - _mm512_min_epi64

| VPMINSQ_ZMMi64_MASKmskw_ZMMi64_ZMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed signed 64-bit integers in "a" and "b", and store packed minimum values in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINUD - _mm512_maskz_min_epu32

| VPMINUD_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 32-bit integers in "a" and "b", and store packed minimum values in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINUQ - _mm512_mask_min_epu64

| VPMINUQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b", and store packed minimum values in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINUQ - _mm512_maskz_min_epu64

| VPMINUQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b", and store packed minimum values in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMINUQ - _mm512_min_epu64

| VPMINUQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compare packed unsigned 64-bit integers in "a" and "b", and store packed minimum values in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVDB - _mm512_cvtepi32_epi8

| VPMOVDB_XMMu8_MASKmskw_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 32-bit integers in "a" to packed 8-bit integers with truncation, and store the results in
"dst".

[algorithm]

FOR j := 0 to 15
    i := 32*j
    k := 8*j
    dst[k+7:k] := Truncate8(a[i+31:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVDB - _mm512_mask_cvtepi32_epi8

| VPMOVDB_XMMu8_MASKmskw_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 32-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := 32*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := Truncate8(a[i+31:i])
    ELSE
        dst[l+7:l] := src[l+7:l]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVDB - _mm512_mask_cvtepi32_storeu_epi8

| VPMOVDB_MEMu8_MASKmskw_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 32-bit integers in "a" to packed 8-bit integers with truncation, and store the active results
(those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 15
    i := 32*j
    l := 8*j
    IF k[j]
        MEM[base_addr+l+7:base_addr+l] := Truncate8(a[i+31:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVDB - _mm512_maskz_cvtepi32_epi8

| VPMOVDB_XMMu8_MASKmskw_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 32-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := 32*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := Truncate8(a[i+31:i])
    ELSE
        dst[l+7:l] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVDW - _mm512_cvtepi32_epi16

| VPMOVDW_YMMu16_MASKmskw_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 32-bit integers in "a" to packed 16-bit integers with truncation, and store the results in
"dst".

[algorithm]

FOR j := 0 to 15
    i := 32*j
    k := 16*j
    dst[k+15:k] := Truncate16(a[i+31:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVDW - _mm512_mask_cvtepi32_epi16

| VPMOVDW_YMMu16_MASKmskw_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 32-bit integers in "a" to packed 16-bit integers with truncation, and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := 32*j
    l := 16*j
    IF k[j]
        dst[l+15:l] := Truncate16(a[i+31:i])
    ELSE
        dst[l+15:l] := src[l+15:l]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVDW - _mm512_mask_cvtepi32_storeu_epi16

| VPMOVDW_MEMu16_MASKmskw_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 32-bit integers in "a" to packed 16-bit integers with truncation, and store the active results
(those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 15
    i := 32*j
    l := 16*j
    IF k[j]
        MEM[base_addr+l+15:base_addr+l] := Truncate16(a[i+31:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVDW - _mm512_maskz_cvtepi32_epi16

| VPMOVDW_YMMu16_MASKmskw_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 32-bit integers in "a" to packed 16-bit integers with truncation, and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := 32*j
    l := 16*j
    IF k[j]
        dst[l+15:l] := Truncate16(a[i+31:i])
    ELSE
        dst[l+15:l] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVQB - _mm512_cvtepi64_epi8

| VPMOVQB_XMMu8_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 64-bit integers in "a" to packed 8-bit integers with truncation, and store the results in
"dst".

[algorithm]

FOR j := 0 to 7
    i := 64*j
    k := 8*j
    dst[k+7:k] := Truncate8(a[i+63:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVQB - _mm512_mask_cvtepi64_epi8

| VPMOVQB_XMMu8_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 64-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := Truncate8(a[i+63:i])
    ELSE
        dst[l+7:l] := src[l+7:l]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVQB - _mm512_mask_cvtepi64_storeu_epi8

| VPMOVQB_MEMu8_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 64-bit integers in "a" to packed 8-bit integers with truncation, and store the active results
(those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 8*j
    IF k[j]
        MEM[base_addr+l+7:base_addr+l] := Truncate8(a[i+63:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVQB - _mm512_maskz_cvtepi64_epi8

| VPMOVQB_XMMu8_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 64-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := Truncate8(a[i+63:i])
    ELSE
        dst[l+7:l] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVQD - _mm512_cvtepi64_epi32

| VPMOVQD_YMMu32_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 64-bit integers in "a" to packed 32-bit integers with truncation, and store the results in
"dst".

[algorithm]

FOR j := 0 to 7
    i := 64*j
    k := 32*j
    dst[k+31:k] := Truncate32(a[i+63:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVQD - _mm512_mask_cvtepi64_epi32

| VPMOVQD_YMMu32_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 64-bit integers in "a" to packed 32-bit integers with truncation, and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 32*j
    IF k[j]
        dst[l+31:l] := Truncate32(a[i+63:i])
    ELSE
        dst[l+31:l] := src[l+31:l]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVQD - _mm512_mask_cvtepi64_storeu_epi32

| VPMOVQD_MEMu32_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 64-bit integers in "a" to packed 32-bit integers with truncation, and store the active results
(those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 32*j
    IF k[j]
        MEM[base_addr+l+31:base_addr+l] := Truncate32(a[i+63:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVQD - _mm512_maskz_cvtepi64_epi32

| VPMOVQD_YMMu32_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 64-bit integers in "a" to packed 32-bit integers with truncation, and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 32*j
    IF k[j]
        dst[l+31:l] := Truncate32(a[i+63:i])
    ELSE
        dst[l+31:l] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVQW - _mm512_cvtepi64_epi16

| VPMOVQW_XMMu16_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 64-bit integers in "a" to packed 16-bit integers with truncation, and store the results in
"dst".

[algorithm]

FOR j := 0 to 7
    i := 64*j
    k := 16*j
    dst[k+15:k] := Truncate16(a[i+63:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVQW - _mm512_mask_cvtepi64_epi16

| VPMOVQW_XMMu16_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 64-bit integers in "a" to packed 16-bit integers with truncation, and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 16*j
    IF k[j]
        dst[l+15:l] := Truncate16(a[i+63:i])
    ELSE
        dst[l+15:l] := src[l+15:l]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVQW - _mm512_mask_cvtepi64_storeu_epi16

| VPMOVQW_MEMu16_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 64-bit integers in "a" to packed 16-bit integers with truncation, and store the active results
(those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 16*j
    IF k[j]
        MEM[base_addr+l+15:base_addr+l] := Truncate16(a[i+63:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVQW - _mm512_maskz_cvtepi64_epi16

| VPMOVQW_XMMu16_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed 64-bit integers in "a" to packed 16-bit integers with truncation, and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 16*j
    IF k[j]
        dst[l+15:l] := Truncate16(a[i+63:i])
    ELSE
        dst[l+15:l] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSDB - _mm512_cvtsepi32_epi8

| VPMOVSDB_XMMi8_MASKmskw_ZMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed 8-bit integers with signed saturation, and store the
results in "dst".

[algorithm]

FOR j := 0 to 15
    i := 32*j
    k := 8*j
    dst[k+7:k] := Saturate8(a[i+31:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSDB - _mm512_mask_cvtsepi32_epi8

| VPMOVSDB_XMMi8_MASKmskw_ZMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed 8-bit integers with signed saturation, and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 15
    i := 32*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := Saturate8(a[i+31:i])
    ELSE
        dst[l+7:l] := src[l+7:l]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSDB - _mm512_mask_cvtsepi32_storeu_epi8

| VPMOVSDB_MEMi8_MASKmskw_ZMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed 8-bit integers with signed saturation, and store the
active results (those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 15
    i := 32*j
    l := 8*j
    IF k[j]
        MEM[base_addr+l+7:base_addr+l] := Saturate8(a[i+31:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVSDB - _mm512_maskz_cvtsepi32_epi8

| VPMOVSDB_XMMi8_MASKmskw_ZMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed 8-bit integers with signed saturation, and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := 32*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := Saturate8(a[i+31:i])
    ELSE
        dst[l+7:l] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSDW - _mm512_cvtsepi32_epi16

| VPMOVSDW_YMMi16_MASKmskw_ZMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed 16-bit integers with signed saturation, and store the
results in "dst".

[algorithm]

FOR j := 0 to 15
    i := 32*j
    k := 16*j
    dst[k+15:k] := Saturate16(a[i+31:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSDW - _mm512_mask_cvtsepi32_epi16

| VPMOVSDW_YMMi16_MASKmskw_ZMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed 16-bit integers with signed saturation, and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 15
    i := 32*j
    l := 16*j
    IF k[j]
        dst[l+15:l] := Saturate16(a[i+31:i])
    ELSE
        dst[l+15:l] := src[l+15:l]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSDW - _mm512_mask_cvtsepi32_storeu_epi16

| VPMOVSDW_MEMi16_MASKmskw_ZMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed 16-bit integers with signed saturation, and store the
active results (those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 15
    i := 32*j
    l := 16*j
    IF k[j]
        MEM[base_addr+l+15:base_addr+l] := Saturate16(a[i+31:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVSDW - _mm512_maskz_cvtsepi32_epi16

| VPMOVSDW_YMMi16_MASKmskw_ZMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 32-bit integers in "a" to packed 16-bit integers with signed saturation, and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := 32*j
    l := 16*j
    IF k[j]
        dst[l+15:l] := Saturate16(a[i+31:i])
    ELSE
        dst[l+15:l] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSQB - _mm512_cvtsepi64_epi8

| VPMOVSQB_XMMi8_MASKmskw_ZMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed 8-bit integers with signed saturation, and store the
results in "dst".

[algorithm]

FOR j := 0 to 7
    i := 64*j
    k := 8*j
    dst[k+7:k] := Saturate8(a[i+63:i])
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSQB - _mm512_mask_cvtsepi64_epi8

| VPMOVSQB_XMMi8_MASKmskw_ZMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed 8-bit integers with signed saturation, and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := Saturate8(a[i+63:i])
    ELSE
        dst[l+7:l] := src[l+7:l]
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSQB - _mm512_mask_cvtsepi64_storeu_epi8

| VPMOVSQB_MEMi8_MASKmskw_ZMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed 8-bit integers with signed saturation, and store the
active results (those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 8*j
    IF k[j]
        MEM[base_addr+l+7:base_addr+l] := Saturate8(a[i+63:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVSQB - _mm512_maskz_cvtsepi64_epi8

| VPMOVSQB_XMMi8_MASKmskw_ZMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed 8-bit integers with signed saturation, and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := Saturate8(a[i+63:i])
    ELSE
        dst[l+7:l] := 0
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSQD - _mm512_cvtsepi64_epi32

| VPMOVSQD_YMMi32_MASKmskw_ZMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed 32-bit integers with signed saturation, and store the
results in "dst".

[algorithm]

FOR j := 0 to 7
    i := 64*j
    k := 32*j
    dst[k+31:k] := Saturate32(a[i+63:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSQD - _mm512_mask_cvtsepi64_epi32

| VPMOVSQD_YMMi32_MASKmskw_ZMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed 32-bit integers with signed saturation, and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 32*j
    IF k[j]
        dst[l+31:l] := Saturate32(a[i+63:i])
    ELSE
        dst[l+31:l] := src[l+31:l]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSQD - _mm512_mask_cvtsepi64_storeu_epi32

| VPMOVSQD_MEMi32_MASKmskw_ZMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed 32-bit integers with signed saturation, and store the
active results (those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 32*j
    IF k[j]
        MEM[base_addr+l+31:base_addr+l] := Saturate32(a[i+63:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVSQD - _mm512_maskz_cvtsepi64_epi32

| VPMOVSQD_YMMi32_MASKmskw_ZMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed 32-bit integers with signed saturation, and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 32*j
    IF k[j]
        dst[l+31:l] := Saturate32(a[i+63:i])
    ELSE
        dst[l+31:l] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSQW - _mm512_cvtsepi64_epi16

| VPMOVSQW_XMMi16_MASKmskw_ZMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed 16-bit integers with signed saturation, and store the
results in "dst".

[algorithm]

FOR j := 0 to 7
    i := 64*j
    k := 16*j
    dst[k+15:k] := Saturate16(a[i+63:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSQW - _mm512_mask_cvtsepi64_epi16

| VPMOVSQW_XMMi16_MASKmskw_ZMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed 16-bit integers with signed saturation, and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 16*j
    IF k[j]
        dst[l+15:l] := Saturate16(a[i+63:i])
    ELSE
        dst[l+15:l] := src[l+15:l]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSQW - _mm512_mask_cvtsepi64_storeu_epi16

| VPMOVSQW_MEMi16_MASKmskw_ZMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed 16-bit integers with signed saturation, and store the
active results (those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 16*j
    IF k[j]
        MEM[base_addr+l+15:base_addr+l] := Saturate16(a[i+63:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVSQW - _mm512_maskz_cvtsepi64_epi16

| VPMOVSQW_XMMi16_MASKmskw_ZMMi64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed signed 64-bit integers in "a" to packed 16-bit integers with signed saturation, and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 16*j
    IF k[j]
        dst[l+15:l] := Saturate16(a[i+63:i])
    ELSE
        dst[l+15:l] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXBD - _mm512_cvtepi8_epi32

| VPMOVSXBD_ZMMi32_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 8-bit integers in "a" to packed 32-bit integers, and store the results in "dst".

[algorithm]

FOR j := 0 to 15
    i := 32*j
    k := 8*j
    dst[i+31:i] := SignExtend32(a[k+7:k])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXBD - _mm512_mask_cvtepi8_epi32

| VPMOVSXBD_ZMMi32_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 8-bit integers in "a" to packed 32-bit integers, and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := 32*j
    l := 8*j
    IF k[j]
        dst[i+31:i] := SignExtend32(a[l+7:l])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXBD - _mm512_maskz_cvtepi8_epi32

| VPMOVSXBD_ZMMi32_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 8-bit integers in "a" to packed 32-bit integers, and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := 32*j
    l := 8*j
    IF k[j]
        dst[i+31:i] := SignExtend32(a[l+7:l])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXBQ - _mm512_cvtepi8_epi64

| VPMOVSXBQ_ZMMi64_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 8-bit integers in the low 8 bytes of "a" to packed 64-bit integers, and store the results
in "dst".

[algorithm]

FOR j := 0 to 7
    i := 64*j
    k := 8*j
    dst[i+63:i] := SignExtend64(a[k+7:k])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXBQ - _mm512_mask_cvtepi8_epi64

| VPMOVSXBQ_ZMMi64_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 8-bit integers in the low 8 bytes of "a" to packed 64-bit integers, and store the results
in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 8*j
    IF k[j]
        dst[i+63:i] := SignExtend64(a[l+7:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXBQ - _mm512_maskz_cvtepi8_epi64

| VPMOVSXBQ_ZMMi64_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 8-bit integers in the low 8 bytes of "a" to packed 64-bit integers, and store the results
in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 8*j
    IF k[j]
        dst[i+63:i] := SignExtend64(a[l+7:l])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXDQ - _mm512_cvtepi32_epi64

| VPMOVSXDQ_ZMMi64_MASKmskw_YMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 32-bit integers in "a" to packed 64-bit integers, and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := 64*j
    k := 32*j
    dst[i+63:i] := SignExtend64(a[k+31:k])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXDQ - _mm512_mask_cvtepi32_epi64

| VPMOVSXDQ_ZMMi64_MASKmskw_YMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 32-bit integers in "a" to packed 64-bit integers, and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 32*j
    IF k[j]
        dst[i+63:i] := SignExtend64(a[l+31:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXDQ - _mm512_maskz_cvtepi32_epi64

| VPMOVSXDQ_ZMMi64_MASKmskw_YMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 32-bit integers in "a" to packed 64-bit integers, and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 32*j
    IF k[j]
        dst[i+63:i] := SignExtend64(a[l+31:l])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXWD - _mm512_cvtepi16_epi32

| VPMOVSXWD_ZMMi32_MASKmskw_YMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 16-bit integers in "a" to packed 32-bit integers, and store the results in "dst".

[algorithm]

FOR j := 0 to 15
    i := 32*j
    k := 16*j
    dst[i+31:i] := SignExtend32(a[k+15:k])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXWD - _mm512_mask_cvtepi16_epi32

| VPMOVSXWD_ZMMi32_MASKmskw_YMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 16-bit integers in "a" to packed 32-bit integers, and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    l := j*16
    IF k[j]
        dst[i+31:i] := SignExtend32(a[l+15:l])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXWD - _mm512_maskz_cvtepi16_epi32

| VPMOVSXWD_ZMMi32_MASKmskw_YMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 16-bit integers in "a" to packed 32-bit integers, and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := 32*j
    l := 16*j
    IF k[j]
        dst[i+31:i] := SignExtend32(a[l+15:l])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXWQ - _mm512_cvtepi16_epi64

| VPMOVSXWQ_ZMMi64_MASKmskw_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 16-bit integers in "a" to packed 64-bit integers, and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := 64*j
    k := 16*j
    dst[i+63:i] := SignExtend64(a[k+15:k])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXWQ - _mm512_mask_cvtepi16_epi64

| VPMOVSXWQ_ZMMi64_MASKmskw_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 16-bit integers in "a" to packed 64-bit integers, and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 16*j
    IF k[j]
        dst[i+63:i] := SignExtend64(a[l+15:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVSXWQ - _mm512_maskz_cvtepi16_epi64

| VPMOVSXWQ_ZMMi64_MASKmskw_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Sign extend packed 16-bit integers in "a" to packed 64-bit integers, and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 16*j
    IF k[j]
        dst[i+63:i] := SignExtend64(a[l+15:l])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSDB - _mm512_cvtusepi32_epi8

| VPMOVUSDB_XMMu8_MASKmskw_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and
store the results in "dst".

[algorithm]

FOR j := 0 to 15
    i := 32*j
    k := 8*j
    dst[k+7:k] := SaturateU8(a[i+31:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSDB - _mm512_mask_cvtusepi32_epi8

| VPMOVUSDB_XMMu8_MASKmskw_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 15
    i := 32*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := SaturateU8(a[i+31:i])
    ELSE
        dst[l+7:l] := src[l+7:l]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSDB - _mm512_mask_cvtusepi32_storeu_epi8

| VPMOVUSDB_MEMu8_MASKmskw_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed 8-bit integers with unsigned saturation, and store
the active results (those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 15
    i := 32*j
    l := 8*j
    IF k[j]
        MEM[base_addr+l+7:base_addr+l] := SaturateU8(a[i+31:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVUSDB - _mm512_maskz_cvtusepi32_epi8

| VPMOVUSDB_XMMu8_MASKmskw_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 15
    i := 32*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := SaturateU8(a[i+31:i])
    ELSE
        dst[l+7:l] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSDW - _mm512_cvtusepi32_epi16

| VPMOVUSDW_YMMu16_MASKmskw_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation,
and store the results in "dst".

[algorithm]

FOR j := 0 to 15
    i := 32*j
    k := 16*j
    dst[k+15:k] := SaturateU16(a[i+31:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSDW - _mm512_mask_cvtusepi32_epi16

| VPMOVUSDW_YMMu16_MASKmskw_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 15
    i := 32*j
    l := 16*j
    IF k[j]
        dst[l+15:l] := SaturateU16(a[i+31:i])
    ELSE
        dst[l+15:l] := src[l+15:l]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSDW - _mm512_mask_cvtusepi32_storeu_epi16

| VPMOVUSDW_MEMu16_MASKmskw_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed 16-bit integers with unsigned saturation, and store
the active results (those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 15
    i := 32*j
    l := 16*j
    IF k[j]
        MEM[base_addr+l+15:base_addr+l] := SaturateU16(a[i+31:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVUSDW - _mm512_maskz_cvtusepi32_epi16

| VPMOVUSDW_YMMu16_MASKmskw_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 32-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 15
    i := 32*j
    l := 16*j
    IF k[j]
        dst[l+15:l] := SaturateU16(a[i+31:i])
    ELSE
        dst[l+15:l] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSQB - _mm512_cvtusepi64_epi8

| VPMOVUSQB_XMMu8_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and
store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := 64*j
    k := 8*j
    dst[k+7:k] := SaturateU8(a[i+63:i])
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSQB - _mm512_mask_cvtusepi64_epi8

| VPMOVUSQB_XMMu8_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set).

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := SaturateU8(a[i+63:i])
    ELSE
        dst[l+7:l] := src[l+7:l]
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSQB - _mm512_mask_cvtusepi64_storeu_epi8

| VPMOVUSQB_MEMu8_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed 8-bit integers with unsigned saturation, and store
the active results (those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 8*j
    IF k[j]
        MEM[base_addr+l+7:base_addr+l] := SaturateU8(a[i+63:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVUSQB - _mm512_maskz_cvtusepi64_epi8

| VPMOVUSQB_XMMu8_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 8*j
    IF k[j]
        dst[l+7:l] := SaturateU8(a[i+63:i])
    ELSE
        dst[l+7:l] := 0
    FI
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSQD - _mm512_cvtusepi64_epi32

| VPMOVUSQD_YMMu32_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed unsigned 32-bit integers with unsigned saturation,
and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := 64*j
    k := 32*j
    dst[k+31:k] := SaturateU32(a[i+63:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSQD - _mm512_mask_cvtusepi64_epi32

| VPMOVUSQD_YMMu32_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed unsigned 32-bit integers with unsigned saturation,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 32*j
    IF k[j]
        dst[l+31:l] := SaturateU32(a[i+63:i])
    ELSE
        dst[l+31:l] := src[l+31:l]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSQD - _mm512_mask_cvtusepi64_storeu_epi32

| VPMOVUSQD_MEMu32_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed 32-bit integers with unsigned saturation, and store
the active results (those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 32*j
    IF k[j]
        MEM[base_addr+l+31:base_addr+l] := SaturateU32(a[i+63:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVUSQD - _mm512_maskz_cvtusepi64_epi32

| VPMOVUSQD_YMMu32_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed unsigned 32-bit integers with unsigned saturation,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 32*j
    IF k[j]
        dst[l+31:l] := SaturateU32(a[i+63:i])
    ELSE
        dst[l+31:l] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSQW - _mm512_cvtusepi64_epi16

| VPMOVUSQW_XMMu16_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation,
and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := 64*j
    k := 16*j
    dst[k+15:k] := SaturateU16(a[i+63:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSQW - _mm512_mask_cvtusepi64_epi16

| VPMOVUSQW_XMMu16_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation,
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 16*j
    IF k[j]
        dst[l+15:l] := SaturateU16(a[i+63:i])
    ELSE
        dst[l+15:l] := src[l+15:l]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVUSQW - _mm512_mask_cvtusepi64_storeu_epi16

| VPMOVUSQW_MEMu16_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed 16-bit integers with unsigned saturation, and store
the active results (those with their respective bit set in writemask "k") to unaligned memory at "base_addr".

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 16*j
    IF k[j]
        MEM[base_addr+l+15:base_addr+l] := SaturateU16(a[i+63:i])
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPMOVUSQW - _mm512_maskz_cvtusepi64_epi16

| VPMOVUSQW_XMMu16_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed unsigned 64-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation,
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 16*j
    IF k[j]
        dst[l+15:l] := SaturateU16(a[i+63:i])
    ELSE
        dst[l+15:l] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXBD - _mm512_cvtepu8_epi32

| VPMOVZXBD_ZMMi32_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 8-bit integers in "a" to packed 32-bit integers, and store the results in "dst".

[algorithm]

FOR j := 0 to 15
    i := 32*j
    k := 8*j
    dst[i+31:i] := ZeroExtend32(a[k+7:k])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXBD - _mm512_mask_cvtepu8_epi32

| VPMOVZXBD_ZMMi32_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 8-bit integers in "a" to packed 32-bit integers, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := 32*j
    l := 8*j
    IF k[j]
        dst[i+31:i] := ZeroExtend32(a[l+7:l])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXBD - _mm512_maskz_cvtepu8_epi32

| VPMOVZXBD_ZMMi32_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 8-bit integers in "a" to packed 32-bit integers, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := 32*j
    l := 8*j
    IF k[j]
        dst[i+31:i] := ZeroExtend32(a[l+7:l])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXBQ - _mm512_cvtepu8_epi64

| VPMOVZXBQ_ZMMi64_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 8-bit integers in the low 8 byte sof "a" to packed 64-bit integers, and store the
results in "dst".

[algorithm]

FOR j := 0 to 7
    i := 64*j
    k := 8*j
    dst[i+63:i] := ZeroExtend64(a[k+7:k])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXBQ - _mm512_mask_cvtepu8_epi64

| VPMOVZXBQ_ZMMi64_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 8-bit integers in the low 8 bytes of "a" to packed 64-bit integers, and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 8*j
    IF k[j]
        dst[i+63:i] := ZeroExtend64(a[l+7:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXBQ - _mm512_maskz_cvtepu8_epi64

| VPMOVZXBQ_ZMMi64_MASKmskw_XMMi8_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 8-bit integers in the low 8 bytes of "a" to packed 64-bit integers, and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 8*j
    IF k[j]
        dst[i+63:i] := ZeroExtend64(a[l+7:l])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXDQ - _mm512_cvtepu32_epi64

| VPMOVZXDQ_ZMMi64_MASKmskw_YMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 32-bit integers in "a" to packed 64-bit integers, and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := 64*j
    k := 32*j
    dst[i+63:i] := ZeroExtend64(a[k+31:k])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXDQ - _mm512_mask_cvtepu32_epi64

| VPMOVZXDQ_ZMMi64_MASKmskw_YMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 32-bit integers in "a" to packed 64-bit integers, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 32*j
    IF k[j]
        dst[i+63:i] := ZeroExtend64(a[l+31:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXDQ - _mm512_maskz_cvtepu32_epi64

| VPMOVZXDQ_ZMMi64_MASKmskw_YMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 32-bit integers in "a" to packed 64-bit integers, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 32*j
    IF k[j]
        dst[i+63:i] := ZeroExtend64(a[l+31:l])
    ELSE 
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXWD - _mm512_cvtepu16_epi32

| VPMOVZXWD_ZMMi32_MASKmskw_YMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 16-bit integers in "a" to packed 32-bit integers, and store the results in "dst".

[algorithm]

FOR j := 0 to 15
    i := 32*j
    k := 16*j
    dst[i+31:i] := ZeroExtend32(a[k+15:k])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXWD - _mm512_mask_cvtepu16_epi32

| VPMOVZXWD_ZMMi32_MASKmskw_YMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 16-bit integers in "a" to packed 32-bit integers, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := 32*j
    l := 16*j
    IF k[j]
        dst[i+31:i] := ZeroExtend32(a[l+15:l])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXWD - _mm512_maskz_cvtepu16_epi32

| VPMOVZXWD_ZMMi32_MASKmskw_YMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 16-bit integers in "a" to packed 32-bit integers, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := 32*j
    l := 16*j
    IF k[j]
        dst[i+31:i] := ZeroExtend32(a[l+15:l])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXWQ - _mm512_cvtepu16_epi64

| VPMOVZXWQ_ZMMi64_MASKmskw_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 16-bit integers in "a" to packed 64-bit integers, and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := 64*j
    k := 16*j
    dst[i+63:i] := ZeroExtend64(a[k+15:k])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXWQ - _mm512_mask_cvtepu16_epi64

| VPMOVZXWQ_ZMMi64_MASKmskw_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 16-bit integers in "a" to packed 64-bit integers, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 16*j
    IF k[j]
        dst[i+63:i] := ZeroExtend64(a[l+15:l])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMOVZXWQ - _mm512_maskz_cvtepu16_epi64

| VPMOVZXWQ_ZMMi64_MASKmskw_XMMi16_AVX512

--------------------------------------------------------------------------------------------------------------
Zero extend packed unsigned 16-bit integers in "a" to packed 64-bit integers, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := 64*j
    l := 16*j
    IF k[j]
        dst[i+63:i] := ZeroExtend64(a[l+15:l])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULDQ - _mm512_mask_mul_epi32

| VPMULDQ_ZMMi64_MASKmskw_ZMMi32_ZMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the low signed 32-bit integers from each packed 64-bit element in "a" and "b", and store the signed
64-bit results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := SignExtend64(a[i+31:i]) * SignExtend64(b[i+31:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULDQ - _mm512_maskz_mul_epi32

| VPMULDQ_ZMMi64_MASKmskw_ZMMi32_ZMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the low signed 32-bit integers from each packed 64-bit element in "a" and "b", and store the signed
64-bit results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := SignExtend64(a[i+31:i]) * SignExtend64(b[i+31:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULDQ - _mm512_mul_epi32

| VPMULDQ_ZMMi64_MASKmskw_ZMMi32_ZMMi32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the low signed 32-bit integers from each packed 64-bit element in "a" and "b", and store the signed
64-bit results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := SignExtend64(a[i+31:i]) * SignExtend64(b[i+31:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULUDQ - _mm512_mask_mul_epu32

| VPMULUDQ_ZMMu64_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the low unsigned 32-bit integers from each packed 64-bit element in "a" and "b", and store the
unsigned 64-bit results in "dst" using writemask "k" (elements are copied from "src" when the corresponding
mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+31:i] * b[i+31:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULUDQ - _mm512_maskz_mul_epu32

| VPMULUDQ_ZMMu64_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the low unsigned 32-bit integers from each packed 64-bit element in "a" and "b", and store the
unsigned 64-bit results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+31:i] * b[i+31:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULUDQ - _mm512_mul_epu32

| VPMULUDQ_ZMMu64_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the low unsigned 32-bit integers from each packed 64-bit element in "a" and "b", and store the
unsigned 64-bit results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := a[i+31:i] * b[i+31:i]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPORD - _mm512_maskz_or_epi32

| VPORD_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of packed 32-bit integers in "a" and "b", and store the results in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] OR b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPORQ - _mm512_maskz_or_epi64

| VPORQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise OR of packed 64-bit integers in "a" and "b", and store the results in "dst" using zeromask
"k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] OR b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPROLD - _mm512_mask_rol_epi32

| VPROLD_ZMMu32_MASKmskw_ZMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 32-bit integer in "a" to the left by the number of bits specified in "imm8",
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

DEFINE LEFT_ROTATE_DWORDS(src, count_src) {
    count := count_src % 32
    RETURN (src &lt;&lt; count) OR (src &gt;&gt; (32 - count))
}
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPROLD - _mm512_maskz_rol_epi32

| VPROLD_ZMMu32_MASKmskw_ZMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 32-bit integer in "a" to the left by the number of bits specified in "imm8",
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

DEFINE LEFT_ROTATE_DWORDS(src, count_src) {
    count := count_src % 32
    RETURN (src &lt;&lt; count) OR (src &gt;&gt; (32 - count))
}
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPROLD - _mm512_rol_epi32

| VPROLD_ZMMu32_MASKmskw_ZMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 32-bit integer in "a" to the left by the number of bits specified in "imm8",
and store the results in "dst".

[algorithm]

DEFINE LEFT_ROTATE_DWORDS(src, count_src) {
    count := count_src % 32
    RETURN (src &lt;&lt; count) OR (src &gt;&gt; (32 - count))
}
FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPROLQ - _mm512_mask_rol_epi64

| VPROLQ_ZMMu64_MASKmskw_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 64-bit integer in "a" to the left by the number of bits specified in "imm8",
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

DEFINE LEFT_ROTATE_QWORDS(src, count_src) {
    count := count_src % 64
    RETURN (src &lt;&lt; count) OR (src &gt;&gt; (64 - count))
}
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPROLQ - _mm512_maskz_rol_epi64

| VPROLQ_ZMMu64_MASKmskw_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 64-bit integer in "a" to the left by the number of bits specified in "imm8",
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

DEFINE LEFT_ROTATE_QWORDS(src, count_src) {
    count := count_src % 64
    RETURN (src &lt;&lt; count) OR (src &gt;&gt; (64 - count))
}
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPROLQ - _mm512_rol_epi64

| VPROLQ_ZMMu64_MASKmskw_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 64-bit integer in "a" to the left by the number of bits specified in "imm8",
and store the results in "dst".

[algorithm]

DEFINE LEFT_ROTATE_QWORDS(src, count_src) {
    count := count_src % 64
    RETURN (src &lt;&lt; count) OR (src &gt;&gt; (64 - count))
}
FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPROLVD - _mm512_mask_rolv_epi32

| VPROLVD_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 32-bit integer in "a" to the left by the number of bits specified in the
corresponding element of "b", and store the results in "dst" using writemask "k" (elements are copied from
"src" when the corresponding mask bit is not set).

[algorithm]

DEFINE LEFT_ROTATE_DWORDS(src, count_src) {
    count := count_src % 32
    RETURN (src &lt;&lt; count) OR (src &gt;&gt; (32 - count))
}
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPROLVD - _mm512_maskz_rolv_epi32

| VPROLVD_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 32-bit integer in "a" to the left by the number of bits specified in the
corresponding element of "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).

[algorithm]

DEFINE LEFT_ROTATE_DWORDS(src, count_src) {
    count := count_src % 32
    RETURN (src &lt;&lt; count) OR (src &gt;&gt; (32 - count))
}
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPROLVD - _mm512_rolv_epi32

| VPROLVD_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 32-bit integer in "a" to the left by the number of bits specified in the
corresponding element of "b", and store the results in "dst".

[algorithm]

DEFINE LEFT_ROTATE_DWORDS(src, count_src) {
    count := count_src % 32
    RETURN (src &lt;&lt; count) OR (src &gt;&gt; (32 - count))
}
FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPROLVQ - _mm512_mask_rolv_epi64

| VPROLVQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 64-bit integer in "a" to the left by the number of bits specified in the
corresponding element of "b", and store the results in "dst" using writemask "k" (elements are copied from
"src" when the corresponding mask bit is not set).

[algorithm]

DEFINE LEFT_ROTATE_QWORDS(src, count_src) {
    count := count_src % 64
    RETURN (src &lt;&lt; count) OR (src &gt;&gt; (64 - count))
}
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPROLVQ - _mm512_maskz_rolv_epi64

| VPROLVQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 64-bit integer in "a" to the left by the number of bits specified in the
corresponding element of "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).

[algorithm]

DEFINE LEFT_ROTATE_QWORDS(src, count_src) {
    count := count_src % 64
    RETURN (src &lt;&lt; count) OR (src &gt;&gt; (64 - count))
}
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPROLVQ - _mm512_rolv_epi64

| VPROLVQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 64-bit integer in "a" to the left by the number of bits specified in the
corresponding element of "b", and store the results in "dst".

[algorithm]

DEFINE LEFT_ROTATE_QWORDS(src, count_src) {
    count := count_src % 64
    RETURN (src &lt;&lt; count) OR (src &gt;&gt; (64 - count))
}
FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPRORD - _mm512_mask_ror_epi32

| VPRORD_ZMMu32_MASKmskw_ZMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 32-bit integer in "a" to the right by the number of bits specified in "imm8",
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

DEFINE RIGHT_ROTATE_DWORDS(src, count_src) {
    count := count_src % 32
    RETURN (src &gt;&gt;count) OR (src &lt;&lt; (32 - count))
}
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPRORD - _mm512_maskz_ror_epi32

| VPRORD_ZMMu32_MASKmskw_ZMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 32-bit integer in "a" to the right by the number of bits specified in "imm8",
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

DEFINE RIGHT_ROTATE_DWORDS(src, count_src) {
    count := count_src % 32
    RETURN (src &gt;&gt;count) OR (src &lt;&lt; (32 - count))
}
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPRORD - _mm512_ror_epi32

| VPRORD_ZMMu32_MASKmskw_ZMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 32-bit integer in "a" to the right by the number of bits specified in "imm8",
and store the results in "dst".

[algorithm]

DEFINE RIGHT_ROTATE_DWORDS(src, count_src) {
    count := count_src % 32
    RETURN (src &gt;&gt;count) OR (src &lt;&lt; (32 - count))
}
FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPRORQ - _mm512_mask_ror_epi64

| VPRORQ_ZMMu64_MASKmskw_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 64-bit integer in "a" to the right by the number of bits specified in "imm8",
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

DEFINE RIGHT_ROTATE_QWORDS(src, count_src) {
    count := count_src % 64
    RETURN (src &gt;&gt; count) OR (src &lt;&lt; (64 - count))
}
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPRORQ - _mm512_maskz_ror_epi64

| VPRORQ_ZMMu64_MASKmskw_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 64-bit integer in "a" to the right by the number of bits specified in "imm8",
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

DEFINE RIGHT_ROTATE_QWORDS(src, count_src) {
    count := count_src % 64
    RETURN (src &gt;&gt; count) OR (src &lt;&lt; (64 - count))
}
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPRORQ - _mm512_ror_epi64

| VPRORQ_ZMMu64_MASKmskw_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 64-bit integer in "a" to the right by the number of bits specified in "imm8",
and store the results in "dst".

[algorithm]

DEFINE RIGHT_ROTATE_QWORDS(src, count_src) {
    count := count_src % 64
    RETURN (src &gt;&gt; count) OR (src &lt;&lt; (64 - count))
}
FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPRORVD - _mm512_mask_rorv_epi32

| VPRORVD_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 32-bit integer in "a" to the right by the number of bits specified in the
corresponding element of "b", and store the results in "dst" using writemask "k" (elements are copied from
"src" when the corresponding mask bit is not set).

[algorithm]

DEFINE RIGHT_ROTATE_DWORDS(src, count_src) {
    count := count_src % 32
    RETURN (src &gt;&gt;count) OR (src &lt;&lt; (32 - count))
}
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPRORVD - _mm512_maskz_rorv_epi32

| VPRORVD_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 32-bit integer in "a" to the right by the number of bits specified in the
corresponding element of "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).

[algorithm]

DEFINE RIGHT_ROTATE_DWORDS(src, count_src) {
    count := count_src % 32
    RETURN (src &gt;&gt;count) OR (src &lt;&lt; (32 - count))
}
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPRORVD - _mm512_rorv_epi32

| VPRORVD_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 32-bit integer in "a" to the right by the number of bits specified in the
corresponding element of "b", and store the results in "dst".

[algorithm]

DEFINE RIGHT_ROTATE_DWORDS(src, count_src) {
    count := count_src % 32
    RETURN (src &gt;&gt;count) OR (src &lt;&lt; (32 - count))
}
FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPRORVQ - _mm512_mask_rorv_epi64

| VPRORVQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 64-bit integer in "a" to the right by the number of bits specified in the
corresponding element of "b", and store the results in "dst" using writemask "k" (elements are copied from
"src" when the corresponding mask bit is not set).

[algorithm]

DEFINE RIGHT_ROTATE_QWORDS(src, count_src) {
    count := count_src % 64
    RETURN (src &gt;&gt; count) OR (src &lt;&lt; (64 - count))
}
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPRORVQ - _mm512_maskz_rorv_epi64

| VPRORVQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 64-bit integer in "a" to the right by the number of bits specified in the
corresponding element of "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).

[algorithm]

DEFINE RIGHT_ROTATE_QWORDS(src, count_src) {
    count := count_src % 64
    RETURN (src &gt;&gt; count) OR (src &lt;&lt; (64 - count))
}
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPRORVQ - _mm512_rorv_epi64

| VPRORVQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Rotate the bits in each packed 64-bit integer in "a" to the right by the number of bits specified in the
corresponding element of "b", and store the results in "dst".

[algorithm]

DEFINE RIGHT_ROTATE_QWORDS(src, count_src) {
    count := count_src % 64
    RETURN (src &gt;&gt; count) OR (src &lt;&lt; (64 - count))
}
FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSCATTERDQ - _mm512_i32scatter_epi64

| VPSCATTERDQ_MEMu64_MASKmskw_ZMMu64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Scatter 64-bit integers from "a" into memory using 32-bit indices. 64-bit elements are stored at addresses
starting at "base_addr" and offset by each 32-bit element in "vindex" (each index is scaled by the factor in
"scale"). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 7
    i := j*64
    m := j*32
    addr := base_addr + SignExtend64(vindex[m+31:m]) * ZeroExtend64(scale) * 8
    MEM[addr+63:addr] := a[i+63:i]
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPSCATTERDQ - _mm512_mask_i32scatter_epi64

| VPSCATTERDQ_MEMu64_MASKmskw_ZMMu64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Scatter 64-bit integers from "a" into memory using 32-bit indices. 64-bit elements are stored at addresses
starting at "base_addr" and offset by each 32-bit element in "vindex" (each index is scaled by the factor in
"scale") subject to mask "k" (elements are not stored when the corresponding mask bit is not set). "scale"
should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 7
    i := j*64
    m := j*32
    IF k[j]
        addr := base_addr + SignExtend64(vindex[m+31:m]) * ZeroExtend64(scale) * 8
        MEM[addr+63:addr] := a[i+63:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPSCATTERQD - _mm512_i64scatter_epi32

| VPSCATTERQD_MEMu32_MASKmskw_YMMu32_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Scatter 32-bit integers from "a" into memory using 64-bit indices. 32-bit elements are stored at addresses
starting at "base_addr" and offset by each 64-bit element in "vindex" (each index is scaled by the factor in
"scale"). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 7
    i := j*32
    m := j*64
    addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
    MEM[addr+31:addr] := a[i+31:i]
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPSCATTERQD - _mm512_mask_i64scatter_epi32

| VPSCATTERQD_MEMu32_MASKmskw_YMMu32_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Scatter 32-bit integers from "a" into memory using 64-bit indices. 32-bit elements are stored at addresses
starting at "base_addr" and offset by each 64-bit element in "vindex" (each index is scaled by the factor in
"scale") subject to mask "k" (elements are not stored when the corresponding mask bit is not set). "scale"
should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 7
    i := j*32
    m := j*64
    IF k[j]
        addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
        MEM[addr+31:addr] := a[i+31:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPSCATTERQQ - _mm512_i64scatter_epi64

| VPSCATTERQQ_MEMu64_MASKmskw_ZMMu64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Scatter 64-bit integers from "a" into memory using 64-bit indices. 64-bit elements are stored at addresses
starting at "base_addr" and offset by each 64-bit element in "vindex" (each index is scaled by the factor in
"scale"). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 7
    i := j*64
    m := j*64
    addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
    MEM[addr+63:addr] := a[i+63:i]
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPSCATTERQQ - _mm512_mask_i64scatter_epi64

| VPSCATTERQQ_MEMu64_MASKmskw_ZMMu64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Scatter 64-bit integers from "a" into memory using 64-bit indices. 64-bit elements are stored at addresses
starting at "base_addr" and offset by each 64-bit element in "vindex" (each index is scaled by the factor in
"scale") subject to mask "k" (elements are not stored when the corresponding mask bit is not set). "scale"
should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 7
    i := j*64
    m := j*64
    IF k[j]
        addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
        MEM[addr+63:addr] := a[i+63:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPSHUFD - _mm512_maskz_shuffle_epi32

| VPSHUFD_ZMMu32_MASKmskw_ZMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 32-bit integers in "a" within 128-bit lanes using the control in "imm8", and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[31:0] := src[31:0]
    1:    tmp[31:0] := src[63:32]
    2:    tmp[31:0] := src[95:64]
    3:    tmp[31:0] := src[127:96]
    ESAC
    RETURN tmp[31:0]
}
tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
tmp_dst[287:256] := SELECT4(a[383:256], imm8[1:0])
tmp_dst[319:288] := SELECT4(a[383:256], imm8[3:2])
tmp_dst[351:320] := SELECT4(a[383:256], imm8[5:4])
tmp_dst[383:352] := SELECT4(a[383:256], imm8[7:6])
tmp_dst[415:384] := SELECT4(a[511:384], imm8[1:0])
tmp_dst[447:416] := SELECT4(a[511:384], imm8[3:2])
tmp_dst[479:448] := SELECT4(a[511:384], imm8[5:4])
tmp_dst[511:480] := SELECT4(a[511:384], imm8[7:6])
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLD - _mm512_mask_sll_epi32

| VPSLLD_ZMMu32_MASKmskw_ZMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        IF count[63:0] &gt; 31
            dst[i+31:i] := 0
        ELSE
            dst[i+31:i] := ZeroExtend32(a[i+31:i] &lt;&lt; count[63:0])
        FI
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLD - _mm512_maskz_sll_epi32

| VPSLLD_ZMMu32_MASKmskw_ZMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        IF count[63:0] &gt; 31
            dst[i+31:i] := 0
        ELSE
            dst[i+31:i] := ZeroExtend32(a[i+31:i] &lt;&lt; count[63:0])
        FI
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLD - _mm512_maskz_slli_epi32

| VPSLLD_ZMMu32_MASKmskw_ZMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" left by "imm8" while shifting in zeros, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        IF imm8[7:0] &gt; 31
            dst[i+31:i] := 0
        ELSE
            dst[i+31:i] := ZeroExtend32(a[i+31:i] &lt;&lt; imm8[7:0])
        FI
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLD - _mm512_sll_epi32

| VPSLLD_ZMMu32_MASKmskw_ZMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF count[63:0] &gt; 31
        dst[i+31:i] := 0
    ELSE
        dst[i+31:i] := ZeroExtend32(a[i+31:i] &lt;&lt; count[63:0])
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLQ - _mm512_mask_sll_epi64

| VPSLLQ_ZMMu64_MASKmskw_ZMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        IF count[63:0] &gt; 63
            dst[i+63:i] := 0
        ELSE
            dst[i+63:i] := ZeroExtend64(a[i+63:i] &lt;&lt; count[63:0])
        FI
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLQ - _mm512_mask_slli_epi64

| VPSLLQ_ZMMu64_MASKmskw_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" left by "imm8" while shifting in zeros, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        IF imm8[7:0] &gt; 63
            dst[i+63:i] := 0
        ELSE
            dst[i+63:i] := ZeroExtend64(a[i+63:i] &lt;&lt; imm8[7:0])
        FI
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLQ - _mm512_maskz_sll_epi64

| VPSLLQ_ZMMu64_MASKmskw_ZMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        IF count[63:0] &gt; 63
            dst[i+63:i] := 0
        ELSE
            dst[i+63:i] := ZeroExtend64(a[i+63:i] &lt;&lt; count[63:0])
        FI
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLQ - _mm512_maskz_slli_epi64

| VPSLLQ_ZMMu64_MASKmskw_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" left by "imm8" while shifting in zeros, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        IF imm8[7:0] &gt; 63
            dst[i+63:i] := 0
        ELSE
            dst[i+63:i] := ZeroExtend64(a[i+63:i] &lt;&lt; imm8[7:0])
        FI
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLQ - _mm512_sll_epi64

| VPSLLQ_ZMMu64_MASKmskw_ZMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF count[63:0] &gt; 63
        dst[i+63:i] := 0
    ELSE
        dst[i+63:i] := ZeroExtend64(a[i+63:i] &lt;&lt; count[63:0])
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLQ - _mm512_slli_epi64

| VPSLLQ_ZMMu64_MASKmskw_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" left by "imm8" while shifting in zeros, and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF imm8[7:0] &gt; 63
        dst[i+63:i] := 0
    ELSE
        dst[i+63:i] := ZeroExtend64(a[i+63:i] &lt;&lt; imm8[7:0])
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLVD - _mm512_maskz_sllv_epi32

| VPSLLVD_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" left by the amount specified by the corresponding element in "count" while
shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        IF count[i+31:i] &lt; 32
            dst[i+31:i] := ZeroExtend32(a[i+31:i] &lt;&lt; count[i+31:i])
        ELSE
            dst[i+31:i] := 0
        FI
    ELSE
        dst[i+31:i] := 0
    FI    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLVQ - _mm512_mask_sllv_epi64

| VPSLLVQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" left by the amount specified by the corresponding element in "count" while
shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        IF count[i+63:i] &lt; 64
            dst[i+63:i] := ZeroExtend64(a[i+63:i] &lt;&lt; count[i+63:i])
        ELSE
            dst[i+63:i] := 0
        FI
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLVQ - _mm512_maskz_sllv_epi64

| VPSLLVQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" left by the amount specified by the corresponding element in "count" while
shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        IF count[i+63:i] &lt; 64
            dst[i+63:i] := ZeroExtend64(a[i+63:i] &lt;&lt; count[i+63:i])
        ELSE
            dst[i+63:i] := 0
        FI
    ELSE
        dst[i+63:i] := 0
    FI    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSLLVQ - _mm512_sllv_epi64

| VPSLLVQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" left by the amount specified by the corresponding element in "count" while
shifting in zeros, and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF count[i+63:i] &lt; 64
        dst[i+63:i] := ZeroExtend64(a[i+63:i] &lt;&lt; count[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAD - _mm512_mask_sra_epi32

| VPSRAD_ZMMu32_MASKmskw_ZMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" right by "count" while shifting in sign bits, and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        IF count[63:0] &gt; 31
            dst[i+31:i] := (a[i+31] ? 0xFFFFFFFF : 0x0)
        ELSE
            dst[i+31:i] := SignExtend32(a[i+31:i] &gt;&gt; count[63:0])
        FI
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAD - _mm512_maskz_sra_epi32

| VPSRAD_ZMMu32_MASKmskw_ZMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" right by "count" while shifting in sign bits, and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        IF count[63:0] &gt; 31
            dst[i+31:i] := (a[i+31] ? 0xFFFFFFFF : 0x0)
        ELSE
            dst[i+31:i] := SignExtend32(a[i+31:i] &gt;&gt; count[63:0])
        FI
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAD - _mm512_maskz_srai_epi32

| VPSRAD_ZMMu32_MASKmskw_ZMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" right by "imm8" while shifting in sign bits, and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        IF imm8[7:0] &gt; 31
            dst[i+31:i] := (a[i+31] ? 0xFFFFFFFF : 0x0)
        ELSE
            dst[i+31:i] := SignExtend32(a[i+31:i] &gt;&gt; imm8[7:0])
        FI
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAD - _mm512_sra_epi32

| VPSRAD_ZMMu32_MASKmskw_ZMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" right by "count" while shifting in sign bits, and store the results in
"dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF count[63:0] &gt; 31
        dst[i+31:i] := (a[i+31] ? 0xFFFFFFFF : 0x0)
    ELSE
        dst[i+31:i] := SignExtend32(a[i+31:i] &gt;&gt; count[63:0])
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAQ - _mm512_mask_sra_epi64

| VPSRAQ_ZMMu64_MASKmskw_ZMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by "count" while shifting in sign bits, and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        IF count[63:0] &gt; 63
            dst[i+63:i] := (a[i+63] ? 0xFFFFFFFFFFFFFFFF : 0x0)
        ELSE
            dst[i+63:i] := SignExtend64(a[i+63:i] &gt;&gt; count[63:0])
        FI
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAQ - _mm512_mask_srai_epi64

| VPSRAQ_ZMMu64_MASKmskw_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by "imm8" while shifting in sign bits, and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        IF imm8[7:0] &gt; 63
            dst[i+63:i] := (a[i+63] ? 0xFFFFFFFFFFFFFFFF : 0x0)
        ELSE
            dst[i+63:i] := SignExtend64(a[i+63:i] &gt;&gt; imm8[7:0])
        FI
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAQ - _mm512_maskz_sra_epi64

| VPSRAQ_ZMMu64_MASKmskw_ZMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by "count" while shifting in sign bits, and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        IF count[63:0] &gt; 63
            dst[i+63:i] := (a[i+63] ? 0xFFFFFFFFFFFFFFFF : 0x0)
        ELSE
            dst[i+63:i] := SignExtend64(a[i+63:i] &gt;&gt; count[63:0])
        FI
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAQ - _mm512_maskz_srai_epi64

| VPSRAQ_ZMMu64_MASKmskw_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by "imm8" while shifting in sign bits, and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        IF imm8[7:0] &gt; 63
            dst[i+63:i] := (a[i+63] ? 0xFFFFFFFFFFFFFFFF : 0x0)
        ELSE
            dst[i+63:i] := SignExtend64(a[i+63:i] &gt;&gt; imm8[7:0])
        FI
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAQ - _mm512_sra_epi64

| VPSRAQ_ZMMu64_MASKmskw_ZMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by "count" while shifting in sign bits, and store the results in
"dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF count[63:0] &gt; 63
        dst[i+63:i] := (a[i+63] ? 0xFFFFFFFFFFFFFFFF : 0x0)
    ELSE
        dst[i+63:i] := SignExtend64(a[i+63:i] &gt;&gt; count[63:0])
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAQ - _mm512_srai_epi64

| VPSRAQ_ZMMu64_MASKmskw_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by "imm8" while shifting in sign bits, and store the results in
"dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF imm8[7:0] &gt; 63
        dst[i+63:i] := (a[i+63] ? 0xFFFFFFFFFFFFFFFF : 0x0)
    ELSE
        dst[i+63:i] := SignExtend64(a[i+63:i] &gt;&gt; imm8[7:0])
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAVD - _mm512_maskz_srav_epi32

| VPSRAVD_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in sign bits, and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        IF count[i+31:i] &lt; 32
            dst[i+31:i] := SignExtend32(a[i+31:i] &gt;&gt; count[i+31:i])
        ELSE
            dst[i+31:i] := (a[i+31] ? 0xFFFFFFFF : 0)
        FI
    ELSE
        dst[i+31:i] := 0
    FI    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAVQ - _mm512_mask_srav_epi64

| VPSRAVQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in sign bits, and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        IF count[i+63:i] &lt; 64
            dst[i+63:i] := SignExtend64(a[i+63:i] &gt;&gt; count[i+63:i])
        ELSE
            dst[i+63:i] := (a[i+63] ? 0xFFFFFFFFFFFFFFFF : 0)
        FI
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAVQ - _mm512_maskz_srav_epi64

| VPSRAVQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in sign bits, and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        IF count[i+63:i] &lt; 64
            dst[i+63:i] := SignExtend64(a[i+63:i] &gt;&gt; count[i+63:i])
        ELSE
            dst[i+63:i] := (a[i+63] ? 0xFFFFFFFFFFFFFFFF : 0)
        FI
    ELSE
        dst[i+63:i] := 0
    FI    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRAVQ - _mm512_srav_epi64

| VPSRAVQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in sign bits, and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF count[i+63:i] &lt; 64
        dst[i+63:i] := SignExtend64(a[i+63:i] &gt;&gt; count[i+63:i])
    ELSE
        dst[i+63:i] := (a[i+63] ? 0xFFFFFFFFFFFFFFFF : 0)
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLD - _mm512_mask_srl_epi32

| VPSRLD_ZMMu32_MASKmskw_ZMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        IF count[63:0] &gt; 31
            dst[i+31:i] := 0
        ELSE
            dst[i+31:i] := ZeroExtend32(a[i+31:i] &gt;&gt; count[63:0])
        FI
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLD - _mm512_maskz_srl_epi32

| VPSRLD_ZMMu32_MASKmskw_ZMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        IF count[63:0] &gt; 31
            dst[i+31:i] := 0
        ELSE
            dst[i+31:i] := ZeroExtend32(a[i+31:i] &gt;&gt; count[63:0])
        FI
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLD - _mm512_maskz_srli_epi32

| VPSRLD_ZMMu32_MASKmskw_ZMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" right by "imm8" while shifting in zeros, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        IF imm8[7:0] &gt; 31
            dst[i+31:i] := 0
        ELSE
            dst[i+31:i] := ZeroExtend32(a[i+31:i] &gt;&gt; imm8[7:0])
        FI
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLD - _mm512_srl_epi32

| VPSRLD_ZMMu32_MASKmskw_ZMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF count[63:0] &gt; 31
        dst[i+31:i] := 0
    ELSE
        dst[i+31:i] := ZeroExtend32(a[i+31:i] &gt;&gt; count[63:0])
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLQ - _mm512_mask_srl_epi64

| VPSRLQ_ZMMu64_MASKmskw_ZMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        IF count[63:0] &gt; 63
            dst[i+63:i] := 0
        ELSE
            dst[i+63:i] := ZeroExtend64(a[i+63:i] &gt;&gt; count[63:0])
        FI
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLQ - _mm512_mask_srli_epi64

| VPSRLQ_ZMMu64_MASKmskw_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by "imm8" while shifting in zeros, and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        IF imm8[7:0] &gt; 63
            dst[i+63:i] := 0
        ELSE
            dst[i+63:i] := ZeroExtend64(a[i+63:i] &gt;&gt; imm8[7:0])
        FI
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLQ - _mm512_maskz_srl_epi64

| VPSRLQ_ZMMu64_MASKmskw_ZMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        IF count[63:0] &gt; 63
            dst[i+63:i] := 0
        ELSE
            dst[i+63:i] := ZeroExtend64(a[i+63:i] &gt;&gt; count[63:0])
        FI
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLQ - _mm512_maskz_srli_epi64

| VPSRLQ_ZMMu64_MASKmskw_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by "imm8" while shifting in zeros, and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        IF imm8[7:0] &gt; 63
            dst[i+63:i] := 0
        ELSE
            dst[i+63:i] := ZeroExtend64(a[i+63:i] &gt;&gt; imm8[7:0])
        FI
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLQ - _mm512_srl_epi64

| VPSRLQ_ZMMu64_MASKmskw_ZMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF count[63:0] &gt; 63
        dst[i+63:i] := 0
    ELSE
        dst[i+63:i] := ZeroExtend64(a[i+63:i] &gt;&gt; count[63:0])
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLQ - _mm512_srli_epi64

| VPSRLQ_ZMMu64_MASKmskw_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by "imm8" while shifting in zeros, and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF imm8[7:0] &gt; 63
        dst[i+63:i] := 0
    ELSE
        dst[i+63:i] := ZeroExtend64(a[i+63:i] &gt;&gt; imm8[7:0])
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLVD - _mm512_maskz_srlv_epi32

| VPSRLVD_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 32-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        IF count[i+31:i] &lt; 32
            dst[i+31:i] := ZeroExtend32(a[i+31:i] &gt;&gt; count[i+31:i])
        ELSE
            dst[i+31:i] := 0
        FI
    ELSE
        dst[i+31:i] := 0
    FI    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLVQ - _mm512_mask_srlv_epi64

| VPSRLVQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        IF count[i+63:i] &lt; 64
            dst[i+63:i] := ZeroExtend64(a[i+63:i] &gt;&gt; count[i+63:i])
        ELSE
            dst[i+63:i] := 0
        FI
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLVQ - _mm512_maskz_srlv_epi64

| VPSRLVQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        IF count[i+63:i] &lt; 64
            dst[i+63:i] := ZeroExtend64(a[i+63:i] &gt;&gt; count[i+63:i])
        ELSE
            dst[i+63:i] := 0
        FI
    ELSE
        dst[i+63:i] := 0
    FI    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSRLVQ - _mm512_srlv_epi64

| VPSRLVQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Shift packed 64-bit integers in "a" right by the amount specified by the corresponding element in "count"
while shifting in zeros, and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF count[i+63:i] &lt; 64
        dst[i+63:i] := ZeroExtend64(a[i+63:i] &gt;&gt; count[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBD - _mm512_maskz_sub_epi32

| VPSUBD_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed 32-bit integers in "b" from packed 32-bit integers in "a", and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] - b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBQ - _mm512_mask_sub_epi64

| VPSUBQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed 64-bit integers in "b" from packed 64-bit integers in "a", and store the results in "dst"
using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] - b[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBQ - _mm512_maskz_sub_epi64

| VPSUBQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed 64-bit integers in "b" from packed 64-bit integers in "a", and store the results in "dst"
using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] - b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSUBQ - _mm512_sub_epi64

| VPSUBQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed 64-bit integers in "b" from packed 64-bit integers in "a", and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := a[i+63:i] - b[i+63:i]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPTERNLOGD - _mm512_mask_ternarylogic_epi32

| VPTERNLOGD_ZMMu32_MASKmskw_ZMMu32_ZMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Bitwise ternary logic that provides the capability to implement any three-operand binary function; the
specific binary function is specified by value in "imm8". For each bit in each packed 32-bit integer, the
corresponding bit from "src", "a", and "b" are used to form a 3 bit index into "imm8", and the value at that
bit in "imm8" is written to the corresponding bit in "dst" using writemask "k" at 32-bit granularity (32-bit
elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        FOR h := 0 to 31
            index[2:0] := (src[i+h] &lt;&lt; 2) OR (a[i+h] &lt;&lt; 1) OR b[i+h]
            dst[i+h] := imm8[index[2:0]]
        ENDFOR
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPTERNLOGD - _mm512_maskz_ternarylogic_epi32

| VPTERNLOGD_ZMMu32_MASKmskw_ZMMu32_ZMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Bitwise ternary logic that provides the capability to implement any three-operand binary function; the
specific binary function is specified by value in "imm8". For each bit in each packed 32-bit integer, the
corresponding bit from "a", "b", and "c" are used to form a 3 bit index into "imm8", and the value at that bit
in "imm8" is written to the corresponding bit in "dst" using zeromask "k" at 32-bit granularity (32-bit
elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        FOR h := 0 to 31
            index[2:0] := (a[i+h] &lt;&lt; 2) OR (b[i+h] &lt;&lt; 1) OR c[i+h]
            dst[i+h] := imm8[index[2:0]]
        ENDFOR
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPTERNLOGD - _mm512_ternarylogic_epi32

| VPTERNLOGD_ZMMu32_MASKmskw_ZMMu32_ZMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Bitwise ternary logic that provides the capability to implement any three-operand binary function; the
specific binary function is specified by value in "imm8". For each bit in each packed 32-bit integer, the
corresponding bit from "a", "b", and "c" are used to form a 3 bit index into "imm8", and the value at that bit
in "imm8" is written to the corresponding bit in "dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    FOR h := 0 to 31
        index[2:0] := (a[i+h] &lt;&lt; 2) OR (b[i+h] &lt;&lt; 1) OR c[i+h]
        dst[i+h] := imm8[index[2:0]]
    ENDFOR
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPTERNLOGQ - _mm512_mask_ternarylogic_epi64

| VPTERNLOGQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Bitwise ternary logic that provides the capability to implement any three-operand binary function; the
specific binary function is specified by value in "imm8". For each bit in each packed 64-bit integer, the
corresponding bit from "src", "a", and "b" are used to form a 3 bit index into "imm8", and the value at that
bit in "imm8" is written to the corresponding bit in "dst" using writemask "k" at 64-bit granularity (64-bit
elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        FOR h := 0 to 63
            index[2:0] := (src[i+h] &lt;&lt; 2) OR (a[i+h] &lt;&lt; 1) OR b[i+h]
            dst[i+h] := imm8[index[2:0]]
        ENDFOR
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPTERNLOGQ - _mm512_maskz_ternarylogic_epi64

| VPTERNLOGQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Bitwise ternary logic that provides the capability to implement any three-operand binary function; the
specific binary function is specified by value in "imm8". For each bit in each packed 64-bit integer, the
corresponding bit from "a", "b", and "c" are used to form a 3 bit index into "imm8", and the value at that bit
in "imm8" is written to the corresponding bit in "dst" using zeromask "k" at 64-bit granularity (64-bit
elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        FOR h := 0 to 63
            index[2:0] := (a[i+h] &lt;&lt; 2) OR (b[i+h] &lt;&lt; 1) OR c[i+h]
            dst[i+h] := imm8[index[2:0]]
        ENDFOR
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPTERNLOGQ - _mm512_ternarylogic_epi64

| VPTERNLOGQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Bitwise ternary logic that provides the capability to implement any three-operand binary function; the
specific binary function is specified by value in "imm8". For each bit in each packed 64-bit integer, the
corresponding bit from "a", "b", and "c" are used to form a 3 bit index into "imm8", and the value at that bit
in "imm8" is written to the corresponding bit in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    FOR h := 0 to 63
        index[2:0] := (a[i+h] &lt;&lt; 2) OR (b[i+h] &lt;&lt; 1) OR c[i+h]
        dst[i+h] := imm8[index[2:0]]
    ENDFOR
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTMQ - _mm512_mask_test_epi64_mask

| VPTESTMQ_MASKmskw_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed 64-bit integers in "a" and "b", producing intermediate 64-bit values, and
set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is non-zero.

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k1[j]
        k[j] := ((a[i+63:i] AND b[i+63:i]) != 0) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTMQ - _mm512_test_epi64_mask

| VPTESTMQ_MASKmskw_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise AND of packed 64-bit integers in "a" and "b", producing intermediate 64-bit values, and
set the corresponding bit in result mask "k" if the intermediate value is non-zero.

[algorithm]

FOR j := 0 to 7
    i := j*64
    k[j] := ((a[i+63:i] AND b[i+63:i]) != 0) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTNMD - _mm512_mask_testn_epi32_mask

| VPTESTNMD_MASKmskw_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NAND of packed 32-bit integers in "a" and "b", producing intermediate 32-bit values, and
set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is zero.

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k1[j]
        k[j] := ((a[i+31:i] AND b[i+31:i]) == 0) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTNMD - _mm512_testn_epi32_mask

| VPTESTNMD_MASKmskw_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NAND of packed 32-bit integers in "a" and "b", producing intermediate 32-bit values, and
set the corresponding bit in result mask "k" if the intermediate value is zero.

[algorithm]

FOR j := 0 to 15
    i := j*32
    k[j] := ((a[i+31:i] AND b[i+31:i]) == 0) ? 1 : 0
ENDFOR
k[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTNMQ - _mm512_mask_testn_epi64_mask

| VPTESTNMQ_MASKmskw_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NAND of packed 64-bit integers in "a" and "b", producing intermediate 64-bit values, and
set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is zero.

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k1[j]
        k[j] := ((a[i+63:i] AND b[i+63:i]) == 0) ? 1 : 0
    ELSE
        k[j] := 0
    FI
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPTESTNMQ - _mm512_testn_epi64_mask

| VPTESTNMQ_MASKmskw_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise NAND of packed 64-bit integers in "a" and "b", producing intermediate 64-bit values, and
set the corresponding bit in result mask "k" if the intermediate value is zero.

[algorithm]

FOR j := 0 to 7
    i := j*64
    k[j] := ((a[i+63:i] AND b[i+63:i]) == 0) ? 1 : 0
ENDFOR
k[MAX:8] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKHDQ - _mm512_mask_unpackhi_epi32

| VPUNPCKHDQ_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 32-bit integers from the high half of each 128-bit lane in "a" and "b", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

DEFINE INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]) {
    dst[31:0] := src1[95:64] 
    dst[63:32] := src2[95:64] 
    dst[95:64] := src1[127:96] 
    dst[127:96] := src2[127:96] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_HIGH_DWORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_HIGH_DWORDS(a[511:384], b[511:384])
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKHDQ - _mm512_maskz_unpackhi_epi32

| VPUNPCKHDQ_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 32-bit integers from the high half of each 128-bit lane in "a" and "b", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]) {
    dst[31:0] := src1[95:64] 
    dst[63:32] := src2[95:64] 
    dst[95:64] := src1[127:96] 
    dst[127:96] := src2[127:96] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_HIGH_DWORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_HIGH_DWORDS(a[511:384], b[511:384])
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKHDQ - _mm512_unpackhi_epi32

| VPUNPCKHDQ_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 32-bit integers from the high half of each 128-bit lane in "a" and "b", and store the
results in "dst".

[algorithm]

DEFINE INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]) {
    dst[31:0] := src1[95:64] 
    dst[63:32] := src2[95:64] 
    dst[95:64] := src1[127:96] 
    dst[127:96] := src2[127:96] 
    RETURN dst[127:0]    
}
dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
dst[383:256] := INTERLEAVE_HIGH_DWORDS(a[383:256], b[383:256])
dst[511:384] := INTERLEAVE_HIGH_DWORDS(a[511:384], b[511:384])
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKHQDQ - _mm512_mask_unpackhi_epi64

| VPUNPCKHQDQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 64-bit integers from the high half of each 128-bit lane in "a" and "b", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

DEFINE INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]) {
    dst[63:0] := src1[127:64] 
    dst[127:64] := src2[127:64] 
    RETURN dst[127:0]    
}
tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_HIGH_QWORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_HIGH_QWORDS(a[511:384], b[511:384])
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKHQDQ - _mm512_maskz_unpackhi_epi64

| VPUNPCKHQDQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 64-bit integers from the high half of each 128-bit lane in "a" and "b", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]) {
    dst[63:0] := src1[127:64] 
    dst[127:64] := src2[127:64] 
    RETURN dst[127:0]    
}
tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_HIGH_QWORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_HIGH_QWORDS(a[511:384], b[511:384])
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKHQDQ - _mm512_unpackhi_epi64

| VPUNPCKHQDQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 64-bit integers from the high half of each 128-bit lane in "a" and "b", and store the
results in "dst".

[algorithm]

DEFINE INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]) {
    dst[63:0] := src1[127:64] 
    dst[127:64] := src2[127:64] 
    RETURN dst[127:0]    
}
dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
dst[383:256] := INTERLEAVE_HIGH_QWORDS(a[383:256], b[383:256])
dst[511:384] := INTERLEAVE_HIGH_QWORDS(a[511:384], b[511:384])
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKLDQ - _mm512_mask_unpacklo_epi32

| VPUNPCKLDQ_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 32-bit integers from the low half of each 128-bit lane in "a" and "b", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

DEFINE INTERLEAVE_DWORDS(src1[127:0], src2[127:0]) {
    dst[31:0] := src1[31:0] 
    dst[63:32] := src2[31:0] 
    dst[95:64] := src1[63:32] 
    dst[127:96] := src2[63:32] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_DWORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_DWORDS(a[511:384], b[511:384])
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKLDQ - _mm512_maskz_unpacklo_epi32

| VPUNPCKLDQ_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 32-bit integers from the low half of each 128-bit lane in "a" and "b", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_DWORDS(src1[127:0], src2[127:0]) {
    dst[31:0] := src1[31:0] 
    dst[63:32] := src2[31:0] 
    dst[95:64] := src1[63:32] 
    dst[127:96] := src2[63:32] 
    RETURN dst[127:0]    
}
tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_DWORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_DWORDS(a[511:384], b[511:384])
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKLDQ - _mm512_unpacklo_epi32

| VPUNPCKLDQ_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 32-bit integers from the low half of each 128-bit lane in "a" and "b", and store the
results in "dst".

[algorithm]

DEFINE INTERLEAVE_DWORDS(src1[127:0], src2[127:0]) {
    dst[31:0] := src1[31:0] 
    dst[63:32] := src2[31:0] 
    dst[95:64] := src1[63:32] 
    dst[127:96] := src2[63:32] 
    RETURN dst[127:0]    
}
dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
dst[383:256] := INTERLEAVE_DWORDS(a[383:256], b[383:256])
dst[511:384] := INTERLEAVE_DWORDS(a[511:384], b[511:384])
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKLQDQ - _mm512_mask_unpacklo_epi64

| VPUNPCKLQDQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 64-bit integers from the low half of each 128-bit lane in "a" and "b", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

DEFINE INTERLEAVE_QWORDS(src1[127:0], src2[127:0]) {
    dst[63:0] := src1[63:0] 
    dst[127:64] := src2[63:0] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_QWORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_QWORDS(a[511:384], b[511:384])
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKLQDQ - _mm512_maskz_unpacklo_epi64

| VPUNPCKLQDQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 64-bit integers from the low half of each 128-bit lane in "a" and "b", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_QWORDS(src1[127:0], src2[127:0]) {
    dst[63:0] := src1[63:0] 
    dst[127:64] := src2[63:0] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_QWORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_QWORDS(a[511:384], b[511:384])
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPUNPCKLQDQ - _mm512_unpacklo_epi64

| VPUNPCKLQDQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave 64-bit integers from the low half of each 128-bit lane in "a" and "b", and store the
results in "dst".

[algorithm]

DEFINE INTERLEAVE_QWORDS(src1[127:0], src2[127:0]) {
    dst[63:0] := src1[63:0] 
    dst[127:64] := src2[63:0] 
    RETURN dst[127:0]
}
dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
dst[383:256] := INTERLEAVE_QWORDS(a[383:256], b[383:256])
dst[511:384] := INTERLEAVE_QWORDS(a[511:384], b[511:384])
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPXORD - _mm512_maskz_xor_epi32

| VPXORD_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XOR of packed 32-bit integers in "a" and "b", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPXORQ - _mm512_maskz_xor_epi64

| VPXORQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the bitwise XOR of packed 64-bit integers in "a" and "b", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP14PD - _mm512_mask_rcp14_pd

| VRCP14PD_ZMMf64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in "a", and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set). The maximum relative error for this approximation is less than 2^-14.

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := (1.0 / a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP14PD - _mm512_maskz_rcp14_pd

| VRCP14PD_ZMMf64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in "a", and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set). The maximum relative error for this approximation is less than 2^-14.

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := (1.0 / a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP14PD - _mm512_rcp14_pd

| VRCP14PD_ZMMf64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in "a", and
store the results in "dst". The maximum relative error for this approximation is less than 2^-14.

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := (1.0 / a[i+63:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP14PS - _mm512_mask_rcp14_ps

| VRCP14PS_ZMMf32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in "a", and
store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit
is not set). The maximum relative error for this approximation is less than 2^-14.

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := (1.0 / a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP14PS - _mm512_maskz_rcp14_ps

| VRCP14PS_ZMMf32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in "a", and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set). The maximum relative error for this approximation is less than 2^-14.

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := (1.0 / a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP14PS - _mm512_rcp14_ps

| VRCP14PS_ZMMf32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in "a", and
store the results in "dst". The maximum relative error for this approximation is less than 2^-14.

[algorithm]

FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := (1.0 / a[i+31:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP14SD - _mm_mask_rcp14_sd

| VRCP14SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of the lower double-precision (64-bit) floating-point element in "b", store
the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0
is not set), and copy the upper element from "a" to the upper element of "dst". The maximum relative error for
this approximation is less than 2^-14.

[algorithm]

IF k[0]
    dst[63:0] := (1.0 / b[63:0])
ELSE
    dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP14SD - _mm_maskz_rcp14_sd

| VRCP14SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of the lower double-precision (64-bit) floating-point element in "b", store
the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not
set), and copy the upper element from "a" to the upper element of "dst". The maximum relative error for this
approximation is less than 2^-14.

[algorithm]

IF k[0]
    dst[63:0] := (1.0 / b[63:0])
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP14SD - _mm_rcp14_sd

| VRCP14SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of the lower double-precision (64-bit) floating-point element in "b", store
the result in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst".
The maximum relative error for this approximation is less than 2^-14.

[algorithm]

dst[63:0] := (1.0 / b[63:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP14SS - _mm_mask_rcp14_ss

| VRCP14SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of the lower single-precision (32-bit) floating-point element in "b", store
the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0
is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". The maximum relative
error for this approximation is less than 2^-14.

[algorithm]

IF k[0]
    dst[31:0] := (1.0 / b[31:0])
ELSE
    dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP14SS - _mm_maskz_rcp14_ss

| VRCP14SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of the lower single-precision (32-bit) floating-point element in "b", store
the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not
set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". The maximum relative error
for this approximation is less than 2^-14.

[algorithm]

IF k[0]
    dst[31:0] := (1.0 / b[31:0])
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRCP14SS - _mm_rcp14_ss

| VRCP14SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal of the lower single-precision (32-bit) floating-point element in "b", store
the result in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements
of "dst". The maximum relative error for this approximation is less than 2^-14.

[algorithm]

dst[31:0] := (1.0 / b[31:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRNDSCALEPD - _mm512_mask_roundscale_pd

| VRNDSCALEPD_ZMMf64_MASKmskw_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Round packed double-precision (64-bit) floating-point elements in "a" to the number of fraction bits specified
by "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set). [round_imm_note]

[algorithm]

DEFINE RoundScaleFP64(src1[63:0], imm8[7:0]) {
    m[63:0] := FP64(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[63:0] := POW(2.0, -m) * ROUND(POW(2.0, m) * src1[63:0], imm8[3:0])
    IF IsInf(tmp[63:0])
        tmp[63:0] := src1[63:0]
    FI
    RETURN tmp[63:0]
}
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := RoundScaleFP64(a[i+63:i], imm8[7:0])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRNDSCALEPD - _mm512_mask_roundscale_round_pd

| VRNDSCALEPD_ZMMf64_MASKmskw_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Round packed double-precision (64-bit) floating-point elements in "a" to the number of fraction bits specified
by "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set). [round_imm_note][sae_note]

[algorithm]

DEFINE RoundScaleFP64(src1[63:0], imm8[7:0]) {
    m[63:0] := FP64(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[63:0] := POW(2.0, -m) * ROUND(POW(2.0, m) * src1[63:0], imm8[3:0])
    IF IsInf(tmp[63:0])
        tmp[63:0] := src1[63:0]
    FI
    RETURN tmp[63:0]
}
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := RoundScaleFP64(a[i+63:i], imm8[7:0])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRNDSCALEPD - _mm512_maskz_roundscale_pd

| VRNDSCALEPD_ZMMf64_MASKmskw_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Round packed double-precision (64-bit) floating-point elements in "a" to the number of fraction bits specified
by "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set). [round_imm_note]

[algorithm]

DEFINE RoundScaleFP64(src1[63:0], imm8[7:0]) {
    m[63:0] := FP64(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[63:0] := POW(2.0, -m) * ROUND(POW(2.0, m) * src1[63:0], imm8[3:0])
    IF IsInf(tmp[63:0])
        tmp[63:0] := src1[63:0]
    FI
    RETURN tmp[63:0]
}
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := RoundScaleFP64(a[i+63:i], imm8[7:0])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRNDSCALEPD - _mm512_maskz_roundscale_round_pd

| VRNDSCALEPD_ZMMf64_MASKmskw_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Round packed double-precision (64-bit) floating-point elements in "a" to the number of fraction bits specified
by "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set). [round_imm_note][sae_note]

[algorithm]

DEFINE RoundScaleFP64(src1[63:0], imm8[7:0]) {
    m[63:0] := FP64(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[63:0] := POW(2.0, -m) * ROUND(POW(2.0, m) * src1[63:0], imm8[3:0])
    IF IsInf(tmp[63:0])
        tmp[63:0] := src1[63:0]
    FI
    RETURN tmp[63:0]
}
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := RoundScaleFP64(a[i+63:i], imm8[7:0])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRNDSCALEPD - _mm512_roundscale_pd

| VRNDSCALEPD_ZMMf64_MASKmskw_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Round packed double-precision (64-bit) floating-point elements in "a" to the number of fraction bits specified
by "imm8", and store the results in "dst". [round_imm_note]

[algorithm]

DEFINE RoundScaleFP64(src1[63:0], imm8[7:0]) {
    m[63:0] := FP64(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[63:0] := POW(2.0, -m) * ROUND(POW(2.0, m) * src1[63:0], imm8[3:0])
    IF IsInf(tmp[63:0])
        tmp[63:0] := src1[63:0]
    FI
    RETURN tmp[63:0]
}
FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := RoundScaleFP64(a[i+63:i], imm8[7:0])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRNDSCALEPD - _mm512_roundscale_round_pd

| VRNDSCALEPD_ZMMf64_MASKmskw_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Round packed double-precision (64-bit) floating-point elements in "a" to the number of fraction bits specified
by "imm8", and store the results in "dst". [round_imm_note][sae_note]

[algorithm]

DEFINE RoundScaleFP64(src1[63:0], imm8[7:0]) {
    m[63:0] := FP64(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[63:0] := POW(2.0, -m) * ROUND(POW(2.0, m) * src1[63:0], imm8[3:0])
    IF IsInf(tmp[63:0])
        tmp[63:0] := src1[63:0]
    FI
    RETURN tmp[63:0]
}
FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := RoundScaleFP64(a[i+63:i], imm8[7:0])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRNDSCALEPS - _mm512_mask_roundscale_ps

| VRNDSCALEPS_ZMMf32_MASKmskw_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Round packed single-precision (32-bit) floating-point elements in "a" to the number of fraction bits specified
by "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set). [round_imm_note]

[algorithm]

DEFINE RoundScaleFP32(src1[31:0], imm8[7:0]) {
    m[31:0] := FP32(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[31:0] := POW(FP32(2.0), -m) * ROUND(POW(FP32(2.0), m) * src1[31:0], imm8[3:0])
    IF IsInf(tmp[31:0])
        tmp[31:0] := src1[31:0]
    FI
    RETURN tmp[31:0]
}
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := RoundScaleFP32(a[i+31:i], imm8[7:0])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRNDSCALEPS - _mm512_mask_roundscale_round_ps

| VRNDSCALEPS_ZMMf32_MASKmskw_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Round packed single-precision (32-bit) floating-point elements in "a" to the number of fraction bits specified
by "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set). [round_imm_note][sae_note]

[algorithm]

DEFINE RoundScaleFP32(src1[31:0], imm8[7:0]) {
    m[31:0] := FP32(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[31:0] := POW(FP32(2.0), -m) * ROUND(POW(FP32(2.0), m) * src1[31:0], imm8[3:0])
    IF IsInf(tmp[31:0])
        tmp[31:0] := src1[31:0]
    FI
    RETURN tmp[31:0]
}
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := RoundScaleFP32(a[i+31:i], imm8[7:0])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRNDSCALEPS - _mm512_maskz_roundscale_ps

| VRNDSCALEPS_ZMMf32_MASKmskw_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Round packed single-precision (32-bit) floating-point elements in "a" to the number of fraction bits specified
by "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set). [round_imm_note]

[algorithm]

DEFINE RoundScaleFP32(src1[31:0], imm8[7:0]) {
    m[31:0] := FP32(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[31:0] := POW(FP32(2.0), -m) * ROUND(POW(FP32(2.0), m) * src1[31:0], imm8[3:0])
    IF IsInf(tmp[31:0])
        tmp[31:0] := src1[31:0]
    FI
    RETURN tmp[31:0]
}
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := RoundScaleFP32(a[i+31:i], imm8[7:0])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRNDSCALEPS - _mm512_maskz_roundscale_round_ps

| VRNDSCALEPS_ZMMf32_MASKmskw_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Round packed single-precision (32-bit) floating-point elements in "a" to the number of fraction bits specified
by "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set). [round_imm_note][sae_note]

[algorithm]

DEFINE RoundScaleFP32(src1[31:0], imm8[7:0]) {
    m[31:0] := FP32(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[31:0] := POW(FP32(2.0), -m) * ROUND(POW(FP32(2.0), m) * src1[31:0], imm8[3:0])
    IF IsInf(tmp[31:0])
        tmp[31:0] := src1[31:0]
    FI
    RETURN tmp[31:0]
}
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := RoundScaleFP32(a[i+31:i], imm8[7:0])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRNDSCALEPS - _mm512_roundscale_ps

| VRNDSCALEPS_ZMMf32_MASKmskw_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Round packed single-precision (32-bit) floating-point elements in "a" to the number of fraction bits specified
by "imm8", and store the results in "dst". [round_imm_note]

[algorithm]

DEFINE RoundScaleFP32(src1[31:0], imm8[7:0]) {
    m[31:0] := FP32(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[31:0] := POW(FP32(2.0), -m) * ROUND(POW(FP32(2.0), m) * src1[31:0], imm8[3:0])
    IF IsInf(tmp[31:0])
        tmp[31:0] := src1[31:0]
    FI
    RETURN tmp[31:0]
}
FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := RoundScaleFP32(a[i+31:i], imm8[7:0])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRNDSCALEPS - _mm512_roundscale_round_ps

| VRNDSCALEPS_ZMMf32_MASKmskw_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Round packed single-precision (32-bit) floating-point elements in "a" to the number of fraction bits specified
by "imm8", and store the results in "dst". [round_imm_note][sae_note]

[algorithm]

DEFINE RoundScaleFP32(src1[31:0], imm8[7:0]) {
    m[31:0] := FP32(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[31:0] := POW(FP32(2.0), -m) * ROUND(POW(FP32(2.0), m) * src1[31:0], imm8[3:0])
    IF IsInf(tmp[31:0])
        tmp[31:0] := src1[31:0]
    FI
    RETURN tmp[31:0]
}
FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := RoundScaleFP32(a[i+31:i], imm8[7:0])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRNDSCALESD - _mm_mask_roundscale_round_sd

| VRNDSCALESD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Round the lower double-precision (64-bit) floating-point element in "b" to the number of fraction bits
specified by "imm8", store the result in the lower element of "dst" using writemask "k" (the element is copied
from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
[round_imm_note][sae_note]

[algorithm]

DEFINE RoundScaleFP64(src1[63:0], imm8[7:0]) {
    m[63:0] := FP64(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[63:0] := POW(2.0, -m) * ROUND(POW(2.0, m) * src1[63:0], imm8[3:0])
    IF IsInf(tmp[63:0])
        tmp[63:0] := src1[63:0]
    FI
    RETURN tmp[63:0]
}
IF k[0]
    dst[63:0] := RoundScaleFP64(b[63:0], imm8[7:0])
ELSE
    dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRNDSCALESD - _mm_mask_roundscale_sd

| VRNDSCALESD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Round the lower double-precision (64-bit) floating-point element in "b" to the number of fraction bits
specified by "imm8", store the result in the lower element of "dst" using writemask "k" (the element is copied
from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
[round_imm_note]

[algorithm]

DEFINE RoundScaleFP64(src1[63:0], imm8[7:0]) {
    m[63:0] := FP64(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[63:0] := POW(2.0, -m) * ROUND(POW(2.0, m) * src1[63:0], imm8[3:0])
    IF IsInf(tmp[63:0])
        tmp[63:0] := src1[63:0]
    FI
    RETURN tmp[63:0]
}
IF k[0]
    dst[63:0] := RoundScaleFP64(b[63:0], imm8[7:0])
ELSE
    dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRNDSCALESD - _mm_maskz_roundscale_round_sd

| VRNDSCALESD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Round the lower double-precision (64-bit) floating-point element in "b" to the number of fraction bits
specified by "imm8", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed
out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
[round_imm_note][sae_note]

[algorithm]

DEFINE RoundScaleFP64(src1[63:0], imm8[7:0]) {
    m[63:0] := FP64(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[63:0] := POW(2.0, -m) * ROUND(POW(2.0, m) * src1[63:0], imm8[3:0])
    IF IsInf(tmp[63:0])
        tmp[63:0] := src1[63:0]
    FI
    RETURN tmp[63:0]
}
IF k[0]
    dst[63:0] := RoundScaleFP64(b[63:0], imm8[7:0])
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRNDSCALESD - _mm_maskz_roundscale_sd

| VRNDSCALESD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Round the lower double-precision (64-bit) floating-point element in "b" to the number of fraction bits
specified by "imm8", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed
out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
[round_imm_note]

[algorithm]

DEFINE RoundScaleFP64(src1[63:0], imm8[7:0]) {
    m[63:0] := FP64(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[63:0] := POW(2.0, -m) * ROUND(POW(2.0, m) * src1[63:0], imm8[3:0])
    IF IsInf(tmp[63:0])
        tmp[63:0] := src1[63:0]
    FI
    RETURN tmp[63:0]
}
IF k[0]
    dst[63:0] := RoundScaleFP64(b[63:0], imm8[7:0])
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRNDSCALESD - _mm_roundscale_round_sd

| VRNDSCALESD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Round the lower double-precision (64-bit) floating-point element in "b" to the number of fraction bits
specified by "imm8", store the result in the lower element of "dst", and copy the upper element from "a" to the
upper element of "dst". [round_imm_note][sae_note]

[algorithm]

DEFINE RoundScaleFP64(src1[63:0], imm8[7:0]) {
    m[63:0] := FP64(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[63:0] := POW(2.0, -m) * ROUND(POW(2.0, m) * src1[63:0], imm8[3:0])
    IF IsInf(tmp[63:0])
        tmp[63:0] := src1[63:0]
    FI
    RETURN tmp[63:0]
}
dst[63:0] := RoundScaleFP64(b[63:0], imm8[7:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRNDSCALESD - _mm_roundscale_sd

| VRNDSCALESD_XMMf64_MASKmskw_XMMf64_XMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Round the lower double-precision (64-bit) floating-point element in "b" to the number of fraction bits
specified by "imm8", store the result in the lower element of "dst", and copy the upper element from "a" to the
upper element of "dst". [round_imm_note]

[algorithm]

DEFINE RoundScaleFP64(src1[63:0], imm8[7:0]) {
    m[63:0] := FP64(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[63:0] := POW(2.0, -m) * ROUND(POW(2.0, m) * src1[63:0], imm8[3:0])
    IF IsInf(tmp[63:0])
        tmp[63:0] := src1[63:0]
    FI
    RETURN tmp[63:0]
}
dst[63:0] := RoundScaleFP64(b[63:0], imm8[7:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRNDSCALESS - _mm_mask_roundscale_round_ss

| VRNDSCALESS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Round the lower single-precision (32-bit) floating-point element in "b" to the number of fraction bits
specified by "imm8", store the result in the lower element of "dst" using writemask "k" (the element is copied
from "src" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of
"dst". [round_imm_note][sae_note]

[algorithm]

DEFINE RoundScaleFP32(src1[31:0], imm8[7:0]) {
    m[31:0] := FP32(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[31:0] := POW(FP32(2.0), -m) * ROUND(POW(FP32(2.0), m) * src1[31:0], imm8[3:0])
    IF IsInf(tmp[31:0])
        tmp[31:0] := src1[31:0]
    FI
    RETURN tmp[31:0]
}
IF k[0]
    dst[31:0] := RoundScaleFP32(b[31:0], imm8[7:0])
ELSE
    dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRNDSCALESS - _mm_mask_roundscale_ss

| VRNDSCALESS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Round the lower single-precision (32-bit) floating-point element in "b" to the number of fraction bits
specified by "imm8", store the result in the lower element of "dst" using writemask "k" (the element is copied
from "src" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of
"dst". [round_imm_note]

[algorithm]

DEFINE RoundScaleFP32(src1[31:0], imm8[7:0]) {
    m[31:0] := FP32(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[31:0] := POW(FP32(2.0), -m) * ROUND(POW(FP32(2.0), m) * src1[31:0], imm8[3:0])
    IF IsInf(tmp[31:0])
        tmp[31:0] := src1[31:0]
    FI
    RETURN tmp[31:0]
}
IF k[0]
    dst[31:0] := RoundScaleFP32(b[31:0], imm8[7:0])
ELSE
    dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRNDSCALESS - _mm_maskz_roundscale_round_ss

| VRNDSCALESS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Round the lower single-precision (32-bit) floating-point element in "b" to the number of fraction bits
specified by "imm8", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed
out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
[round_imm_note][sae_note]

[algorithm]

DEFINE RoundScaleFP32(src1[31:0], imm8[7:0]) {
    m[31:0] := FP32(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[31:0] := POW(FP32(2.0), -m) * ROUND(POW(FP32(2.0), m) * src1[31:0], imm8[3:0])
    IF IsInf(tmp[31:0])
        tmp[31:0] := src1[31:0]
    FI
    RETURN tmp[31:0]
}
IF k[0]
    dst[31:0] := RoundScaleFP32(b[31:0], imm8[7:0])
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRNDSCALESS - _mm_maskz_roundscale_ss

| VRNDSCALESS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Round the lower single-precision (32-bit) floating-point element in "b" to the number of fraction bits
specified by "imm8", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed
out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
[round_imm_note]

[algorithm]

DEFINE RoundScaleFP32(src1[31:0], imm8[7:0]) {
    m[31:0] := FP32(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[31:0] := POW(FP32(2.0), -m) * ROUND(POW(FP32(2.0), m) * src1[31:0], imm8[3:0])
    IF IsInf(tmp[31:0])
        tmp[31:0] := src1[31:0]
    FI
    RETURN tmp[31:0]
}
IF k[0]
    dst[31:0] := RoundScaleFP32(b[31:0], imm8[7:0])
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRNDSCALESS - _mm_roundscale_round_ss

| VRNDSCALESS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Round the lower single-precision (32-bit) floating-point element in "b" to the number of fraction bits
specified by "imm8", store the result in the lower element of "dst", and copy the upper 3 packed elements from
"a" to the upper elements of "dst". [round_imm_note][sae_note]

[algorithm]

DEFINE RoundScaleFP32(src1[31:0], imm8[7:0]) {
    m[31:0] := FP32(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[31:0] := POW(FP32(2.0), -m) * ROUND(POW(FP32(2.0), m) * src1[31:0], imm8[3:0])
    IF IsInf(tmp[31:0])
        tmp[31:0] := src1[31:0]
    FI
    RETURN tmp[31:0]
}
dst[31:0] := RoundScaleFP32(b[31:0], imm8[7:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRNDSCALESS - _mm_roundscale_ss

| VRNDSCALESS_XMMf32_MASKmskw_XMMf32_XMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Round the lower single-precision (32-bit) floating-point element in "b" to the number of fraction bits
specified by "imm8", store the result in the lower element of "dst", and copy the upper 3 packed elements from
"a" to the upper elements of "dst". [round_imm_note]

[algorithm]

DEFINE RoundScaleFP32(src1[31:0], imm8[7:0]) {
    m[31:0] := FP32(imm8[7:4]) // number of fraction bits after the binary point to be preserved
    tmp[31:0] := POW(FP32(2.0), -m) * ROUND(POW(FP32(2.0), m) * src1[31:0], imm8[3:0])
    IF IsInf(tmp[31:0])
        tmp[31:0] := src1[31:0]
    FI
    RETURN tmp[31:0]
}
dst[31:0] := RoundScaleFP32(b[31:0], imm8[7:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRSQRT14PD - _mm512_mask_rsqrt14_pd

| VRSQRT14PD_ZMMf64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of packed double-precision (64-bit) floating-point elements in
"a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding
mask bit is not set). The maximum relative error for this approximation is less than 2^-14.

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := (1.0 / SQRT(a[i+63:i]))
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRSQRT14PD - _mm512_maskz_rsqrt14_pd

| VRSQRT14PD_ZMMf64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of packed double-precision (64-bit) floating-point elements in
"a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit
is not set). The maximum relative error for this approximation is less than 2^-14.

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := (1.0 / SQRT(a[i+63:i]))
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRSQRT14PD - _mm512_rsqrt14_pd

| VRSQRT14PD_ZMMf64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of packed double-precision (64-bit) floating-point elements in
"a", and store the results in "dst". The maximum relative error for this approximation is less than 2^-14.

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := (1.0 / SQRT(a[i+63:i]))
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRSQRT14PS - _mm512_mask_rsqrt14_ps

| VRSQRT14PS_ZMMf32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of packed single-precision (32-bit) floating-point elements in
"a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding
mask bit is not set). The maximum relative error for this approximation is less than 2^-14.

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := (1.0 / SQRT(a[i+31:i]))
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRSQRT14PS - _mm512_maskz_rsqrt14_ps

| VRSQRT14PS_ZMMf32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of packed single-precision (32-bit) floating-point elements in
"a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit
is not set). The maximum relative error for this approximation is less than 2^-14.

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := (1.0 / SQRT(a[i+31:i]))
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRSQRT14PS - _mm512_rsqrt14_ps

| VRSQRT14PS_ZMMf32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of packed single-precision (32-bit) floating-point elements in
"a", and store the results in "dst". The maximum relative error for this approximation is less than 2^-14.

[algorithm]

FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := (1.0 / SQRT(a[i+31:i]))
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VRSQRT14SD - _mm_mask_rsqrt14_sd

| VRSQRT14SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of the lower double-precision (64-bit) floating-point element
in "b", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src"
when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst". The maximum
relative error for this approximation is less than 2^-14.

[algorithm]

IF k[0]
    dst[63:0] := (1.0 / SQRT(b[63:0]))
ELSE
    dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRSQRT14SD - _mm_maskz_rsqrt14_sd

| VRSQRT14SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of the lower double-precision (64-bit) floating-point element
in "b", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask
bit 0 is not set), and copy the upper element from "a" to the upper element of "dst". The maximum relative
error for this approximation is less than 2^-14.

[algorithm]

IF k[0]
    dst[63:0] := (1.0 / SQRT(b[63:0]))
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRSQRT14SD - _mm_rsqrt14_sd

| VRSQRT14SD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of the lower double-precision (64-bit) floating-point element
in "b", store the result in the lower element of "dst", and copy the upper element from "a" to the upper
element of "dst". The maximum relative error for this approximation is less than 2^-14.

[algorithm]

dst[63:0] := (1.0 / SQRT(b[63:0]))
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRSQRT14SS - _mm_mask_rsqrt14_ss

| VRSQRT14SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of the lower single-precision (32-bit) floating-point element
in "b", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src"
when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". The
maximum relative error for this approximation is less than 2^-14.

[algorithm]

IF k[0]
    dst[31:0] := (1.0 / SQRT(b[31:0]))
ELSE
    dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRSQRT14SS - _mm_maskz_rsqrt14_ss

| VRSQRT14SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of the lower single-precision (32-bit) floating-point element
in "b", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask
bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". The maximum
relative error for this approximation is less than 2^-14.

[algorithm]

IF k[0]
    dst[31:0] := (1.0 / SQRT(b[31:0]))
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VRSQRT14SS - _mm_rsqrt14_ss

| VRSQRT14SS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the approximate reciprocal square root of the lower single-precision (32-bit) floating-point element
in "b", store the result in the lower element of "dst", and copy the upper 3 packed elements from "a" to the
upper elements of "dst". The maximum relative error for this approximation is less than 2^-14.

[algorithm]

dst[31:0] := (1.0 / SQRT(b[31:0]))
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSCALEFPD - _mm512_mask_scalef_pd

| VSCALEFPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Scale the packed double-precision (64-bit) floating-point elements in "a" using values from "b", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

DEFINE SCALE(src1, src2) {
    IF (src2 == NaN)
        IF (src2 == SNaN)
            RETURN QNAN(src2)
        FI
    ELSE IF (src1 == NaN)
        IF (src1 == SNaN)
            RETURN QNAN(src1)
        FI
        IF (src2 != INF)
            RETURN QNAN(src1)
        FI
    ELSE
        tmp_src2 := src2
        tmp_src1 := src1
        IF (IS_DENORMAL(src2) AND MXCSR.DAZ)
            tmp_src2 := 0
        FI
        IF (IS_DENORMAL(src1) AND MXCSR.DAZ)
            tmp_src1 := 0
        FI
    FI
    dst[63:0] := tmp_src1[63:0] * POW(2.0, FLOOR(tmp_src2[63:0]))
    RETURN dst[63:0]
}
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSCALEFPD - _mm512_mask_scalef_round_pd

| VSCALEFPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Scale the packed double-precision (64-bit) floating-point elements in "a" using values from "b", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).
	[round_note]

[algorithm]

DEFINE SCALE(src1, src2) {
    IF (src2 == NaN)
        IF (src2 == SNaN)
            RETURN QNAN(src2)
        FI
    ELSE IF (src1 == NaN)
        IF (src1 == SNaN)
            RETURN QNAN(src1)
        FI
        IF (src2 != INF)
            RETURN QNAN(src1)
        FI
    ELSE
        tmp_src2 := src2
        tmp_src1 := src1
        IF (IS_DENORMAL(src2) AND MXCSR.DAZ)
            tmp_src2 := 0
        FI
        IF (IS_DENORMAL(src1) AND MXCSR.DAZ)
            tmp_src1 := 0
        FI
    FI
    dst[63:0] := tmp_src1[63:0] * POW(2.0, FLOOR(tmp_src2[63:0]))
    RETURN dst[63:0]
}
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSCALEFPD - _mm512_maskz_scalef_pd

| VSCALEFPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Scale the packed double-precision (64-bit) floating-point elements in "a" using values from "b", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE SCALE(src1, src2) {
    IF (src2 == NaN)
        IF (src2 == SNaN)
            RETURN QNAN(src2)
        FI
    ELSE IF (src1 == NaN)
        IF (src1 == SNaN)
            RETURN QNAN(src1)
        FI
        IF (src2 != INF)
            RETURN QNAN(src1)
        FI
    ELSE
        tmp_src2 := src2
        tmp_src1 := src1
        IF (IS_DENORMAL(src2) AND MXCSR.DAZ)
            tmp_src2 := 0
        FI
        IF (IS_DENORMAL(src1) AND MXCSR.DAZ)
            tmp_src1 := 0
        FI
    FI
    dst[63:0] := tmp_src1[63:0] * POW(2.0, FLOOR(tmp_src2[63:0]))
    RETURN dst[63:0]
}
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSCALEFPD - _mm512_maskz_scalef_round_pd

| VSCALEFPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Scale the packed double-precision (64-bit) floating-point elements in "a" using values from "b", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).
	[round_note]

[algorithm]

DEFINE SCALE(src1, src2) {
    IF (src2 == NaN)
        IF (src2 == SNaN)
            RETURN QNAN(src2)
        FI
    ELSE IF (src1 == NaN)
        IF (src1 == SNaN)
            RETURN QNAN(src1)
        FI
        IF (src2 != INF)
            RETURN QNAN(src1)
        FI
    ELSE
        tmp_src2 := src2
        tmp_src1 := src1
        IF (IS_DENORMAL(src2) AND MXCSR.DAZ)
            tmp_src2 := 0
        FI
        IF (IS_DENORMAL(src1) AND MXCSR.DAZ)
            tmp_src1 := 0
        FI
    FI
    dst[63:0] := tmp_src1[63:0] * POW(2.0, FLOOR(tmp_src2[63:0]))
    RETURN dst[63:0]
}
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSCALEFPD - _mm512_scalef_pd

| VSCALEFPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Scale the packed double-precision (64-bit) floating-point elements in "a" using values from "b", and store the
results in "dst".

[algorithm]

DEFINE SCALE(src1, src2) {
    IF (src2 == NaN)
        IF (src2 == SNaN)
            RETURN QNAN(src2)
        FI
    ELSE IF (src1 == NaN)
        IF (src1 == SNaN)
            RETURN QNAN(src1)
        FI
        IF (src2 != INF)
            RETURN QNAN(src1)
        FI
    ELSE
        tmp_src2 := src2
        tmp_src1 := src1
        IF (IS_DENORMAL(src2) AND MXCSR.DAZ)
            tmp_src2 := 0
        FI
        IF (IS_DENORMAL(src1) AND MXCSR.DAZ)
            tmp_src1 := 0
        FI
    FI
    dst[63:0] := tmp_src1[63:0] * POW(2.0, FLOOR(tmp_src2[63:0]))
    RETURN dst[63:0]
}
FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSCALEFPD - _mm512_scalef_round_pd

| VSCALEFPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Scale the packed double-precision (64-bit) floating-point elements in "a" using values from "b", and store the
results in "dst".
	[round_note]

[algorithm]

DEFINE SCALE(src1, src2) {
    IF (src2 == NaN)
        IF (src2 == SNaN)
            RETURN QNAN(src2)
        FI
    ELSE IF (src1 == NaN)
        IF (src1 == SNaN)
            RETURN QNAN(src1)
        FI
        IF (src2 != INF)
            RETURN QNAN(src1)
        FI
    ELSE
        tmp_src2 := src2
        tmp_src1 := src1
        IF (IS_DENORMAL(src2) AND MXCSR.DAZ)
            tmp_src2 := 0
        FI
        IF (IS_DENORMAL(src1) AND MXCSR.DAZ)
            tmp_src1 := 0
        FI
    FI
    dst[63:0] := tmp_src1[63:0] * POW(2.0, FLOOR(tmp_src2[63:0]))
    RETURN dst[63:0]
}
FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSCALEFPS - _mm512_mask_scalef_ps

| VSCALEFPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Scale the packed single-precision (32-bit) floating-point elements in "a" using values from "b", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

DEFINE SCALE(src1, src2) {
    IF (src2 == NaN)
        IF (src2 == SNaN)
            RETURN QNAN(src2)
        FI
    ELSE IF (src1 == NaN)
        IF (src1 == SNaN)
            RETURN QNAN(src1)
        FI
        IF (src2 != INF)
            RETURN QNAN(src1)
        FI
    ELSE
        tmp_src2 := src2
        tmp_src1 := src1
        IF (IS_DENORMAL(src2) AND MXCSR.DAZ)
            tmp_src2 := 0
        FI
        IF (IS_DENORMAL(src1) AND MXCSR.DAZ)
            tmp_src1 := 0
        FI
    FI
    dst[31:0] := tmp_src1[31:0] * POW(2.0, FLOOR(tmp_src2[31:0]))
    RETURN dst[31:0]
}
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSCALEFPS - _mm512_mask_scalef_round_ps

| VSCALEFPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Scale the packed single-precision (32-bit) floating-point elements in "a" using values from "b", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).
	[round_note]

[algorithm]

DEFINE SCALE(src1, src2) {
    IF (src2 == NaN)
        IF (src2 == SNaN)
            RETURN QNAN(src2)
        FI
    ELSE IF (src1 == NaN)
        IF (src1 == SNaN)
            RETURN QNAN(src1)
        FI
        IF (src2 != INF)
            RETURN QNAN(src1)
        FI
    ELSE
        tmp_src2 := src2
        tmp_src1 := src1
        IF (IS_DENORMAL(src2) AND MXCSR.DAZ)
            tmp_src2 := 0
        FI
        IF (IS_DENORMAL(src1) AND MXCSR.DAZ)
            tmp_src1 := 0
        FI
    FI
    dst[31:0] := tmp_src1[31:0] * POW(2.0, FLOOR(tmp_src2[31:0]))
    RETURN dst[31:0]
}
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSCALEFPS - _mm512_maskz_scalef_ps

| VSCALEFPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Scale the packed single-precision (32-bit) floating-point elements in "a" using values from "b", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE SCALE(src1, src2) {
    IF (src2 == NaN)
        IF (src2 == SNaN)
            RETURN QNAN(src2)
        FI
    ELSE IF (src1 == NaN)
        IF (src1 == SNaN)
            RETURN QNAN(src1)
        FI
        IF (src2 != INF)
            RETURN QNAN(src1)
        FI
    ELSE
        tmp_src2 := src2
        tmp_src1 := src1
        IF (IS_DENORMAL(src2) AND MXCSR.DAZ)
            tmp_src2 := 0
        FI
        IF (IS_DENORMAL(src1) AND MXCSR.DAZ)
            tmp_src1 := 0
        FI
    FI
    dst[31:0] := tmp_src1[31:0] * POW(2.0, FLOOR(tmp_src2[31:0]))
    RETURN dst[31:0]
}
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSCALEFPS - _mm512_maskz_scalef_round_ps

| VSCALEFPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Scale the packed single-precision (32-bit) floating-point elements in "a" using values from "b", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).
	[round_note]

[algorithm]

DEFINE SCALE(src1, src2) {
    IF (src2 == NaN)
        IF (src2 == SNaN)
            RETURN QNAN(src2)
        FI
    ELSE IF (src1 == NaN)
        IF (src1 == SNaN)
            RETURN QNAN(src1)
        FI
        IF (src2 != INF)
            RETURN QNAN(src1)
        FI
    ELSE
        tmp_src2 := src2
        tmp_src1 := src1
        IF (IS_DENORMAL(src2) AND MXCSR.DAZ)
            tmp_src2 := 0
        FI
        IF (IS_DENORMAL(src1) AND MXCSR.DAZ)
            tmp_src1 := 0
        FI
    FI
    dst[31:0] := tmp_src1[31:0] * POW(2.0, FLOOR(tmp_src2[31:0]))
    RETURN dst[31:0]
}
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSCALEFPS - _mm512_scalef_ps

| VSCALEFPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Scale the packed single-precision (32-bit) floating-point elements in "a" using values from "b", and store the
results in "dst".

[algorithm]

DEFINE SCALE(src1, src2) {
    IF (src2 == NaN)
        IF (src2 == SNaN)
            RETURN QNAN(src2)
        FI
    ELSE IF (src1 == NaN)
        IF (src1 == SNaN)
            RETURN QNAN(src1)
        FI
        IF (src2 != INF)
            RETURN QNAN(src1)
        FI
    ELSE
        tmp_src2 := src2
        tmp_src1 := src1
        IF (IS_DENORMAL(src2) AND MXCSR.DAZ)
            tmp_src2 := 0
        FI
        IF (IS_DENORMAL(src1) AND MXCSR.DAZ)
            tmp_src1 := 0
        FI
    FI
    dst[31:0] := tmp_src1[31:0] * POW(2.0, FLOOR(tmp_src2[31:0]))
    RETURN dst[31:0]
}
FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSCALEFPS - _mm512_scalef_round_ps

| VSCALEFPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Scale the packed single-precision (32-bit) floating-point elements in "a" using values from "b", and store the
results in "dst".
	[round_note]

[algorithm]

DEFINE SCALE(src1, src2) {
    IF (src2 == NaN)
        IF (src2 == SNaN)
            RETURN QNAN(src2)
        FI
    ELSE IF (src1 == NaN)
        IF (src1 == SNaN)
            RETURN QNAN(src1)
        FI
        IF (src2 != INF)
            RETURN QNAN(src1)
        FI
    ELSE
        tmp_src2 := src2
        tmp_src1 := src1
        IF (IS_DENORMAL(src2) AND MXCSR.DAZ)
            tmp_src2 := 0
        FI
        IF (IS_DENORMAL(src1) AND MXCSR.DAZ)
            tmp_src1 := 0
        FI
    FI
    dst[31:0] := tmp_src1[31:0] * POW(2.0, FLOOR(tmp_src2[31:0]))
    RETURN dst[31:0]
}
FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSCALEFSD - _mm_mask_scalef_round_sd

| VSCALEFSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Scale the packed double-precision (64-bit) floating-point elements in "a" using values from "b", store the
result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is
not set), and copy the upper element from "a" to the upper element of "dst".
	[round_note]

[algorithm]

DEFINE SCALE(src1, src2) {
    IF (src2 == NaN)
        IF (src2 == SNaN)
            RETURN QNAN(src2)
        FI
    ELSE IF (src1 == NaN)
        IF (src1 == SNaN)
            RETURN QNAN(src1)
        FI
        IF (src2 != INF)
            RETURN QNAN(src1)
        FI
    ELSE
        tmp_src2 := src2
        tmp_src1 := src1
        IF (IS_DENORMAL(src2) AND MXCSR.DAZ)
            tmp_src2 := 0
        FI
        IF (IS_DENORMAL(src1) AND MXCSR.DAZ)
            tmp_src1 := 0
        FI
    FI
    dst[63:0] := tmp_src1[63:0] * POW(2.0, FLOOR(tmp_src2[63:0]))
    RETURN dst[63:0]
}
IF k[0]
    dst[63:0] := SCALE(a[63:0], b[63:0])
ELSE
    dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSCALEFSD - _mm_mask_scalef_sd

| VSCALEFSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Scale the packed double-precision (64-bit) floating-point elements in "a" using values from "b", store the
result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is
not set), and copy the upper element from "a" to the upper element of "dst".

[algorithm]

DEFINE SCALE(src1, src2) {
    IF (src2 == NaN)
        IF (src2 == SNaN)
            RETURN QNAN(src2)
        FI
    ELSE IF (src1 == NaN)
        IF (src1 == SNaN)
            RETURN QNAN(src1)
        FI
        IF (src2 != INF)
            RETURN QNAN(src1)
        FI
    ELSE
        tmp_src2 := src2
        tmp_src1 := src1
        IF (IS_DENORMAL(src2) AND MXCSR.DAZ)
            tmp_src2 := 0
        FI
        IF (IS_DENORMAL(src1) AND MXCSR.DAZ)
            tmp_src1 := 0
        FI
    FI
    dst[63:0] := tmp_src1[63:0] * POW(2.0, FLOOR(tmp_src2[63:0]))
    RETURN dst[63:0]
}
IF k[0]
    dst[63:0] := SCALE(a[63:0], b[63:0])
ELSE
    dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSCALEFSD - _mm_maskz_scalef_round_sd

| VSCALEFSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Scale the packed double-precision (64-bit) floating-point elements in "a" using values from "b", store the
result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set),
and copy the upper element from "a" to the upper element of "dst".
	[round_note]

[algorithm]

DEFINE SCALE(src1, src2) {
    IF (src2 == NaN)
        IF (src2 == SNaN)
            RETURN QNAN(src2)
        FI
    ELSE IF (src1 == NaN)
        IF (src1 == SNaN)
            RETURN QNAN(src1)
        FI
        IF (src2 != INF)
            RETURN QNAN(src1)
        FI
    ELSE
        tmp_src2 := src2
        tmp_src1 := src1
        IF (IS_DENORMAL(src2) AND MXCSR.DAZ)
            tmp_src2 := 0
        FI
        IF (IS_DENORMAL(src1) AND MXCSR.DAZ)
            tmp_src1 := 0
        FI
    FI
    dst[63:0] := tmp_src1[63:0] * POW(2.0, FLOOR(tmp_src2[63:0]))
    RETURN dst[63:0]
}
IF k[0]
    dst[63:0] := SCALE(a[63:0], b[63:0])
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSCALEFSD - _mm_maskz_scalef_sd

| VSCALEFSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Scale the packed double-precision (64-bit) floating-point elements in "a" using values from "b", store the
result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set),
and copy the upper element from "a" to the upper element of "dst".

[algorithm]

DEFINE SCALE(src1, src2) {
    IF (src2 == NaN)
        IF (src2 == SNaN)
            RETURN QNAN(src2)
        FI
    ELSE IF (src1 == NaN)
        IF (src1 == SNaN)
            RETURN QNAN(src1)
        FI
        IF (src2 != INF)
            RETURN QNAN(src1)
        FI
    ELSE
        tmp_src2 := src2
        tmp_src1 := src1
        IF (IS_DENORMAL(src2) AND MXCSR.DAZ)
            tmp_src2 := 0
        FI
        IF (IS_DENORMAL(src1) AND MXCSR.DAZ)
            tmp_src1 := 0
        FI
    FI
    dst[63:0] := tmp_src1[63:0] * POW(2.0, FLOOR(tmp_src2[63:0]))
    RETURN dst[63:0]
}
IF k[0]
    dst[63:0] := SCALE(a[63:0], b[63:0])
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSCALEFSD - _mm_scalef_round_sd

| VSCALEFSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Scale the packed double-precision (64-bit) floating-point elements in "a" using values from "b", store the
result in the lower element of "dst", and copy the upper element from "a" to the upper element of
"dst".
	[round_note]

[algorithm]

DEFINE SCALE(src1, src2) {
    IF (src2 == NaN)
        IF (src2 == SNaN)
            RETURN QNAN(src2)
        FI
    ELSE IF (src1 == NaN)
        IF (src1 == SNaN)
            RETURN QNAN(src1)
        FI
        IF (src2 != INF)
            RETURN QNAN(src1)
        FI
    ELSE
        tmp_src2 := src2
        tmp_src1 := src1
        IF (IS_DENORMAL(src2) AND MXCSR.DAZ)
            tmp_src2 := 0
        FI
        IF (IS_DENORMAL(src1) AND MXCSR.DAZ)
            tmp_src1 := 0
        FI
    FI
    dst[63:0] := tmp_src1[63:0] * POW(2.0, FLOOR(tmp_src2[63:0]))
    RETURN dst[63:0]
}
dst[63:0] := SCALE(a[63:0], b[63:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSCALEFSD - _mm_scalef_sd

| VSCALEFSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Scale the packed double-precision (64-bit) floating-point elements in "a" using values from "b", store the
result in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst".

[algorithm]

DEFINE SCALE(src1, src2) {
    IF (src2 == NaN)
        IF (src2 == SNaN)
            RETURN QNAN(src2)
        FI
    ELSE IF (src1 == NaN)
        IF (src1 == SNaN)
            RETURN QNAN(src1)
        FI
        IF (src2 != INF)
            RETURN QNAN(src1)
        FI
    ELSE
        tmp_src2 := src2
        tmp_src1 := src1
        IF (IS_DENORMAL(src2) AND MXCSR.DAZ)
            tmp_src2 := 0
        FI
        IF (IS_DENORMAL(src1) AND MXCSR.DAZ)
            tmp_src1 := 0
        FI
    FI
    dst[63:0] := tmp_src1[63:0] * POW(2.0, FLOOR(tmp_src2[63:0]))
    RETURN dst[63:0]
}
dst[63:0] := SCALE(a[63:0], b[63:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSCALEFSS - _mm_mask_scalef_round_ss

| VSCALEFSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Scale the packed single-precision (32-bit) floating-point elements in "a" using values from "b", store the
result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is
not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
	[round_note]

[algorithm]

DEFINE SCALE(src1, src2) {
    IF (src2 == NaN)
        IF (src2 == SNaN)
            RETURN QNAN(src2)
        FI
    ELSE IF (src1 == NaN)
        IF (src1 == SNaN)
            RETURN QNAN(src1)
        FI
        IF (src2 != INF)
            RETURN QNAN(src1)
        FI
    ELSE
        tmp_src2 := src2
        tmp_src1 := src1
        IF (IS_DENORMAL(src2) AND MXCSR.DAZ)
            tmp_src2 := 0
        FI
        IF (IS_DENORMAL(src1) AND MXCSR.DAZ)
            tmp_src1 := 0
        FI
    FI
    dst[31:0] := tmp_src1[31:0] * POW(2.0, FLOOR(tmp_src2[31:0]))
    RETURN dst[63:0]
}
IF k[0]
    dst[31:0] := SCALE(a[31:0], b[31:0])
ELSE
    dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSCALEFSS - _mm_mask_scalef_ss

| VSCALEFSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Scale the packed single-precision (32-bit) floating-point elements in "a" using values from "b", store the
result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is
not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".

[algorithm]

DEFINE SCALE(src1, src2) {
    IF (src2 == NaN)
        IF (src2 == SNaN)
            RETURN QNAN(src2)
        FI
    ELSE IF (src1 == NaN)
        IF (src1 == SNaN)
            RETURN QNAN(src1)
        FI
        IF (src2 != INF)
            RETURN QNAN(src1)
        FI
    ELSE
        tmp_src2 := src2
        tmp_src1 := src1
        IF (IS_DENORMAL(src2) AND MXCSR.DAZ)
            tmp_src2 := 0
        FI
        IF (IS_DENORMAL(src1) AND MXCSR.DAZ)
            tmp_src1 := 0
        FI
    FI
    dst[31:0] := tmp_src1[31:0] * POW(2.0, FLOOR(tmp_src2[31:0]))
    RETURN dst[63:0]
}
IF k[0]
    dst[31:0] := SCALE(a[31:0], b[31:0])
ELSE
    dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSCALEFSS - _mm_maskz_scalef_round_ss

| VSCALEFSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Scale the packed single-precision (32-bit) floating-point elements in "a" using values from "b", store the
result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set),
and copy the upper 3 packed elements from "a" to the upper elements of "dst".
	[round_note]

[algorithm]

DEFINE SCALE(src1, src2) {
    IF (src2 == NaN)
        IF (src2 == SNaN)
            RETURN QNAN(src2)
        FI
    ELSE IF (src1 == NaN)
        IF (src1 == SNaN)
            RETURN QNAN(src1)
        FI
        IF (src2 != INF)
            RETURN QNAN(src1)
        FI
    ELSE
        tmp_src2 := src2
        tmp_src1 := src1
        IF (IS_DENORMAL(src2) AND MXCSR.DAZ)
            tmp_src2 := 0
        FI
        IF (IS_DENORMAL(src1) AND MXCSR.DAZ)
            tmp_src1 := 0
        FI
    FI
    dst[31:0] := tmp_src1[31:0] * POW(2.0, FLOOR(tmp_src2[31:0]))
    RETURN dst[63:0]
}
IF k[0]
    dst[31:0] := SCALE(a[31:0], b[31:0])
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSCALEFSS - _mm_maskz_scalef_ss

| VSCALEFSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Scale the packed single-precision (32-bit) floating-point elements in "a" using values from "b", store the
result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set),
and copy the upper 3 packed elements from "a" to the upper elements of "dst".

[algorithm]

DEFINE SCALE(src1, src2) {
    IF (src2 == NaN)
        IF (src2 == SNaN)
            RETURN QNAN(src2)
        FI
    ELSE IF (src1 == NaN)
        IF (src1 == SNaN)
            RETURN QNAN(src1)
        FI
        IF (src2 != INF)
            RETURN QNAN(src1)
        FI
    ELSE
        tmp_src2 := src2
        tmp_src1 := src1
        IF (IS_DENORMAL(src2) AND MXCSR.DAZ)
            tmp_src2 := 0
        FI
        IF (IS_DENORMAL(src1) AND MXCSR.DAZ)
            tmp_src1 := 0
        FI
    FI
    dst[31:0] := tmp_src1[31:0] * POW(2.0, FLOOR(tmp_src2[31:0]))
    RETURN dst[63:0]
}
IF k[0]
    dst[31:0] := SCALE(a[31:0], b[31:0])
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSCALEFSS - _mm_scalef_round_ss

| VSCALEFSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Scale the packed single-precision (32-bit) floating-point elements in "a" using values from "b", store the
result in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of
"dst".
	[round_note]

[algorithm]

DEFINE SCALE(src1, src2) {
    IF (src2 == NaN)
        IF (src2 == SNaN)
            RETURN QNAN(src2)
        FI
    ELSE IF (src1 == NaN)
        IF (src1 == SNaN)
            RETURN QNAN(src1)
        FI
        IF (src2 != INF)
            RETURN QNAN(src1)
        FI
    ELSE
        tmp_src2 := src2
        tmp_src1 := src1
        IF (IS_DENORMAL(src2) AND MXCSR.DAZ)
            tmp_src2 := 0
        FI
        IF (IS_DENORMAL(src1) AND MXCSR.DAZ)
            tmp_src1 := 0
        FI
    FI
    dst[31:0] := tmp_src1[31:0] * POW(2.0, FLOOR(tmp_src2[31:0]))
    RETURN dst[63:0]
}
dst[31:0] := SCALE(a[31:0], b[31:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSCALEFSS - _mm_scalef_ss

| VSCALEFSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Scale the packed single-precision (32-bit) floating-point elements in "a" using values from "b", store the
result in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of
"dst".

[algorithm]

DEFINE SCALE(src1, src2) {
    IF (src2 == NaN)
        IF (src2 == SNaN)
            RETURN QNAN(src2)
        FI
    ELSE IF (src1 == NaN)
        IF (src1 == SNaN)
            RETURN QNAN(src1)
        FI
        IF (src2 != INF)
            RETURN QNAN(src1)
        FI
    ELSE
        tmp_src2 := src2
        tmp_src1 := src1
        IF (IS_DENORMAL(src2) AND MXCSR.DAZ)
            tmp_src2 := 0
        FI
        IF (IS_DENORMAL(src1) AND MXCSR.DAZ)
            tmp_src1 := 0
        FI
    FI
    dst[31:0] := tmp_src1[31:0] * POW(2.0, FLOOR(tmp_src2[31:0]))
    RETURN dst[63:0]
}
dst[31:0] := SCALE(a[31:0], b[31:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSCATTERDPD - _mm512_i32scatter_pd

| VSCATTERDPD_MEMf64_MASKmskw_ZMMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Scatter double-precision (64-bit) floating-point elements from "a" into memory using 32-bit indices. 64-bit
elements are stored at addresses starting at "base_addr" and offset by each 32-bit element in "vindex" (each
index is scaled by the factor in "scale"). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 7
    i := j*64
    m := j*32
    addr := base_addr + SignExtend64(vindex[m+31:m]) * ZeroExtend64(scale) * 8
    MEM[addr+63:addr] := a[i+63:i]
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VSCATTERDPD - _mm512_mask_i32scatter_pd

| VSCATTERDPD_MEMf64_MASKmskw_ZMMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Scatter double-precision (64-bit) floating-point elements from "a" into memory using 32-bit indices. 64-bit
elements are stored at addresses starting at "base_addr" and offset by each 32-bit element in "vindex" (each
index is scaled by the factor in "scale") subject to mask "k" (elements are not stored when the corresponding
mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 7
    i := j*64
    m := j*32
    IF k[j]
        addr := base_addr + SignExtend64(vindex[m+31:m]) * ZeroExtend64(scale) * 8
        MEM[addr+63:addr] := a[i+63:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VSCATTERQPD - _mm512_i64scatter_pd

| VSCATTERQPD_MEMf64_MASKmskw_ZMMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Scatter double-precision (64-bit) floating-point elements from "a" into memory using 64-bit indices. 64-bit
elements are stored at addresses starting at "base_addr" and offset by each 64-bit element in "vindex" (each
index is scaled by the factor in "scale"). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 7
    i := j*64
    m := j*64
    addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
    MEM[addr+63:addr] := a[i+63:i]
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VSCATTERQPD - _mm512_mask_i64scatter_pd

| VSCATTERQPD_MEMf64_MASKmskw_ZMMf64_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Scatter double-precision (64-bit) floating-point elements from "a" into memory using 64-bit indices. 64-bit
elements are stored at addresses starting at "base_addr" and offset by each 64-bit element in "vindex" (each
index is scaled by the factor in "scale") subject to mask "k" (elements are not stored when the corresponding
mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 7
    i := j*64
    m := j*64
    IF k[j]
        addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
        MEM[addr+63:addr] := a[i+63:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VSCATTERQPS - _mm512_i64scatter_ps

| VSCATTERQPS_MEMf32_MASKmskw_YMMf32_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Scatter single-precision (32-bit) floating-point elements from "a" into memory using 64-bit indices. 32-bit
elements are stored at addresses starting at "base_addr" and offset by each 64-bit element in "vindex" (each
index is scaled by the factor in "scale") subject to mask "k" (elements are not stored when the corresponding
mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 7
    i := j*32
    m := j*64
    addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
    MEM[addr+31:addr] := a[i+31:i]
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VSCATTERQPS - _mm512_mask_i64scatter_ps

| VSCATTERQPS_MEMf32_MASKmskw_YMMf32_AVX512_VL512

--------------------------------------------------------------------------------------------------------------
Scatter single-precision (32-bit) floating-point elements from "a" into memory using 64-bit indices. 32-bit
elements are stored at addresses starting at "base_addr" and offset by each 64-bit element in "vindex" (each
index is scaled by the factor in "scale") subject to mask "k" (elements are not stored when the corresponding
mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 7
    i := j*32
    m := j*64
    IF k[j]
        addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
        MEM[addr+31:addr] := a[i+31:i]
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VSHUFF32X4 - _mm512_mask_shuffle_f32x4

| VSHUFF32X4_ZMMf32_MASKmskw_ZMMf32_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 128-bits (composed of 4 single-precision (32-bit) floating-point elements) selected by "imm8" from "a"
and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[127:0] := src[127:0]
    1:    tmp[127:0] := src[255:128]
    2:    tmp[127:0] := src[383:256]
    3:    tmp[127:0] := src[511:384]
    ESAC
    RETURN tmp[127:0]
}
tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSHUFF32X4 - _mm512_maskz_shuffle_f32x4

| VSHUFF32X4_ZMMf32_MASKmskw_ZMMf32_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 128-bits (composed of 4 single-precision (32-bit) floating-point elements) selected by "imm8" from "a"
and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask
bit is not set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[127:0] := src[127:0]
    1:    tmp[127:0] := src[255:128]
    2:    tmp[127:0] := src[383:256]
    3:    tmp[127:0] := src[511:384]
    ESAC
    RETURN tmp[127:0]
}
tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSHUFF32X4 - _mm512_shuffle_f32x4

| VSHUFF32X4_ZMMf32_MASKmskw_ZMMf32_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 128-bits (composed of 4 single-precision (32-bit) floating-point elements) selected by "imm8" from "a"
and "b", and store the results in "dst".

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[127:0] := src[127:0]
    1:    tmp[127:0] := src[255:128]
    2:    tmp[127:0] := src[383:256]
    3:    tmp[127:0] := src[511:384]
    ESAC
    RETURN tmp[127:0]
}
dst[127:0] := SELECT4(a[511:0], imm8[1:0])
dst[255:128] := SELECT4(a[511:0], imm8[3:2])
dst[383:256] := SELECT4(b[511:0], imm8[5:4])
dst[511:384] := SELECT4(b[511:0], imm8[7:6])
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSHUFF64X2 - _mm512_mask_shuffle_f64x2

| VSHUFF64X2_ZMMf64_MASKmskw_ZMMf64_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 128-bits (composed of 2 double-precision (64-bit) floating-point elements) selected by "imm8" from "a"
and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[127:0] := src[127:0]
    1:    tmp[127:0] := src[255:128]
    2:    tmp[127:0] := src[383:256]
    3:    tmp[127:0] := src[511:384]
    ESAC
    RETURN tmp[127:0]
}
tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSHUFF64X2 - _mm512_maskz_shuffle_f64x2

| VSHUFF64X2_ZMMf64_MASKmskw_ZMMf64_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 128-bits (composed of 2 double-precision (64-bit) floating-point elements) selected by "imm8" from "a"
and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask
bit is not set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[127:0] := src[127:0]
    1:    tmp[127:0] := src[255:128]
    2:    tmp[127:0] := src[383:256]
    3:    tmp[127:0] := src[511:384]
    ESAC
    RETURN tmp[127:0]
}
tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSHUFF64X2 - _mm512_shuffle_f64x2

| VSHUFF64X2_ZMMf64_MASKmskw_ZMMf64_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 128-bits (composed of 2 double-precision (64-bit) floating-point elements) selected by "imm8" from "a"
and "b", and store the results in "dst".

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[127:0] := src[127:0]
    1:    tmp[127:0] := src[255:128]
    2:    tmp[127:0] := src[383:256]
    3:    tmp[127:0] := src[511:384]
    ESAC
    RETURN tmp[127:0]
}
dst[127:0] := SELECT4(a[511:0], imm8[1:0])
dst[255:128] := SELECT4(a[511:0], imm8[3:2])
dst[383:256] := SELECT4(b[511:0], imm8[5:4])
dst[511:384] := SELECT4(b[511:0], imm8[7:6])
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSHUFI32X4 - _mm512_mask_shuffle_i32x4

| VSHUFI32X4_ZMMu32_MASKmskw_ZMMu32_ZMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 128-bits (composed of 4 32-bit integers) selected by "imm8" from "a" and "b", and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[127:0] := src[127:0]
    1:    tmp[127:0] := src[255:128]
    2:    tmp[127:0] := src[383:256]
    3:    tmp[127:0] := src[511:384]
    ESAC
    RETURN tmp[127:0]
}
tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSHUFI32X4 - _mm512_maskz_shuffle_i32x4

| VSHUFI32X4_ZMMu32_MASKmskw_ZMMu32_ZMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 128-bits (composed of 4 32-bit integers) selected by "imm8" from "a" and "b", and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[127:0] := src[127:0]
    1:    tmp[127:0] := src[255:128]
    2:    tmp[127:0] := src[383:256]
    3:    tmp[127:0] := src[511:384]
    ESAC
    RETURN tmp[127:0]
}
tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSHUFI32X4 - _mm512_shuffle_i32x4

| VSHUFI32X4_ZMMu32_MASKmskw_ZMMu32_ZMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 128-bits (composed of 4 32-bit integers) selected by "imm8" from "a" and "b", and store the results in
"dst".

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[127:0] := src[127:0]
    1:    tmp[127:0] := src[255:128]
    2:    tmp[127:0] := src[383:256]
    3:    tmp[127:0] := src[511:384]
    ESAC
    RETURN tmp[127:0]
}
dst[127:0] := SELECT4(a[511:0], imm8[1:0])
dst[255:128] := SELECT4(a[511:0], imm8[3:2])
dst[383:256] := SELECT4(b[511:0], imm8[5:4])
dst[511:384] := SELECT4(b[511:0], imm8[7:6])
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSHUFI64X2 - _mm512_mask_shuffle_i64x2

| VSHUFI64X2_ZMMu64_MASKmskw_ZMMu64_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 128-bits (composed of 2 64-bit integers) selected by "imm8" from "a" and "b", and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[127:0] := src[127:0]
    1:    tmp[127:0] := src[255:128]
    2:    tmp[127:0] := src[383:256]
    3:    tmp[127:0] := src[511:384]
    ESAC
    RETURN tmp[127:0]
}
tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSHUFI64X2 - _mm512_maskz_shuffle_i64x2

| VSHUFI64X2_ZMMu64_MASKmskw_ZMMu64_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 128-bits (composed of 2 64-bit integers) selected by "imm8" from "a" and "b", and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[127:0] := src[127:0]
    1:    tmp[127:0] := src[255:128]
    2:    tmp[127:0] := src[383:256]
    3:    tmp[127:0] := src[511:384]
    ESAC
    RETURN tmp[127:0]
}
tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSHUFI64X2 - _mm512_shuffle_i64x2

| VSHUFI64X2_ZMMu64_MASKmskw_ZMMu64_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 128-bits (composed of 2 64-bit integers) selected by "imm8" from "a" and "b", and store the results in
"dst".

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[127:0] := src[127:0]
    1:    tmp[127:0] := src[255:128]
    2:    tmp[127:0] := src[383:256]
    3:    tmp[127:0] := src[511:384]
    ESAC
    RETURN tmp[127:0]
}
dst[127:0] := SELECT4(a[511:0], imm8[1:0])
dst[255:128] := SELECT4(a[511:0], imm8[3:2])
dst[383:256] := SELECT4(b[511:0], imm8[5:4])
dst[511:384] := SELECT4(b[511:0], imm8[7:6])
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSHUFPD - _mm512_mask_shuffle_pd

| VSHUFPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements within 128-bit lanes using the control in "imm8",
and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask
bit is not set).

[algorithm]

tmp_dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
tmp_dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
tmp_dst[191:128] := (imm8[2] == 0) ? a[191:128] : a[255:192]
tmp_dst[255:192] := (imm8[3] == 0) ? b[191:128] : b[255:192]
tmp_dst[319:256] := (imm8[4] == 0) ? a[319:256] : a[383:320]
tmp_dst[383:320] := (imm8[5] == 0) ? b[319:256] : b[383:320]
tmp_dst[447:384] := (imm8[6] == 0) ? a[447:384] : a[511:448]
tmp_dst[511:448] := (imm8[7] == 0) ? b[447:384] : b[511:448]
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSHUFPD - _mm512_maskz_shuffle_pd

| VSHUFPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements within 128-bit lanes using the control in "imm8",
and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is
not set).

[algorithm]

tmp_dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
tmp_dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
tmp_dst[191:128] := (imm8[2] == 0) ? a[191:128] : a[255:192]
tmp_dst[255:192] := (imm8[3] == 0) ? b[191:128] : b[255:192]
tmp_dst[319:256] := (imm8[4] == 0) ? a[319:256] : a[383:320]
tmp_dst[383:320] := (imm8[5] == 0) ? b[319:256] : b[383:320]
tmp_dst[447:384] := (imm8[6] == 0) ? a[447:384] : a[511:448]
tmp_dst[511:448] := (imm8[7] == 0) ? b[447:384] : b[511:448]
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSHUFPD - _mm512_shuffle_pd

| VSHUFPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle double-precision (64-bit) floating-point elements within 128-bit lanes using the control in "imm8",
and store the results in "dst".

[algorithm]

dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
dst[191:128] := (imm8[2] == 0) ? a[191:128] : a[255:192]
dst[255:192] := (imm8[3] == 0) ? b[191:128] : b[255:192]
dst[319:256] := (imm8[4] == 0) ? a[319:256] : a[383:320]
dst[383:320] := (imm8[5] == 0) ? b[319:256] : b[383:320]
dst[447:384] := (imm8[6] == 0) ? a[447:384] : a[511:448]
dst[511:448] := (imm8[7] == 0) ? b[447:384] : b[511:448]
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSHUFPS - _mm512_mask_shuffle_ps

| VSHUFPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" within 128-bit lanes using the control in
"imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[31:0] := src[31:0]
    1:    tmp[31:0] := src[63:32]
    2:    tmp[31:0] := src[95:64]
    3:    tmp[31:0] := src[127:96]
    ESAC
    RETURN tmp[31:0]
}
tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
tmp_dst[95:64] := SELECT4(b[127:0], imm8[5:4])
tmp_dst[127:96] := SELECT4(b[127:0], imm8[7:6])
tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
tmp_dst[223:192] := SELECT4(b[255:128], imm8[5:4])
tmp_dst[255:224] := SELECT4(b[255:128], imm8[7:6])
tmp_dst[287:256] := SELECT4(a[383:256], imm8[1:0])
tmp_dst[319:288] := SELECT4(a[383:256], imm8[3:2])
tmp_dst[351:320] := SELECT4(b[383:256], imm8[5:4])
tmp_dst[383:352] := SELECT4(b[383:256], imm8[7:6])
tmp_dst[415:384] := SELECT4(a[511:384], imm8[1:0])
tmp_dst[447:416] := SELECT4(a[511:384], imm8[3:2])
tmp_dst[479:448] := SELECT4(b[511:384], imm8[5:4])
tmp_dst[511:480] := SELECT4(b[511:384], imm8[7:6])
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSHUFPS - _mm512_maskz_shuffle_ps

| VSHUFPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" within 128-bit lanes using the control in
"imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask
bit is not set).

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[31:0] := src[31:0]
    1:    tmp[31:0] := src[63:32]
    2:    tmp[31:0] := src[95:64]
    3:    tmp[31:0] := src[127:96]
    ESAC
    RETURN tmp[31:0]
}
tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
tmp_dst[95:64] := SELECT4(b[127:0], imm8[5:4])
tmp_dst[127:96] := SELECT4(b[127:0], imm8[7:6])
tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
tmp_dst[223:192] := SELECT4(b[255:128], imm8[5:4])
tmp_dst[255:224] := SELECT4(b[255:128], imm8[7:6])
tmp_dst[287:256] := SELECT4(a[383:256], imm8[1:0])
tmp_dst[319:288] := SELECT4(a[383:256], imm8[3:2])
tmp_dst[351:320] := SELECT4(b[383:256], imm8[5:4])
tmp_dst[383:352] := SELECT4(b[383:256], imm8[7:6])
tmp_dst[415:384] := SELECT4(a[511:384], imm8[1:0])
tmp_dst[447:416] := SELECT4(a[511:384], imm8[3:2])
tmp_dst[479:448] := SELECT4(b[511:384], imm8[5:4])
tmp_dst[511:480] := SELECT4(b[511:384], imm8[7:6])
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSHUFPS - _mm512_shuffle_ps

| VSHUFPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle single-precision (32-bit) floating-point elements in "a" within 128-bit lanes using the control in
"imm8", and store the results in "dst".

[algorithm]

DEFINE SELECT4(src, control) {
    CASE(control[1:0]) OF
    0:    tmp[31:0] := src[31:0]
    1:    tmp[31:0] := src[63:32]
    2:    tmp[31:0] := src[95:64]
    3:    tmp[31:0] := src[127:96]
    ESAC
    RETURN tmp[31:0]
}
dst[31:0] := SELECT4(a[127:0], imm8[1:0])
dst[63:32] := SELECT4(a[127:0], imm8[3:2])
dst[95:64] := SELECT4(b[127:0], imm8[5:4])
dst[127:96] := SELECT4(b[127:0], imm8[7:6])
dst[159:128] := SELECT4(a[255:128], imm8[1:0])
dst[191:160] := SELECT4(a[255:128], imm8[3:2])
dst[223:192] := SELECT4(b[255:128], imm8[5:4])
dst[255:224] := SELECT4(b[255:128], imm8[7:6])
dst[287:256] := SELECT4(a[383:256], imm8[1:0])
dst[319:288] := SELECT4(a[383:256], imm8[3:2])
dst[351:320] := SELECT4(b[383:256], imm8[5:4])
dst[383:352] := SELECT4(b[383:256], imm8[7:6])
dst[415:384] := SELECT4(a[511:384], imm8[1:0])
dst[447:416] := SELECT4(a[511:384], imm8[3:2])
dst[479:448] := SELECT4(b[511:384], imm8[5:4])
dst[511:480] := SELECT4(b[511:384], imm8[7:6])
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSQRTPD - _mm512_mask_sqrt_pd

| VSQRTPD_ZMMf64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the square root of packed double-precision (64-bit) floating-point elements in "a", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := SQRT(a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSQRTPD - _mm512_mask_sqrt_round_pd

| VSQRTPD_ZMMf64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the square root of packed double-precision (64-bit) floating-point elements in "a", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := SQRT(a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSQRTPD - _mm512_maskz_sqrt_pd

| VSQRTPD_ZMMf64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the square root of packed double-precision (64-bit) floating-point elements in "a", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := SQRT(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSQRTPD - _mm512_maskz_sqrt_round_pd

| VSQRTPD_ZMMf64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the square root of packed double-precision (64-bit) floating-point elements in "a", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).
	[round_note].

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := SQRT(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSQRTPD - _mm512_sqrt_pd

| VSQRTPD_ZMMf64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the square root of packed double-precision (64-bit) floating-point elements in "a", and store the
results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := SQRT(a[i+63:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSQRTPD - _mm512_sqrt_round_pd

| VSQRTPD_ZMMf64_MASKmskw_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the square root of packed double-precision (64-bit) floating-point elements in "a", and store the
results in "dst".
	[round_note].

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := SQRT(a[i+63:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSQRTPS - _mm512_mask_sqrt_ps

| VSQRTPS_ZMMf32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the square root of packed single-precision (32-bit) floating-point elements in "a", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := SQRT(a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSQRTPS - _mm512_mask_sqrt_round_ps

| VSQRTPS_ZMMf32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the square root of packed single-precision (32-bit) floating-point elements in "a", and store the
results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).
	[round_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := SQRT(a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSQRTPS - _mm512_maskz_sqrt_ps

| VSQRTPS_ZMMf32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the square root of packed single-precision (32-bit) floating-point elements in "a", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := SQRT(a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSQRTPS - _mm512_maskz_sqrt_round_ps

| VSQRTPS_ZMMf32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the square root of packed single-precision (32-bit) floating-point elements in "a", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).
	[round_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := SQRT(a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSQRTPS - _mm512_sqrt_ps

| VSQRTPS_ZMMf32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the square root of packed single-precision (32-bit) floating-point elements in "a", and store the
results in "dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := SQRT(a[i+31:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSQRTPS - _mm512_sqrt_round_ps

| VSQRTPS_ZMMf32_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the square root of packed single-precision (32-bit) floating-point elements in "a", and store the
results in "dst".
	[round_note].

[algorithm]

FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := SQRT(a[i+31:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSQRTSD - _mm_mask_sqrt_round_sd

| VSQRTSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the square root of the lower double-precision (64-bit) floating-point element in "b", store the result
in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not
set), and copy the upper element from "a" to the upper element of "dst".
	[round_note]

[algorithm]

IF k[0]
    dst[63:0] := SQRT(b[63:0])
ELSE
    dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSQRTSD - _mm_mask_sqrt_sd

| VSQRTSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the square root of the lower double-precision (64-bit) floating-point element in "b", store the result
in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not
set), and copy the upper element from "a" to the upper element of "dst".

[algorithm]

IF k[0]
    dst[63:0] := SQRT(b[63:0])
ELSE
    dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSQRTSD - _mm_maskz_sqrt_round_sd

| VSQRTSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the square root of the lower double-precision (64-bit) floating-point element in "b", store the result
in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and
copy the upper element from "a" to the upper element of "dst".
	[round_note]

[algorithm]

IF k[0]
    dst[63:0] := SQRT(b[63:0])
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSQRTSD - _mm_maskz_sqrt_sd

| VSQRTSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the square root of the lower double-precision (64-bit) floating-point element in "b", store the result
in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and
copy the upper element from "a" to the upper element of "dst".

[algorithm]

IF k[0]
    dst[63:0] := SQRT(b[63:0])
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSQRTSD - _mm_sqrt_round_sd

| VSQRTSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the square root of the lower double-precision (64-bit) floating-point element in "b", store the result
in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst".
	[round_note]

[algorithm]

dst[63:0] := SQRT(b[63:0])
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSQRTSS - _mm_mask_sqrt_round_ss

| VSQRTSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the square root of the lower single-precision (32-bit) floating-point element in "b", store the result
in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not
set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
	[round_note]

[algorithm]

IF k[0]
    dst[31:0] := SQRT(b[31:0])
ELSE
    dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSQRTSS - _mm_mask_sqrt_ss

| VSQRTSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the square root of the lower single-precision (32-bit) floating-point element in "b", store the result
in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not
set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".

[algorithm]

IF k[0]
    dst[31:0] := SQRT(b[31:0])
ELSE
    dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSQRTSS - _mm_maskz_sqrt_round_ss

| VSQRTSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the square root of the lower single-precision (32-bit) floating-point element in "b", store the result
in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and
copy the upper 3 packed elements from "a" to the upper elements of "dst".
	[round_note]

[algorithm]

IF k[0]
    dst[31:0] := SQRT(b[31:0])
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSQRTSS - _mm_maskz_sqrt_ss

| VSQRTSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the square root of the lower single-precision (32-bit) floating-point element in "b", store the result
in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and
copy the upper 3 packed elements from "a" to the upper elements of "dst".

[algorithm]

IF k[0]
    dst[31:0] := SQRT(b[31:0])
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSQRTSS - _mm_sqrt_round_ss

| VSQRTSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute the square root of the lower single-precision (32-bit) floating-point element in "b", store the result
in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of
"dst".
	[round_note]

[algorithm]

dst[31:0] := SQRT(b[31:0])
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSUBPD - _mm512_maskz_sub_pd

| VSUBPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed double-precision (64-bit) floating-point elements in "b" from packed double-precision (64-bit)
floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] - b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSUBPD - _mm512_maskz_sub_round_pd

| VSUBPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed double-precision (64-bit) floating-point elements in "b" from packed double-precision (64-bit)
floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).
	[round_note]

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] - b[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSUBPS - _mm512_maskz_sub_ps

| VSUBPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed single-precision (32-bit) floating-point elements in "b" from packed single-precision (32-bit)
floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] - b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSUBPS - _mm512_maskz_sub_round_ps

| VSUBPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract packed single-precision (32-bit) floating-point elements in "b" from packed single-precision (32-bit)
floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).
	[round_note]

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := a[i+31:i] - b[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI    
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VSUBSD - _mm_mask_sub_round_sd

| VSUBSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract the lower double-precision (64-bit) floating-point element in "b" from the lower double-precision
(64-bit) floating-point element in "a", store the result in the lower element of "dst" using writemask "k" (the
element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper
element of "dst".
	[round_note]

[algorithm]

IF k[0]
    dst[63:0] := a[63:0] - b[63:0]
ELSE
    dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSUBSD - _mm_mask_sub_sd

| VSUBSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract the lower double-precision (64-bit) floating-point element in "b" from the lower double-precision
(64-bit) floating-point element in "a", store the result in the lower element of "dst" using writemask "k" (the
element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper
element of "dst".

[algorithm]

IF k[0]
    dst[63:0] := a[63:0] - b[63:0]
ELSE
    dst[63:0] := src[63:0]
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSUBSD - _mm_maskz_sub_round_sd

| VSUBSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract the lower double-precision (64-bit) floating-point element in "b" from the lower double-precision
(64-bit) floating-point element in "a", store the result in the lower element of "dst" using zeromask "k" (the
element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of
"dst".
	[round_note]

[algorithm]

IF k[0]
    dst[63:0] := a[63:0] - b[63:0]
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSUBSD - _mm_maskz_sub_sd

| VSUBSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract the lower double-precision (64-bit) floating-point element in "b" from the lower double-precision
(64-bit) floating-point element in "a", store the result in the lower element of "dst" using zeromask "k" (the
element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of
"dst".

[algorithm]

IF k[0]
    dst[63:0] := a[63:0] - b[63:0]
ELSE
    dst[63:0] := 0
FI
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSUBSD - _mm_sub_round_sd

| VSUBSD_XMMf64_MASKmskw_XMMf64_XMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract the lower double-precision (64-bit) floating-point element in "b" from the lower double-precision
(64-bit) floating-point element in "a", store the result in the lower element of "dst", and copy the upper
element from "a" to the upper element of "dst".
	[round_note]

[algorithm]

dst[63:0] := a[63:0] - b[63:0]
dst[127:64] := a[127:64]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSUBSS - _mm_mask_sub_round_ss

| VSUBSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract the lower single-precision (32-bit) floating-point element in "b" from the lower single-precision
(32-bit) floating-point element in "a", store the result in the lower element of "dst" using writemask "k" (the
element is copied from "src" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the
upper elements of "dst".
	[round_note]

[algorithm]

IF k[0]
    dst[31:0] := a[31:0] - b[31:0]
ELSE
    dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSUBSS - _mm_mask_sub_ss

| VSUBSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract the lower single-precision (32-bit) floating-point element in "b" from the lower single-precision
(32-bit) floating-point element in "a", store the result in the lower element of "dst" using writemask "k" (the
element is copied from "src" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the
upper elements of "dst".

[algorithm]

IF k[0]
    dst[31:0] := a[31:0] - b[31:0]
ELSE
    dst[31:0] := src[31:0]
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSUBSS - _mm_maskz_sub_round_ss

| VSUBSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract the lower single-precision (32-bit) floating-point element in "b" from the lower single-precision
(32-bit) floating-point element in "a", store the result in the lower element of "dst" using zeromask "k" (the
element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper
elements of "dst".
	[round_note]

[algorithm]

IF k[0]
    dst[31:0] := a[31:0] - b[31:0]
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSUBSS - _mm_maskz_sub_ss

| VSUBSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract the lower single-precision (32-bit) floating-point element in "b" from the lower single-precision
(32-bit) floating-point element in "a", store the result in the lower element of "dst" using zeromask "k" (the
element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper
elements of "dst".

[algorithm]

IF k[0]
    dst[31:0] := a[31:0] - b[31:0]
ELSE
    dst[31:0] := 0
FI
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VSUBSS - _mm_sub_round_ss

| VSUBSS_XMMf32_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Subtract the lower single-precision (32-bit) floating-point element in "b" from the lower single-precision
(32-bit) floating-point element in "a", store the result in the lower element of "dst", and copy the upper 3
packed elements from "a" to the upper elements of "dst".
	[round_note]

[algorithm]

dst[31:0] := a[31:0] - b[31:0]
dst[127:32] := a[127:32]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VUNPCKHPD - _mm512_mask_unpackhi_pd

| VUNPCKHPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave double-precision (64-bit) floating-point elements from the high half of each 128-bit
lane in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]) {
    dst[63:0] := src1[127:64] 
    dst[127:64] := src2[127:64] 
    RETURN dst[127:0]    
}
tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_HIGH_QWORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_HIGH_QWORDS(a[511:384], b[511:384])
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VUNPCKHPD - _mm512_maskz_unpackhi_pd

| VUNPCKHPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave double-precision (64-bit) floating-point elements from the high half of each 128-bit
lane in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]) {
    dst[63:0] := src1[127:64] 
    dst[127:64] := src2[127:64] 
    RETURN dst[127:0]    
}
tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_HIGH_QWORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_HIGH_QWORDS(a[511:384], b[511:384])
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VUNPCKHPD - _mm512_unpackhi_pd

| VUNPCKHPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave double-precision (64-bit) floating-point elements from the high half of each 128-bit
lane in "a" and "b", and store the results in "dst".

[algorithm]

DEFINE INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]) {
    dst[63:0] := src1[127:64] 
    dst[127:64] := src2[127:64] 
    RETURN dst[127:0]    
}
dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
dst[383:256] := INTERLEAVE_HIGH_QWORDS(a[383:256], b[383:256])
dst[511:384] := INTERLEAVE_HIGH_QWORDS(a[511:384], b[511:384])
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VUNPCKHPS - _mm512_mask_unpackhi_ps

| VUNPCKHPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave single-precision (32-bit) floating-point elements from the high half of each 128-bit
lane in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]) {
    dst[31:0] := src1[95:64] 
    dst[63:32] := src2[95:64] 
    dst[95:64] := src1[127:96] 
    dst[127:96] := src2[127:96] 
    RETURN dst[127:0]    
}
tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_HIGH_DWORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_HIGH_DWORDS(a[511:384], b[511:384])
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VUNPCKHPS - _mm512_maskz_unpackhi_ps

| VUNPCKHPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave single-precision (32-bit) floating-point elements from the high half of each 128-bit
lane in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]) {
    dst[31:0] := src1[95:64] 
    dst[63:32] := src2[95:64] 
    dst[95:64] := src1[127:96] 
    dst[127:96] := src2[127:96] 
    RETURN dst[127:0]    
}
tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_HIGH_DWORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_HIGH_DWORDS(a[511:384], b[511:384])
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VUNPCKHPS - _mm512_unpackhi_ps

| VUNPCKHPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave single-precision (32-bit) floating-point elements from the high half of each 128-bit
lane in "a" and "b", and store the results in "dst".

[algorithm]

DEFINE INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]) {
    dst[31:0] := src1[95:64] 
    dst[63:32] := src2[95:64] 
    dst[95:64] := src1[127:96] 
    dst[127:96] := src2[127:96] 
    RETURN dst[127:0]    
}
dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
dst[383:256] := INTERLEAVE_HIGH_DWORDS(a[383:256], b[383:256])
dst[511:384] := INTERLEAVE_HIGH_DWORDS(a[511:384], b[511:384])
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VUNPCKLPD - _mm512_mask_unpacklo_pd

| VUNPCKLPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave double-precision (64-bit) floating-point elements from the low half of each 128-bit lane
in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_QWORDS(src1[127:0], src2[127:0]) {
    dst[63:0] := src1[63:0] 
    dst[127:64] := src2[63:0] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_QWORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_QWORDS(a[511:384], b[511:384])
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VUNPCKLPD - _mm512_maskz_unpacklo_pd

| VUNPCKLPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave double-precision (64-bit) floating-point elements from the low half of each 128-bit lane
in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_QWORDS(src1[127:0], src2[127:0]) {
    dst[63:0] := src1[63:0] 
    dst[127:64] := src2[63:0] 
    RETURN dst[127:0]
}
tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_QWORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_QWORDS(a[511:384], b[511:384])
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := tmp_dst[i+63:i]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VUNPCKLPD - _mm512_unpacklo_pd

| VUNPCKLPD_ZMMf64_MASKmskw_ZMMf64_ZMMf64_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave double-precision (64-bit) floating-point elements from the low half of each 128-bit lane
in "a" and "b", and store the results in "dst".

[algorithm]

DEFINE INTERLEAVE_QWORDS(src1[127:0], src2[127:0]) {
    dst[63:0] := src1[63:0] 
    dst[127:64] := src2[63:0] 
    RETURN dst[127:0]
}
dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
dst[383:256] := INTERLEAVE_QWORDS(a[383:256], b[383:256])
dst[511:384] := INTERLEAVE_QWORDS(a[511:384], b[511:384])
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VUNPCKLPS - _mm512_mask_unpacklo_ps

| VUNPCKLPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave single-precision (32-bit) floating-point elements from the low half of each 128-bit lane
in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_DWORDS(src1[127:0], src2[127:0]) {
    dst[31:0] := src1[31:0] 
    dst[63:32] := src2[31:0] 
    dst[95:64] := src1[63:32] 
    dst[127:96] := src2[63:32] 
    RETURN dst[127:0]    
}
tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_DWORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_DWORDS(a[511:384], b[511:384])
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VUNPCKLPS - _mm512_maskz_unpacklo_ps

| VUNPCKLPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave single-precision (32-bit) floating-point elements from the low half of each 128-bit lane
in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

DEFINE INTERLEAVE_DWORDS(src1[127:0], src2[127:0]) {
    dst[31:0] := src1[31:0] 
    dst[63:32] := src2[31:0] 
    dst[95:64] := src1[63:32] 
    dst[127:96] := src2[63:32] 
    RETURN dst[127:0]    
}
tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
tmp_dst[383:256] := INTERLEAVE_DWORDS(a[383:256], b[383:256])
tmp_dst[511:384] := INTERLEAVE_DWORDS(a[511:384], b[511:384])
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := tmp_dst[i+31:i]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VUNPCKLPS - _mm512_unpacklo_ps

| VUNPCKLPS_ZMMf32_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Unpack and interleave single-precision (32-bit) floating-point elements from the low half of each 128-bit lane
in "a" and "b", and store the results in "dst".

[algorithm]

DEFINE INTERLEAVE_DWORDS(src1[127:0], src2[127:0]) {
    dst[31:0] := src1[31:0] 
    dst[63:32] := src2[31:0] 
    dst[95:64] := src1[63:32] 
    dst[127:96] := src2[63:32] 
    RETURN dst[127:0]    
}
dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
dst[383:256] := INTERLEAVE_DWORDS(a[383:256], b[383:256])
dst[511:384] := INTERLEAVE_DWORDS(a[511:384], b[511:384])
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## _mm512_castpd128_pd512

--------------------------------------------------------------------------------------------------------------
Cast vector of type __m128d to type __m512d; the upper 384 bits of the result are undefined. 
	This intrinsic
is only used for compilation and does not generate any instructions, thus it has zero latency.

[missing]

--------------------------------------------------------------------------------------------------------------

## _mm512_castpd256_pd512

--------------------------------------------------------------------------------------------------------------
Cast vector of type __m256d to type __m512d; the upper 256 bits of the result are undefined. 
	This intrinsic
is only used for compilation and does not generate any instructions, thus it has zero latency.

[missing]

--------------------------------------------------------------------------------------------------------------

## _mm512_castpd512_pd128

--------------------------------------------------------------------------------------------------------------
Cast vector of type __m512d to type __m128d. 
	This intrinsic is only used for compilation and does not
generate any instructions, thus it has zero latency.

[missing]

--------------------------------------------------------------------------------------------------------------

## _mm512_castps512_ps128

--------------------------------------------------------------------------------------------------------------
Cast vector of type __m512 to type __m128. 
	This intrinsic is only used for compilation and does not generate
any instructions, thus it has zero latency.

[missing]

--------------------------------------------------------------------------------------------------------------

## _mm512_castpd512_pd256

--------------------------------------------------------------------------------------------------------------
Cast vector of type __m512d to type __m256d. 
	This intrinsic is only used for compilation and does not
generate any instructions, thus it has zero latency.

[missing]

--------------------------------------------------------------------------------------------------------------

## _mm512_castps128_ps512

--------------------------------------------------------------------------------------------------------------
Cast vector of type __m128 to type __m512; the upper 384 bits of the result are undefined. 
	This intrinsic is
only used for compilation and does not generate any instructions, thus it has zero latency.

[missing]

--------------------------------------------------------------------------------------------------------------

## _mm512_castps256_ps512

--------------------------------------------------------------------------------------------------------------
Cast vector of type __m256 to type __m512; the upper 256 bits of the result are undefined. 
	This intrinsic is
only used for compilation and does not generate any instructions, thus it has zero latency.

[missing]

--------------------------------------------------------------------------------------------------------------

## _mm512_castps512_ps256

--------------------------------------------------------------------------------------------------------------
Cast vector of type __m512 to type __m256. 
	This intrinsic is only used for compilation and does not generate
any instructions, thus it has zero latency.

[missing]

--------------------------------------------------------------------------------------------------------------

## _mm512_castsi128_si512

--------------------------------------------------------------------------------------------------------------
Cast vector of type __m128i to type __m512i; the upper 384 bits of the result are undefined. 
	This intrinsic
is only used for compilation and does not generate any instructions, thus it has zero latency.

[missing]

--------------------------------------------------------------------------------------------------------------

## _mm512_castsi256_si512

--------------------------------------------------------------------------------------------------------------
Cast vector of type __m256i to type __m512i; the upper 256 bits of the result are undefined.
	 This intrinsic
is only used for compilation and does not generate any instructions, thus it has zero latency.

[missing]

--------------------------------------------------------------------------------------------------------------

## _mm512_castsi512_si128

--------------------------------------------------------------------------------------------------------------
Cast vector of type __m512i to type __m128i.
	 This intrinsic is only used for compilation and does not
generate any instructions, thus it has zero latency.

[missing]

--------------------------------------------------------------------------------------------------------------

## _mm512_castsi512_si256

--------------------------------------------------------------------------------------------------------------
Cast vector of type __m512i to type __m256i.
	 This intrinsic is only used for compilation and does not
generate any instructions, thus it has zero latency.

[missing]

--------------------------------------------------------------------------------------------------------------

## _mm512_zextpd128_pd512

--------------------------------------------------------------------------------------------------------------
Cast vector of type __m128d to type __m512d; the upper 384 bits of the result are zeroed. This intrinsic is
only used for compilation and does not generate any instructions, thus it has zero latency.

[missing]

--------------------------------------------------------------------------------------------------------------

## _mm512_zextps128_ps512

--------------------------------------------------------------------------------------------------------------
Cast vector of type __m128 to type __m512; the upper 384 bits of the result are zeroed. This intrinsic is only
used for compilation and does not generate any instructions, thus it has zero latency.

[missing]

--------------------------------------------------------------------------------------------------------------

## _mm512_zextsi128_si512

--------------------------------------------------------------------------------------------------------------
Cast vector of type __m128i to type __m512i; the upper 384 bits of the result are zeroed. This intrinsic is
only used for compilation and does not generate any instructions, thus it has zero latency.

[missing]

--------------------------------------------------------------------------------------------------------------

## _mm512_zextpd256_pd512

--------------------------------------------------------------------------------------------------------------
Cast vector of type __m256d to type __m512d; the upper 256 bits of the result are zeroed. This intrinsic is
only used for compilation and does not generate any instructions, thus it has zero latency.

[missing]

--------------------------------------------------------------------------------------------------------------

## _mm512_zextps256_ps512

--------------------------------------------------------------------------------------------------------------
Cast vector of type __m256 to type __m512; the upper 256 bits of the result are zeroed. This intrinsic is only
used for compilation and does not generate any instructions, thus it has zero latency.

[missing]

--------------------------------------------------------------------------------------------------------------

## _mm512_zextsi256_si512

--------------------------------------------------------------------------------------------------------------
Cast vector of type __m256i to type __m512i; the upper 256 bits of the result are zeroed. This intrinsic is
only used for compilation and does not generate any instructions, thus it has zero latency.

[missing]

--------------------------------------------------------------------------------------------------------------

## _mm512_set1_pd

--------------------------------------------------------------------------------------------------------------
Broadcast double-precision (64-bit) floating-point value "a" to all elements of "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := a[63:0]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## _mm512_set1_ps

--------------------------------------------------------------------------------------------------------------
Broadcast single-precision (32-bit) floating-point value "a" to all elements of "dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := a[31:0]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## _mm512_set4_epi32

--------------------------------------------------------------------------------------------------------------
Set packed 32-bit integers in "dst" with the repeated 4 element sequence.

[algorithm]

dst[31:0] := a
dst[63:32] := b
dst[95:64] := c
dst[127:96] := d
dst[159:128] := a
dst[191:160] := b
dst[223:192] := c
dst[255:224] := d
dst[287:256] := a
dst[319:288] := b
dst[351:320] := c
dst[383:352] := d
dst[415:384] := a
dst[447:416] := b
dst[479:448] := c
dst[511:480] := d
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## _mm512_set4_epi64

--------------------------------------------------------------------------------------------------------------
Set packed 64-bit integers in "dst" with the repeated 4 element sequence.

[algorithm]

dst[63:0] := a
dst[127:64] := b
dst[191:128] := c
dst[255:192] := d
dst[319:256] := a
dst[383:320] := b
dst[447:384] := c
dst[511:448] := d
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## _mm512_set4_pd

--------------------------------------------------------------------------------------------------------------
Set packed double-precision (64-bit) floating-point elements in "dst" with the repeated 4 element sequence.

[algorithm]

dst[63:0] := a
dst[127:64] := b
dst[191:128] := c
dst[255:192] := d
dst[319:256] := a
dst[383:320] := b
dst[447:384] := c
dst[511:448] := d
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## _mm512_set4_ps

--------------------------------------------------------------------------------------------------------------
Set packed single-precision (32-bit) floating-point elements in "dst" with the repeated 4 element sequence.

[algorithm]

dst[31:0] := a
dst[63:32] := b
dst[95:64] := c
dst[127:96] := d
dst[159:128] := a
dst[191:160] := b
dst[223:192] := c
dst[255:224] := d
dst[287:256] := a
dst[319:288] := b
dst[351:320] := c
dst[383:352] := d
dst[415:384] := a
dst[447:416] := b
dst[479:448] := c
dst[511:480] := d
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## _mm512_set_epi8

--------------------------------------------------------------------------------------------------------------
Set packed 8-bit integers in "dst" with the supplied values.

[algorithm]

dst[7:0] := e0
dst[15:8] := e1
dst[23:16] := e2
dst[31:24] := e3
dst[39:32] := e4
dst[47:40] := e5
dst[55:48] := e6
dst[63:56] := e7
dst[71:64] := e8
dst[79:72] := e9
dst[87:80] := e10
dst[95:88] := e11
dst[103:96] := e12
dst[111:104] := e13
dst[119:112] := e14
dst[127:120] := e15
dst[135:128] := e16
dst[143:136] := e17
dst[151:144] := e18
dst[159:152] := e19
dst[167:160] := e20
dst[175:168] := e21
dst[183:176] := e22
dst[191:184] := e23
dst[199:192] := e24
dst[207:200] := e25
dst[215:208] := e26
dst[223:216] := e27
dst[231:224] := e28
dst[239:232] := e29
dst[247:240] := e30
dst[255:248] := e31
dst[263:256] := e32
dst[271:264] := e33
dst[279:272] := e34
dst[287:280] := e35
dst[295:288] := e36
dst[303:296] := e37
dst[311:304] := e38
dst[319:312] := e39
dst[327:320] := e40
dst[335:328] := e41
dst[343:336] := e42
dst[351:344] := e43
dst[359:352] := e44
dst[367:360] := e45
dst[375:368] := e46
dst[383:376] := e47
dst[391:384] := e48
dst[399:392] := e49
dst[407:400] := e50
dst[415:408] := e51
dst[423:416] := e52
dst[431:424] := e53
dst[439:432] := e54
dst[447:440] := e55
dst[455:448] := e56
dst[463:456] := e57
dst[471:464] := e58
dst[479:472] := e59
dst[487:480] := e60
dst[495:488] := e61
dst[503:496] := e62
dst[511:504] := e63
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## _mm512_set_epi16

--------------------------------------------------------------------------------------------------------------
Set packed 16-bit integers in "dst" with the supplied values.

[algorithm]

dst[15:0] := e0
dst[31:16] := e1
dst[47:32] := e2
dst[63:48] := e3
dst[79:64] := e4
dst[95:80] := e5
dst[111:96] := e6
dst[127:112] := e7
dst[143:128] := e8
dst[159:144] := e9
dst[175:160] := e10
dst[191:176] := e11
dst[207:192] := e12
dst[223:208] := e13
dst[239:224] := e14
dst[255:240] := e15
dst[271:256] := e16
dst[287:272] := e17
dst[303:288] := e18
dst[319:304] := e19
dst[335:320] := e20
dst[351:336] := e21
dst[367:352] := e22
dst[383:368] := e23
dst[399:384] := e24
dst[415:400] := e25
dst[431:416] := e26
dst[447:432] := e27
dst[463:448] := e28
dst[479:464] := e29
dst[495:480] := e30
dst[511:496] := e31
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## _mm512_set_epi32

--------------------------------------------------------------------------------------------------------------
Set packed 32-bit integers in "dst" with the supplied values.

[algorithm]

dst[31:0] := e0
dst[63:32] := e1
dst[95:64] := e2
dst[127:96] := e3
dst[159:128] := e4
dst[191:160] := e5
dst[223:192] := e6
dst[255:224] := e7
dst[287:256] := e8
dst[319:288] := e9
dst[351:320] := e10
dst[383:352] := e11
dst[415:384] := e12
dst[447:416] := e13
dst[479:448] := e14
dst[511:480] := e15
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## _mm512_set_epi64

--------------------------------------------------------------------------------------------------------------
Set packed 64-bit integers in "dst" with the supplied values.

[algorithm]

dst[63:0] := e0
dst[127:64] := e1
dst[191:128] := e2
dst[255:192] := e3
dst[319:256] := e4
dst[383:320] := e5
dst[447:384] := e6
dst[511:448] := e7
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## _mm512_set_pd

--------------------------------------------------------------------------------------------------------------
Set packed double-precision (64-bit) floating-point elements in "dst" with the supplied values.

[algorithm]

dst[63:0] := e0
dst[127:64] := e1
dst[191:128] := e2
dst[255:192] := e3
dst[319:256] := e4
dst[383:320] := e5
dst[447:384] := e6
dst[511:448] := e7
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## _mm512_set_ps

--------------------------------------------------------------------------------------------------------------
Set packed single-precision (32-bit) floating-point elements in "dst" with the supplied values.

[algorithm]

dst[31:0] := e0
dst[63:32] := e1
dst[95:64] := e2
dst[127:96] := e3
dst[159:128] := e4
dst[191:160] := e5
dst[223:192] := e6
dst[255:224] := e7
dst[287:256] := e8
dst[319:288] := e9
dst[351:320] := e10
dst[383:352] := e11
dst[415:384] := e12
dst[447:416] := e13
dst[479:448] := e14
dst[511:480] := e15
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## _mm512_setr4_epi32

--------------------------------------------------------------------------------------------------------------
Set packed 32-bit integers in "dst" with the repeated 4 element sequence in reverse order.

[algorithm]

dst[31:0] := d
dst[63:32] := c
dst[95:64] := b
dst[127:96] := a
dst[159:128] := d
dst[191:160] := c
dst[223:192] := b
dst[255:224] := a
dst[287:256] := d
dst[319:288] := c
dst[351:320] := b
dst[383:352] := a
dst[415:384] := d
dst[447:416] := c
dst[479:448] := b
dst[511:480] := a
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## _mm512_setr4_epi64

--------------------------------------------------------------------------------------------------------------
Set packed 64-bit integers in "dst" with the repeated 4 element sequence in reverse order.

[algorithm]

dst[63:0] := d
dst[127:64] := c
dst[191:128] := b
dst[255:192] := a
dst[319:256] := d
dst[383:320] := c
dst[447:384] := b
dst[511:448] := a
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## _mm512_setr4_pd

--------------------------------------------------------------------------------------------------------------
Set packed double-precision (64-bit) floating-point elements in "dst" with the repeated 4 element sequence in
reverse order.

[algorithm]

dst[63:0] := d
dst[127:64] := c
dst[191:128] := b
dst[255:192] := a
dst[319:256] := d
dst[383:320] := c
dst[447:384] := b
dst[511:448] := a
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## _mm512_setr4_ps

--------------------------------------------------------------------------------------------------------------
Set packed single-precision (32-bit) floating-point elements in "dst" with the repeated 4 element sequence in
reverse order.

[algorithm]

dst[31:0] := d
dst[63:32] := c
dst[95:64] := b
dst[127:96] := a
dst[159:128] := d
dst[191:160] := c
dst[223:192] := b
dst[255:224] := a
dst[287:256] := d
dst[319:288] := c
dst[351:320] := b
dst[383:352] := a
dst[415:384] := d
dst[447:416] := c
dst[479:448] := b
dst[511:480] := a
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## _mm512_setr_epi32

--------------------------------------------------------------------------------------------------------------
Set packed 32-bit integers in "dst" with the supplied values in reverse order.

[algorithm]

dst[31:0] := e15
dst[63:32] := e14
dst[95:64] := e13
dst[127:96] := e12
dst[159:128] := e11
dst[191:160] := e10
dst[223:192] := e9
dst[255:224] := e8
dst[287:256] := e7
dst[319:288] := e6
dst[351:320] := e5
dst[383:352] := e4
dst[415:384] := e3
dst[447:416] := e2
dst[479:448] := e1
dst[511:480] := e0
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## _mm512_setr_epi64

--------------------------------------------------------------------------------------------------------------
Set packed 64-bit integers in "dst" with the supplied values in reverse order.

[algorithm]

dst[63:0] := e7
dst[127:64] := e6
dst[191:128] := e5
dst[255:192] := e4
dst[319:256] := e3
dst[383:320] := e2
dst[447:384] := e1
dst[511:448] := e0
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## _mm512_setr_pd

--------------------------------------------------------------------------------------------------------------
Set packed double-precision (64-bit) floating-point elements in "dst" with the supplied values in reverse
order.

[algorithm]

dst[63:0] := e7
dst[127:64] := e6
dst[191:128] := e5
dst[255:192] := e4
dst[319:256] := e3
dst[383:320] := e2
dst[447:384] := e1
dst[511:448] := e0
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## _mm512_setr_ps

--------------------------------------------------------------------------------------------------------------
Set packed single-precision (32-bit) floating-point elements in "dst" with the supplied values in reverse
order.

[algorithm]

dst[31:0] := e15
dst[63:32] := e14
dst[95:64] := e13
dst[127:96] := e12
dst[159:128] := e11
dst[191:160] := e10
dst[223:192] := e9
dst[255:224] := e8
dst[287:256] := e7
dst[319:288] := e6
dst[351:320] := e5
dst[383:352] := e4
dst[415:384] := e3
dst[447:416] := e2
dst[479:448] := e1
dst[511:480] := e0
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPXORQ - _mm512_setzero

| VPXORQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Return vector of type __m512 with all elements set to zero.

[algorithm]

dst[MAX:0] := 0

--------------------------------------------------------------------------------------------------------------

## VPXORQ - _mm512_setzero_epi32

| VPXORQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Return vector of type __m512i with all elements set to zero.

[algorithm]

dst[MAX:0] := 0

--------------------------------------------------------------------------------------------------------------

## VPXORQ - _mm512_setzero_pd

| VPXORQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Return vector of type __m512d with all elements set to zero.

[algorithm]

dst[MAX:0] := 0

--------------------------------------------------------------------------------------------------------------

## VPXORQ - _mm512_setzero_ps

| VPXORQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Return vector of type __m512 with all elements set to zero.

[algorithm]

dst[MAX:0] := 0

--------------------------------------------------------------------------------------------------------------

## VPXORQ - _mm512_setzero_si512

| VPXORQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Return vector of type __m512i with all elements set to zero.

[algorithm]

dst[MAX:0] := 0

--------------------------------------------------------------------------------------------------------------

## _mm512_undefined

--------------------------------------------------------------------------------------------------------------
Return vector of type __m512 with undefined elements.

[missing]

--------------------------------------------------------------------------------------------------------------

## _mm512_undefined_epi32

--------------------------------------------------------------------------------------------------------------
Return vector of type __m512i with undefined elements.

[missing]

--------------------------------------------------------------------------------------------------------------

## _mm512_undefined_pd

--------------------------------------------------------------------------------------------------------------
Return vector of type __m512d with undefined elements.

[missing]

--------------------------------------------------------------------------------------------------------------

## _mm512_undefined_ps

--------------------------------------------------------------------------------------------------------------
Return vector of type __m512 with undefined elements.

[missing]

--------------------------------------------------------------------------------------------------------------

## KORTESTW - _mm512_kortestz

| KORTESTW_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Performs bitwise OR between "k1" and "k2", storing the result in "dst". ZF flag is set if "dst" is 0.

[algorithm]

dst[15:0] := k1[15:0] | k2[15:0]
IF dst == 0
    SetZF()
FI

--------------------------------------------------------------------------------------------------------------

## KORTESTW - _mm512_kortestc

| KORTESTW_MASKmskw_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Performs bitwise OR between "k1" and "k2", storing the result in "dst". CF flag is set if "dst" consists of
all 1's.

[algorithm]

dst[15:0] := k1[15:0] | k2[15:0]
IF PopCount(dst[15:0]) == 16
    SetCF()
FI

--------------------------------------------------------------------------------------------------------------

## KMOVW - _mm512_mask2int

| KMOVW_GPR32u32_MASKmskw_AVX512

--------------------------------------------------------------------------------------------------------------
Converts bit mask "k1" into an integer value, storing the results in "dst".

[algorithm]

dst := ZeroExtend32(k1)

--------------------------------------------------------------------------------------------------------------

## KMOVW - _mm512_int2mask

| KMOVW_MASKmskw_GPR32u32_AVX512

--------------------------------------------------------------------------------------------------------------
Converts integer "mask" into bitmask, storing the result in "dst".

[algorithm]

dst := mask[15:0]

--------------------------------------------------------------------------------------------------------------

## _mm512_mullox_epi64

--------------------------------------------------------------------------------------------------------------
Multiplies elements in packed 64-bit integer vectors "a" and "b" together, storing the lower 64 bits of the
result in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := a[i+63:i] * b[i+63:i]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## _mm512_mask_mullox_epi64

--------------------------------------------------------------------------------------------------------------
Multiplies elements in packed 64-bit integer vectors "a" and "b" together, storing the lower 64 bits of the
result in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := a[i+63:i] * b[i+63:i]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VMOVSS - _mm512_cvtss_f32

| VMOVSS_MEMf32_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Copy the lower single-precision (32-bit) floating-point element of "a" to "dst".

[algorithm]

dst[31:0] := a[31:0]

--------------------------------------------------------------------------------------------------------------

## VMOVSD - _mm512_cvtsd_f64

| VMOVSD_MEMq_XMMq

--------------------------------------------------------------------------------------------------------------
Copy the lower double-precision (64-bit) floating-point element of "a" to "dst".

[algorithm]

dst[63:0] := a[63:0]

--------------------------------------------------------------------------------------------------------------

## VMOVD - _mm512_cvtsi512_si32

| VMOVD_GPR32u32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Copy the lower 32-bit integer in "a" to "dst".

[algorithm]

dst[31:0] := a[31:0]

--------------------------------------------------------------------------------------------------------------

## VPMADD52LUQ - _mm512_madd52lo_epu64

| VPMADD52LUQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed unsigned 52-bit integers in each 64-bit element of "b" and "c" to form a 104-bit intermediate
result. Add the low 52-bit unsigned integer from the intermediate result with the corresponding unsigned 64-bit
integer in "a", and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
    dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[51:0])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMADD52LUQ - _mm512_mask_madd52lo_epu64

| VPMADD52LUQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed unsigned 52-bit integers in each 64-bit element of "b" and "c" to form a 104-bit intermediate
result. Add the low 52-bit unsigned integer from the intermediate result with the corresponding unsigned 64-bit
integer in "a", and store the results in "dst" using writemask "k" (elements are copied from "a" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
        dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[51:0])
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMADD52LUQ - _mm512_maskz_madd52lo_epu64

| VPMADD52LUQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed unsigned 52-bit integers in each 64-bit element of "b" and "c" to form a 104-bit intermediate
result. Add the low 52-bit unsigned integer from the intermediate result with the corresponding unsigned 64-bit
integer in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
        dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[51:0])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMADD52LUQ - _mm256_madd52lo_epu64

| VPMADD52LUQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed unsigned 52-bit integers in each 64-bit element of "b" and "c" to form a 104-bit intermediate
result. Add the low 52-bit unsigned integer from the intermediate result with the corresponding unsigned 64-bit
integer in "a", and store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
    dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[51:0])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMADD52LUQ - _mm256_mask_madd52lo_epu64

| VPMADD52LUQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed unsigned 52-bit integers in each 64-bit element of "b" and "c" to form a 104-bit intermediate
result. Add the low 52-bit unsigned integer from the intermediate result with the corresponding unsigned 64-bit
integer in "a", and store the results in "dst" using writemask "k" (elements are copied from "a" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
        dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[51:0])
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMADD52LUQ - _mm256_maskz_madd52lo_epu64

| VPMADD52LUQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed unsigned 52-bit integers in each 64-bit element of "b" and "c" to form a 104-bit intermediate
result. Add the low 52-bit unsigned integer from the intermediate result with the corresponding unsigned 64-bit
integer in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
        dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[51:0])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMADD52LUQ - _mm_madd52lo_epu64

| VPMADD52LUQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed unsigned 52-bit integers in each 64-bit element of "b" and "c" to form a 104-bit intermediate
result. Add the low 52-bit unsigned integer from the intermediate result with the corresponding unsigned 64-bit
integer in "a", and store the results in "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
    dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[51:0])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMADD52LUQ - _mm_mask_madd52lo_epu64

| VPMADD52LUQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed unsigned 52-bit integers in each 64-bit element of "b" and "c" to form a 104-bit intermediate
result. Add the low 52-bit unsigned integer from the intermediate result with the corresponding unsigned 64-bit
integer in "a", and store the results in "dst" using writemask "k" (elements are copied from "a" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
        dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[51:0])
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMADD52LUQ - _mm_maskz_madd52lo_epu64

| VPMADD52LUQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed unsigned 52-bit integers in each 64-bit element of "b" and "c" to form a 104-bit intermediate
result. Add the low 52-bit unsigned integer from the intermediate result with the corresponding unsigned 64-bit
integer in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
        dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[51:0])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMADD52HUQ - _mm512_madd52hi_epu64

| VPMADD52HUQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed unsigned 52-bit integers in each 64-bit element of "b" and "c" to form a 104-bit intermediate
result. Add the high 52-bit unsigned integer from the intermediate result with the corresponding unsigned
64-bit integer in "a", and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
    dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[103:52])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMADD52HUQ - _mm512_mask_madd52hi_epu64

| VPMADD52HUQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed unsigned 52-bit integers in each 64-bit element of "b" and "c" to form a 104-bit intermediate
result. Add the high 52-bit unsigned integer from the intermediate result with the corresponding unsigned
64-bit integer in "a", and store the results in "dst" using writemask "k" (elements are copied from "a" when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
        dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[103:52])
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMADD52HUQ - _mm512_maskz_madd52hi_epu64

| VPMADD52HUQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed unsigned 52-bit integers in each 64-bit element of "b" and "c" to form a 104-bit intermediate
result. Add the high 52-bit unsigned integer from the intermediate result with the corresponding unsigned
64-bit integer in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
        dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[103:52])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMADD52HUQ - _mm256_madd52hi_epu64

| VPMADD52HUQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed unsigned 52-bit integers in each 64-bit element of "b" and "c" to form a 104-bit intermediate
result. Add the high 52-bit unsigned integer from the intermediate result with the corresponding unsigned
64-bit integer in "a", and store the results in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
    dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[103:52])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMADD52HUQ - _mm256_mask_madd52hi_epu64

| VPMADD52HUQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed unsigned 52-bit integers in each 64-bit element of "b" and "c" to form a 104-bit intermediate
result. Add the high 52-bit unsigned integer from the intermediate result with the corresponding unsigned
64-bit integer in "a", and store the results in "dst" using writemask "k" (elements are copied from "a" when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
        dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[103:52])
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMADD52HUQ - _mm256_maskz_madd52hi_epu64

| VPMADD52HUQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed unsigned 52-bit integers in each 64-bit element of "b" and "c" to form a 104-bit intermediate
result. Add the high 52-bit unsigned integer from the intermediate result with the corresponding unsigned
64-bit integer in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
        dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[103:52])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMADD52HUQ - _mm_madd52hi_epu64

| VPMADD52HUQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed unsigned 52-bit integers in each 64-bit element of "b" and "c" to form a 104-bit intermediate
result. Add the high 52-bit unsigned integer from the intermediate result with the corresponding unsigned
64-bit integer in "a", and store the results in "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
    dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[103:52])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMADD52HUQ - _mm_mask_madd52hi_epu64

| VPMADD52HUQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed unsigned 52-bit integers in each 64-bit element of "b" and "c" to form a 104-bit intermediate
result. Add the high 52-bit unsigned integer from the intermediate result with the corresponding unsigned
64-bit integer in "a", and store the results in "dst" using writemask "k" (elements are copied from "a" when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
        dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[103:52])
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMADD52HUQ - _mm_maskz_madd52hi_epu64

| VPMADD52HUQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed unsigned 52-bit integers in each 64-bit element of "b" and "c" to form a 104-bit intermediate
result. Add the high 52-bit unsigned integer from the intermediate result with the corresponding unsigned
64-bit integer in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
        dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[103:52])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VGATHERPF0QPS - _mm512_prefetch_i64gather_ps

| VGATHERPF0QPS_MEMf32_MASKmskw_AVX512PF_VL512 | VGATHERPF1QPS_MEMf32_MASKmskw_AVX512PF_VL512

--------------------------------------------------------------------------------------------------------------
Prefetch single-precision (32-bit) floating-point elements from memory using 64-bit indices. 32-bit elements
are loaded from addresses starting at "base_addr" and offset by each 64-bit element in "vindex" (each index is
scaled by the factor in "scale"). Gathered elements are merged in cache. "scale" should be 1, 2, 4 or 8. The
"hint" parameter may be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for prefetching to L2
cache.

[algorithm]

FOR j:= 0 to 7
    i := j*64
    m := j*64
    addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
    Prefetch(MEM[addr+31:addr], hint)
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VGATHERPF0QPS - _mm512_mask_prefetch_i64gather_ps

| VGATHERPF0QPS_MEMf32_MASKmskw_AVX512PF_VL512 | VGATHERPF1QPS_MEMf32_MASKmskw_AVX512PF_VL512

--------------------------------------------------------------------------------------------------------------
Prefetch single-precision (32-bit) floating-point elements from memory using 64-bit indices. 32-bit elements
are loaded from addresses starting at "base_addr" and offset by each 64-bit element in "vindex" (each index is
scaled by the factor in "scale"). Gathered elements are merged in cache using writemask "k" (elements are only
brought into cache when their corresponding mask bit is set). "scale" should be 1, 2, 4 or 8.. The "hint"
parameter may be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for prefetching to L2 cache.

[algorithm]

FOR j:= 0 to 7
    i := j*64
    m := j*64
    IF k[j]
        addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
        Prefetch(MEM[addr+31:addr], hint)
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VSCATTERPF0QPS - _mm512_prefetch_i64scatter_ps

| VSCATTERPF0QPS_MEMf32_MASKmskw_AVX512PF_VL512 | VSCATTERPF1QPS_MEMf32_MASKmskw_AVX512PF_VL512

--------------------------------------------------------------------------------------------------------------
Prefetch single-precision (32-bit) floating-point elements with intent to write into memory using 64-bit
indices. Elements are prefetched into cache level "hint", where "hint" is 0 or 1. 32-bit elements are stored at
addresses starting at "base_addr" and offset by each 64-bit element in "vindex" (each index is scaled by the
factor in "scale"). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 7
    i := j*64
    m := j*64
    addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
    Prefetch(MEM[addr+31:addr], hint)
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VSCATTERPF0QPS - _mm512_mask_prefetch_i64scatter_ps

| VSCATTERPF0QPS_MEMf32_MASKmskw_AVX512PF_VL512 | VSCATTERPF1QPS_MEMf32_MASKmskw_AVX512PF_VL512

--------------------------------------------------------------------------------------------------------------
Prefetch single-precision (32-bit) floating-point elements with intent to write into memory using 64-bit
indices. The "hint" parameter may be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for
prefetching to L2 cache. 32-bit elements are stored at addresses starting at "base_addr" and offset by each
64-bit element in "vindex" (each index is scaled by the factor in "scale") subject to mask "k" (elements are
not brought into cache when the corresponding mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 7
    i := j*64
    m := j*64
    IF k[j]
        addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
        Prefetch(MEM[addr+31:addr], hint)
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VGATHERPF0DPD - _mm512_prefetch_i32gather_pd

| VGATHERPF0DPD_MEMf64_MASKmskw_AVX512PF_VL512 | VGATHERPF1DPD_MEMf64_MASKmskw_AVX512PF_VL512

--------------------------------------------------------------------------------------------------------------
Prefetch double-precision (64-bit) floating-point elements from memory using 32-bit indices. 64-bit elements
are loaded from addresses starting at "base_addr" and offset by each 32-bit element in "vindex" (each index is
scaled by the factor in "scale"). Gathered elements are merged in cache. "scale" should be 1, 2, 4 or 8. The
"hint" parameter may be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for prefetching to L2
cache.

[algorithm]

FOR j := 0 to 7
    i := j*64
    m := j*32
    addr := base_addr + SignExtend64(vindex[m+31:m]) * ZeroExtend64(scale) * 8
    Prefetch(MEM[addr+63:addr], hint)
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VGATHERPF0DPD - _mm512_mask_prefetch_i32gather_pd

| VGATHERPF0DPD_MEMf64_MASKmskw_AVX512PF_VL512 | VGATHERPF1DPD_MEMf64_MASKmskw_AVX512PF_VL512

--------------------------------------------------------------------------------------------------------------
Prefetch double-precision (64-bit) floating-point elements from memory using 32-bit indices. 64-bit elements
are loaded from addresses starting at "base_addr" and offset by each 32-bit element in "vindex" (each index is
scaled by the factor in "scale"). Gathered elements are merged in cache using writemask "k" (elements are
brought into cache only when their corresponding mask bits are set). "scale" should be 1, 2, 4 or 8. The "hint"
parameter may be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for prefetching to L2 cache.

[algorithm]

FOR j := 0 to 7
    i := j*64
    m := j*32
    IF k[j]
        addr := base_addr + SignExtend64(vindex[m+31:m]) * ZeroExtend64(scale) * 8
        Prefetch(MEM[addr+63:addr], hint)
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VSCATTERPF0DPD - _mm512_prefetch_i32scatter_pd

| VSCATTERPF0DPD_MEMf64_MASKmskw_AVX512PF_VL512 | VSCATTERPF1DPD_MEMf64_MASKmskw_AVX512PF_VL512

--------------------------------------------------------------------------------------------------------------
Prefetch double-precision (64-bit) floating-point elements with intent to write using 32-bit indices. The
"hint" parameter may be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for prefetching to L2
cache. 64-bit elements are brought into cache from addresses starting at "base_addr" and offset by each 32-bit
element in "vindex" (each index is scaled by the factor in "scale"). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 7
    i := j*64
    m := j*32
    addr := base_addr + SignExtend64(vindex[m+31:m]) * ZeroExtend64(scale) * 8
    Prefetch(MEM[addr+63:addr], hint)
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VSCATTERPF0DPD - _mm512_mask_prefetch_i32scatter_pd

| VSCATTERPF0DPD_MEMf64_MASKmskw_AVX512PF_VL512 | VSCATTERPF1DPD_MEMf64_MASKmskw_AVX512PF_VL512

--------------------------------------------------------------------------------------------------------------
Prefetch double-precision (64-bit) floating-point elements with intent to write using 32-bit indices. The
"hint" parameter may be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for prefetching to L2
cache. 64-bit elements are brought into cache from addresses starting at "base_addr" and offset by each 32-bit
element in "vindex" (each index is scaled by the factor in "scale") subject to mask "k" (elements are not
brought into cache when the corresponding mask bit is not set). "scale" should be 1, 2, 4 or 8.

[algorithm]

FOR j := 0 to 7
    i := j*64
    m := j*32
    IF k[j]
        addr := base_addr + SignExtend64(vindex[m+31:m]) * ZeroExtend64(scale) * 8
        Prefetch(MEM[addr+63:addr], hint)
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VGATHERPF0QPD - _mm512_prefetch_i64gather_pd

| VGATHERPF0QPD_MEMf64_MASKmskw_AVX512PF_VL512 | VGATHERPF1QPD_MEMf64_MASKmskw_AVX512PF_VL512

--------------------------------------------------------------------------------------------------------------
Prefetch double-precision (64-bit) floating-point elements from memory into cache level specified by "hint"
using 64-bit indices. 64-bit elements are loaded from addresses starting at "base_addr" and offset by each
64-bit element in "vindex" (each index is scaled by the factor in "scale"). "scale" should be 1, 2, 4 or 8. The
"hint" parameter may be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for prefetching to L2
cache.

[algorithm]

FOR j := 0 to 7
    i := j*64
    m := j*64
    addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
    Prefetch(MEM[addr+63:addr], hint)
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VGATHERPF0QPD - _mm512_mask_prefetch_i64gather_pd

| VGATHERPF0QPD_MEMf64_MASKmskw_AVX512PF_VL512 | VGATHERPF1QPD_MEMf64_MASKmskw_AVX512PF_VL512

--------------------------------------------------------------------------------------------------------------
Prefetch double-precision (64-bit) floating-point elements from memory into cache level specified by "hint"
using 64-bit indices. 64-bit elements are loaded from addresses starting at "base_addr" and offset by each
64-bit element in "vindex" (each index is scaled by the factor in "scale"). Prefetched elements are merged in
cache using writemask "k" (elements are copied from memory when the corresponding mask bit is set). "scale"
should be 1, 2, 4 or 8. The "hint" parameter may be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2
(_MM_HINT_T1) for prefetching to L2 cache.

[algorithm]

FOR j := 0 to 7
    i := j*64
    m := j*64
    IF k[j]
        addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
        Prefetch(MEM[addr+63:addr], hint)
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VSCATTERPF0QPD - _mm512_prefetch_i64scatter_pd

| VSCATTERPF0QPD_MEMf64_MASKmskw_AVX512PF_VL512 | VSCATTERPF1QPD_MEMf64_MASKmskw_AVX512PF_VL512

--------------------------------------------------------------------------------------------------------------
Prefetch double-precision (64-bit) floating-point elements with intent to write into memory using 64-bit
indices. The "hint" parameter may be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for
prefetching to L2 cache. 64-bit elements are brought into cache from addresses starting at "base_addr" and
offset by each 64-bit element in "vindex" (each index is scaled by the factor in "scale"). "scale" should be 1,
2, 4 or 8.

[algorithm]

FOR j := 0 to 7
    i := j*64
    m := j*64
    addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
    Prefetch(MEM[addr+63:addr], hint)
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VSCATTERPF0QPD - _mm512_mask_prefetch_i64scatter_pd

| VSCATTERPF0QPD_MEMf64_MASKmskw_AVX512PF_VL512 | VSCATTERPF1QPD_MEMf64_MASKmskw_AVX512PF_VL512

--------------------------------------------------------------------------------------------------------------
Prefetch double-precision (64-bit) floating-point elements with intent to write into memory using 64-bit
indices. The "hint" parameter may be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for
prefetching to L2 cache. 64-bit elements are brought into cache from addresses starting at "base_addr" and
offset by each 64-bit element in "vindex" (each index is scaled by the factor in "scale") subject to mask "k"
(elements are not brought into cache when the corresponding mask bit is not set). "scale" should be 1, 2, 4 or
8.

[algorithm]

FOR j := 0 to 7
    i := j*64
    m := j*64
    IF k[j]
        addr := base_addr + vindex[m+63:m] * ZeroExtend64(scale) * 8
        Prefetch(MEM[addr+63:addr], hint)
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPOPCNTQ - _mm256_maskz_popcnt_epi64

| VPOPCNTQ_YMMu64_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Count the number of logical 1 bits in packed 64-bit integers in "a", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE POPCNT(a) {
    count := 0
    DO WHILE a &gt; 0
        count += a[0]
        a &gt;&gt;= 1
    OD
    RETURN count
}
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := POPCNT(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPOPCNTQ - _mm256_mask_popcnt_epi64

| VPOPCNTQ_YMMu64_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Count the number of logical 1 bits in packed 64-bit integers in "a", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

DEFINE POPCNT(a) {
    count := 0
    DO WHILE a &gt; 0
        count += a[0]
        a &gt;&gt;= 1
    OD
    RETURN count
}
FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := POPCNT(a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPOPCNTQ - _mm256_popcnt_epi64

| VPOPCNTQ_YMMu64_MASKmskw_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Count the number of logical 1 bits in packed 64-bit integers in "a", and store the results in "dst".

[algorithm]

DEFINE POPCNT(a) {
    count := 0
    DO WHILE a &gt; 0
        count += a[0]
        a &gt;&gt;= 1
    OD
    RETURN count
}
FOR j := 0 to 3
    i := j*64
    dst[i+63:i] := POPCNT(a[i+63:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPOPCNTQ - _mm_maskz_popcnt_epi64

| VPOPCNTQ_XMMu64_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Count the number of logical 1 bits in packed 64-bit integers in "a", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE POPCNT(a) {
    count := 0
    DO WHILE a &gt; 0
        count += a[0]
        a &gt;&gt;= 1
    OD
    RETURN count
}
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := POPCNT(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPOPCNTQ - _mm_mask_popcnt_epi64

| VPOPCNTQ_XMMu64_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Count the number of logical 1 bits in packed 64-bit integers in "a", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

DEFINE POPCNT(a) {
    count := 0
    DO WHILE a &gt; 0
        count += a[0]
        a &gt;&gt;= 1
    OD
    RETURN count
}
FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := POPCNT(a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPOPCNTQ - _mm_popcnt_epi64

| VPOPCNTQ_XMMu64_MASKmskw_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Count the number of logical 1 bits in packed 64-bit integers in "a", and store the results in "dst".

[algorithm]

DEFINE POPCNT(a) {
    count := 0
    DO WHILE a &gt; 0
        count += a[0]
        a &gt;&gt;= 1
    OD
    RETURN count
}
FOR j := 0 to 1
    i := j*64
    dst[i+63:i] := POPCNT(a[i+63:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPOPCNTD - _mm256_popcnt_epi32

| VPOPCNTD_YMMu32_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Count the number of logical 1 bits in packed 32-bit integers in "a", and store the results in "dst".

[algorithm]

DEFINE POPCNT(a) {
    count := 0
    DO WHILE a &gt; 0
        count += a[0]
        a &gt;&gt;= 1
    OD
    RETURN count
}
FOR j := 0 to 7
    i := j*32
    dst[i+31:i] := POPCNT(a[i+31:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPOPCNTD - _mm256_mask_popcnt_epi32

| VPOPCNTD_YMMu32_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Count the number of logical 1 bits in packed 32-bit integers in "a", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

DEFINE POPCNT(a) {
    count := 0
    DO WHILE a &gt; 0
        count += a[0]
        a &gt;&gt;= 1
    OD
    RETURN count
}
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := POPCNT(a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPOPCNTD - _mm256_maskz_popcnt_epi32

| VPOPCNTD_YMMu32_MASKmskw_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Count the number of logical 1 bits in packed 32-bit integers in "a", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE POPCNT(a) {
    count := 0
    DO WHILE a &gt; 0
        count += a[0]
        a &gt;&gt;= 1
    OD
    RETURN count
}
FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := POPCNT(a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPOPCNTD - _mm_popcnt_epi32

| VPOPCNTD_XMMu32_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Count the number of logical 1 bits in packed 32-bit integers in "a", and store the results in "dst".

[algorithm]

DEFINE POPCNT(a) {
    count := 0
    DO WHILE a &gt; 0
        count += a[0]
        a &gt;&gt;= 1
    OD
    RETURN count
}
FOR j := 0 to 3
    i := j*32
    dst[i+31:i] := POPCNT(a[i+31:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPOPCNTD - _mm_mask_popcnt_epi32

| VPOPCNTD_XMMu32_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Count the number of logical 1 bits in packed 32-bit integers in "a", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

DEFINE POPCNT(a) {
    count := 0
    DO WHILE a &gt; 0
        count += a[0]
        a &gt;&gt;= 1
    OD
    RETURN count
}
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := POPCNT(a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPOPCNTD - _mm_maskz_popcnt_epi32

| VPOPCNTD_XMMu32_MASKmskw_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Count the number of logical 1 bits in packed 32-bit integers in "a", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE POPCNT(a) {
    count := 0
    DO WHILE a &gt; 0
        count += a[0]
        a &gt;&gt;= 1
    OD
    RETURN count
}
FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := POPCNT(a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPOPCNTD - _mm512_popcnt_epi32

| VPOPCNTD_ZMMu32_MASKmskw_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Count the number of logical 1 bits in packed 32-bit integers in "a", and store the results in "dst".

[algorithm]

DEFINE POPCNT(a) {
    count := 0
    DO WHILE a &gt; 0
        count += a[0]
        a &gt;&gt;= 1
    OD
    RETURN count
}
FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := POPCNT(a[i+31:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPOPCNTD - _mm512_mask_popcnt_epi32

| VPOPCNTD_ZMMu32_MASKmskw_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Count the number of logical 1 bits in packed 32-bit integers in "a", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

DEFINE POPCNT(a) {
    count := 0
    DO WHILE a &gt; 0
        count += a[0]
        a &gt;&gt;= 1
    OD
    RETURN count
}
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := POPCNT(a[i+31:i])
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPOPCNTD - _mm512_maskz_popcnt_epi32

| VPOPCNTD_ZMMu32_MASKmskw_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Count the number of logical 1 bits in packed 32-bit integers in "a", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE POPCNT(a) {
    count := 0
    DO WHILE a &gt; 0
        count += a[0]
        a &gt;&gt;= 1
    OD
    RETURN count
}
FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := POPCNT(a[i+31:i])
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPOPCNTQ - _mm512_popcnt_epi64

| VPOPCNTQ_ZMMu64_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Count the number of logical 1 bits in packed 64-bit integers in "a", and store the results in "dst".

[algorithm]

DEFINE POPCNT(a) {
    count := 0
    DO WHILE a &gt; 0
        count += a[0]
        a &gt;&gt;= 1
    OD
    RETURN count
}
FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := POPCNT(a[i+63:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPOPCNTQ - _mm512_mask_popcnt_epi64

| VPOPCNTQ_ZMMu64_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Count the number of logical 1 bits in packed 64-bit integers in "a", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

DEFINE POPCNT(a) {
    count := 0
    DO WHILE a &gt; 0
        count += a[0]
        a &gt;&gt;= 1
    OD
    RETURN count
}
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := POPCNT(a[i+63:i])
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPOPCNTQ - _mm512_maskz_popcnt_epi64

| VPOPCNTQ_ZMMu64_MASKmskw_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Count the number of logical 1 bits in packed 64-bit integers in "a", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE POPCNT(a) {
    count := 0
    DO WHILE a &gt; 0
        count += a[0]
        a &gt;&gt;= 1
    OD
    RETURN count
}
FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := POPCNT(a[i+63:i])
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## V4FMADDPS - _mm512_4fmadd_ps

| V4FMADDPS_ZMMf32_MASKmskw_ZMMf32_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements specified in 4 consecutive operands "a0"
through "a3" by the 4 corresponding packed elements in "b", accumulate with the corresponding elements in
"src", and store the results in "dst".

[algorithm]

dst[511:0] := src[511:0]
FOR i := 0 to 15
    FOR m := 0 to 3
        addr := b + m * 32
        dst.fp32[i] := dst.fp32[i] + a{m}.fp32[i] * Cast_FP32(MEM[addr+31:addr])
    ENDFOR
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## V4FMADDPS - _mm512_mask_4fmadd_ps

| V4FMADDPS_ZMMf32_MASKmskw_ZMMf32_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements specified in 4 consecutive operands "a0"
through "a3" by the 4 corresponding packed elements in "b", accumulate with the corresponding elements in
"src", and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding
mask bit is not set).

[algorithm]

dst[511:0] := src[511:0]
FOR i := 0 to 15
    FOR m := 0 to 3
        addr := b + m * 32
        IF k[i]
            dst.fp32[i] := dst.fp32[i] + a{m}.fp32[i] * Cast_FP32(MEM[addr+31:addr])
        FI
    ENDFOR
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## V4FMADDPS - _mm512_maskz_4fmadd_ps

| V4FMADDPS_ZMMf32_MASKmskw_ZMMf32_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements specified in 4 consecutive operands "a0"
through "a3" by the 4 corresponding packed elements in "b", accumulate with the corresponding elements in
"src", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask
bit is not set).

[algorithm]

dst[511:0] := src[511:0]
FOR i := 0 to 15
    FOR m := 0 to 3
        addr := b + m * 32
        IF k[i]
            dst.fp32[i] := dst.fp32[i] + a{m}.fp32[i] * Cast_FP32(MEM[addr+31:addr])
        ELSE
            dst.fp32[i] := 0
        FI
    ENDFOR
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## V4FNMADDPS - _mm512_4fnmadd_ps

| V4FNMADDPS_ZMMf32_MASKmskw_ZMMf32_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements specified in 4 consecutive operands "a0"
through "a3" by the 4 corresponding packed elements in "b", accumulate the negated intermediate result with the
corresponding elements in "src", and store the results in "dst".

[algorithm]

dst[511:0] := src[511:0]
FOR i := 0 to 15
    FOR m := 0 to 3
        addr := b + m * 32
        dst.fp32[i] := dst.fp32[i] - a{m}.fp32[i] * Cast_FP32(MEM[addr+31:addr])
    ENDFOR
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## V4FNMADDPS - _mm512_mask_4fnmadd_ps

| V4FNMADDPS_ZMMf32_MASKmskw_ZMMf32_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements specified in 4 consecutive operands "a0"
through "a3" by the 4 corresponding packed elements in "b", accumulate the negated intermediate result with the
corresponding elements in "src", and store the results in "dst" using writemask "k" (elements are copied from
"a" when the corresponding mask bit is not set).

[algorithm]

dst[511:0] := src[511:0]
FOR i := 0 to 15
    FOR m := 0 to 3
        addr := b + m * 32
        IF k[i]
            dst.fp32[i] := dst.fp32[i] - a{m}.fp32[i] * Cast_FP32(MEM[addr+31:addr])
        FI
    ENDFOR
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## V4FNMADDPS - _mm512_maskz_4fnmadd_ps

| V4FNMADDPS_ZMMf32_MASKmskw_ZMMf32_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply packed single-precision (32-bit) floating-point elements specified in 4 consecutive operands "a0"
through "a3" by the 4 corresponding packed elements in "b", accumulate the negated intermediate result with the
corresponding elements in "src", and store the results in "dst" using zeromask "k" (elements are zeroed out
when the corresponding mask bit is not set).

[algorithm]

dst[511:0] := src[511:0]
FOR i := 0 to 15
    FOR m := 0 to 3
        addr := b + m * 32
        IF k[i]
            dst.fp32[i] := dst.fp32[i] - a{m}.fp32[i] * Cast_FP32(MEM[addr+31:addr])
        ELSE
            dst.fp32[i] := 0
        FI
    ENDFOR
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## V4FMADDSS - _mm_4fmadd_ss

| V4FMADDSS_XMMf32_MASKmskw_XMMf32_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point elements specified in 4 consecutive operands "a0"
through "a3" by corresponding element in "b", accumulate with the lower element in "a", and store the result in
the lower element of "dst".

[algorithm]

dst[127:0] := src[127:0]
FOR m := 0 to 3
    addr := b + m * 32
    dst.fp32[0] := dst.fp32[0] + a{m}.fp32[0] * Cast_FP32(MEM[addr+31:addr])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## V4FMADDSS - _mm_mask_4fmadd_ss

| V4FMADDSS_XMMf32_MASKmskw_XMMf32_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point elements specified in 4 consecutive operands "a0"
through "a3" by corresponding element in "b", accumulate with the lower element in "a", and store the result in
the lower element of "dst" using writemask "k" (the element is copied from "a" when mask bit 0 is not set).

[algorithm]

dst[127:0] := src[127:0]
IF k[0]
    FOR m := 0 to 3
        addr := b + m * 32
        dst.fp32[0] := dst.fp32[0] + a{m}.fp32[0] * Cast_FP32(MEM[addr+31:addr])
    ENDFOR
FI
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## V4FMADDSS - _mm_maskz_4fmadd_ss

| V4FMADDSS_XMMf32_MASKmskw_XMMf32_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point elements specified in 4 consecutive operands "a0"
through "a3" by corresponding element in "b", accumulate with the lower element in "a", and store the result in
the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set).

[algorithm]

dst[127:0] := src[127:0]
IF k[0]
    FOR m := 0 to 3
        addr := b + m * 32
        dst.fp32[0] := dst.fp32[0] + a{m}.fp32[0] * Cast_FP32(MEM[addr+31:addr])
    ENDFOR
ELSE
    dst.fp32[0] := 0
FI
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## V4FNMADDSS - _mm_4fnmadd_ss

| V4FNMADDSS_XMMf32_MASKmskw_XMMf32_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point elements specified in 4 consecutive operands "a0"
through "a3" by corresponding element in "b", accumulate the negated intermediate result with the lower element
in "src", and store the result in the lower element of "dst".

[algorithm]

dst[127:0] := src[127:0]
FOR m := 0 to 3
    addr := b + m * 32
    dst.fp32[0] := dst.fp32[0] - a{m}.fp32[0] * Cast_FP32(MEM[addr+31:addr])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## V4FNMADDSS - _mm_mask_4fnmadd_ss

| V4FNMADDSS_XMMf32_MASKmskw_XMMf32_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point elements specified in 4 consecutive operands "a0"
through "a3" by corresponding element in "b", accumulate the negated intermediate result with the lower element
in "src", and store the result in the lower element of "dst" using writemask "k" (the element is copied from
"a" when mask bit 0 is not set).

[algorithm]

dst[127:0] := src[127:0]
IF k[0]
    FOR m := 0 to 3
        addr := b + m * 32
        dst.fp32[0] := dst.fp32[0] - a{m}.fp32[0] * Cast_FP32(MEM[addr+31:addr])
    ENDFOR
FI
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## V4FNMADDSS - _mm_maskz_4fnmadd_ss

| V4FNMADDSS_XMMf32_MASKmskw_XMMf32_MEMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply the lower single-precision (32-bit) floating-point elements specified in 4 consecutive operands "a0"
through "a3" by corresponding element in "b", accumulate the negated intermediate result with the lower element
in "src", and store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when
mask bit 0 is not set).

[algorithm]

dst[127:0] := src[127:0]
IF k[0]
    FOR m := 0 to 3
        addr := b + m * 32
        dst.fp32[0] := dst.fp32[0] - a{m}.fp32[0] * Cast_FP32(MEM[addr+31:addr])
    ENDFOR
ELSE
    dst.fp32[0] := 0
FI
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VP4DPWSSD - _mm512_4dpwssd_epi32

| VP4DPWSSD_ZMMi32_MASKmskw_ZMMi16_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute 4 sequential operand source-block dot-products of two signed 16-bit element operands with 32-bit
element accumulation, and store the results in "dst".

[algorithm]

dst[511:0] := src[511:0]
FOR i := 0 to 15
    FOR m := 0 to 3
        lim_base := b + m*32
        t.dword  := MEM[lim_base+31:lim_base]
        p1.dword := SignExtend32(a{m}.word[2*i+0]) * SignExtend32(Cast_Int16(t.word[0]))
        p2.dword := SignExtend32(a{m}.word[2*i+1]) * SignExtend32(Cast_Int16(t.word[1]))
        dst.dword[i] := dst.dword[i] + p1.dword + p2.dword
    ENDFOR
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VP4DPWSSD - _mm512_mask_4dpwssd_epi32

| VP4DPWSSD_ZMMi32_MASKmskw_ZMMi16_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute 4 sequential operand source-block dot-products of two signed 16-bit element operands with 32-bit
element accumulation with mask, and store the results in "dst" using writemask "k" (elements are copied from
"src" when the corresponding mask bit is not set).

[algorithm]

dst[511:0] := src[511:0]
FOR i := 0 to 15
    IF k[i]
        FOR m := 0 to 3
            lim_base := b + m*32
            t.dword  := MEM[lim_base+31:lim_base]
            p1.dword := SignExtend32(a{m}.word[2*i+0]) * SignExtend32(Cast_Int16(t.word[0]))
            p2.dword := SignExtend32(a{m}.word[2*i+1]) * SignExtend32(Cast_Int16(t.word[1]))
            dst.dword[i] := dst.dword[i] + p1.dword + p2.dword
        ENDFOR
    ELSE
        dst.dword[i] := src.dword[i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VP4DPWSSD - _mm512_maskz_4dpwssd_epi32

| VP4DPWSSD_ZMMi32_MASKmskw_ZMMi16_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute 4 sequential operand source-block dot-products of two signed 16-bit element operands with 32-bit
element accumulation with mask, and store the results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).

[algorithm]

dst[511:0] := src[511:0]
FOR i := 0 to 15
    IF k[i]
        FOR m := 0 to 3
            lim_base := b + m*32
            t.dword  := MEM[lim_base+31:lim_base]
            p1.dword := SignExtend32(a{m}.word[2*i+0]) * SignExtend32(Cast_Int16(t.word[0]))
            p2.dword := SignExtend32(a{m}.word[2*i+1]) * SignExtend32(Cast_Int16(t.word[1]))
            dst.dword[i] := dst.dword[i] + p1.dword + p2.dword
        ENDFOR
    ELSE
        dst.dword[i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VP4DPWSSDS - _mm512_4dpwssds_epi32

| VP4DPWSSDS_ZMMi32_MASKmskw_ZMMi16_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute 4 sequential operand source-block dot-products of two signed 16-bit element operands with 32-bit
element accumulation and signed saturation, and store the results in "dst".

[algorithm]

dst[511:0] := src[511:0]
FOR i := 0 to 15
    FOR m := 0 to 3
        lim_base := b + m*32
        t.dword  := MEM[lim_base+31:lim_base]
        p1.dword := SignExtend32(a{m}.word[2*i+0]) * SignExtend32(Cast_Int16(t.word[0]))
        p2.dword := SignExtend32(a{m}.word[2*i+1]) * SignExtend32(Cast_Int16(t.word[1]))
        dst.dword[i] := Saturate32(dst.dword[i] + p1.dword + p2.dword)
    ENDFOR
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VP4DPWSSDS - _mm512_mask_4dpwssds_epi32

| VP4DPWSSDS_ZMMi32_MASKmskw_ZMMi16_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute 4 sequential operand source-block dot-products of two signed 16-bit element operands with 32-bit
element accumulation with mask and signed saturation, and store the results in "dst" using writemask "k"
(elements are copied from "src" when the corresponding mask bit is not set)..

[algorithm]

dst[511:0] := src[511:0]
FOR i := 0 to 15
    IF k[i]
        FOR m := 0 to 3
            lim_base := b + m*32
            t.dword  := MEM[lim_base+31:lim_base]
            p1.dword := SignExtend32(a{m}.word[2*i+0]) * SignExtend32(Cast_Int16(t.word[0]))
            p2.dword := SignExtend32(a{m}.word[2*i+1]) * SignExtend32(Cast_Int16(t.word[1]))
            dst.dword[i] := Saturate32(dst.dword[i] + p1.dword + p2.dword)
        ENDFOR
    ELSE
        dst.dword[i] := src.dword[i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VP4DPWSSDS - _mm512_maskz_4dpwssds_epi32

| VP4DPWSSDS_ZMMi32_MASKmskw_ZMMi16_MEMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute 4 sequential operand source-block dot-products of two signed 16-bit element operands with 32-bit
element accumulation with mask and signed saturation, and store the results in "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set)..

[algorithm]

dst[511:0] := src[511:0]
FOR i := 0 to 15
    IF k[i]
        FOR m := 0 to 3
            lim_base := b + m*32
            t.dword  := MEM[lim_base+31:lim_base]
            p1.dword := SignExtend32(a{m}.word[2*i+0]) * SignExtend32(Cast_Int16(t.word[0]))
            p2.dword := SignExtend32(a{m}.word[2*i+1]) * SignExtend32(Cast_Int16(t.word[1]))
            dst.dword[i] := Saturate32(dst.dword[i] + p1.dword + p2.dword)
        ENDFOR
    ELSE
        dst.dword[i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTNE2PS2BF16 - _mm_cvtne2ps_pbh

| VCVTNE2PS2BF16_XMMbf16_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in two vectors "a" and "b" to packed BF16
(16-bit) floating-point elements, and store the results in single vector "dst".

[algorithm]

FOR j := 0 to 7
    IF j &lt; 4
        t := b.fp32[j]
    ELSE
        t := a.fp32[j-4]
    FI
    dst.word[j] := Convert_FP32_To_BF16(t)
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTNE2PS2BF16 - _mm_mask_cvtne2ps_pbh

| VCVTNE2PS2BF16_XMMbf16_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in two vectors "a" and "b" to packed BF16
(16-bit) floating-point elements, and store the results in single vector "dst" using writemask "k" (elements
are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    IF k[j]
        IF j &lt; 4
            t := b.fp32[j]
        ELSE
            t := a.fp32[j-4]
        FI
        dst.word[j] := Convert_FP32_To_BF16(t)
    ELSE
        dst.word[j] := src.word[j]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTNE2PS2BF16 - _mm_maskz_cvtne2ps_pbh

| VCVTNE2PS2BF16_XMMbf16_MASKmskw_XMMf32_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in two vectors "a" and "b" to packed BF16
(16-bit) floating-point elements, and store the results in single vector "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    IF k[j]
        IF j &lt; 4
            t := b.fp32[j]
        ELSE
            t := a.fp32[j-4]
        FI
        dst.word[j] := Convert_FP32_To_BF16(t)
    ELSE
        dst.word[j] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTNE2PS2BF16 - _mm256_cvtne2ps_pbh

| VCVTNE2PS2BF16_YMMbf16_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in two vectors "a" and "b" to packed BF16
(16-bit) floating-point elements, and store the results in single vector "dst".

[algorithm]

FOR j := 0 to 15
    IF j &lt; 8
        t := b.fp32[j]
    ELSE
        t := a.fp32[j-8]
    FI
    dst.word[j] := Convert_FP32_To_BF16(t)
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTNE2PS2BF16 - _mm256_mask_cvtne2ps_pbh

| VCVTNE2PS2BF16_YMMbf16_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in two vectors "a" and "b" to packed BF16
(16-bit) floating-point elements, and store the results in single vector "dst" using writemask "k" (elements
are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    IF k[j]
        IF j &lt; 8
            t := b.fp32[j]
        ELSE
            t := a.fp32[j-8]
        FI
        dst.word[j] := Convert_FP32_To_BF16(t)
    ELSE
        dst.word[j] := src.word[j]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTNE2PS2BF16 - _mm256_maskz_cvtne2ps_pbh

| VCVTNE2PS2BF16_YMMbf16_MASKmskw_YMMf32_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in two vectors "a" and "b" to packed BF16
(16-bit) floating-point elements, and store the results in single vector "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    IF k[j]
        IF j &lt; 8
            t := b.fp32[j]
        ELSE
            t := a.fp32[j-8]
        FI
        dst.word[j] := Convert_FP32_To_BF16(t)
    ELSE
        dst.word[j] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTNE2PS2BF16 - _mm512_cvtne2ps_pbh

| VCVTNE2PS2BF16_ZMMbf16_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in two vectors "a" and "b" to packed BF16
(16-bit) floating-point elements, and store the results in single vector "dst".

[algorithm]

FOR j := 0 to 31
    IF j &lt; 16
        t := b.fp32[j]
    ELSE
        t := a.fp32[j-16]
    FI
    dst.word[j] := Convert_FP32_To_BF16(t)
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTNE2PS2BF16 - _mm512_mask_cvtne2ps_pbh

| VCVTNE2PS2BF16_ZMMbf16_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in two vectors "a" and "b" to packed BF16
(16-bit) floating-point elements, and store the results in single vector "dst" using writemask "k" (elements
are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    IF k[j]
        IF j &lt; 16
            t := b.fp32[j]
        ELSE
            t := a.fp32[j-16]
        FI
        dst.word[j] := Convert_FP32_To_BF16(t)
    ELSE
        dst.word[j] := src.word[j]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTNE2PS2BF16 - _mm512_maskz_cvtne2ps_pbh

| VCVTNE2PS2BF16_ZMMbf16_MASKmskw_ZMMf32_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in two vectors "a" and "b" to packed BF16
(16-bit) floating-point elements, and store the results in single vector "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    IF k[j]
        IF j &lt; 16
            t := b.fp32[j]
        ELSE
            t := a.fp32[j-16]
        FI
        dst.word[j] := Convert_FP32_To_BF16(t)
    ELSE
        dst.word[j] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTNEPS2BF16 - _mm_cvtneps_pbh

| VCVTNEPS2BF16_XMMbf16_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed BF16 (16-bit) floating-point
elements, and store the results in "dst".

[algorithm]

FOR j := 0 to 3
    dst.word[j] := Convert_FP32_To_BF16(a.fp32[j])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTNEPS2BF16 - _mm_mask_cvtneps_pbh

| VCVTNEPS2BF16_XMMbf16_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed BF16 (16-bit) floating-point
elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    IF k[j]
        dst.word[j] := Convert_FP32_To_BF16(a.fp32[j])
    ELSE
        dst.word[j] := src.word[j]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTNEPS2BF16 - _mm_maskz_cvtneps_pbh

| VCVTNEPS2BF16_XMMbf16_MASKmskw_XMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed BF16 (16-bit) floating-point
elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set).

[algorithm]

FOR j := 0 to 3
    IF k[j]
        dst.word[j] := Convert_FP32_To_BF16(a.fp32[j])
    ELSE
        dst.word[j] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTNEPS2BF16 - _mm256_cvtneps_pbh

| VCVTNEPS2BF16_XMMbf16_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed BF16 (16-bit) floating-point
elements, and store the results in "dst".

[algorithm]

FOR j := 0 to 7
    dst.word[j] := Convert_FP32_To_BF16(a.fp32[j])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTNEPS2BF16 - _mm256_mask_cvtneps_pbh

| VCVTNEPS2BF16_XMMbf16_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed BF16 (16-bit) floating-point
elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    IF k[j]
        dst.word[j] := Convert_FP32_To_BF16(a.fp32[j])
    ELSE
        dst.word[j] := src.word[j]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTNEPS2BF16 - _mm256_maskz_cvtneps_pbh

| VCVTNEPS2BF16_XMMbf16_MASKmskw_YMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed BF16 (16-bit) floating-point
elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set).

[algorithm]

FOR j := 0 to 7
    IF k[j]
        dst.word[j] := Convert_FP32_To_BF16(a.fp32[j])
    ELSE
        dst.word[j] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTNEPS2BF16 - _mm512_cvtneps_pbh

| VCVTNEPS2BF16_YMMbf16_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed BF16 (16-bit) floating-point
elements, and store the results in "dst".

[algorithm]

FOR j := 0 to 15
    dst.word[j] := Convert_FP32_To_BF16(a.fp32[j])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTNEPS2BF16 - _mm512_mask_cvtneps_pbh

| VCVTNEPS2BF16_YMMbf16_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed BF16 (16-bit) floating-point
elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    IF k[j]
        dst.word[j] := Convert_FP32_To_BF16(a.fp32[j])
    ELSE
        dst.word[j] := src.word[j]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VCVTNEPS2BF16 - _mm512_maskz_cvtneps_pbh

| VCVTNEPS2BF16_YMMbf16_MASKmskw_ZMMf32_AVX512

--------------------------------------------------------------------------------------------------------------
Convert packed single-precision (32-bit) floating-point elements in "a" to packed BF16 (16-bit) floating-point
elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set).

[algorithm]

FOR j := 0 to 15
    IF k[j]
        dst.word[j] := Convert_FP32_To_BF16(a.fp32[j])
    ELSE
        dst.word[j] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VDPBF16PS - _mm_dpbf16_ps

| VDPBF16PS_XMMf32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute dot-product of BF16 (16-bit) floating-point pairs in "a" and "b", accumulating the intermediate
single-precision (32-bit) floating-point elements with elements in "src", and store the results in "dst".

[algorithm]

DEFINE make_fp32(x[15:0]) {
    y.fp32  := 0.0
    y[31:16] := x[15:0]
    RETURN y
}
dst := src
FOR j := 0 to 3
    dst.fp32[j] += make_fp32(a.bf16[2*j+1]) * make_fp32(b.bf16[2*j+1])
    dst.fp32[j] += make_fp32(a.bf16[2*j+0]) * make_fp32(b.bf16[2*j+0])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VDPBF16PS - _mm_mask_dpbf16_ps

| VDPBF16PS_XMMf32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute dot-product of BF16 (16-bit) floating-point pairs in "a" and "b", accumulating the intermediate
single-precision (32-bit) floating-point elements with elements in "src", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

DEFINE make_fp32(x[15:0]) {
    y.fp32  := 0.0
    y[31:16] := x[15:0]
    RETURN y
}
dst := src
FOR j := 0 to 3
    IF k[j]
        dst.fp32[j] += make_fp32(a.bf16[2*j+1]) * make_fp32(b.bf16[2*j+1])
        dst.fp32[j] += make_fp32(a.bf16[2*j+0]) * make_fp32(b.bf16[2*j+0])
    ELSE
        dst.dword[j] := src.dword[j]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VDPBF16PS - _mm_maskz_dpbf16_ps

| VDPBF16PS_XMMf32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute dot-product of BF16 (16-bit) floating-point pairs in "a" and "b", accumulating the intermediate
single-precision (32-bit) floating-point elements with elements in "src", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE make_fp32(x[15:0]) {
    y.fp32  := 0.0
    y[31:16] := x[15:0]
    RETURN y
}
dst := src
FOR j := 0 to 3
    IF k[j]
        dst.fp32[j] += make_fp32(a.bf16[2*j+1]) * make_fp32(b.bf16[2*j+1])
        dst.fp32[j] += make_fp32(a.bf16[2*j+0]) * make_fp32(b.bf16[2*j+0])
    ELSE
        dst.dword[j] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VDPBF16PS - _mm256_dpbf16_ps

| VDPBF16PS_YMMf32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute dot-product of BF16 (16-bit) floating-point pairs in "a" and "b", accumulating the intermediate
single-precision (32-bit) floating-point elements with elements in "src", and store the results in "dst".

[algorithm]

DEFINE make_fp32(x[15:0]) {
    y.fp32  := 0.0
    y[31:16] := x[15:0]
    RETURN y
}
dst := src
FOR j := 0 to 7
    dst.fp32[j] += make_fp32(a.bf16[2*j+1]) * make_fp32(b.bf16[2*j+1])
    dst.fp32[j] += make_fp32(a.bf16[2*j+0]) * make_fp32(b.bf16[2*j+0])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VDPBF16PS - _mm256_mask_dpbf16_ps

| VDPBF16PS_YMMf32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute dot-product of BF16 (16-bit) floating-point pairs in "a" and "b", accumulating the intermediate
single-precision (32-bit) floating-point elements with elements in "src", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

DEFINE make_fp32(x[15:0]) {
    y.fp32  := 0.0
    y[31:16] := x[15:0]
    RETURN y
}
dst := src
FOR j := 0 to 7
    IF k[j]
        dst.fp32[j] += make_fp32(a.bf16[2*j+1]) * make_fp32(b.bf16[2*j+1])
        dst.fp32[j] += make_fp32(a.bf16[2*j+0]) * make_fp32(b.bf16[2*j+0])
    ELSE
        dst.dword[j] := src.dword[j]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VDPBF16PS - _mm256_maskz_dpbf16_ps

| VDPBF16PS_YMMf32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute dot-product of BF16 (16-bit) floating-point pairs in "a" and "b", accumulating the intermediate
single-precision (32-bit) floating-point elements with elements in "src", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE make_fp32(x[15:0]) {
    y.fp32  := 0.0
    y[31:16] := x[15:0]
    RETURN y
}
dst := src
FOR j := 0 to 7
    IF k[j]
        dst.fp32[j] += make_fp32(a.bf16[2*j+1]) * make_fp32(b.bf16[2*j+1])
        dst.fp32[j] += make_fp32(a.bf16[2*j+0]) * make_fp32(b.bf16[2*j+0])
    ELSE
        dst.dword[j] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VDPBF16PS - _mm512_dpbf16_ps

| VDPBF16PS_ZMMf32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute dot-product of BF16 (16-bit) floating-point pairs in "a" and "b", accumulating the intermediate
single-precision (32-bit) floating-point elements with elements in "src", and store the results in "dst".

[algorithm]

DEFINE make_fp32(x[15:0]) {
    y.fp32  := 0.0
    y[31:16] := x[15:0]
    RETURN y
}
dst := src
FOR j := 0 to 15
    dst.fp32[j] += make_fp32(a.bf16[2*j+1]) * make_fp32(b.bf16[2*j+1])
    dst.fp32[j] += make_fp32(a.bf16[2*j+0]) * make_fp32(b.bf16[2*j+0])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VDPBF16PS - _mm512_mask_dpbf16_ps

| VDPBF16PS_ZMMf32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute dot-product of BF16 (16-bit) floating-point pairs in "a" and "b", accumulating the intermediate
single-precision (32-bit) floating-point elements with elements in "src", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

DEFINE make_fp32(x[15:0]) {
    y.fp32  := 0.0
    y[31:16] := x[15:0]
    RETURN y
}
dst := src
FOR j := 0 to 15
    IF k[j]
        dst.fp32[j] += make_fp32(a.bf16[2*j+1]) * make_fp32(b.bf16[2*j+1])
        dst.fp32[j] += make_fp32(a.bf16[2*j+0]) * make_fp32(b.bf16[2*j+0])
    ELSE
        dst.dword[j] := src.dword[j]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VDPBF16PS - _mm512_maskz_dpbf16_ps

| VDPBF16PS_ZMMf32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute dot-product of BF16 (16-bit) floating-point pairs in "a" and "b", accumulating the intermediate
single-precision (32-bit) floating-point elements with elements in "src", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE make_fp32(x[15:0]) {
    y.fp32  := 0.0
    y[31:16] := x[15:0]
    RETURN y
}
dst := src
FOR j := 0 to 15
    IF k[j]
        dst.fp32[j] += make_fp32(a.bf16[2*j+1]) * make_fp32(b.bf16[2*j+1])
        dst.fp32[j] += make_fp32(a.bf16[2*j+0]) * make_fp32(b.bf16[2*j+0])
    ELSE
        dst.dword[j] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHUFBITQMB - _mm512_mask_bitshuffle_epi64_mask

| VPSHUFBITQMB_MASKmskw_MASKmskw_ZMMu64_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Gather 64 bits from "b" using selection bits in "c". For each 64-bit element in "b", gather 8 bits from the
64-bit element in "b" at 8 bit position controlled by the 8 corresponding 8-bit elements of "c", and store the
result in the corresponding 8-bit element of "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR i := 0 to 7 //Qword
    FOR j := 0 to 7 // Byte
        IF k[i*8+j]
            m := c.qword[i].byte[j] &amp; 0x3F
            dst[i*8+j] := b.qword[i].bit[m]
        ELSE
            dst[i*8+j] := 0
        FI
    ENDFOR
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHUFBITQMB - _mm512_bitshuffle_epi64_mask

| VPSHUFBITQMB_MASKmskw_MASKmskw_ZMMu64_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Gather 64 bits from "b" using selection bits in "c". For each 64-bit element in "b", gather 8 bits from the
64-bit element in "b" at 8 bit position controlled by the 8 corresponding 8-bit elements of "c", and store the
result in the corresponding 8-bit element of "dst".

[algorithm]

FOR i := 0 to 7 //Qword
    FOR j := 0 to 7 // Byte
        m := c.qword[i].byte[j] &amp; 0x3F
        dst[i*8+j] := b.qword[i].bit[m]
    ENDFOR
ENDFOR
dst[MAX:64] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHUFBITQMB - _mm256_mask_bitshuffle_epi64_mask

| VPSHUFBITQMB_MASKmskw_MASKmskw_YMMu64_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Gather 64 bits from "b" using selection bits in "c". For each 64-bit element in "b", gather 8 bits from the
64-bit element in "b" at 8 bit position controlled by the 8 corresponding 8-bit elements of "c", and store the
result in the corresponding 8-bit element of "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR i := 0 to 3 //Qword
    FOR j := 0 to 7 // Byte
        IF k[i*8+j]
            m := c.qword[i].byte[j] &amp; 0x3F
            dst[i*8+j] := b.qword[i].bit[m]
        ELSE
            dst[i*8+j] := 0
        FI
    ENDFOR
ENDFOR
dst[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHUFBITQMB - _mm256_bitshuffle_epi64_mask

| VPSHUFBITQMB_MASKmskw_MASKmskw_YMMu64_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Gather 64 bits from "b" using selection bits in "c". For each 64-bit element in "b", gather 8 bits from the
64-bit element in "b" at 8 bit position controlled by the 8 corresponding 8-bit elements of "c", and store the
result in the corresponding 8-bit element of "dst".

[algorithm]

FOR i := 0 to 3 //Qword
    FOR j := 0 to 7 // Byte
        m := c.qword[i].byte[j] &amp; 0x3F
        dst[i*8+j] := b.qword[i].bit[m]
    ENDFOR
ENDFOR
dst[MAX:32] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHUFBITQMB - _mm_mask_bitshuffle_epi64_mask

| VPSHUFBITQMB_MASKmskw_MASKmskw_XMMu64_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Gather 64 bits from "b" using selection bits in "c". For each 64-bit element in "b", gather 8 bits from the
64-bit element in "b" at 8 bit position controlled by the 8 corresponding 8-bit elements of "c", and store the
result in the corresponding 8-bit element of "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR i := 0 to 1 //Qword
    FOR j := 0 to 7 // Byte
        IF k[i*8+j]
            m := c.qword[i].byte[j] &amp; 0x3F
            dst[i*8+j] := b.qword[i].bit[m]
        ELSE
            dst[i*8+j] := 0
        FI
    ENDFOR
ENDFOR
dst[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHUFBITQMB - _mm_bitshuffle_epi64_mask

| VPSHUFBITQMB_MASKmskw_MASKmskw_XMMu64_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Gather 64 bits from "b" using selection bits in "c". For each 64-bit element in "b", gather 8 bits from the
64-bit element in "b" at 8 bit position controlled by the 8 corresponding 8-bit elements of "c", and store the
result in the corresponding 8-bit element of "dst".

[algorithm]

FOR i := 0 to 1 //Qword
    FOR j := 0 to 7 // Byte
        m := c.qword[i].byte[j] &amp; 0x3F
        dst[i*8+j] := b.qword[i].bit[m]
    ENDFOR
ENDFOR
dst[MAX:16] := 0

--------------------------------------------------------------------------------------------------------------

## VPOPCNTW - _mm512_popcnt_epi16

| VPOPCNTW_ZMMu16_MASKmskw_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Count the number of logical 1 bits in packed 16-bit integers in "a", and store the results in "dst".

[algorithm]

DEFINE POPCNT(a) {
    count := 0
    DO WHILE a &gt; 0
        count += a[0]
        a &gt;&gt;= 1
    OD
    RETURN count
}
FOR j := 0 to 31
    i := j*16
    dst[i+15:i] := POPCNT(a[i+15:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPOPCNTW - _mm512_mask_popcnt_epi16

| VPOPCNTW_ZMMu16_MASKmskw_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Count the number of logical 1 bits in packed 16-bit integers in "a", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

DEFINE POPCNT(a) {
    count := 0
    DO WHILE a &gt; 0
        count += a[0]
        a &gt;&gt;= 1
    OD
    RETURN count
}
FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := POPCNT(a[i+15:i])
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPOPCNTW - _mm512_maskz_popcnt_epi16

| VPOPCNTW_ZMMu16_MASKmskw_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Count the number of logical 1 bits in packed 16-bit integers in "a", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE POPCNT(a) {
    count := 0
    DO WHILE a &gt; 0
        count += a[0]
        a &gt;&gt;= 1
    OD
    RETURN count
}
FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := POPCNT(a[i+15:i])
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPOPCNTW - _mm256_popcnt_epi16

| VPOPCNTW_YMMu16_MASKmskw_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Count the number of logical 1 bits in packed 16-bit integers in "a", and store the results in "dst".

[algorithm]

DEFINE POPCNT(a) {
    count := 0
    DO WHILE a &gt; 0
        count += a[0]
        a &gt;&gt;= 1
    OD
    RETURN count
}
FOR j := 0 to 15
    i := j*16
    dst[i+15:i] := POPCNT(a[i+15:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPOPCNTW - _mm256_mask_popcnt_epi16

| VPOPCNTW_YMMu16_MASKmskw_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Count the number of logical 1 bits in packed 16-bit integers in "a", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

DEFINE POPCNT(a) {
    count := 0
    DO WHILE a &gt; 0
        count += a[0]
        a &gt;&gt;= 1
    OD
    RETURN count
}
FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := POPCNT(a[i+15:i])
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPOPCNTW - _mm256_maskz_popcnt_epi16

| VPOPCNTW_YMMu16_MASKmskw_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Count the number of logical 1 bits in packed 16-bit integers in "a", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE POPCNT(a) {
    count := 0
    DO WHILE a &gt; 0
        count += a[0]
        a &gt;&gt;= 1
    OD
    RETURN count
}
FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := POPCNT(a[i+15:i])
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPOPCNTW - _mm_popcnt_epi16

| VPOPCNTW_XMMu16_MASKmskw_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Count the number of logical 1 bits in packed 16-bit integers in "a", and store the results in "dst".

[algorithm]

DEFINE POPCNT(a) {
    count := 0
    DO WHILE a &gt; 0
        count += a[0]
        a &gt;&gt;= 1
    OD
    RETURN count
}
FOR j := 0 to 7
    i := j*16
    dst[i+15:i] := POPCNT(a[i+15:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPOPCNTW - _mm_mask_popcnt_epi16

| VPOPCNTW_XMMu16_MASKmskw_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Count the number of logical 1 bits in packed 16-bit integers in "a", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

DEFINE POPCNT(a) {
    count := 0
    DO WHILE a &gt; 0
        count += a[0]
        a &gt;&gt;= 1
    OD
    RETURN count
}
FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := POPCNT(a[i+15:i])
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPOPCNTW - _mm_maskz_popcnt_epi16

| VPOPCNTW_XMMu16_MASKmskw_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Count the number of logical 1 bits in packed 16-bit integers in "a", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE POPCNT(a) {
    count := 0
    DO WHILE a &gt; 0
        count += a[0]
        a &gt;&gt;= 1
    OD
    RETURN count
}
FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := POPCNT(a[i+15:i])
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPOPCNTB - _mm512_popcnt_epi8

| VPOPCNTB_ZMMu8_MASKmskw_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Count the number of logical 1 bits in packed 8-bit integers in "a", and store the results in "dst".

[algorithm]

DEFINE POPCNT(a) {
    count := 0
    DO WHILE a &gt; 0
        count += a[0]
        a &gt;&gt;= 1
    OD
    RETURN count
}
FOR j := 0 to 63
    i := j*8
    dst[i+7:i] := POPCNT(a[i+7:i])
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPOPCNTB - _mm512_mask_popcnt_epi8

| VPOPCNTB_ZMMu8_MASKmskw_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Count the number of logical 1 bits in packed 8-bit integers in "a", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

DEFINE POPCNT(a) {
    count := 0
    DO WHILE a &gt; 0
        count += a[0]
        a &gt;&gt;= 1
    OD
    RETURN count
}
FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := POPCNT(a[i+7:i])
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPOPCNTB - _mm512_maskz_popcnt_epi8

| VPOPCNTB_ZMMu8_MASKmskw_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Count the number of logical 1 bits in packed 8-bit integers in "a", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE POPCNT(a) {
    count := 0
    DO WHILE a &gt; 0
        count += a[0]
        a &gt;&gt;= 1
    OD
    RETURN count
}
FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := POPCNT(a[i+7:i])
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPOPCNTB - _mm256_popcnt_epi8

| VPOPCNTB_YMMu8_MASKmskw_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Count the number of logical 1 bits in packed 8-bit integers in "a", and store the results in "dst".

[algorithm]

DEFINE POPCNT(a) {
    count := 0
    DO WHILE a &gt; 0
        count += a[0]
        a &gt;&gt;= 1
    OD
    RETURN count
}
FOR j := 0 to 31
    i := j*8
    dst[i+7:i] := POPCNT(a[i+7:i])
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPOPCNTB - _mm256_mask_popcnt_epi8

| VPOPCNTB_YMMu8_MASKmskw_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Count the number of logical 1 bits in packed 8-bit integers in "a", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

DEFINE POPCNT(a) {
    count := 0
    DO WHILE a &gt; 0
        count += a[0]
        a &gt;&gt;= 1
    OD
    RETURN count
}
FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := POPCNT(a[i+7:i])
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPOPCNTB - _mm256_maskz_popcnt_epi8

| VPOPCNTB_YMMu8_MASKmskw_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Count the number of logical 1 bits in packed 8-bit integers in "a", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE POPCNT(a) {
    count := 0
    DO WHILE a &gt; 0
        count += a[0]
        a &gt;&gt;= 1
    OD
    RETURN count
}
FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := POPCNT(a[i+7:i])
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPOPCNTB - _mm_popcnt_epi8

| VPOPCNTB_XMMu8_MASKmskw_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Count the number of logical 1 bits in packed 8-bit integers in "a", and store the results in "dst".

[algorithm]

DEFINE POPCNT(a) {
    count := 0
    DO WHILE a &gt; 0
        count += a[0]
        a &gt;&gt;= 1
    OD
    RETURN count
}
FOR j := 0 to 15
    i := j*8
    dst[i+7:i] := POPCNT(a[i+7:i])
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPOPCNTB - _mm_mask_popcnt_epi8

| VPOPCNTB_XMMu8_MASKmskw_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Count the number of logical 1 bits in packed 8-bit integers in "a", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

DEFINE POPCNT(a) {
    count := 0
    DO WHILE a &gt; 0
        count += a[0]
        a &gt;&gt;= 1
    OD
    RETURN count
}
FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := POPCNT(a[i+7:i])
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPOPCNTB - _mm_maskz_popcnt_epi8

| VPOPCNTB_XMMu8_MASKmskw_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Count the number of logical 1 bits in packed 8-bit integers in "a", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

DEFINE POPCNT(a) {
    count := 0
    DO WHILE a &gt; 0
        count += a[0]
        a &gt;&gt;= 1
    OD
    RETURN count
}
FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := POPCNT(a[i+7:i])
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULTISHIFTQB - _mm512_multishift_epi64_epi8

| VPMULTISHIFTQB_ZMMu8_MASKmskw_ZMMu8_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
For each 64-bit element in "b", select 8 unaligned bytes using a byte-granular shift control within the
corresponding 64-bit element of "a", and store the 8 assembled bytes to the corresponding 64-bit element of
"dst".

[algorithm]

FOR i := 0 to 7
    q := i * 64
    FOR j := 0 to 7
        tmp8 := 0
        ctrl := a[q+j*8+7:q+j*8] &amp; 63
        FOR l := 0 to 7
            tmp8[l] := b[q+((ctrl+l) &amp; 63)]
        ENDFOR
        dst[q+j*8+7:q+j*8] := tmp8[7:0]
    ENDFOR
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULTISHIFTQB - _mm512_mask_multishift_epi64_epi8

| VPMULTISHIFTQB_ZMMu8_MASKmskw_ZMMu8_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
For each 64-bit element in "b", select 8 unaligned bytes using a byte-granular shift control within the
corresponding 64-bit element of "a", and store the 8 assembled bytes to the corresponding 64-bit element of
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR i := 0 to 7
    q := i * 64
    FOR j := 0 to 7
        tmp8 := 0
        ctrl := a[q+j*8+7:q+j*8] &amp; 63
        FOR l := 0 to 7
            tmp8[l] := b[q+((ctrl+l) &amp; 63)]
        ENDFOR
        IF k[i*8+j]
            dst[q+j*8+7:q+j*8] := tmp8[7:0]
        ELSE
            dst[q+j*8+7:q+j*8] := src[q+j*8+7:q+j*8]
        FI
    ENDFOR
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULTISHIFTQB - _mm512_maskz_multishift_epi64_epi8

| VPMULTISHIFTQB_ZMMu8_MASKmskw_ZMMu8_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
For each 64-bit element in "b", select 8 unaligned bytes using a byte-granular shift control within the
corresponding 64-bit element of "a", and store the 8 assembled bytes to the corresponding 64-bit element of
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR i := 0 to 7
    q := i * 64
    FOR j := 0 to 7
        tmp8 := 0
        ctrl := a[q+j*8+7:q+j*8] &amp; 63
        FOR l := 0 to 7
            tmp8[l] := b[q+((ctrl+l) &amp; 63)]
        ENDFOR
        IF k[i*8+j]
            dst[q+j*8+7:q+j*8] := tmp8[7:0]
        ELSE
            dst[q+j*8+7:q+j*8] := 0
        FI
    ENDFOR
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULTISHIFTQB - _mm256_multishift_epi64_epi8

| VPMULTISHIFTQB_YMMu8_MASKmskw_YMMu8_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
For each 64-bit element in "b", select 8 unaligned bytes using a byte-granular shift control within the
corresponding 64-bit element of "a", and store the 8 assembled bytes to the corresponding 64-bit element of
"dst".

[algorithm]

FOR i := 0 to 3
    q := i * 64
    FOR j := 0 to 7
        tmp8 := 0
        ctrl := a[q+j*8+7:q+j*8] &amp; 63
        FOR l := 0 to 7
            tmp8[l] := b[q+((ctrl+l) &amp; 63)]
        ENDFOR
        dst[q+j*8+7:q+j*8] := tmp8[7:0]
    ENDFOR
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULTISHIFTQB - _mm256_mask_multishift_epi64_epi8

| VPMULTISHIFTQB_YMMu8_MASKmskw_YMMu8_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
For each 64-bit element in "b", select 8 unaligned bytes using a byte-granular shift control within the
corresponding 64-bit element of "a", and store the 8 assembled bytes to the corresponding 64-bit element of
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR i := 0 to 3
    q := i * 64
    FOR j := 0 to 7
        tmp8 := 0
        ctrl := a[q+j*8+7:q+j*8] &amp; 63
        FOR l := 0 to 7
            tmp8[l] := b[q+((ctrl+l) &amp; 63)]
        ENDFOR
        IF k[i*8+j]
            dst[q+j*8+7:q+j*8] := tmp8[7:0]
        ELSE
            dst[q+j*8+7:q+j*8] := src[q+j*8+7:q+j*8]
        FI
    ENDFOR
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULTISHIFTQB - _mm256_maskz_multishift_epi64_epi8

| VPMULTISHIFTQB_YMMu8_MASKmskw_YMMu8_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
For each 64-bit element in "b", select 8 unaligned bytes using a byte-granular shift control within the
corresponding 64-bit element of "a", and store the 8 assembled bytes to the corresponding 64-bit element of
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR i := 0 to 3
    q := i * 64
    FOR j := 0 to 7
        tmp8 := 0
        ctrl := a[q+j*8+7:q+j*8] &amp; 63
        FOR l := 0 to 7
            tmp8[l] := b[q+((ctrl+l) &amp; 63)]
        ENDFOR
        IF k[i*8+j]
            dst[q+j*8+7:q+j*8] := tmp8[7:0]
        ELSE
            dst[q+j*8+7:q+j*8] := 0
        FI
    ENDFOR
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULTISHIFTQB - _mm_multishift_epi64_epi8

| VPMULTISHIFTQB_XMMu8_MASKmskw_XMMu8_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
For each 64-bit element in "b", select 8 unaligned bytes using a byte-granular shift control within the
corresponding 64-bit element of "a", and store the 8 assembled bytes to the corresponding 64-bit element of
"dst".

[algorithm]

FOR i := 0 to 1
    q := i * 64
    FOR j := 0 to 7
        tmp8 := 0
        ctrl := a[q+j*8+7:q+j*8] &amp; 63
        FOR l := 0 to 7
            tmp8[l] := b[q+((ctrl+l) &amp; 63)]
        ENDFOR
        dst[q+j*8+7:q+j*8] := tmp8[7:0]
    ENDFOR
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULTISHIFTQB - _mm_mask_multishift_epi64_epi8

| VPMULTISHIFTQB_XMMu8_MASKmskw_XMMu8_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
For each 64-bit element in "b", select 8 unaligned bytes using a byte-granular shift control within the
corresponding 64-bit element of "a", and store the 8 assembled bytes to the corresponding 64-bit element of
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR i := 0 to 1
    q := i * 64
    FOR j := 0 to 7
        tmp8 := 0
        ctrl := a[q+j*8+7:q+j*8] &amp; 63
        FOR l := 0 to 7
            tmp8[l] := b[q+((ctrl+l) &amp; 63)]
        ENDFOR
        IF k[i*8+j]
            dst[q+j*8+7:q+j*8] := tmp8[7:0]
        ELSE
            dst[q+j*8+7:q+j*8] := src[q+j*8+7:q+j*8]
        FI
    ENDFOR
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPMULTISHIFTQB - _mm_maskz_multishift_epi64_epi8

| VPMULTISHIFTQB_XMMu8_MASKmskw_XMMu8_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
For each 64-bit element in "b", select 8 unaligned bytes using a byte-granular shift control within the
corresponding 64-bit element of "a", and store the 8 assembled bytes to the corresponding 64-bit element of
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR i := 0 to 1
    q := i * 64
    FOR j := 0 to 7
        tmp8 := 0
        ctrl := a[q+j*8+7:q+j*8] &amp; 63
        FOR l := 0 to 7
            tmp8[l] := b[q+((ctrl+l) &amp; 63)]
        ENDFOR
        IF k[i*8+j]
            dst[q+j*8+7:q+j*8] := tmp8[7:0]
        ELSE
            dst[q+j*8+7:q+j*8] := 0
        FI
    ENDFOR
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMB - _mm512_permutexvar_epi8

| VPERMB_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 8-bit integers in "a" across lanes using the corresponding index in "idx", and store the results in
"dst".

[algorithm]

FOR j := 0 to 63
    i := j*8
    id := idx[i+5:i]*8
    dst[i+7:i] := a[id+7:id]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMB - _mm512_mask_permutexvar_epi8

| VPERMB_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 8-bit integers in "a" across lanes using the corresponding index in "idx", and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    id := idx[i+5:i]*8
    IF k[j]
        dst[i+7:i] := a[id+7:id]
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMB - _mm512_maskz_permutexvar_epi8

| VPERMB_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 8-bit integers in "a" across lanes using the corresponding index in "idx", and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    id := idx[i+5:i]*8
    IF k[j]
        dst[i+7:i] := a[id+7:id]
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMB - _mm256_permutexvar_epi8

| VPERMB_YMMu8_MASKmskw_YMMu8_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 8-bit integers in "a" across lanes using the corresponding index in "idx", and store the results in
"dst".

[algorithm]

FOR j := 0 to 31
    i := j*8
    id := idx[i+4:i]*8
    dst[i+7:i] := a[id+7:id]
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMB - _mm256_mask_permutexvar_epi8

| VPERMB_YMMu8_MASKmskw_YMMu8_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 8-bit integers in "a" across lanes using the corresponding index in "idx", and store the results in
"dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    id := idx[i+4:i]*8
    IF k[j]
        dst[i+7:i] := a[id+7:id]
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMB - _mm256_maskz_permutexvar_epi8

| VPERMB_YMMu8_MASKmskw_YMMu8_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 8-bit integers in "a" across lanes using the corresponding index in "idx", and store the results in
"dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    id := idx[i+4:i]*8
    IF k[j]
        dst[i+7:i] := a[id+7:id]
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMB - _mm_permutexvar_epi8

| VPERMB_XMMu8_MASKmskw_XMMu8_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 8-bit integers in "a" using the corresponding index in "idx", and store the results in "dst".

[algorithm]

FOR j := 0 to 15
    i := j*8
    id := idx[i+3:i]*8
    dst[i+7:i] := a[id+7:id]
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMB - _mm_mask_permutexvar_epi8

| VPERMB_XMMu8_MASKmskw_XMMu8_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 8-bit integers in "a" using the corresponding index in "idx", and store the results in "dst" using
writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    id := idx[i+3:i]*8
    IF k[j]
        dst[i+7:i] := a[id+7:id]
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMB - _mm_maskz_permutexvar_epi8

| VPERMB_XMMu8_MASKmskw_XMMu8_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 8-bit integers in "a" using the corresponding index in "idx", and store the results in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    id := idx[i+3:i]*8
    IF k[j]
        dst[i+7:i] := a[id+7:id]
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2B - _mm512_permutex2var_epi8

| VPERMI2B_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 8-bit integers in "a" and "b" across lanes using the corresponding selector and index in "idx", and
store the results in "dst".

[algorithm]

FOR j := 0 to 63
    i := j*8
    off := 8*idx[i+5:i]
    dst[i+7:i] := idx[i+6] ? b[off+7:off] : a[off+7:off]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMT2B - _mm512_mask_permutex2var_epi8

| VPERMT2B_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 8-bit integers in "a" and "b" across lanes using the corresponding selector and index in "idx", and
store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        off := 8*idx[i+5:i]
        dst[i+7:i] := idx[i+6] ? b[off+7:off] : a[off+7:off]
    ELSE
        dst[i+7:i] := a[i+7:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2B - _mm512_mask2_permutex2var_epi8

| VPERMI2B_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 8-bit integers in "a" and "b" across lanes using the corresponding selector and index in "idx", and
store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        off := 8*idx[i+5:i]
        dst[i+7:i] := idx[i+6] ? b[off+7:off] : a[off+7:off]
    ELSE
        dst[i+7:i] := idx[i+7:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2B - _mm512_maskz_permutex2var_epi8

| VPERMI2B_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512 | VPERMT2B_ZMMu8_MASKmskw_ZMMu8_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 8-bit integers in "a" and "b" across lanes using the corresponding selector and index in "idx", and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 63
    i := j*8
    IF k[j]
        off := 8*idx[i+5:i]
        dst[i+7:i] := idx[i+6] ? b[off+7:off] : a[off+7:off]
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2B - _mm256_permutex2var_epi8

| VPERMI2B_YMMu8_MASKmskw_YMMu8_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 8-bit integers in "a" and "b" across lanes using the corresponding selector and index in "idx", and
store the results in "dst".

[algorithm]

FOR j := 0 to 31
    i := j*8
    off := 8*idx[i+4:i]
    dst[i+7:i] := idx[i+5] ? b[off+7:off] : a[off+7:off]
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMT2B - _mm256_mask_permutex2var_epi8

| VPERMT2B_YMMu8_MASKmskw_YMMu8_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 8-bit integers in "a" and "b" across lanes using the corresponding selector and index in "idx", and
store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        off := 8*idx[i+4:i]
        dst[i+7:i] := idx[i+5] ? b[off+7:off] : a[off+7:off]
    ELSE
        dst[i+7:i] := a[i+7:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2B - _mm256_mask2_permutex2var_epi8

| VPERMI2B_YMMu8_MASKmskw_YMMu8_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 8-bit integers in "a" and "b" across lanes using the corresponding selector and index in "idx", and
store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is
not set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        off := 8*idx[i+4:i]
        dst[i+7:i] := idx[i+5] ? b[off+7:off] : a[off+7:off]
    ELSE
        dst[i+7:i] := idx[i+7:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2B - _mm256_maskz_permutex2var_epi8

| VPERMI2B_YMMu8_MASKmskw_YMMu8_YMMu8_AVX512 | VPERMT2B_YMMu8_MASKmskw_YMMu8_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 8-bit integers in "a" and "b" across lanes using the corresponding selector and index in "idx", and
store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not
set).

[algorithm]

FOR j := 0 to 31
    i := j*8
    IF k[j]
        off := 8*idx[i+4:i]
        dst[i+7:i] := idx[i+5] ? b[off+7:off] : a[off+7:off]
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2B - _mm_permutex2var_epi8

| VPERMI2B_XMMu8_MASKmskw_XMMu8_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 8-bit integers in "a" and "b" using the corresponding selector and index in "idx", and store the
results in "dst".

[algorithm]

FOR j := 0 to 15
    i := j*8
    off := 8*idx[i+3:i]
    dst[i+7:i] := idx[i+4] ? b[off+7:off] : a[off+7:off]
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMT2B - _mm_mask_permutex2var_epi8

| VPERMT2B_XMMu8_MASKmskw_XMMu8_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 8-bit integers in "a" and "b" using the corresponding selector and index in "idx", and store the
results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        off := 8*idx[i+3:i]
        dst[i+7:i] := idx[i+4] ? b[off+7:off] : a[off+7:off]
    ELSE
        dst[i+7:i] := a[i+7:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2B - _mm_mask2_permutex2var_epi8

| VPERMI2B_XMMu8_MASKmskw_XMMu8_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 8-bit integers in "a" and "b" using the corresponding selector and index in "idx", and store the
results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        off := 8*idx[i+3:i]
        dst[i+7:i] := idx[i+4] ? b[off+7:off] : a[off+7:off]
    ELSE
        dst[i+7:i] := idx[i+7:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPERMI2B - _mm_maskz_permutex2var_epi8

| VPERMI2B_XMMu8_MASKmskw_XMMu8_XMMu8_AVX512 | VPERMT2B_XMMu8_MASKmskw_XMMu8_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Shuffle 8-bit integers in "a" and "b" using the corresponding selector and index in "idx", and store the
results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*8
    IF k[j]
        off := 8*idx[i+3:i]
        dst[i+7:i] := idx[i+4] ? b[off+7:off] : a[off+7:off]
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDVQ - _mm512_maskz_shrdv_epi64

| VPSHRDVQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 64-bit integers in "b" and "a" producing an intermediate 128-bit result. Shift the result
right by the amount specified in the corresponding element of "c", and store the lower 64-bits in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := ((b[i+63:i] &lt;&lt; 64)[127:0] | a[i+63:i]) &gt;&gt; (c[i+63:i] &amp; 63)
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDVQ - _mm512_mask_shrdv_epi64

| VPSHRDVQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 64-bit integers in "b" and "a" producing an intermediate 128-bit result. Shift the result
right by the amount specified in the corresponding element of "c", and store the lower 64-bits in "dst" using
writemask "k" (elements are copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := ((b[i+63:i] &lt;&lt; 64)[127:0] | a[i+63:i]) &gt;&gt; (c[i+63:i] &amp; 63)
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDVQ - _mm512_shrdv_epi64

| VPSHRDVQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 64-bit integers in "b" and "a" producing an intermediate 128-bit result. Shift the result
right by the amount specified in the corresponding element of "c", and store the lower 64-bits in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := ((b[i+63:i] &lt;&lt; 64)[127:0] | a[i+63:i]) &gt;&gt; (c[i+63:i] &amp; 63)
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDVQ - _mm256_maskz_shrdv_epi64

| VPSHRDVQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 64-bit integers in "b" and "a" producing an intermediate 128-bit result. Shift the result
right by the amount specified in the corresponding element of "c", and store the lower 64-bits in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := ((b[i+63:i] &lt;&lt; 64)[127:0] | a[i+63:i]) &gt;&gt; (c[i+63:i] &amp; 63)
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDVQ - _mm256_mask_shrdv_epi64

| VPSHRDVQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 64-bit integers in "b" and "a" producing an intermediate 128-bit result. Shift the result
right by the amount specified in the corresponding element of "c", and store the lower 64-bits in "dst" using
writemask "k" (elements are copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := ((b[i+63:i] &lt;&lt; 64)[127:0] | a[i+63:i]) &gt;&gt; (c[i+63:i] &amp; 63)
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDVQ - _mm256_shrdv_epi64

| VPSHRDVQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 64-bit integers in "b" and "a" producing an intermediate 128-bit result. Shift the result
right by the amount specified in the corresponding element of "c", and store the lower 64-bits in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    dst[i+63:i] := ((b[i+63:i] &lt;&lt; 64)[127:0] | a[i+63:i]) &gt;&gt; (c[i+63:i] &amp; 63)
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDVQ - _mm_maskz_shrdv_epi64

| VPSHRDVQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 64-bit integers in "b" and "a" producing an intermediate 128-bit result. Shift the result
right by the amount specified in the corresponding element of "c", and store the lower 64-bits in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := ((b[i+63:i] &lt;&lt; 64)[127:0] | a[i+63:i]) &gt;&gt; (c[i+63:i] &amp; 63)
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDVQ - _mm_mask_shrdv_epi64

| VPSHRDVQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 64-bit integers in "b" and "a" producing an intermediate 128-bit result. Shift the result
right by the amount specified in the corresponding element of "c", and store the lower 64-bits in "dst" using
writemask "k" (elements are copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := ((b[i+63:i] &lt;&lt; 64)[127:0] | a[i+63:i]) &gt;&gt; (c[i+63:i] &amp; 63)
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDVQ - _mm_shrdv_epi64

| VPSHRDVQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 64-bit integers in "b" and "a" producing an intermediate 128-bit result. Shift the result
right by the amount specified in the corresponding element of "c", and store the lower 64-bits in "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    dst[i+63:i] := ((b[i+63:i] &lt;&lt; 64)[127:0] | a[i+63:i]) &gt;&gt; (c[i+63:i] &amp; 63)
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDVD - _mm512_maskz_shrdv_epi32

| VPSHRDVD_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 32-bit integers in "b" and "a" producing an intermediate 64-bit result. Shift the result
right by the amount specified in the corresponding element of "c", and store the lower 32-bits in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := ((b[i+31:i] &lt;&lt; 32)[63:0] | a[i+31:i]) &gt;&gt; (c[i+31:i] &amp; 31)
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDVD - _mm512_mask_shrdv_epi32

| VPSHRDVD_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 32-bit integers in "b" and "a" producing an intermediate 64-bit result. Shift the result
right by the amount specified in the corresponding element of "c", and store the lower 32-bits in "dst" using
writemask "k" (elements are copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := ((b[i+31:i] &lt;&lt; 32)[63:0] | a[i+31:i]) &gt;&gt; (c[i+31:i] &amp; 31)
    ELSE
        dst[i+31:i] := a[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDVD - _mm512_shrdv_epi32

| VPSHRDVD_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 32-bit integers in "b" and "a" producing an intermediate 64-bit result. Shift the result
right by the amount specified in the corresponding element of "c", and store the lower 32-bits in "dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := ((b[i+31:i] &lt;&lt; 32)[63:0] | a[i+31:i]) &gt;&gt; (c[i+31:i] &amp; 31)
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDVD - _mm256_maskz_shrdv_epi32

| VPSHRDVD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 32-bit integers in "b" and "a" producing an intermediate 64-bit result. Shift the result
right by the amount specified in the corresponding element of "c", and store the lower 32-bits in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := ((b[i+31:i] &lt;&lt; 32)[63:0] | a[i+31:i]) &gt;&gt; (c[i+31:i] &amp; 31)
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDVD - _mm256_mask_shrdv_epi32

| VPSHRDVD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 32-bit integers in "b" and "a" producing an intermediate 64-bit result. Shift the result
right by the amount specified in the corresponding element of "c", and store the lower 32-bits in "dst" using
writemask "k" (elements are copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := ((b[i+31:i] &lt;&lt; 32)[63:0] | a[i+31:i]) &gt;&gt; (c[i+31:i] &amp; 31)
    ELSE
        dst[i+31:i] := a[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDVD - _mm256_shrdv_epi32

| VPSHRDVD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 32-bit integers in "b" and "a" producing an intermediate 64-bit result. Shift the result
right by the amount specified in the corresponding element of "c", and store the lower 32-bits in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*32
    dst[i+31:i] := ((b[i+31:i] &lt;&lt; 32)[63:0] | a[i+31:i]) &gt;&gt; (c[i+31:i] &amp; 31)
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDVD - _mm_maskz_shrdv_epi32

| VPSHRDVD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 32-bit integers in "b" and "a" producing an intermediate 64-bit result. Shift the result
right by the amount specified in the corresponding element of "c", and store the lower 32-bits in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := ((b[i+31:i] &lt;&lt; 32)[63:0] | a[i+31:i]) &gt;&gt; (c[i+31:i] &amp; 31)
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDVD - _mm_mask_shrdv_epi32

| VPSHRDVD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 32-bit integers in "b" and "a" producing an intermediate 64-bit result. Shift the result
right by the amount specified in the corresponding element of "c", and store the lower 32-bits in "dst" using
writemask "k" (elements are copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := ((b[i+31:i] &lt;&lt; 32)[63:0] | a[i+31:i]) &gt;&gt; (c[i+31:i] &amp; 31)
    ELSE
        dst[i+31:i] := a[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDVD - _mm_shrdv_epi32

| VPSHRDVD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 32-bit integers in "b" and "a" producing an intermediate 64-bit result. Shift the result
right by the amount specified in the corresponding element of "c", and store the lower 32-bits in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*32
    dst[i+31:i] := ((b[i+31:i] &lt;&lt; 32)[63:0] | a[i+31:i]) &gt;&gt; (c[i+31:i] &amp; 31)
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDVW - _mm512_maskz_shrdv_epi16

| VPSHRDVW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 16-bit integers in "b" and "a" producing an intermediate 32-bit result. Shift the result
right by the amount specified in the corresponding element of "c", and store the lower 16-bits in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := ((b[i+15:i] &lt;&lt; 16)[31:0] | a[i+15:i]) &gt;&gt; (c[i+15:i] &amp; 15)
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDVW - _mm512_mask_shrdv_epi16

| VPSHRDVW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 16-bit integers in "b" and "a" producing an intermediate 32-bit result. Shift the result
right by the amount specified in the corresponding element of "c", and store the lower 16-bits in "dst" using
writemask "k" (elements are copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := ((b[i+15:i] &lt;&lt; 16)[31:0] | a[i+15:i]) &gt;&gt; (c[i+15:i] &amp; 15)
    ELSE
        dst[i+15:i] := a[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDVW - _mm512_shrdv_epi16

| VPSHRDVW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 16-bit integers in "b" and "a" producing an intermediate 32-bit result. Shift the result
right by the amount specified in the corresponding element of "c", and store the lower 16-bits in "dst".

[algorithm]

FOR j := 0 to 31
    i := j*16
    dst[i+15:i] := ((b[i+15:i] &lt;&lt; 16)[31:0] | a[i+15:i]) &gt;&gt; (c[i+15:i] &amp; 15)
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDVW - _mm256_maskz_shrdv_epi16

| VPSHRDVW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 16-bit integers in "b" and "a" producing an intermediate 32-bit result. Shift the result
right by the amount specified in the corresponding element of "c", and store the lower 16-bits in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := ((b[i+15:i] &lt;&lt; 16)[31:0] | a[i+15:i]) &gt;&gt; (c[i+15:i] &amp; 15)
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDVW - _mm256_mask_shrdv_epi16

| VPSHRDVW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 16-bit integers in "b" and "a" producing an intermediate 32-bit result. Shift the result
right by the amount specified in the corresponding element of "c", and store the lower 16-bits in "dst" using
writemask "k" (elements are copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := ((b[i+15:i] &lt;&lt; 16)[31:0] | a[i+15:i]) &gt;&gt; (c[i+15:i] &amp; 15)
    ELSE
        dst[i+15:i] := a[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDVW - _mm256_shrdv_epi16

| VPSHRDVW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 16-bit integers in "b" and "a" producing an intermediate 32-bit result. Shift the result
right by the amount specified in the corresponding element of "c", and store the lower 16-bits in "dst".

[algorithm]

FOR j := 0 to 15
    i := j*16
    dst[i+15:i] := ((b[i+15:i] &lt;&lt; 16)[31:0] | a[i+15:i]) &gt;&gt; (c[i+15:i] &amp; 15)
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDVW - _mm_maskz_shrdv_epi16

| VPSHRDVW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 16-bit integers in "b" and "a" producing an intermediate 32-bit result. Shift the result
right by the amount specified in the corresponding element of "c", and store the lower 16-bits in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := ((b[i+15:i] &lt;&lt; 16)[31:0] | a[i+15:i]) &gt;&gt; (c[i+15:i] &amp; 15)
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDVW - _mm_mask_shrdv_epi16

| VPSHRDVW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 16-bit integers in "b" and "a" producing an intermediate 32-bit result. Shift the result
right by the amount specified in the corresponding element of "c", and store the lower 16-bits in "dst" using
writemask "k" (elements are copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := ((b[i+15:i] &lt;&lt; 16)[31:0] | a[i+15:i]) &gt;&gt; (c[i+15:i] &amp; 15)
    ELSE
        dst[i+15:i] := a[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDVW - _mm_shrdv_epi16

| VPSHRDVW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 16-bit integers in "b" and "a" producing an intermediate 32-bit result. Shift the result
right by the amount specified in the corresponding element of "c", and store the lower 16-bits in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*16
    dst[i+15:i] := ((b[i+15:i] &lt;&lt; 16)[31:0] | a[i+15:i]) &gt;&gt; (c[i+15:i] &amp; 15)
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDQ - _mm512_maskz_shrdi_epi64

| VPSHRDQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 64-bit integers in "b" and "a" producing an intermediate 128-bit result. Shift the result
right by "imm8" bits, and store the lower 64-bits in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := ((b[i+63:i] &lt;&lt; 64)[127:0] | a[i+63:i]) &gt;&gt; imm8[5:0]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDQ - _mm512_mask_shrdi_epi64

| VPSHRDQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 64-bit integers in "b" and "a" producing an intermediate 128-bit result. Shift the result
right by "imm8" bits, and store the lower 64-bits in "dst" using writemask "k" (elements are copied from "src""
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        dst[i+63:i] := ((b[i+63:i] &lt;&lt; 64)[127:0] | a[i+63:i]) &gt;&gt; imm8[5:0]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDQ - _mm512_shrdi_epi64

| VPSHRDQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 64-bit integers in "b" and "a" producing an intermediate 128-bit result. Shift the result
right by "imm8" bits, and store the lower 64-bits in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    dst[i+63:i] := ((b[i+63:i] &lt;&lt; 64)[127:0] | a[i+63:i]) &gt;&gt; imm8[5:0]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDQ - _mm256_maskz_shrdi_epi64

| VPSHRDQ_YMMu64_MASKmskw_YMMu64_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 64-bit integers in "b" and "a" producing an intermediate 128-bit result. Shift the result
right by "imm8" bits, and store the lower 64-bits in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := ((b[i+63:i] &lt;&lt; 64)[127:0] | a[i+63:i]) &gt;&gt; imm8[5:0]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDQ - _mm256_mask_shrdi_epi64

| VPSHRDQ_YMMu64_MASKmskw_YMMu64_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 64-bit integers in "b" and "a" producing an intermediate 128-bit result. Shift the result
right by "imm8" bits, and store the lower 64-bits in "dst" using writemask "k" (elements are copied from "src""
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        dst[i+63:i] := ((b[i+63:i] &lt;&lt; 64)[127:0] | a[i+63:i]) &gt;&gt; imm8[5:0]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDQ - _mm256_shrdi_epi64

| VPSHRDQ_YMMu64_MASKmskw_YMMu64_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 64-bit integers in "b" and "a" producing an intermediate 128-bit result. Shift the result
right by "imm8" bits, and store the lower 64-bits in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    dst[i+63:i] := ((b[i+63:i] &lt;&lt; 64)[127:0] | a[i+63:i]) &gt;&gt; imm8[5:0]
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDQ - _mm_maskz_shrdi_epi64

| VPSHRDQ_XMMu64_MASKmskw_XMMu64_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 64-bit integers in "b" and "a" producing an intermediate 128-bit result. Shift the result
right by "imm8" bits, and store the lower 64-bits in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := ((b[i+63:i] &lt;&lt; 64)[127:0] | a[i+63:i]) &gt;&gt; imm8[5:0]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDQ - _mm_mask_shrdi_epi64

| VPSHRDQ_XMMu64_MASKmskw_XMMu64_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 64-bit integers in "b" and "a" producing an intermediate 128-bit result. Shift the result
right by "imm8" bits, and store the lower 64-bits in "dst" using writemask "k" (elements are copied from "src""
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        dst[i+63:i] := ((b[i+63:i] &lt;&lt; 64)[127:0] | a[i+63:i]) &gt;&gt; imm8[5:0]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDQ - _mm_shrdi_epi64

| VPSHRDQ_XMMu64_MASKmskw_XMMu64_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 64-bit integers in "b" and "a" producing an intermediate 128-bit result. Shift the result
right by "imm8" bits, and store the lower 64-bits in "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    dst[i+63:i] := ((b[i+63:i] &lt;&lt; 64)[127:0] | a[i+63:i]) &gt;&gt; imm8[5:0]
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDD - _mm512_maskz_shrdi_epi32

| VPSHRDD_ZMMu32_MASKmskw_ZMMu32_ZMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 32-bit integers in "b" and "a" producing an intermediate 64-bit result. Shift the result
right by "imm8" bits, and store the lower 32-bits in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := ((b[i+31:i] &lt;&lt; 32)[63:0] | a[i+31:i]) &gt;&gt; imm8[4:0]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDD - _mm512_mask_shrdi_epi32

| VPSHRDD_ZMMu32_MASKmskw_ZMMu32_ZMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 32-bit integers in "b" and "a" producing an intermediate 64-bit result. Shift the result
right by "imm8" bits, and store the lower 32-bits in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        dst[i+31:i] := ((b[i+31:i] &lt;&lt; 32)[63:0] | a[i+31:i]) &gt;&gt; imm8[4:0]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDD - _mm512_shrdi_epi32

| VPSHRDD_ZMMu32_MASKmskw_ZMMu32_ZMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 32-bit integers in "b" and "a" producing an intermediate 64-bit result. Shift the result
right by "imm8" bits, and store the lower 32-bits in "dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    dst[i+31:i] := ((b[i+31:i] &lt;&lt; 32)[63:0] | a[i+31:i]) &gt;&gt; imm8[4:0]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDD - _mm256_maskz_shrdi_epi32

| VPSHRDD_YMMu32_MASKmskw_YMMu32_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 32-bit integers in "b" and "a" producing an intermediate 64-bit result. Shift the result
right by "imm8" bits, and store the lower 32-bits in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := ((b[i+31:i] &lt;&lt; 32)[63:0] | a[i+31:i]) &gt;&gt; imm8[4:0]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDD - _mm256_mask_shrdi_epi32

| VPSHRDD_YMMu32_MASKmskw_YMMu32_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 32-bit integers in "b" and "a" producing an intermediate 64-bit result. Shift the result
right by "imm8" bits, and store the lower 32-bits in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        dst[i+31:i] := ((b[i+31:i] &lt;&lt; 32)[63:0] | a[i+31:i]) &gt;&gt; imm8[4:0]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDD - _mm256_shrdi_epi32

| VPSHRDD_YMMu32_MASKmskw_YMMu32_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 32-bit integers in "b" and "a" producing an intermediate 64-bit result. Shift the result
right by "imm8" bits, and store the lower 32-bits in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*32
    dst[i+31:i] := ((b[i+31:i] &lt;&lt; 32)[63:0] | a[i+31:i]) &gt;&gt; imm8[4:0]
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDD - _mm_maskz_shrdi_epi32

| VPSHRDD_XMMu32_MASKmskw_XMMu32_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 32-bit integers in "b" and "a" producing an intermediate 64-bit result. Shift the result
right by "imm8" bits, and store the lower 32-bits in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := ((b[i+31:i] &lt;&lt; 32)[63:0] | a[i+31:i]) &gt;&gt; imm8[4:0]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDD - _mm_mask_shrdi_epi32

| VPSHRDD_XMMu32_MASKmskw_XMMu32_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 32-bit integers in "b" and "a" producing an intermediate 64-bit result. Shift the result
right by "imm8" bits, and store the lower 32-bits in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        dst[i+31:i] := ((b[i+31:i] &lt;&lt; 32)[63:0] | a[i+31:i]) &gt;&gt; imm8[4:0]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDD - _mm_shrdi_epi32

| VPSHRDD_XMMu32_MASKmskw_XMMu32_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 32-bit integers in "b" and "a" producing an intermediate 64-bit result. Shift the result
right by "imm8" bits, and store the lower 32-bits in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*32
    dst[i+31:i] := ((b[i+31:i] &lt;&lt; 32)[63:0] | a[i+31:i]) &gt;&gt; imm8[4:0]
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDW - _mm512_maskz_shrdi_epi16

| VPSHRDW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 16-bit integers in "b" and "a" producing an intermediate 32-bit result. Shift the result
right by "imm8" bits, and store the lower 16-bits in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := ((b[i+15:i] &lt;&lt; 16)[31:0] | a[i+15:i]) &gt;&gt; imm8[3:0]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDW - _mm512_mask_shrdi_epi16

| VPSHRDW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 16-bit integers in "b" and "a" producing an intermediate 32-bit result. Shift the result
right by "imm8" bits, and store the lower 16-bits in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := ((b[i+15:i] &lt;&lt; 16)[31:0] | a[i+15:i]) &gt;&gt; imm8[3:0]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDW - _mm512_shrdi_epi16

| VPSHRDW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 16-bit integers in "b" and "a" producing an intermediate 32-bit result. Shift the result
right by "imm8" bits, and store the lower 16-bits in "dst".

[algorithm]

FOR j := 0 to 31
    i := j*16
    dst[i+15:i] := ((b[i+15:i] &lt;&lt; 16)[31:0] | a[i+15:i]) &gt;&gt; imm8[3:0]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDW - _mm256_maskz_shrdi_epi16

| VPSHRDW_YMMu16_MASKmskw_YMMu16_YMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 16-bit integers in "b" and "a" producing an intermediate 32-bit result. Shift the result
right by "imm8" bits, and store the lower 16-bits in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := ((b[i+15:i] &lt;&lt; 16)[31:0] | a[i+15:i]) &gt;&gt; imm8[3:0]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDW - _mm256_mask_shrdi_epi16

| VPSHRDW_YMMu16_MASKmskw_YMMu16_YMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 16-bit integers in "b" and "a" producing an intermediate 32-bit result. Shift the result
right by "imm8" bits, and store the lower 16-bits in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := ((b[i+15:i] &lt;&lt; 16)[31:0] | a[i+15:i]) &gt;&gt; imm8[3:0]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDW - _mm256_shrdi_epi16

| VPSHRDW_YMMu16_MASKmskw_YMMu16_YMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 16-bit integers in "b" and "a" producing an intermediate 32-bit result. Shift the result
right by "imm8" bits, and store the lower 16-bits in "dst".

[algorithm]

FOR j := 0 to 15
    i := j*16
    dst[i+15:i] := ((b[i+15:i] &lt;&lt; 16)[31:0] | a[i+15:i]) &gt;&gt; imm8[3:0]
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDW - _mm_maskz_shrdi_epi16

| VPSHRDW_XMMu16_MASKmskw_XMMu16_XMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 16-bit integers in "b" and "a" producing an intermediate 32-bit result. Shift the result
right by "imm8" bits, and store the lower 16-bits in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := ((b[i+15:i] &lt;&lt; 16)[31:0] | a[i+15:i]) &gt;&gt; imm8[3:0]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDW - _mm_mask_shrdi_epi16

| VPSHRDW_XMMu16_MASKmskw_XMMu16_XMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 16-bit integers in "b" and "a" producing an intermediate 32-bit result. Shift the result
right by "imm8" bits, and store the lower 16-bits in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := ((b[i+15:i] &lt;&lt; 16)[31:0] | a[i+15:i]) &gt;&gt; imm8[3:0]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHRDW - _mm_shrdi_epi16

| VPSHRDW_XMMu16_MASKmskw_XMMu16_XMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 16-bit integers in "b" and "a" producing an intermediate 32-bit result. Shift the result
right by "imm8" bits, and store the lower 16-bits in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*16
    dst[i+15:i] := ((b[i+15:i] &lt;&lt; 16)[31:0] | a[i+15:i]) &gt;&gt; imm8[3:0]
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDVQ - _mm512_maskz_shldv_epi64

| VPSHLDVQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 64-bit integers in "a" and "b" producing an intermediate 128-bit result. Shift the result
left by the amount specified in the corresponding element of "c", and store the upper 64-bits in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        tmp[127:0] := ((a[i+63:i] &lt;&lt; 64)[127:0] | b[i+63:i]) &lt;&lt; (c[i+63:i] &amp; 63)
        dst[i+63:i] := tmp[127:64]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDVQ - _mm512_mask_shldv_epi64

| VPSHLDVQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 64-bit integers in "a" and "b" producing an intermediate 128-bit result. Shift the result
left by the amount specified in the corresponding element of "c", and store the upper 64-bits in "dst" using
writemask "k" (elements are copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        tmp[127:0] := ((a[i+63:i] &lt;&lt; 64)[127:0] | b[i+63:i]) &lt;&lt; (c[i+63:i] &amp; 63)
        dst[i+63:i] := tmp[127:64]
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDVQ - _mm512_shldv_epi64

| VPSHLDVQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 64-bit integers in "a" and "b" producing an intermediate 128-bit result. Shift the result
left by the amount specified in the corresponding element of "c", and store the upper 64-bits in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*64
    tmp[127:0] := ((a[i+63:i] &lt;&lt; 64)[127:0] | b[i+63:i]) &lt;&lt; (c[i+63:i] &amp; 63)
    dst[i+63:i] := tmp[127:64]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDVQ - _mm256_maskz_shldv_epi64

| VPSHLDVQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 64-bit integers in "a" and "b" producing an intermediate 128-bit result. Shift the result
left by the amount specified in the corresponding element of "c", and store the upper 64-bits in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        tmp[127:0] := ((a[i+63:i] &lt;&lt; 64)[127:0] | b[i+63:i]) &lt;&lt; (c[i+63:i] &amp; 63)
        dst[i+63:i] := tmp[127:64]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDVQ - _mm256_mask_shldv_epi64

| VPSHLDVQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 64-bit integers in "a" and "b" producing an intermediate 128-bit result. Shift the result
left by the amount specified in the corresponding element of "c", and store the upper 64-bits in "dst" using
writemask "k" (elements are copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        tmp[127:0] := ((a[i+63:i] &lt;&lt; 64)[127:0] | b[i+63:i]) &lt;&lt; (c[i+63:i] &amp; 63)
        dst[i+63:i] := tmp[127:64]
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDVQ - _mm256_shldv_epi64

| VPSHLDVQ_YMMu64_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 64-bit integers in "a" and "b" producing an intermediate 128-bit result. Shift the result
left by the amount specified in the corresponding element of "c", and store the upper 64-bits in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*64
    tmp[127:0] := ((a[i+63:i] &lt;&lt; 64)[127:0] | b[i+63:i]) &lt;&lt; (c[i+63:i] &amp; 63)
    dst[i+63:i] := tmp[127:64]
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDVQ - _mm_maskz_shldv_epi64

| VPSHLDVQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 64-bit integers in "a" and "b" producing an intermediate 128-bit result. Shift the result
left by the amount specified in the corresponding element of "c", and store the upper 64-bits in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        tmp[127:0] := ((a[i+63:i] &lt;&lt; 64)[127:0] | b[i+63:i]) &lt;&lt; (c[i+63:i] &amp; 63)
        dst[i+63:i] := tmp[127:64]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDVQ - _mm_mask_shldv_epi64

| VPSHLDVQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 64-bit integers in "a" and "b" producing an intermediate 128-bit result. Shift the result
left by the amount specified in the corresponding element of "c", and store the upper 64-bits in "dst" using
writemask "k" (elements are copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        tmp[127:0] := ((a[i+63:i] &lt;&lt; 64)[127:0] | b[i+63:i]) &lt;&lt; (c[i+63:i] &amp; 63)
        dst[i+63:i] := tmp[127:64]
    ELSE
        dst[i+63:i] := a[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDVQ - _mm_shldv_epi64

| VPSHLDVQ_XMMu64_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 64-bit integers in "a" and "b" producing an intermediate 128-bit result. Shift the result
left by the amount specified in the corresponding element of "c", and store the upper 64-bits in "dst".

[algorithm]

FOR j := 0 to 1
    i := j*64
    tmp[127:0] := ((a[i+63:i] &lt;&lt; 64)[127:0] | b[i+63:i]) &lt;&lt; (c[i+63:i] &amp; 63)
    dst[i+63:i] := tmp[127:64]
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDVD - _mm512_maskz_shldv_epi32

| VPSHLDVD_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 32-bit integers in "a" and "b" producing an intermediate 64-bit result. Shift the result
left by the amount specified in the corresponding element of "c", and store the upper 32-bits in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        tmp[63:0] := ((a[i+31:i] &lt;&lt; 32)[63:0] | b[i+31:i]) &lt;&lt; (c[i+31:i] &amp; 31)
        dst[i+31:i] := tmp[63:32]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDVD - _mm512_mask_shldv_epi32

| VPSHLDVD_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 32-bit integers in "a" and "b" producing an intermediate 64-bit result. Shift the result
left by the amount specified in the corresponding element of "c", and store the upper 32-bits in "dst" using
writemask "k" (elements are copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        tmp[63:0] := ((a[i+31:i] &lt;&lt; 32)[63:0] | b[i+31:i]) &lt;&lt; (c[i+31:i] &amp; 31)
        dst[i+31:i] := tmp[63:32]
    ELSE
        dst[i+31:i] := a[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDVD - _mm512_shldv_epi32

| VPSHLDVD_ZMMu32_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 32-bit integers in "a" and "b" producing an intermediate 64-bit result. Shift the result
left by the amount specified in the corresponding element of "c", and store the upper 32-bits in "dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    tmp[63:0] := ((a[i+31:i] &lt;&lt; 32)[63:0] | b[i+31:i]) &lt;&lt; (c[i+31:i] &amp; 31)
    dst[i+31:i] := tmp[63:32]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDVD - _mm256_maskz_shldv_epi32

| VPSHLDVD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 32-bit integers in "a" and "b" producing an intermediate 64-bit result. Shift the result
left by the amount specified in the corresponding element of "c", and store the upper 32-bits in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        tmp[63:0] := ((a[i+31:i] &lt;&lt; 32)[63:0] | b[i+31:i]) &lt;&lt; (c[i+31:i] &amp; 31)
        dst[i+31:i] := tmp[63:32]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDVD - _mm256_mask_shldv_epi32

| VPSHLDVD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 32-bit integers in "a" and "b" producing an intermediate 64-bit result. Shift the result
left by the amount specified in the corresponding element of "c", and store the upper 32-bits in "dst" using
writemask "k" (elements are copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        tmp[63:0] := ((a[i+31:i] &lt;&lt; 32)[63:0] | b[i+31:i]) &lt;&lt; (c[i+31:i] &amp; 31)
        dst[i+31:i] := tmp[63:32]
    ELSE
        dst[i+31:i] := a[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDVD - _mm256_shldv_epi32

| VPSHLDVD_YMMu32_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 32-bit integers in "a" and "b" producing an intermediate 64-bit result. Shift the result
left by the amount specified in the corresponding element of "c", and store the upper 32-bits in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*32
    tmp[63:0] := ((a[i+31:i] &lt;&lt; 32)[63:0] | b[i+31:i]) &lt;&lt; (c[i+31:i] &amp; 31)
    dst[i+31:i] := tmp[63:32]
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDVD - _mm_maskz_shldv_epi32

| VPSHLDVD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 32-bit integers in "a" and "b" producing an intermediate 64-bit result. Shift the result
left by the amount specified in the corresponding element of "c", and store the upper 32-bits in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        tmp[63:0] := ((a[i+31:i] &lt;&lt; 32)[63:0] | b[i+31:i]) &lt;&lt; (c[i+31:i] &amp; 31)
        dst[i+31:i] := tmp[63:32]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDVD - _mm_mask_shldv_epi32

| VPSHLDVD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 32-bit integers in "a" and "b" producing an intermediate 64-bit result. Shift the result
left by the amount specified in the corresponding element of "c", and store the upper 32-bits in "dst" using
writemask "k" (elements are copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        tmp[63:0] := ((a[i+31:i] &lt;&lt; 32)[63:0] | b[i+31:i]) &lt;&lt; (c[i+31:i] &amp; 31)
        dst[i+31:i] := tmp[63:32]
    ELSE
        dst[i+31:i] := a[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDVD - _mm_shldv_epi32

| VPSHLDVD_XMMu32_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 32-bit integers in "a" and "b" producing an intermediate 64-bit result. Shift the result
left by the amount specified in the corresponding element of "c", and store the upper 32-bits in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*32
    tmp[63:0] := ((a[i+31:i] &lt;&lt; 32)[63:0] | b[i+31:i]) &lt;&lt; (c[i+31:i] &amp; 31)
    dst[i+31:i] := tmp[63:32]
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDVW - _mm512_maskz_shldv_epi16

| VPSHLDVW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 16-bit integers in "a" and "b" producing an intermediate 32-bit result. Shift the result
left by the amount specified in the corresponding element of "c", and store the upper 16-bits in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        tmp[31:0] := ((a[i+15:i] &lt;&lt; 16)[31:0] | b[i+15:i]) &lt;&lt; (c[i+15:i] &amp; 15)
        dst[i+15:i] := tmp[31:16]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDVW - _mm512_mask_shldv_epi16

| VPSHLDVW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 16-bit integers in "a" and "b" producing an intermediate 32-bit result. Shift the result
left by the amount specified in the corresponding element of "c", and store the upper 16-bits in "dst" using
writemask "k" (elements are copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        tmp[31:0] := ((a[i+15:i] &lt;&lt; 16)[31:0] | b[i+15:i]) &lt;&lt; (c[i+15:i] &amp; 15)
        dst[i+15:i] := tmp[31:16]
    ELSE
        dst[i+15:i] := a[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDVW - _mm512_shldv_epi16

| VPSHLDVW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 16-bit integers in "a" and "b" producing an intermediate 32-bit result. Shift the result
left by the amount specified in the corresponding element of "c", and store the upper 16-bits in "dst".

[algorithm]

FOR j := 0 to 31
    i := j*16
    tmp[31:0] := ((a[i+15:i] &lt;&lt; 16)[31:0] | b[i+15:i]) &lt;&lt; (c[i+15:i] &amp; 15)
    dst[i+15:i] := tmp[31:16]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDVW - _mm256_maskz_shldv_epi16

| VPSHLDVW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 16-bit integers in "a" and "b" producing an intermediate 32-bit result. Shift the result
left by the amount specified in the corresponding element of "c", and store the upper 16-bits in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        tmp[31:0] := ((a[i+15:i] &lt;&lt; 16)[31:0] | b[i+15:i]) &lt;&lt; (c[i+15:i] &amp; 15)
        dst[i+15:i] := tmp[31:16]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDVW - _mm256_mask_shldv_epi16

| VPSHLDVW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 16-bit integers in "a" and "b" producing an intermediate 32-bit result. Shift the result
left by the amount specified in the corresponding element of "c", and store the upper 16-bits in "dst" using
writemask "k" (elements are copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        tmp[31:0] := ((a[i+15:i] &lt;&lt; 16)[31:0] | b[i+15:i]) &lt;&lt; (c[i+15:i] &amp; 15)
        dst[i+15:i] := tmp[31:16]
    ELSE
        dst[i+15:i] := a[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDVW - _mm256_shldv_epi16

| VPSHLDVW_YMMu16_MASKmskw_YMMu16_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 16-bit integers in "a" and "b" producing an intermediate 32-bit result. Shift the result
left by the amount specified in the corresponding element of "c", and store the upper 16-bits in "dst".

[algorithm]

FOR j := 0 to 15
    i := j*16
    tmp[31:0] := ((a[i+15:i] &lt;&lt; 16)[31:0] | b[i+15:i]) &lt;&lt; (c[i+15:i] &amp; 15)
    dst[i+15:i] := tmp[31:16]
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDVW - _mm_maskz_shldv_epi16

| VPSHLDVW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 16-bit integers in "a" and "b" producing an intermediate 32-bit result. Shift the result
left by the amount specified in the corresponding element of "c", and store the upper 16-bits in "dst" using
zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        tmp[31:0] := ((a[i+15:i] &lt;&lt; 16)[31:0] | b[i+15:i]) &lt;&lt; (c[i+15:i] &amp; 15)
        dst[i+15:i] := tmp[31:16]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDVW - _mm_mask_shldv_epi16

| VPSHLDVW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 16-bit integers in "a" and "b" producing an intermediate 32-bit result. Shift the result
left by the amount specified in the corresponding element of "c", and store the upper 16-bits in "dst" using
writemask "k" (elements are copied from "a" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        tmp[31:0] := ((a[i+15:i] &lt;&lt; 16)[31:0] | b[i+15:i]) &lt;&lt; (c[i+15:i] &amp; 15)
        dst[i+15:i] := tmp[31:16]
    ELSE
        dst[i+15:i] := a[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDVW - _mm_shldv_epi16

| VPSHLDVW_XMMu16_MASKmskw_XMMu16_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 16-bit integers in "a" and "b" producing an intermediate 32-bit result. Shift the result
left by the amount specified in the corresponding element of "c", and store the upper 16-bits in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*16
    tmp[31:0] := ((a[i+15:i] &lt;&lt; 16)[31:0] | b[i+15:i]) &lt;&lt; (c[i+15:i] &amp; 15)
    dst[i+15:i] := tmp[31:16]
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDQ - _mm512_maskz_shldi_epi64

| VPSHLDQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 64-bit integers in "a" and "b" producing an intermediate 128-bit result. Shift the result
left by "imm8" bits, and store the upper 64-bits in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        tmp[127:0] := ((a[i+63:i] &lt;&lt; 64)[127:0] | b[i+63:i]) &lt;&lt; imm8[5:0]
        dst[i+63:i] := tmp[127:64]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDQ - _mm512_mask_shldi_epi64

| VPSHLDQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 64-bit integers in "a" and "b" producing an intermediate 128-bit result. Shift the result
left by "imm8" bits, and store the upper 64-bits in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*64
    IF k[j]
        tmp[127:0] := ((a[i+63:i] &lt;&lt; 64)[127:0] | b[i+63:i]) &lt;&lt; imm8[5:0]
        dst[i+63:i] := tmp[127:64]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDQ - _mm512_shldi_epi64

| VPSHLDQ_ZMMu64_MASKmskw_ZMMu64_ZMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 64-bit integers in "a" and "b" producing an intermediate 128-bit result. Shift the result
left by "imm8" bits, and store the upper 64-bits in "dst").

[algorithm]

FOR j := 0 to 7
    i := j*64
    tmp[127:0] := ((a[i+63:i] &lt;&lt; 64)[127:0] | b[i+63:i]) &lt;&lt; imm8[5:0]
    dst[i+63:i] := tmp[127:64]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDQ - _mm256_maskz_shldi_epi64

| VPSHLDQ_YMMu64_MASKmskw_YMMu64_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 64-bit integers in "a" and "b" producing an intermediate 128-bit result. Shift the result
left by "imm8" bits, and store the upper 64-bits in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        tmp[127:0] := ((a[i+63:i] &lt;&lt; 64)[127:0] | b[i+63:i]) &lt;&lt; imm8[5:0]
        dst[i+63:i] := tmp[127:64]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDQ - _mm256_mask_shldi_epi64

| VPSHLDQ_YMMu64_MASKmskw_YMMu64_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 64-bit integers in "a" and "b" producing an intermediate 128-bit result. Shift the result
left by "imm8" bits, and store the upper 64-bits in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*64
    IF k[j]
        tmp[127:0] := ((a[i+63:i] &lt;&lt; 64)[127:0] | b[i+63:i]) &lt;&lt; imm8[5:0]
        dst[i+63:i] := tmp[127:64]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDQ - _mm256_shldi_epi64

| VPSHLDQ_YMMu64_MASKmskw_YMMu64_YMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 64-bit integers in "a" and "b" producing an intermediate 128-bit result. Shift the result
left by "imm8" bits, and store the upper 64-bits in "dst").

[algorithm]

FOR j := 0 to 3
    i := j*64
    tmp[127:0] := ((a[i+63:i] &lt;&lt; 64)[127:0] | b[i+63:i]) &lt;&lt; imm8[5:0]
    dst[i+63:i] := tmp[127:64]
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDQ - _mm_maskz_shldi_epi64

| VPSHLDQ_XMMu64_MASKmskw_XMMu64_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 64-bit integers in "a" and "b" producing an intermediate 128-bit result. Shift the result
left by "imm8" bits, and store the upper 64-bits in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        tmp[127:0] := ((a[i+63:i] &lt;&lt; 64)[127:0] | b[i+63:i]) &lt;&lt; imm8[5:0]
        dst[i+63:i] := tmp[127:64]
    ELSE
        dst[i+63:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDQ - _mm_mask_shldi_epi64

| VPSHLDQ_XMMu64_MASKmskw_XMMu64_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 64-bit integers in "a" and "b" producing an intermediate 128-bit result. Shift the result
left by "imm8" bits, and store the upper 64-bits in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 1
    i := j*64
    IF k[j]
        tmp[127:0] := ((a[i+63:i] &lt;&lt; 64)[127:0] | b[i+63:i]) &lt;&lt; imm8[5:0]
        dst[i+63:i] := tmp[127:64]
    ELSE
        dst[i+63:i] := src[i+63:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDQ - _mm_shldi_epi64

| VPSHLDQ_XMMu64_MASKmskw_XMMu64_XMMu64_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 64-bit integers in "a" and "b" producing an intermediate 128-bit result. Shift the result
left by "imm8" bits, and store the upper 64-bits in "dst").

[algorithm]

FOR j := 0 to 1
    i := j*64
    tmp[127:0] := ((a[i+63:i] &lt;&lt; 64)[127:0] | b[i+63:i]) &lt;&lt; imm8[5:0]
    dst[i+63:i] := tmp[127:64]
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDD - _mm512_maskz_shldi_epi32

| VPSHLDD_ZMMu32_MASKmskw_ZMMu32_ZMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 32-bit integers in "a" and "b" producing an intermediate 64-bit result. Shift the result
left by "imm8" bits, and store the upper 32-bits in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        tmp[63:0] := ((a[i+31:i] &lt;&lt; 32)[63:0] | b[i+31:i]) &lt;&lt; imm8[4:0]
        dst[i+31:i] := tmp[63:32]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDD - _mm512_mask_shldi_epi32

| VPSHLDD_ZMMu32_MASKmskw_ZMMu32_ZMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 32-bit integers in "a" and "b" producing an intermediate 64-bit result. Shift the result
left by "imm8" bits, and store the upper 32-bits in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*32
    IF k[j]
        tmp[63:0] := ((a[i+31:i] &lt;&lt; 32)[63:0] | b[i+31:i]) &lt;&lt; imm8[4:0]
        dst[i+31:i] := tmp[63:32]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDD - _mm512_shldi_epi32

| VPSHLDD_ZMMu32_MASKmskw_ZMMu32_ZMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 32-bit integers in "a" and "b" producing an intermediate 64-bit result. Shift the result
left by "imm8" bits, and store the upper 32-bits in "dst".

[algorithm]

FOR j := 0 to 15
    i := j*32
    tmp[63:0] := ((a[i+31:i] &lt;&lt; 32)[63:0] | b[i+31:i]) &lt;&lt; imm8[4:0]
    dst[i+31:i] := tmp[63:32]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDD - _mm256_maskz_shldi_epi32

| VPSHLDD_YMMu32_MASKmskw_YMMu32_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 32-bit integers in "a" and "b" producing an intermediate 64-bit result. Shift the result
left by "imm8" bits, and store the upper 32-bits in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        tmp[63:0] := ((a[i+31:i] &lt;&lt; 32)[63:0] | b[i+31:i]) &lt;&lt; imm8[4:0]
        dst[i+31:i] := tmp[63:32]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDD - _mm256_mask_shldi_epi32

| VPSHLDD_YMMu32_MASKmskw_YMMu32_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 32-bit integers in "a" and "b" producing an intermediate 64-bit result. Shift the result
left by "imm8" bits, and store the upper 32-bits in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*32
    IF k[j]
        tmp[63:0] := ((a[i+31:i] &lt;&lt; 32)[63:0] | b[i+31:i]) &lt;&lt; imm8[4:0]
        dst[i+31:i] := tmp[63:32]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDD - _mm256_shldi_epi32

| VPSHLDD_YMMu32_MASKmskw_YMMu32_YMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 32-bit integers in "a" and "b" producing an intermediate 64-bit result. Shift the result
left by "imm8" bits, and store the upper 32-bits in "dst".

[algorithm]

FOR j := 0 to 7
    i := j*32
    tmp[63:0] := ((a[i+31:i] &lt;&lt; 32)[63:0] | b[i+31:i]) &lt;&lt; imm8[4:0]
    dst[i+31:i] := tmp[63:32]
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDD - _mm_maskz_shldi_epi32

| VPSHLDD_XMMu32_MASKmskw_XMMu32_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 32-bit integers in "a" and "b" producing an intermediate 64-bit result. Shift the result
left by "imm8" bits, and store the upper 32-bits in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        tmp[63:0] := ((a[i+31:i] &lt;&lt; 32)[63:0] | b[i+31:i]) &lt;&lt; imm8[4:0]
        dst[i+31:i] := tmp[63:32]
    ELSE
        dst[i+31:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDD - _mm_mask_shldi_epi32

| VPSHLDD_XMMu32_MASKmskw_XMMu32_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 32-bit integers in "a" and "b" producing an intermediate 64-bit result. Shift the result
left by "imm8" bits, and store the upper 32-bits in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    i := j*32
    IF k[j]
        tmp[63:0] := ((a[i+31:i] &lt;&lt; 32)[63:0] | b[i+31:i]) &lt;&lt; imm8[4:0]
        dst[i+31:i] := tmp[63:32]
    ELSE
        dst[i+31:i] := src[i+31:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDD - _mm_shldi_epi32

| VPSHLDD_XMMu32_MASKmskw_XMMu32_XMMu32_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 32-bit integers in "a" and "b" producing an intermediate 64-bit result. Shift the result
left by "imm8" bits, and store the upper 32-bits in "dst".

[algorithm]

FOR j := 0 to 3
    i := j*32
    tmp[63:0] := ((a[i+31:i] &lt;&lt; 32)[63:0] | b[i+31:i]) &lt;&lt; imm8[4:0]
    dst[i+31:i] := tmp[63:32]
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDW - _mm512_maskz_shldi_epi16

| VPSHLDW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 16-bit integers in "a" and "b" producing an intermediate 32-bit result. Shift the result
left by "imm8" bits, and store the upper 16-bits in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        tmp[31:0] := ((a[i+15:i] &lt;&lt; 16)[31:0] | b[i+15:i]) &lt;&lt; imm8[3:0]
        dst[i+15:i] := tmp[31:16]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDW - _mm512_mask_shldi_epi16

| VPSHLDW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 16-bit integers in "a" and "b" producing an intermediate 32-bit result. Shift the result
left by "imm8" bits, and store the upper 16-bits in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 31
    i := j*16
    IF k[j]
        tmp[31:0] := ((a[i+15:i] &lt;&lt; 16)[31:0] | b[i+15:i]) &lt;&lt; imm8[3:0]
        dst[i+15:i] := tmp[31:16]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDW - _mm512_shldi_epi16

| VPSHLDW_ZMMu16_MASKmskw_ZMMu16_ZMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 16-bit integers in "a" and "b" producing an intermediate 32-bit result. Shift the result
left by "imm8" bits, and store the upper 16-bits in "dst").

[algorithm]

FOR j := 0 to 31
    i := j*16
    tmp[31:0] := ((a[i+15:i] &lt;&lt; 16)[31:0] | b[i+15:i]) &lt;&lt; imm8[3:0]
    dst[i+15:i] := tmp[31:16]
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDW - _mm256_maskz_shldi_epi16

| VPSHLDW_YMMu16_MASKmskw_YMMu16_YMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 16-bit integers in "a" and "b" producing an intermediate 32-bit result. Shift the result
left by "imm8" bits, and store the upper 16-bits in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        tmp[31:0] := ((a[i+15:i] &lt;&lt; 16)[31:0] | b[i+15:i]) &lt;&lt; imm8[3:0]
        dst[i+15:i] := tmp[31:16]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDW - _mm256_mask_shldi_epi16

| VPSHLDW_YMMu16_MASKmskw_YMMu16_YMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 16-bit integers in "a" and "b" producing an intermediate 32-bit result. Shift the result
left by "imm8" bits, and store the upper 16-bits in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    i := j*16
    IF k[j]
        tmp[31:0] := ((a[i+15:i] &lt;&lt; 16)[31:0] | b[i+15:i]) &lt;&lt; imm8[3:0]
        dst[i+15:i] := tmp[31:16]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDW - _mm256_shldi_epi16

| VPSHLDW_YMMu16_MASKmskw_YMMu16_YMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 16-bit integers in "a" and "b" producing an intermediate 32-bit result. Shift the result
left by "imm8" bits, and store the upper 16-bits in "dst").

[algorithm]

FOR j := 0 to 15
    i := j*16
    tmp[31:0] := ((a[i+15:i] &lt;&lt; 16)[31:0] | b[i+15:i]) &lt;&lt; imm8[3:0]
    dst[i+15:i] := tmp[31:16]
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDW - _mm_maskz_shldi_epi16

| VPSHLDW_XMMu16_MASKmskw_XMMu16_XMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 16-bit integers in "a" and "b" producing an intermediate 32-bit result. Shift the result
left by "imm8" bits, and store the upper 16-bits in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        tmp[31:0] := ((a[i+15:i] &lt;&lt; 16)[31:0] | b[i+15:i]) &lt;&lt; imm8[3:0]
        dst[i+15:i] := tmp[31:16]
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDW - _mm_mask_shldi_epi16

| VPSHLDW_XMMu16_MASKmskw_XMMu16_XMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 16-bit integers in "a" and "b" producing an intermediate 32-bit result. Shift the result
left by "imm8" bits, and store the upper 16-bits in "dst" using writemask "k" (elements are copied from "src"
when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    i := j*16
    IF k[j]
        tmp[31:0] := ((a[i+15:i] &lt;&lt; 16)[31:0] | b[i+15:i]) &lt;&lt; imm8[3:0]
        dst[i+15:i] := tmp[31:16]
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPSHLDW - _mm_shldi_epi16

| VPSHLDW_XMMu16_MASKmskw_XMMu16_XMMu16_IMM8_AVX512

--------------------------------------------------------------------------------------------------------------
Concatenate packed 16-bit integers in "a" and "b" producing an intermediate 32-bit result. Shift the result
left by "imm8" bits, and store the upper 16-bits in "dst").

[algorithm]

FOR j := 0 to 7
    i := j*16
    tmp[31:0] := ((a[i+15:i] &lt;&lt; 16)[31:0] | b[i+15:i]) &lt;&lt; imm8[3:0]
    dst[i+15:i] := tmp[31:16]
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDW - _mm512_maskz_expandloadu_epi16

| VPEXPANDW_ZMMu16_MASKmskw_MEMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 16-bit integers from unaligned memory at "mem_addr" (those with their respective bit
set in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := MEM[mem_addr+m+15:mem_addr+m]
        m := m + 16
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDW - _mm512_mask_expandloadu_epi16

| VPEXPANDW_ZMMu16_MASKmskw_MEMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 16-bit integers from unaligned memory at "mem_addr" (those with their respective bit
set in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := MEM[mem_addr+m+15:mem_addr+m]
        m := m + 16
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDW - _mm512_maskz_expand_epi16

| VPEXPANDW_ZMMu16_MASKmskw_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 16-bit integers from "a" (those with their respective bit set in mask "k"), and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := a[m+15:m]
        m := m + 16
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDW - _mm512_mask_expand_epi16

| VPEXPANDW_ZMMu16_MASKmskw_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 16-bit integers from "a" (those with their respective bit set in mask "k"), and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

m := 0
FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[i+15:i] := a[m+15:m]
        m := m + 16
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDW - _mm256_maskz_expandloadu_epi16

| VPEXPANDW_YMMu16_MASKmskw_MEMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 16-bit integers from unaligned memory at "mem_addr" (those with their respective bit
set in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := MEM[mem_addr+m+15:mem_addr+m]
        m := m + 16
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDW - _mm256_mask_expandloadu_epi16

| VPEXPANDW_YMMu16_MASKmskw_MEMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 16-bit integers from unaligned memory at "mem_addr" (those with their respective bit
set in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := MEM[mem_addr+m+15:mem_addr+m]
        m := m + 16
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDW - _mm256_maskz_expand_epi16

| VPEXPANDW_YMMu16_MASKmskw_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 16-bit integers from "a" (those with their respective bit set in mask "k"), and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := a[m+15:m]
        m := m + 16
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDW - _mm256_mask_expand_epi16

| VPEXPANDW_YMMu16_MASKmskw_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 16-bit integers from "a" (those with their respective bit set in mask "k"), and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

m := 0
FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[i+15:i] := a[m+15:m]
        m := m + 16
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDW - _mm_maskz_expandloadu_epi16

| VPEXPANDW_XMMu16_MASKmskw_MEMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 16-bit integers from unaligned memory at "mem_addr" (those with their respective bit
set in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := MEM[mem_addr+m+15:mem_addr+m]
        m := m + 16
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDW - _mm_mask_expandloadu_epi16

| VPEXPANDW_XMMu16_MASKmskw_MEMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 16-bit integers from unaligned memory at "mem_addr" (those with their respective bit
set in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := MEM[mem_addr+m+15:mem_addr+m]
        m := m + 16
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDW - _mm_maskz_expand_epi16

| VPEXPANDW_XMMu16_MASKmskw_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 16-bit integers from "a" (those with their respective bit set in mask "k"), and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := a[m+15:m]
        m := m + 16
    ELSE
        dst[i+15:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDW - _mm_mask_expand_epi16

| VPEXPANDW_XMMu16_MASKmskw_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 16-bit integers from "a" (those with their respective bit set in mask "k"), and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

m := 0
FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[i+15:i] := a[m+15:m]
        m := m + 16
    ELSE
        dst[i+15:i] := src[i+15:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDB - _mm512_maskz_expandloadu_epi8

| VPEXPANDB_ZMMu8_MASKmskw_MEMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 8-bit integers from unaligned memory at "mem_addr" (those with their respective bit set
in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := MEM[mem_addr+m+7:mem_addr+m]
        m := m + 8
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDB - _mm512_mask_expandloadu_epi8

| VPEXPANDB_ZMMu8_MASKmskw_MEMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 8-bit integers from unaligned memory at "mem_addr" (those with their respective bit set
in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := MEM[mem_addr+m+7:mem_addr+m]
        m := m + 8
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDB - _mm256_maskz_expandloadu_epi8

| VPEXPANDB_YMMu8_MASKmskw_MEMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 8-bit integers from unaligned memory at "mem_addr" (those with their respective bit set
in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := MEM[mem_addr+m+7:mem_addr+m]
        m := m + 8
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDB - _mm256_mask_expandloadu_epi8

| VPEXPANDB_YMMu8_MASKmskw_MEMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 8-bit integers from unaligned memory at "mem_addr" (those with their respective bit set
in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := MEM[mem_addr+m+7:mem_addr+m]
        m := m + 8
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDB - _mm_maskz_expandloadu_epi8

| VPEXPANDB_XMMu8_MASKmskw_MEMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 8-bit integers from unaligned memory at "mem_addr" (those with their respective bit set
in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding
mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := MEM[mem_addr+m+7:mem_addr+m]
        m := m + 8
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDB - _mm_mask_expandloadu_epi8

| VPEXPANDB_XMMu8_MASKmskw_MEMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 8-bit integers from unaligned memory at "mem_addr" (those with their respective bit set
in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when the
corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := MEM[mem_addr+m+7:mem_addr+m]
        m := m + 8
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDB - _mm512_maskz_expand_epi8

| VPEXPANDB_ZMMu8_MASKmskw_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 8-bit integers from "a" (those with their respective bit set in mask "k"), and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := a[m+7:m]
        m := m + 8
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDB - _mm512_mask_expand_epi8

| VPEXPANDB_ZMMu8_MASKmskw_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 8-bit integers from "a" (those with their respective bit set in mask "k"), and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

m := 0
FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[i+7:i] := a[m+7:m]
        m := m + 8
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDB - _mm256_maskz_expand_epi8

| VPEXPANDB_YMMu8_MASKmskw_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 8-bit integers from "a" (those with their respective bit set in mask "k"), and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := a[m+7:m]
        m := m + 8
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDB - _mm256_mask_expand_epi8

| VPEXPANDB_YMMu8_MASKmskw_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 8-bit integers from "a" (those with their respective bit set in mask "k"), and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

m := 0
FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[i+7:i] := a[m+7:m]
        m := m + 8
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDB - _mm_maskz_expand_epi8

| VPEXPANDB_XMMu8_MASKmskw_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 8-bit integers from "a" (those with their respective bit set in mask "k"), and store
the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

m := 0
FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := a[m+7:m]
        m := m + 8
    ELSE
        dst[i+7:i] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPEXPANDB - _mm_mask_expand_epi8

| VPEXPANDB_XMMu8_MASKmskw_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Load contiguous active 8-bit integers from "a" (those with their respective bit set in mask "k"), and store
the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not
set).

[algorithm]

m := 0
FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[i+7:i] := a[m+7:m]
        m := m + 8
    ELSE
        dst[i+7:i] := src[i+7:i]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPCOMPRESSW - _mm512_mask_compressstoreu_epi16

| VPCOMPRESSW_MEMu16_MASKmskw_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active 16-bit integers in "a" (those with their respective bit set in writemask "k") to
unaligned memory at "base_addr".

[algorithm]

size := 16
m := base_addr
FOR j := 0 to 31
    i := j*16
    IF k[j]
        MEM[m+size-1:m] := a[i+15:i]
        m := m + size
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPCOMPRESSW - _mm256_mask_compressstoreu_epi16

| VPCOMPRESSW_MEMu16_MASKmskw_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active 16-bit integers in "a" (those with their respective bit set in writemask "k") to
unaligned memory at "base_addr".

[algorithm]

size := 16
m := base_addr
FOR j := 0 to 15
    i := j*16
    IF k[j]
        MEM[m+size-1:m] := a[i+15:i]
        m := m + size
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPCOMPRESSW - _mm_mask_compressstoreu_epi16

| VPCOMPRESSW_MEMu16_MASKmskw_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active 16-bit integers in "a" (those with their respective bit set in writemask "k") to
unaligned memory at "base_addr".

[algorithm]

size := 16
m := base_addr
FOR j := 0 to 7
    i := j*16
    IF k[j]
        MEM[m+size-1:m] := a[i+15:i]
        m := m + size
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPCOMPRESSW - _mm512_maskz_compress_epi16

| VPCOMPRESSW_ZMMu16_MASKmskw_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active 16-bit integers in "a" (those with their respective bit set in zeromask "k") to
"dst", and set the remaining elements to zero.

[algorithm]

size := 16
m := 0
FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[m+size-1:m] := a[i+15:i]
        m := m + size
    FI
ENDFOR
dst[511:m] := 0
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPCOMPRESSW - _mm512_mask_compress_epi16

| VPCOMPRESSW_ZMMu16_MASKmskw_ZMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active 16-bit integers in "a" (those with their respective bit set in writemask "k") to
"dst", and pass through the remaining elements from "src".

[algorithm]

size := 16
m := 0
FOR j := 0 to 31
    i := j*16
    IF k[j]
        dst[m+size-1:m] := a[i+15:i]
        m := m + size
    FI
ENDFOR
dst[511:m] := src[511:m]
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPCOMPRESSW - _mm256_maskz_compress_epi16

| VPCOMPRESSW_YMMu16_MASKmskw_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active 16-bit integers in "a" (those with their respective bit set in zeromask "k") to
"dst", and set the remaining elements to zero.

[algorithm]

size := 16
m := 0
FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[m+size-1:m] := a[i+15:i]
        m := m + size
    FI
ENDFOR
dst[255:m] := 0
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPCOMPRESSW - _mm256_mask_compress_epi16

| VPCOMPRESSW_YMMu16_MASKmskw_YMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active 16-bit integers in "a" (those with their respective bit set in writemask "k") to
"dst", and pass through the remaining elements from "src".

[algorithm]

size := 16
m := 0
FOR j := 0 to 15
    i := j*16
    IF k[j]
        dst[m+size-1:m] := a[i+15:i]
        m := m + size
    FI
ENDFOR
dst[255:m] := src[255:m]
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPCOMPRESSW - _mm_maskz_compress_epi16

| VPCOMPRESSW_XMMu16_MASKmskw_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active 16-bit integers in "a" (those with their respective bit set in zeromask "k") to
"dst", and set the remaining elements to zero.

[algorithm]

size := 16
m := 0
FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[m+size-1:m] := a[i+15:i]
        m := m + size
    FI
ENDFOR
dst[127:m] := 0
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPCOMPRESSW - _mm_mask_compress_epi16

| VPCOMPRESSW_XMMu16_MASKmskw_XMMu16_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active 16-bit integers in "a" (those with their respective bit set in writemask "k") to
"dst", and pass through the remaining elements from "src".

[algorithm]

size := 16
m := 0
FOR j := 0 to 7
    i := j*16
    IF k[j]
        dst[m+size-1:m] := a[i+15:i]
        m := m + size
    FI
ENDFOR
dst[127:m] := src[127:m]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPCOMPRESSB - _mm512_mask_compressstoreu_epi8

| VPCOMPRESSB_MEMu8_MASKmskw_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active 8-bit integers in "a" (those with their respective bit set in writemask "k") to
unaligned memory at "base_addr".

[algorithm]

size := 8
m := base_addr
FOR j := 0 to 63
    i := j*8
    IF k[j]
        MEM[m+size-1:m] := a[i+7:i]
        m := m + size
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPCOMPRESSB - _mm256_mask_compressstoreu_epi8

| VPCOMPRESSB_MEMu8_MASKmskw_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active 8-bit integers in "a" (those with their respective bit set in writemask "k") to
unaligned memory at "base_addr".

[algorithm]

size := 8
m := base_addr
FOR j := 0 to 31
    i := j*8
    IF k[j]
        MEM[m+size-1:m] := a[i+7:i]
        m := m + size
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPCOMPRESSB - _mm_mask_compressstoreu_epi8

| VPCOMPRESSB_MEMu8_MASKmskw_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active 8-bit integers in "a" (those with their respective bit set in writemask "k") to
unaligned memory at "base_addr".

[algorithm]

size := 8
m := base_addr
FOR j := 0 to 15
    i := j*8
    IF k[j]
        MEM[m+size-1:m] := a[i+7:i]
        m := m + size
    FI
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VPCOMPRESSB - _mm512_maskz_compress_epi8

| VPCOMPRESSB_ZMMu8_MASKmskw_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active 8-bit integers in "a" (those with their respective bit set in zeromask "k") to
"dst", and set the remaining elements to zero.

[algorithm]

size := 8
m := 0
FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[m+size-1:m] := a[i+7:i]
        m := m + size
    FI
ENDFOR
dst[511:m] := 0
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPCOMPRESSB - _mm512_mask_compress_epi8

| VPCOMPRESSB_ZMMu8_MASKmskw_ZMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active 8-bit integers in "a" (those with their respective bit set in writemask "k") to
"dst", and pass through the remaining elements from "src".

[algorithm]

size := 8
m := 0
FOR j := 0 to 63
    i := j*8
    IF k[j]
        dst[m+size-1:m] := a[i+7:i]
        m := m + size
    FI
ENDFOR
dst[511:m] := src[511:m]
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPCOMPRESSB - _mm256_maskz_compress_epi8

| VPCOMPRESSB_YMMu8_MASKmskw_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active 8-bit integers in "a" (those with their respective bit set in zeromask "k") to
"dst", and set the remaining elements to zero.

[algorithm]

size := 8
m := 0
FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[m+size-1:m] := a[i+7:i]
        m := m + size
    FI
ENDFOR
dst[255:m] := 0
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPCOMPRESSB - _mm256_mask_compress_epi8

| VPCOMPRESSB_YMMu8_MASKmskw_YMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active 8-bit integers in "a" (those with their respective bit set in writemask "k") to
"dst", and pass through the remaining elements from "src".

[algorithm]

size := 8
m := 0
FOR j := 0 to 31
    i := j*8
    IF k[j]
        dst[m+size-1:m] := a[i+7:i]
        m := m + size
    FI
ENDFOR
dst[255:m] := src[255:m]
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPCOMPRESSB - _mm_maskz_compress_epi8

| VPCOMPRESSB_XMMu8_MASKmskw_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active 8-bit integers in "a" (those with their respective bit set in zeromask "k") to
"dst", and set the remaining elements to zero.

[algorithm]

size := 8
m := 0
FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[m+size-1:m] := a[i+7:i]
        m := m + size
    FI
ENDFOR
dst[127:m] := 0
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPCOMPRESSB - _mm_mask_compress_epi8

| VPCOMPRESSB_XMMu8_MASKmskw_XMMu8_AVX512

--------------------------------------------------------------------------------------------------------------
Contiguously store the active 8-bit integers in "a" (those with their respective bit set in writemask "k") to
"dst", and pass through the remaining elements from "src".

[algorithm]

size := 8
m := 0
FOR j := 0 to 15
    i := j*8
    IF k[j]
        dst[m+size-1:m] := a[i+7:i]
        m := m + size
    FI
ENDFOR
dst[127:m] := src[127:m]
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPDPWSSDS - _mm512_maskz_dpwssds_epi32

| VPDPWSSDS_ZMMi32_MASKmskw_ZMMi16_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply groups of 2 adjacent pairs of signed 16-bit integers in "a" with corresponding 16-bit integers in
"b", producing 2 intermediate signed 32-bit results. Sum these 2 results with the corresponding 32-bit integer
in "src" using signed saturation, and store the packed 32-bit results in "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    IF k[j]
        tmp1.dword := SignExtend32(a.word[2*j]) * SignExtend32(b.word[2*j])
        tmp2.dword := SignExtend32(a.word[2*j+1]) * SignExtend32(b.word[2*j+1])
        dst.dword[j] := Saturate32(src.dword[j] + tmp1 + tmp2)
    ELSE
        dst.dword[j] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPDPWSSDS - _mm512_mask_dpwssds_epi32

| VPDPWSSDS_ZMMi32_MASKmskw_ZMMi16_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply groups of 2 adjacent pairs of signed 16-bit integers in "a" with corresponding 16-bit integers in
"b", producing 2 intermediate signed 32-bit results. Sum these 2 results with the corresponding 32-bit integer
in "src" using signed saturation, and store the packed 32-bit results in "dst" using writemask "k" (elements
are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    IF k[j]
        tmp1.dword := SignExtend32(a.word[2*j]) * SignExtend32(b.word[2*j])
        tmp2.dword := SignExtend32(a.word[2*j+1]) * SignExtend32(b.word[2*j+1])
        dst.dword[j] := Saturate32(src.dword[j] + tmp1 + tmp2)
    ELSE
        dst.dword[j] := src.dword[j]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPDPWSSDS - _mm512_dpwssds_epi32

| VPDPWSSDS_ZMMi32_MASKmskw_ZMMi16_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply groups of 2 adjacent pairs of signed 16-bit integers in "a" with corresponding 16-bit integers in
"b", producing 2 intermediate signed 32-bit results. Sum these 2 results with the corresponding 32-bit integer
in "src" using signed saturation, and store the packed 32-bit results in "dst".

[algorithm]

FOR j := 0 to 15
    tmp1.dword := SignExtend32(a.word[2*j]) * SignExtend32(b.word[2*j])
    tmp2.dword := SignExtend32(a.word[2*j+1]) * SignExtend32(b.word[2*j+1])
    dst.dword[j] := Saturate32(src.dword[j] + tmp1 + tmp2)
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPDPWSSDS - _mm256_maskz_dpwssds_epi32

| VPDPWSSDS_YMMi32_MASKmskw_YMMi16_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply groups of 2 adjacent pairs of signed 16-bit integers in "a" with corresponding 16-bit integers in
"b", producing 2 intermediate signed 32-bit results. Sum these 2 results with the corresponding 32-bit integer
in "src" using signed saturation, and store the packed 32-bit results in "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    IF k[j]
        tmp1.dword := SignExtend32(a.word[2*j]) * SignExtend32(b.word[2*j])
        tmp2.dword := SignExtend32(a.word[2*j+1]) * SignExtend32(b.word[2*j+1])
        dst.dword[j] := Saturate32(src.dword[j] + tmp1 + tmp2)
    ELSE
        dst.dword[j] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPDPWSSDS - _mm256_mask_dpwssds_epi32

| VPDPWSSDS_YMMi32_MASKmskw_YMMi16_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply groups of 2 adjacent pairs of signed 16-bit integers in "a" with corresponding 16-bit integers in
"b", producing 2 intermediate signed 32-bit results. Sum these 2 results with the corresponding 32-bit integer
in "src" using signed saturation, and store the packed 32-bit results in "dst" using writemask "k" (elements
are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    IF k[j]
        tmp1.dword := SignExtend32(a.word[2*j]) * SignExtend32(b.word[2*j])
        tmp2.dword := SignExtend32(a.word[2*j+1]) * SignExtend32(b.word[2*j+1])
        dst.dword[j] := Saturate32(src.dword[j] + tmp1 + tmp2)
    ELSE
        dst.dword[j] := src.dword[j]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPDPWSSDS - _mm256_dpwssds_epi32

| VPDPWSSDS_YMMi32_MASKmskw_YMMi16_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply groups of 2 adjacent pairs of signed 16-bit integers in "a" with corresponding 16-bit integers in
"b", producing 2 intermediate signed 32-bit results. Sum these 2 results with the corresponding 32-bit integer
in "src" using signed saturation, and store the packed 32-bit results in "dst".

[algorithm]

FOR j := 0 to 7
    tmp1.dword := SignExtend32(a.word[2*j]) * SignExtend32(b.word[2*j])
    tmp2.dword := SignExtend32(a.word[2*j+1]) * SignExtend32(b.word[2*j+1])
    dst.dword[j] := Saturate32(src.dword[j] + tmp1 + tmp2)
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPDPWSSDS - _mm_maskz_dpwssds_epi32

| VPDPWSSDS_XMMi32_MASKmskw_XMMi16_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply groups of 2 adjacent pairs of signed 16-bit integers in "a" with corresponding 16-bit integers in
"b", producing 2 intermediate signed 32-bit results. Sum these 2 results with the corresponding 32-bit integer
in "src" using signed saturation, and store the packed 32-bit results in "dst" using zeromask "k" (elements are
zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    IF k[j]
        tmp1.dword := SignExtend32(a.word[2*j]) * SignExtend32(b.word[2*j])
        tmp2.dword := SignExtend32(a.word[2*j+1]) * SignExtend32(b.word[2*j+1])
        dst.dword[j] := Saturate32(src.dword[j] + tmp1 + tmp2)
    ELSE
        dst.dword[j] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPDPWSSDS - _mm_mask_dpwssds_epi32

| VPDPWSSDS_XMMi32_MASKmskw_XMMi16_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply groups of 2 adjacent pairs of signed 16-bit integers in "a" with corresponding 16-bit integers in
"b", producing 2 intermediate signed 32-bit results. Sum these 2 results with the corresponding 32-bit integer
in "src" using signed saturation, and store the packed 32-bit results in "dst" using writemask "k" (elements
are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    IF k[j]
        tmp1.dword := SignExtend32(a.word[2*j]) * SignExtend32(b.word[2*j])
        tmp2.dword := SignExtend32(a.word[2*j+1]) * SignExtend32(b.word[2*j+1])
        dst.dword[j] := Saturate32(src.dword[j] + tmp1 + tmp2)
    ELSE
        dst.dword[j] := src.dword[j]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPDPWSSDS - _mm_dpwssds_epi32

| VPDPWSSDS_XMMi32_MASKmskw_XMMi16_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply groups of 2 adjacent pairs of signed 16-bit integers in "a" with corresponding 16-bit integers in
"b", producing 2 intermediate signed 32-bit results. Sum these 2 results with the corresponding 32-bit integer
in "src" using signed saturation, and store the packed 32-bit results in "dst".

[algorithm]

FOR j := 0 to 3
    tmp1.dword := SignExtend32(a.word[2*j]) * SignExtend32(b.word[2*j])
    tmp2.dword := SignExtend32(a.word[2*j+1]) * SignExtend32(b.word[2*j+1])
    dst.dword[j] := Saturate32(src.dword[j] + tmp1 + tmp2)
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPDPWSSD - _mm512_maskz_dpwssd_epi32

| VPDPWSSD_ZMMi32_MASKmskw_ZMMi16_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply groups of 2 adjacent pairs of signed 16-bit integers in "a" with corresponding 16-bit integers in
"b", producing 2 intermediate signed 32-bit results. Sum these 2 results with the corresponding 32-bit integer
in "src", and store the packed 32-bit results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    IF k[j]
        tmp1.dword := SignExtend32(a.word[2*j]) * SignExtend32(b.word[2*j])
        tmp2.dword := SignExtend32(a.word[2*j+1]) * SignExtend32(b.word[2*j+1])
        dst.dword[j] := src.dword[j] + tmp1 + tmp2
    ELSE
        dst.dword[j] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPDPWSSD - _mm512_mask_dpwssd_epi32

| VPDPWSSD_ZMMi32_MASKmskw_ZMMi16_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply groups of 2 adjacent pairs of signed 16-bit integers in "a" with corresponding 16-bit integers in
"b", producing 2 intermediate signed 32-bit results. Sum these 2 results with the corresponding 32-bit integer
in "src", and store the packed 32-bit results in "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    IF k[j]
        tmp1.dword := SignExtend32(a.word[2*j]) * SignExtend32(b.word[2*j])
        tmp2.dword := SignExtend32(a.word[2*j+1]) * SignExtend32(b.word[2*j+1])
        dst.dword[j] := src.dword[j] + tmp1 + tmp2
    ELSE
        dst.dword[j] := src.dword[j]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPDPWSSD - _mm512_dpwssd_epi32

| VPDPWSSD_ZMMi32_MASKmskw_ZMMi16_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply groups of 2 adjacent pairs of signed 16-bit integers in "a" with corresponding 16-bit integers in
"b", producing 2 intermediate signed 32-bit results. Sum these 2 results with the corresponding 32-bit integer
in "src", and store the packed 32-bit results in "dst".

[algorithm]

FOR j := 0 to 15
    tmp1.dword := SignExtend32(a.word[2*j]) * SignExtend32(b.word[2*j])
    tmp2.dword := SignExtend32(a.word[2*j+1]) * SignExtend32(b.word[2*j+1])
    dst.dword[j] := src.dword[j] + tmp1 + tmp2
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPDPWSSD - _mm256_maskz_dpwssd_epi32

| VPDPWSSD_YMMi32_MASKmskw_YMMi16_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply groups of 2 adjacent pairs of signed 16-bit integers in "a" with corresponding 16-bit integers in
"b", producing 2 intermediate signed 32-bit results. Sum these 2 results with the corresponding 32-bit integer
in "src", and store the packed 32-bit results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    IF k[j]
        tmp1.dword := SignExtend32(a.word[2*j]) * SignExtend32(b.word[2*j])
        tmp2.dword := SignExtend32(a.word[2*j+1]) * SignExtend32(b.word[2*j+1])
        dst.dword[j] := src.dword[j] + tmp1 + tmp2
    ELSE
        dst.dword[j] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPDPWSSD - _mm256_mask_dpwssd_epi32

| VPDPWSSD_YMMi32_MASKmskw_YMMi16_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply groups of 2 adjacent pairs of signed 16-bit integers in "a" with corresponding 16-bit integers in
"b", producing 2 intermediate signed 32-bit results. Sum these 2 results with the corresponding 32-bit integer
in "src", and store the packed 32-bit results in "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    IF k[j]
        tmp1.dword := SignExtend32(a.word[2*j]) * SignExtend32(b.word[2*j])
        tmp2.dword := SignExtend32(a.word[2*j+1]) * SignExtend32(b.word[2*j+1])
        dst.dword[j] := src.dword[j] + tmp1 + tmp2
    ELSE
        dst.dword[j] := src.dword[j]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPDPWSSD - _mm256_dpwssd_epi32

| VPDPWSSD_YMMi32_MASKmskw_YMMi16_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply groups of 2 adjacent pairs of signed 16-bit integers in "a" with corresponding 16-bit integers in
"b", producing 2 intermediate signed 32-bit results. Sum these 2 results with the corresponding 32-bit integer
in "src", and store the packed 32-bit results in "dst".

[algorithm]

FOR j := 0 to 7
    tmp1.dword := SignExtend32(a.word[2*j]) * SignExtend32(b.word[2*j])
    tmp2.dword := SignExtend32(a.word[2*j+1]) * SignExtend32(b.word[2*j+1])
    dst.dword[j] := src.dword[j] + tmp1 + tmp2
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPDPWSSD - _mm_maskz_dpwssd_epi32

| VPDPWSSD_XMMi32_MASKmskw_XMMi16_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply groups of 2 adjacent pairs of signed 16-bit integers in "a" with corresponding 16-bit integers in
"b", producing 2 intermediate signed 32-bit results. Sum these 2 results with the corresponding 32-bit integer
in "src", and store the packed 32-bit results in "dst" using zeromask "k" (elements are zeroed out when the
corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    IF k[j]
        tmp1.dword := SignExtend32(a.word[2*j]) * SignExtend32(b.word[2*j])
        tmp2.dword := SignExtend32(a.word[2*j+1]) * SignExtend32(b.word[2*j+1])
        dst.dword[j] := src.dword[j] + tmp1 + tmp2
    ELSE
        dst.dword[j] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPDPWSSD - _mm_mask_dpwssd_epi32

| VPDPWSSD_XMMi32_MASKmskw_XMMi16_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply groups of 2 adjacent pairs of signed 16-bit integers in "a" with corresponding 16-bit integers in
"b", producing 2 intermediate signed 32-bit results. Sum these 2 results with the corresponding 32-bit integer
in "src", and store the packed 32-bit results in "dst" using writemask "k" (elements are copied from "src" when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    IF k[j]
        tmp1.dword := SignExtend32(a.word[2*j]) * SignExtend32(b.word[2*j])
        tmp2.dword := SignExtend32(a.word[2*j+1]) * SignExtend32(b.word[2*j+1])
        dst.dword[j] := src.dword[j] + tmp1 + tmp2
    ELSE
        dst.dword[j] := src.dword[j]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPDPWSSD - _mm_dpwssd_epi32

| VPDPWSSD_XMMi32_MASKmskw_XMMi16_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply groups of 2 adjacent pairs of signed 16-bit integers in "a" with corresponding 16-bit integers in
"b", producing 2 intermediate signed 32-bit results. Sum these 2 results with the corresponding 32-bit integer
in "src", and store the packed 32-bit results in "dst".

[algorithm]

FOR j := 0 to 3
    tmp1.dword := SignExtend32(a.word[2*j]) * SignExtend32(b.word[2*j])
    tmp2.dword := SignExtend32(a.word[2*j+1]) * SignExtend32(b.word[2*j+1])
    dst.dword[j] := src.dword[j] + tmp1 + tmp2
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPDPBUSDS - _mm512_maskz_dpbusds_epi32

| VPDPBUSDS_ZMMi32_MASKmskw_ZMMu8_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply groups of 4 adjacent pairs of unsigned 8-bit integers in "a" with corresponding signed 8-bit integers
in "b", producing 4 intermediate signed 16-bit results. Sum these 4 results with the corresponding 32-bit
integer in "src" using signed saturation, and store the packed 32-bit results in "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    IF k[j]
        tmp1.word := Signed(ZeroExtend16(a.byte[4*j]) * SignExtend16(b.byte[4*j]))
        tmp2.word := Signed(ZeroExtend16(a.byte[4*j+1]) * SignExtend16(b.byte[4*j+1]))
        tmp3.word := Signed(ZeroExtend16(a.byte[4*j+2]) * SignExtend16(b.byte[4*j+2]))
        tmp4.word := Signed(ZeroExtend16(a.byte[4*j+3]) * SignExtend16(b.byte[4*j+3]))
        dst.dword[j] := Saturate32(src.dword[j] + tmp1 + tmp2 + tmp3 + tmp4)
    ELSE
        dst.dword[j] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPDPBUSDS - _mm512_mask_dpbusds_epi32

| VPDPBUSDS_ZMMi32_MASKmskw_ZMMu8_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply groups of 4 adjacent pairs of unsigned 8-bit integers in "a" with corresponding signed 8-bit integers
in "b", producing 4 intermediate signed 16-bit results. Sum these 4 results with the corresponding 32-bit
integer in "src" using signed saturation, and store the packed 32-bit results in "dst" using writemask "k"
(elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    IF k[j]
        tmp1.word := Signed(ZeroExtend16(a.byte[4*j]) * SignExtend16(b.byte[4*j]))
        tmp2.word := Signed(ZeroExtend16(a.byte[4*j+1]) * SignExtend16(b.byte[4*j+1]))
        tmp3.word := Signed(ZeroExtend16(a.byte[4*j+2]) * SignExtend16(b.byte[4*j+2]))
        tmp4.word := Signed(ZeroExtend16(a.byte[4*j+3]) * SignExtend16(b.byte[4*j+3]))
        dst.dword[j] := Saturate32(src.dword[j] + tmp1 + tmp2 + tmp3 + tmp4)
    ELSE
        dst.dword[j] := src.dword[j]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPDPBUSDS - _mm512_dpbusds_epi32

| VPDPBUSDS_ZMMi32_MASKmskw_ZMMu8_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply groups of 4 adjacent pairs of unsigned 8-bit integers in "a" with corresponding signed 8-bit integers
in "b", producing 4 intermediate signed 16-bit results. Sum these 4 results with the corresponding 32-bit
integer in "src" using signed saturation, and store the packed 32-bit results in "dst".

[algorithm]

FOR j := 0 to 15
    tmp1.word := Signed(ZeroExtend16(a.byte[4*j]) * SignExtend16(b.byte[4*j]))
    tmp2.word := Signed(ZeroExtend16(a.byte[4*j+1]) * SignExtend16(b.byte[4*j+1]))
    tmp3.word := Signed(ZeroExtend16(a.byte[4*j+2]) * SignExtend16(b.byte[4*j+2]))
    tmp4.word := Signed(ZeroExtend16(a.byte[4*j+3]) * SignExtend16(b.byte[4*j+3]))
    dst.dword[j] := Saturate32(src.dword[j] + tmp1 + tmp2 + tmp3 + tmp4)
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPDPBUSDS - _mm256_maskz_dpbusds_epi32

| VPDPBUSDS_YMMi32_MASKmskw_YMMu8_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply groups of 4 adjacent pairs of unsigned 8-bit integers in "a" with corresponding signed 8-bit integers
in "b", producing 4 intermediate signed 16-bit results. Sum these 4 results with the corresponding 32-bit
integer in "src" using signed saturation, and store the packed 32-bit results in "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    IF k[j]
        tmp1.word := Signed(ZeroExtend16(a.byte[4*j]) * SignExtend16(b.byte[4*j]))
        tmp2.word := Signed(ZeroExtend16(a.byte[4*j+1]) * SignExtend16(b.byte[4*j+1]))
        tmp3.word := Signed(ZeroExtend16(a.byte[4*j+2]) * SignExtend16(b.byte[4*j+2]))
        tmp4.word := Signed(ZeroExtend16(a.byte[4*j+3]) * SignExtend16(b.byte[4*j+3]))
        dst.dword[j] := Saturate32(src.dword[j] + tmp1 + tmp2 + tmp3 + tmp4)
    ELSE
        dst.dword[j] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPDPBUSDS - _mm256_mask_dpbusds_epi32

| VPDPBUSDS_YMMi32_MASKmskw_YMMu8_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply groups of 4 adjacent pairs of unsigned 8-bit integers in "a" with corresponding signed 8-bit integers
in "b", producing 4 intermediate signed 16-bit results. Sum these 4 results with the corresponding 32-bit
integer in "src" using signed saturation, and store the packed 32-bit results in "dst" using writemask "k"
(elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    IF k[j]
        tmp1.word := Signed(ZeroExtend16(a.byte[4*j]) * SignExtend16(b.byte[4*j]))
        tmp2.word := Signed(ZeroExtend16(a.byte[4*j+1]) * SignExtend16(b.byte[4*j+1]))
        tmp3.word := Signed(ZeroExtend16(a.byte[4*j+2]) * SignExtend16(b.byte[4*j+2]))
        tmp4.word := Signed(ZeroExtend16(a.byte[4*j+3]) * SignExtend16(b.byte[4*j+3]))
        dst.dword[j] := Saturate32(src.dword[j] + tmp1 + tmp2 + tmp3 + tmp4)
    ELSE
        dst.dword[j] := src.dword[j]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPDPBUSDS - _mm256_dpbusds_epi32

| VPDPBUSDS_YMMi32_MASKmskw_YMMu8_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply groups of 4 adjacent pairs of unsigned 8-bit integers in "a" with corresponding signed 8-bit integers
in "b", producing 4 intermediate signed 16-bit results. Sum these 4 results with the corresponding 32-bit
integer in "src" using signed saturation, and store the packed 32-bit results in "dst".

[algorithm]

FOR j := 0 to 7
    tmp1.word := Signed(ZeroExtend16(a.byte[4*j]) * SignExtend16(b.byte[4*j]))
    tmp2.word := Signed(ZeroExtend16(a.byte[4*j+1]) * SignExtend16(b.byte[4*j+1]))
    tmp3.word := Signed(ZeroExtend16(a.byte[4*j+2]) * SignExtend16(b.byte[4*j+2]))
    tmp4.word := Signed(ZeroExtend16(a.byte[4*j+3]) * SignExtend16(b.byte[4*j+3]))
    dst.dword[j] := Saturate32(src.dword[j] + tmp1 + tmp2 + tmp3 + tmp4)
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPDPBUSDS - _mm_maskz_dpbusds_epi32

| VPDPBUSDS_XMMi32_MASKmskw_XMMu8_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply groups of 4 adjacent pairs of unsigned 8-bit integers in "a" with corresponding signed 8-bit integers
in "b", producing 4 intermediate signed 16-bit results. Sum these 4 results with the corresponding 32-bit
integer in "src" using signed saturation, and store the packed 32-bit results in "dst" using zeromask "k"
(elements are zeroed out when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    IF k[j]
        tmp1.word := Signed(ZeroExtend16(a.byte[4*j]) * SignExtend16(b.byte[4*j]))
        tmp2.word := Signed(ZeroExtend16(a.byte[4*j+1]) * SignExtend16(b.byte[4*j+1]))
        tmp3.word := Signed(ZeroExtend16(a.byte[4*j+2]) * SignExtend16(b.byte[4*j+2]))
        tmp4.word := Signed(ZeroExtend16(a.byte[4*j+3]) * SignExtend16(b.byte[4*j+3]))
        dst.dword[j] := Saturate32(src.dword[j] + tmp1 + tmp2 + tmp3 + tmp4)
    ELSE
        dst.dword[j] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPDPBUSDS - _mm_mask_dpbusds_epi32

| VPDPBUSDS_XMMi32_MASKmskw_XMMu8_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply groups of 4 adjacent pairs of unsigned 8-bit integers in "a" with corresponding signed 8-bit integers
in "b", producing 4 intermediate signed 16-bit results. Sum these 4 results with the corresponding 32-bit
integer in "src" using signed saturation, and store the packed 32-bit results in "dst" using writemask "k"
(elements are copied from "src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    IF k[j]
        tmp1.word := Signed(ZeroExtend16(a.byte[4*j]) * SignExtend16(b.byte[4*j]))
        tmp2.word := Signed(ZeroExtend16(a.byte[4*j+1]) * SignExtend16(b.byte[4*j+1]))
        tmp3.word := Signed(ZeroExtend16(a.byte[4*j+2]) * SignExtend16(b.byte[4*j+2]))
        tmp4.word := Signed(ZeroExtend16(a.byte[4*j+3]) * SignExtend16(b.byte[4*j+3]))
        dst.dword[j] := Saturate32(src.dword[j] + tmp1 + tmp2 + tmp3 + tmp4)
    ELSE
        dst.dword[j] := src.dword[j]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPDPBUSDS - _mm_dpbusds_epi32

| VPDPBUSDS_XMMi32_MASKmskw_XMMu8_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply groups of 4 adjacent pairs of unsigned 8-bit integers in "a" with corresponding signed 8-bit integers
in "b", producing 4 intermediate signed 16-bit results. Sum these 4 results with the corresponding 32-bit
integer in "src" using signed saturation, and store the packed 32-bit results in "dst".

[algorithm]

FOR j := 0 to 3
    tmp1.word := Signed(ZeroExtend16(a.byte[4*j]) * SignExtend16(b.byte[4*j]))
    tmp2.word := Signed(ZeroExtend16(a.byte[4*j+1]) * SignExtend16(b.byte[4*j+1]))
    tmp3.word := Signed(ZeroExtend16(a.byte[4*j+2]) * SignExtend16(b.byte[4*j+2]))
    tmp4.word := Signed(ZeroExtend16(a.byte[4*j+3]) * SignExtend16(b.byte[4*j+3]))
    dst.dword[j] := Saturate32(src.dword[j] + tmp1 + tmp2 + tmp3 + tmp4)
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPDPBUSD - _mm512_maskz_dpbusd_epi32

| VPDPBUSD_ZMMi32_MASKmskw_ZMMu8_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply groups of 4 adjacent pairs of unsigned 8-bit integers in "a" with corresponding signed 8-bit integers
in "b", producing 4 intermediate signed 16-bit results. Sum these 4 results with the corresponding 32-bit
integer in "src", and store the packed 32-bit results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    IF k[j]
        tmp1.word := Signed(ZeroExtend16(a.byte[4*j]) * SignExtend16(b.byte[4*j]))
        tmp2.word := Signed(ZeroExtend16(a.byte[4*j+1]) * SignExtend16(b.byte[4*j+1]))
        tmp3.word := Signed(ZeroExtend16(a.byte[4*j+2]) * SignExtend16(b.byte[4*j+2]))
        tmp4.word := Signed(ZeroExtend16(a.byte[4*j+3]) * SignExtend16(b.byte[4*j+3]))
        dst.dword[j] := src.dword[j] + tmp1 + tmp2 + tmp3 + tmp4
    ELSE
        dst.dword[j] := 0
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPDPBUSD - _mm512_mask_dpbusd_epi32

| VPDPBUSD_ZMMi32_MASKmskw_ZMMu8_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply groups of 4 adjacent pairs of unsigned 8-bit integers in "a" with corresponding signed 8-bit integers
in "b", producing 4 intermediate signed 16-bit results. Sum these 4 results with the corresponding 32-bit
integer in "src", and store the packed 32-bit results in "dst" using writemask "k" (elements are copied from
"src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 15
    IF k[j]
        tmp1.word := Signed(ZeroExtend16(a.byte[4*j]) * SignExtend16(b.byte[4*j]))
        tmp2.word := Signed(ZeroExtend16(a.byte[4*j+1]) * SignExtend16(b.byte[4*j+1]))
        tmp3.word := Signed(ZeroExtend16(a.byte[4*j+2]) * SignExtend16(b.byte[4*j+2]))
        tmp4.word := Signed(ZeroExtend16(a.byte[4*j+3]) * SignExtend16(b.byte[4*j+3]))
        dst.dword[j] := src.dword[j] + tmp1 + tmp2 + tmp3 + tmp4
    ELSE
        dst.dword[j] := src.dword[j]
    FI
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPDPBUSD - _mm512_dpbusd_epi32

| VPDPBUSD_ZMMi32_MASKmskw_ZMMu8_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply groups of 4 adjacent pairs of unsigned 8-bit integers in "a" with corresponding signed 8-bit integers
in "b", producing 4 intermediate signed 16-bit results. Sum these 4 results with the corresponding 32-bit
integer in "src", and store the packed 32-bit results in "dst".

[algorithm]

FOR j := 0 to 15
    tmp1.word := Signed(ZeroExtend16(a.byte[4*j]) * SignExtend16(b.byte[4*j]))
    tmp2.word := Signed(ZeroExtend16(a.byte[4*j+1]) * SignExtend16(b.byte[4*j+1]))
    tmp3.word := Signed(ZeroExtend16(a.byte[4*j+2]) * SignExtend16(b.byte[4*j+2]))
    tmp4.word := Signed(ZeroExtend16(a.byte[4*j+3]) * SignExtend16(b.byte[4*j+3]))
    dst.dword[j] := src.dword[j] + tmp1 + tmp2 + tmp3 + tmp4
ENDFOR
dst[MAX:512] := 0

--------------------------------------------------------------------------------------------------------------

## VPDPBUSD - _mm256_maskz_dpbusd_epi32

| VPDPBUSD_YMMi32_MASKmskw_YMMu8_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply groups of 4 adjacent pairs of unsigned 8-bit integers in "a" with corresponding signed 8-bit integers
in "b", producing 4 intermediate signed 16-bit results. Sum these 4 results with the corresponding 32-bit
integer in "src", and store the packed 32-bit results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    IF k[j]
        tmp1.word := Signed(ZeroExtend16(a.byte[4*j]) * SignExtend16(b.byte[4*j]))
        tmp2.word := Signed(ZeroExtend16(a.byte[4*j+1]) * SignExtend16(b.byte[4*j+1]))
        tmp3.word := Signed(ZeroExtend16(a.byte[4*j+2]) * SignExtend16(b.byte[4*j+2]))
        tmp4.word := Signed(ZeroExtend16(a.byte[4*j+3]) * SignExtend16(b.byte[4*j+3]))
        dst.dword[j] := src.dword[j] + tmp1 + tmp2 + tmp3 + tmp4
    ELSE
        dst.dword[j] := 0
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPDPBUSD - _mm256_mask_dpbusd_epi32

| VPDPBUSD_YMMi32_MASKmskw_YMMu8_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply groups of 4 adjacent pairs of unsigned 8-bit integers in "a" with corresponding signed 8-bit integers
in "b", producing 4 intermediate signed 16-bit results. Sum these 4 results with the corresponding 32-bit
integer in "src", and store the packed 32-bit results in "dst" using writemask "k" (elements are copied from
"src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 7
    IF k[j]
        tmp1.word := Signed(ZeroExtend16(a.byte[4*j]) * SignExtend16(b.byte[4*j]))
        tmp2.word := Signed(ZeroExtend16(a.byte[4*j+1]) * SignExtend16(b.byte[4*j+1]))
        tmp3.word := Signed(ZeroExtend16(a.byte[4*j+2]) * SignExtend16(b.byte[4*j+2]))
        tmp4.word := Signed(ZeroExtend16(a.byte[4*j+3]) * SignExtend16(b.byte[4*j+3]))
        dst.dword[j] := src.dword[j] + tmp1 + tmp2 + tmp3 + tmp4
    ELSE
        dst.dword[j] := src.dword[j]
    FI
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPDPBUSD - _mm256_dpbusd_epi32

| VPDPBUSD_YMMi32_MASKmskw_YMMu8_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply groups of 4 adjacent pairs of unsigned 8-bit integers in "a" with corresponding signed 8-bit integers
in "b", producing 4 intermediate signed 16-bit results. Sum these 4 results with the corresponding 32-bit
integer in "src", and store the packed 32-bit results in "dst".

[algorithm]

FOR j := 0 to 7
    tmp1.word := Signed(ZeroExtend16(a.byte[4*j]) * SignExtend16(b.byte[4*j]))
    tmp2.word := Signed(ZeroExtend16(a.byte[4*j+1]) * SignExtend16(b.byte[4*j+1]))
    tmp3.word := Signed(ZeroExtend16(a.byte[4*j+2]) * SignExtend16(b.byte[4*j+2]))
    tmp4.word := Signed(ZeroExtend16(a.byte[4*j+3]) * SignExtend16(b.byte[4*j+3]))
    dst.dword[j] := src.dword[j] + tmp1 + tmp2 + tmp3 + tmp4
ENDFOR
dst[MAX:256] := 0

--------------------------------------------------------------------------------------------------------------

## VPDPBUSD - _mm_maskz_dpbusd_epi32

| VPDPBUSD_XMMi32_MASKmskw_XMMu8_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply groups of 4 adjacent pairs of unsigned 8-bit integers in "a" with corresponding signed 8-bit integers
in "b", producing 4 intermediate signed 16-bit results. Sum these 4 results with the corresponding 32-bit
integer in "src", and store the packed 32-bit results in "dst" using zeromask "k" (elements are zeroed out when
the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    IF k[j]
        tmp1.word := Signed(ZeroExtend16(a.byte[4*j]) * SignExtend16(b.byte[4*j]))
        tmp2.word := Signed(ZeroExtend16(a.byte[4*j+1]) * SignExtend16(b.byte[4*j+1]))
        tmp3.word := Signed(ZeroExtend16(a.byte[4*j+2]) * SignExtend16(b.byte[4*j+2]))
        tmp4.word := Signed(ZeroExtend16(a.byte[4*j+3]) * SignExtend16(b.byte[4*j+3]))
        dst.dword[j] := src.dword[j] + tmp1 + tmp2 + tmp3 + tmp4
    ELSE
        dst.dword[j] := 0
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPDPBUSD - _mm_mask_dpbusd_epi32

| VPDPBUSD_XMMi32_MASKmskw_XMMu8_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply groups of 4 adjacent pairs of unsigned 8-bit integers in "a" with corresponding signed 8-bit integers
in "b", producing 4 intermediate signed 16-bit results. Sum these 4 results with the corresponding 32-bit
integer in "src", and store the packed 32-bit results in "dst" using writemask "k" (elements are copied from
"src" when the corresponding mask bit is not set).

[algorithm]

FOR j := 0 to 3
    IF k[j]
        tmp1.word := Signed(ZeroExtend16(a.byte[4*j]) * SignExtend16(b.byte[4*j]))
        tmp2.word := Signed(ZeroExtend16(a.byte[4*j+1]) * SignExtend16(b.byte[4*j+1]))
        tmp3.word := Signed(ZeroExtend16(a.byte[4*j+2]) * SignExtend16(b.byte[4*j+2]))
        tmp4.word := Signed(ZeroExtend16(a.byte[4*j+3]) * SignExtend16(b.byte[4*j+3]))
        dst.dword[j] := src.dword[j] + tmp1 + tmp2 + tmp3 + tmp4
    ELSE
        dst.dword[j] := src.dword[j]
    FI
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VPDPBUSD - _mm_dpbusd_epi32

| VPDPBUSD_XMMi32_MASKmskw_XMMu8_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Multiply groups of 4 adjacent pairs of unsigned 8-bit integers in "a" with corresponding signed 8-bit integers
in "b", producing 4 intermediate signed 16-bit results. Sum these 4 results with the corresponding 32-bit
integer in "src", and store the packed 32-bit results in "dst".

[algorithm]

FOR j := 0 to 3
    tmp1.word := Signed(ZeroExtend16(a.byte[4*j]) * SignExtend16(b.byte[4*j]))
    tmp2.word := Signed(ZeroExtend16(a.byte[4*j+1]) * SignExtend16(b.byte[4*j+1]))
    tmp3.word := Signed(ZeroExtend16(a.byte[4*j+2]) * SignExtend16(b.byte[4*j+2]))
    tmp4.word := Signed(ZeroExtend16(a.byte[4*j+3]) * SignExtend16(b.byte[4*j+3]))
    dst.dword[j] := src.dword[j] + tmp1 + tmp2 + tmp3 + tmp4
ENDFOR
dst[MAX:128] := 0

--------------------------------------------------------------------------------------------------------------

## VP2INTERSECTD - _mm_2intersect_epi32

| VP2INTERSECTD_MASKmskw_XMMu32_XMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute intersection of packed 32-bit integer vectors "a" and "b", and store indication of match in the
corresponding bit of two mask registers specified by "k1" and "k2". A match in corresponding elements of "a"
and "b" is indicated by a set bit in the corresponding bit of the mask registers.

[algorithm]

MEM[k1+7:k1] := 0
MEM[k2+7:k2] := 0
FOR i := 0 TO 3
    FOR j := 0 TO 3
        match := (a.dword[i] == b.dword[j] ? 1 : 0)
        MEM[k1+7:k1].bit[i] |= match
        MEM[k2+7:k2].bit[j] |= match
    ENDFOR
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VP2INTERSECTD - _mm256_2intersect_epi32

| VP2INTERSECTD_MASKmskw_YMMu32_YMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute intersection of packed 32-bit integer vectors "a" and "b", and store indication of match in the
corresponding bit of two mask registers specified by "k1" and "k2". A match in corresponding elements of "a"
and "b" is indicated by a set bit in the corresponding bit of the mask registers.

[algorithm]

MEM[k1+7:k1] := 0
MEM[k2+7:k2] := 0
FOR i := 0 TO 7
    FOR j := 0 TO 7
        match := (a.dword[i] == b.dword[j] ? 1 : 0)
        MEM[k1+7:k1].bit[i] |= match
        MEM[k2+7:k2].bit[j] |= match
    ENDFOR
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VP2INTERSECTD - _mm512_2intersect_epi32

| VP2INTERSECTD_MASKmskw_ZMMu32_ZMMu32_AVX512

--------------------------------------------------------------------------------------------------------------
Compute intersection of packed 32-bit integer vectors "a" and "b", and store indication of match in the
corresponding bit of two mask registers specified by "k1" and "k2". A match in corresponding elements of "a"
and "b" is indicated by a set bit in the corresponding bit of the mask registers.

[algorithm]

MEM[k1+15:k1] := 0
MEM[k2+15:k2] := 0
FOR i := 0 TO 15
    FOR j := 0 TO 15
        match := (a.dword[i] == b.dword[j] ? 1 : 0)
        MEM[k1+15:k1].bit[i] |= match
        MEM[k2+15:k2].bit[j] |= match
    ENDFOR
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VP2INTERSECTQ - _mm_2intersect_epi64

| VP2INTERSECTQ_MASKmskw_XMMu64_XMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute intersection of packed 64-bit integer vectors "a" and "b", and store indication of match in the
corresponding bit of two mask registers specified by "k1" and "k2". A match in corresponding elements of "a"
and "b" is indicated by a set bit in the corresponding bit of the mask registers.

[algorithm]

MEM[k1+7:k1] := 0
MEM[k2+7:k2] := 0
FOR i := 0 TO 1
    FOR j := 0 TO 1
        match := (a.qword[i] == b.qword[j] ? 1 : 0)
        MEM[k1+7:k1].bit[i] |= match
        MEM[k2+7:k2].bit[j] |= match
    ENDFOR
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VP2INTERSECTQ - _mm256_2intersect_epi64

| VP2INTERSECTQ_MASKmskw_YMMu64_YMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute intersection of packed 64-bit integer vectors "a" and "b", and store indication of match in the
corresponding bit of two mask registers specified by "k1" and "k2". A match in corresponding elements of "a"
and "b" is indicated by a set bit in the corresponding bit of the mask registers.

[algorithm]

MEM[k1+7:k1] := 0
MEM[k2+7:k2] := 0
FOR i := 0 TO 3
    FOR j := 0 TO 3
        match := (a.qword[i] == b.qword[j] ? 1 : 0)
        MEM[k1+7:k1].bit[i] |= match
        MEM[k2+7:k2].bit[j] |= match
    ENDFOR
ENDFOR

--------------------------------------------------------------------------------------------------------------

## VP2INTERSECTQ - _mm512_2intersect_epi64

| VP2INTERSECTQ_MASKmskw_ZMMu64_ZMMu64_AVX512

--------------------------------------------------------------------------------------------------------------
Compute intersection of packed 64-bit integer vectors "a" and "b", and store indication of match in the
corresponding bit of two mask registers specified by "k1" and "k2". A match in corresponding elements of "a"
and "b" is indicated by a set bit in the corresponding bit of the mask registers.

[algorithm]

MEM[k1+7:k1] := 0
MEM[k2+7:k2] := 0
FOR i := 0 TO 7
    FOR j := 0 TO 7
        match := (a.qword[i] == b.qword[j] ? 1 : 0)
        MEM[k1+7:k1].bit[i] |= match
        MEM[k2+7:k2].bit[j] |= match
    ENDFOR
ENDFOR

--------------------------------------------------------------------------------------------------------------
