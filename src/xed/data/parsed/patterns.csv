AAA                  | DECIMAL        | BASE           | I86            | 0x37 not64                                                                       | REG0=XED_REG_AL:rw:SUPP REG1=XED_REG_AH:rw:SUPP
AAD                  | DECIMAL        | BASE           | I86            | 0xD5 not64 UIMM8()                                                               | IMM0:r:b:i8 REG0=XED_REG_AL:rw:SUPP REG1=XED_REG_AH:rw:SUPP
AAM                  | DECIMAL        | BASE           | I86            | 0xD4 not64 UIMM8()                                                               | IMM0:r:b:i8 REG0=XED_REG_AL:rw:SUPP REG1=XED_REG_AH:w:SUPP
AAS                  | DECIMAL        | BASE           | I86            | 0x3F not64                                                                       | REG0=XED_REG_AL:rw:SUPP REG1=XED_REG_AH:rw:SUPP
ADC                  | BINARY         | BASE           | I86            | 0x80 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM() SIMM8() nolock_prefix             | MEM0:rw:b IMM0:r:b:i8
ADC                  | BINARY         | BASE           | I86            | 0x80 MOD[0b11] MOD=3 REG[0b010] RM[nnn] SIMM8()                                  | REG0=GPR8_B():rw IMM0:r:b:i8
ADC                  | BINARY         | BASE           | I86            | 0x81 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM() SIMMz() nolock_prefix             | MEM0:rw:v IMM0:r:z
ADC                  | BINARY         | BASE           | I86            | 0x81 MOD[0b11] MOD=3 REG[0b010] RM[nnn] SIMMz()                                  | REG0=GPRv_B():rw IMM0:r:z
ADC                  | BINARY         | BASE           | I86            | 0x82 MOD[mm] MOD!=3 REG[0b010] RM[nnn] not64 MODRM() SIMM8() nolock_prefix       | MEM0:rw:b IMM0:r:b:i8
ADC                  | BINARY         | BASE           | I86            | 0x82 MOD[0b11] MOD=3 REG[0b010] RM[nnn] not64 SIMM8()                            | REG0=GPR8_B():rw IMM0:r:b:i8
ADC                  | BINARY         | BASE           | I86            | 0x83 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM() SIMM8() nolock_prefix             | MEM0:rw:v IMM0:r:b:i8
ADC                  | BINARY         | BASE           | I86            | 0x83 MOD[0b11] MOD=3 REG[0b010] RM[nnn] SIMM8()                                  | REG0=GPRv_B():rw IMM0:r:b:i8
ADC                  | BINARY         | BASE           | I86            | 0x10 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                       | MEM0:rw:b REG0=GPR8_R():r
ADC                  | BINARY         | BASE           | I86            | 0x10 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPR8_B():rw REG1=GPR8_R():r
ADC                  | BINARY         | BASE           | I86            | 0x11 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                       | MEM0:rw:v REG0=GPRv_R():r
ADC                  | BINARY         | BASE           | I86            | 0x11 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPRv_B():rw REG1=GPRv_R():r
ADC                  | BINARY         | BASE           | I86            | 0x12 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=GPR8_R():rw MEM0:r:b
ADC                  | BINARY         | BASE           | I86            | 0x12 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPR8_R():rw REG1=GPR8_B():r
ADC                  | BINARY         | BASE           | I86            | 0x13 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=GPRv_R():rw MEM0:r:v
ADC                  | BINARY         | BASE           | I86            | 0x13 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPRv_R():rw REG1=GPRv_B():r
ADC                  | BINARY         | BASE           | I86            | 0x14 SIMM8()                                                                     | REG0=XED_REG_AL:rw:IMPL IMM0:r:b:i8
ADC                  | BINARY         | BASE           | I86            | 0x15 SIMMz()                                                                     | REG0=OrAX():rw:IMPL IMM0:r:z
ADC_LOCK             | BINARY         | BASE           | I86            | 0x80 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM() SIMM8() lock_prefix               | MEM0:rw:b IMM0:r:b:i8
ADC_LOCK             | BINARY         | BASE           | I86            | 0x81 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM() SIMMz() lock_prefix               | MEM0:rw:v IMM0:r:z
ADC_LOCK             | BINARY         | BASE           | I86            | 0x82 MOD[mm] MOD!=3 REG[0b010] RM[nnn] not64 MODRM() SIMM8() lock_prefix         | MEM0:rw:b IMM0:r:b:i8
ADC_LOCK             | BINARY         | BASE           | I86            | 0x83 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM() SIMM8() lock_prefix               | MEM0:rw:v IMM0:r:b:i8
ADC_LOCK             | BINARY         | BASE           | I86            | 0x10 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                         | MEM0:rw:b REG0=GPR8_R():r
ADC_LOCK             | BINARY         | BASE           | I86            | 0x11 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                         | MEM0:rw:v REG0=GPRv_R():r
ADCX                 | ADOX_ADCX      | ADOX_ADCX      | ADOX_ADCX      | 0x0F 0x38 0xF6  MOD[0b11] MOD=3 REG[rrr] RM[nnn] osz_refining_prefix W0  IMMUNE66() | REG0=GPR32_R():rw:d REG1=GPR32_B():r:d
ADCX                 | ADOX_ADCX      | ADOX_ADCX      | ADOX_ADCX      | 0x0F 0x38 0xF6   MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() osz_refining_prefix W0  IMMUNE66() | REG0=GPR32_R():rw:d MEM0:r:d
ADCX                 | ADOX_ADCX      | ADOX_ADCX      | ADOX_ADCX      | 0x0F 0x38 0xF6  MOD[0b11] MOD=3 REG[rrr] RM[nnn] osz_refining_prefix  W1 IMMUNE66() mode64 | REG0=GPR64_R():rw:q  REG1=GPR64_B():r:q
ADCX                 | ADOX_ADCX      | ADOX_ADCX      | ADOX_ADCX      | 0x0F 0x38 0xF6  MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM() osz_refining_prefix  W1  IMMUNE66() mode64 | REG0=GPR64_R():rw:q  MEM0:r:q
ADD                  | BINARY         | BASE           | I86            | 0x80 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() SIMM8() nolock_prefix             | MEM0:rw:b IMM0:r:b:i8
ADD                  | BINARY         | BASE           | I86            | 0x80 MOD[0b11] MOD=3 REG[0b000] RM[nnn] SIMM8()                                  | REG0=GPR8_B():rw IMM0:r:b
ADD                  | BINARY         | BASE           | I86            | 0x81 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() SIMMz() nolock_prefix             | MEM0:rw:v IMM0:r:z
ADD                  | BINARY         | BASE           | I86            | 0x81 MOD[0b11] MOD=3 REG[0b000] RM[nnn] SIMMz()                                  | REG0=GPRv_B():rw IMM0:r:z
ADD                  | BINARY         | BASE           | I86            | 0x82 MOD[mm] MOD!=3 REG[0b000] RM[nnn] not64 MODRM() SIMM8() nolock_prefix       | MEM0:rw:b IMM0:r:b:i8
ADD                  | BINARY         | BASE           | I86            | 0x82 MOD[0b11] MOD=3 REG[0b000] RM[nnn] not64 SIMM8()                            | REG0=GPR8_B():rw IMM0:r:b:i8
ADD                  | BINARY         | BASE           | I86            | 0x83 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() SIMM8() nolock_prefix             | MEM0:rw:v IMM0:r:b:i8
ADD                  | BINARY         | BASE           | I86            | 0x83 MOD[0b11] MOD=3 REG[0b000] RM[nnn] SIMM8()                                  | REG0=GPRv_B():rw IMM0:r:b:i8
ADD                  | BINARY         | BASE           | I86            | 0x00 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                       | MEM0:rw:b REG0=GPR8_R():r
ADD                  | BINARY         | BASE           | I86            | 0x00 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPR8_B():rw REG1=GPR8_R():r
ADD                  | BINARY         | BASE           | I86            | 0x01 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                       | MEM0:rw:v REG0=GPRv_R():r
ADD                  | BINARY         | BASE           | I86            | 0x01 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPRv_B():rw REG1=GPRv_R():r
ADD                  | BINARY         | BASE           | I86            | 0x02 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=GPR8_R():rw MEM0:r:b
ADD                  | BINARY         | BASE           | I86            | 0x02 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPR8_R():rw REG1=GPR8_B():r
ADD                  | BINARY         | BASE           | I86            | 0x03 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=GPRv_R():rw MEM0:r:v
ADD                  | BINARY         | BASE           | I86            | 0x03 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPRv_R():rw REG1=GPRv_B():r
ADD                  | BINARY         | BASE           | I86            | 0x04 SIMM8()                                                                     | REG0=XED_REG_AL:rw:IMPL IMM0:r:b:i8
ADD                  | BINARY         | BASE           | I86            | 0x05 SIMMz()                                                                     | REG0=OrAX():rw:IMPL IMM0:r:z
ADD_LOCK             | BINARY         | BASE           | I86            | 0x80 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() SIMM8() lock_prefix               | MEM0:rw:b IMM0:r:b:i8
ADD_LOCK             | BINARY         | BASE           | I86            | 0x81 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() SIMMz() lock_prefix               | MEM0:rw:v IMM0:r:z
ADD_LOCK             | BINARY         | BASE           | I86            | 0x82 MOD[mm] MOD!=3 REG[0b000] RM[nnn] not64 MODRM() SIMM8() lock_prefix         | MEM0:rw:b IMM0:r:b:i8
ADD_LOCK             | BINARY         | BASE           | I86            | 0x83 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() SIMM8() lock_prefix               | MEM0:rw:v IMM0:r:b:i8
ADD_LOCK             | BINARY         | BASE           | I86            | 0x00 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                         | MEM0:rw:b REG0=GPR8_R():r
ADD_LOCK             | BINARY         | BASE           | I86            | 0x01 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                         | MEM0:rw:v REG0=GPRv_R():r
ADOX                 | ADOX_ADCX      | ADOX_ADCX      | ADOX_ADCX      | 0x0F 0x38 0xF6  MOD[0b11] MOD=3 REG[rrr] RM[nnn] refining_f3  W0 IMMUNE66()      | REG0=GPR32_R():rw:d  REG1=GPR32_B():r:d
ADOX                 | ADOX_ADCX      | ADOX_ADCX      | ADOX_ADCX      | 0x0F 0x38 0xF6  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() refining_f3 W0 IMMUNE66() | REG0=GPR32_R():rw:d MEM0:r:d
ADOX                 | ADOX_ADCX      | ADOX_ADCX      | ADOX_ADCX      | 0x0F 0x38 0xF6 MOD[0b11] MOD=3 REG[rrr] RM[nnn] refining_f3 W1 IMMUNE66()  mode64 | REG0=GPR64_R():rw:q  REG1=GPR64_B():r:q
ADOX                 | ADOX_ADCX      | ADOX_ADCX      | ADOX_ADCX      | 0x0F 0x38 0xF6 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM() refining_f3 W1   IMMUNE66() mode64 | REG0=GPR64_R():rw:q  MEM0:r:q
AND                  | LOGICAL        | BASE           | I86            | 0x80 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM() UIMM8() nolock_prefix             | MEM0:rw:b IMM0:r:b
AND                  | LOGICAL        | BASE           | I86            | 0x80 MOD[0b11] MOD=3 REG[0b100] RM[nnn] UIMM8()                                  | REG0=GPR8_B():rw IMM0:r:b
AND                  | LOGICAL        | BASE           | I86            | 0x81 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM() SIMMz() nolock_prefix             | MEM0:rw:v IMM0:r:z
AND                  | LOGICAL        | BASE           | I86            | 0x81 MOD[0b11] MOD=3 REG[0b100] RM[nnn] SIMMz()                                  | REG0=GPRv_B():rw IMM0:r:z
AND                  | LOGICAL        | BASE           | I86            | 0x82 MOD[mm] MOD!=3 REG[0b100] RM[nnn] not64 MODRM() UIMM8() nolock_prefix       | MEM0:rw:b IMM0:r:b
AND                  | LOGICAL        | BASE           | I86            | 0x82 MOD[0b11] MOD=3 REG[0b100] RM[nnn] not64 UIMM8()                            | REG0=GPR8_B():rw IMM0:r:b
AND                  | LOGICAL        | BASE           | I86            | 0x83 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM() SIMM8() nolock_prefix             | MEM0:rw:v IMM0:r:b:i8
AND                  | LOGICAL        | BASE           | I86            | 0x83 MOD[0b11] MOD=3 REG[0b100] RM[nnn] SIMM8()                                  | REG0=GPRv_B():rw IMM0:r:b:i8
AND                  | LOGICAL        | BASE           | I86            | 0x20 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                       | MEM0:rw:b REG0=GPR8_R():r
AND                  | LOGICAL        | BASE           | I86            | 0x20 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPR8_B():rw REG1=GPR8_R():r
AND                  | LOGICAL        | BASE           | I86            | 0x21 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                       | MEM0:rw:v REG0=GPRv_R():r
AND                  | LOGICAL        | BASE           | I86            | 0x21 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPRv_B():rw REG1=GPRv_R():r
AND                  | LOGICAL        | BASE           | I86            | 0x22 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPR8_R():rw REG1=GPR8_B():r
AND                  | LOGICAL        | BASE           | I86            | 0x22 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=GPR8_R():rw MEM0:r:b
AND                  | LOGICAL        | BASE           | I86            | 0x23 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPRv_R():rw REG1=GPRv_B():r
AND                  | LOGICAL        | BASE           | I86            | 0x23 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=GPRv_R():rw MEM0:r:v
AND                  | LOGICAL        | BASE           | I86            | 0x24 SIMM8()                                                                     | REG0=XED_REG_AL:rw:IMPL IMM0:r:b:i8
AND                  | LOGICAL        | BASE           | I86            | 0x25 SIMMz()                                                                     | REG0=OrAX():rw:IMPL IMM0:r:z
AND_LOCK             | LOGICAL        | BASE           | I86            | 0x80 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM() UIMM8() lock_prefix               | MEM0:rw:b IMM0:r:b
AND_LOCK             | LOGICAL        | BASE           | I86            | 0x81 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM() SIMMz() lock_prefix               | MEM0:rw:v IMM0:r:z
AND_LOCK             | LOGICAL        | BASE           | I86            | 0x82 MOD[mm] MOD!=3 REG[0b100] RM[nnn] not64 MODRM() UIMM8() lock_prefix         | MEM0:rw:b IMM0:r:b
AND_LOCK             | LOGICAL        | BASE           | I86            | 0x83 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM() SIMM8() lock_prefix               | MEM0:rw:v IMM0:r:b:i8
AND_LOCK             | LOGICAL        | BASE           | I86            | 0x20 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                         | MEM0:rw:b REG0=GPR8_R():r
AND_LOCK             | LOGICAL        | BASE           | I86            | 0x21 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                         | MEM0:rw:v REG0=GPRv_R():r
ARPL                 | SYSTEM         | BASE           | I286PROTECTED  | 0x63 MOD[mm] MOD!=3 REG[rrr] RM[nnn] not64 MODRM()                               | MEM0:rw:w REG0=GPR16_R():r
ARPL                 | SYSTEM         | BASE           | I286PROTECTED  | 0x63 MOD[0b11] MOD=3 REG[rrr] RM[nnn] not64                                      | REG0=GPR16_B():rw REG1=GPR16_R():r
BEXTR_XOP            | TBM            | TBM            | TBM            | XOPV 0x10 VNP not64 VL128 NOVSR XMAPA MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM32() | REG0=GPR32_R():w:d MEM0:r:d IMM0:r:d
BEXTR_XOP            | TBM            | TBM            | TBM            | XOPV 0x10 VNP mode64  VL128 NOVSR XMAPA MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM32() | REG0=GPRy_R():w:y MEM0:r:y IMM0:r:d
BEXTR_XOP            | TBM            | TBM            | TBM            | XOPV 0x10 VNP not64 VL128 NOVSR XMAPA MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM32()  | REG0=GPR32_R():w:d REG1=GPR32_B():r:d IMM0:r:d
BEXTR_XOP            | TBM            | TBM            | TBM            | XOPV 0x10 VNP mode64 VL128 NOVSR XMAPA MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM32() | REG0=GPRy_R():w:y REG1=GPRy_B():r:y IMM0:r:d
BLCFILL              | TBM            | TBM            | TBM            | XOPV 0x01 VNP not64 VL128  XMAP9 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM()       | REG0=VGPR32_N():w:d MEM0:r:d
BLCFILL              | TBM            | TBM            | TBM            | XOPV 0x01 VNP mode64 VL128  XMAP9 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM()      | REG0=VGPRy_N():w:y MEM0:r:y
BLCFILL              | TBM            | TBM            | TBM            | XOPV 0x01 VNP not64 VL128  XMAP9 MOD[0b11] MOD=3 REG[0b001] RM[nnn]              | REG0=VGPR32_N():w:d REG1=GPR32_B():r:d
BLCFILL              | TBM            | TBM            | TBM            | XOPV 0x01 VNP mode64 VL128  XMAP9 MOD[0b11] MOD=3 REG[0b001] RM[nnn]             | REG0=VGPRy_N():w:y REG1=GPRy_B():r:y
BLCI                 | TBM            | TBM            | TBM            | XOPV 0x02 VNP not64 VL128  XMAP9 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM()       | REG0=VGPR32_N():w:d MEM0:r:d
BLCI                 | TBM            | TBM            | TBM            | XOPV 0x02 VNP mode64 VL128  XMAP9 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM()      | REG0=VGPRy_N():w:y MEM0:r:y
BLCI                 | TBM            | TBM            | TBM            | XOPV 0x02 VNP not64 VL128  XMAP9 MOD[0b11] MOD=3 REG[0b110] RM[nnn]              | REG0=VGPR32_N():w:d REG1=GPR32_B():r:d
BLCI                 | TBM            | TBM            | TBM            | XOPV 0x02 VNP mode64 VL128  XMAP9 MOD[0b11] MOD=3 REG[0b110] RM[nnn]             | REG0=VGPRy_N():w:y REG1=GPRy_B():r:y
BLCIC                | TBM            | TBM            | TBM            | XOPV 0x01 VNP not64 VL128  XMAP9 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM()       | REG0=VGPR32_N():w:d MEM0:r:d
BLCIC                | TBM            | TBM            | TBM            | XOPV 0x01 VNP mode64 VL128  XMAP9 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM()      | REG0=VGPRy_N():w:y MEM0:r:y
BLCIC                | TBM            | TBM            | TBM            | XOPV 0x01 VNP not64 VL128  XMAP9 MOD[0b11] MOD=3 REG[0b101] RM[nnn]              | REG0=VGPR32_N():w:d REG1=GPR32_B():r:d
BLCIC                | TBM            | TBM            | TBM            | XOPV 0x01 VNP mode64 VL128  XMAP9 MOD[0b11] MOD=3 REG[0b101] RM[nnn]             | REG0=VGPRy_N():w:y REG1=GPRy_B():r:y
BLCMSK               | TBM            | TBM            | TBM            | XOPV 0x02 VNP not64 VL128  XMAP9 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM()       | REG0=VGPR32_N():w:d MEM0:r:d
BLCMSK               | TBM            | TBM            | TBM            | XOPV 0x02 VNP mode64 VL128  XMAP9 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM()      | REG0=VGPRy_N():w:y MEM0:r:y
BLCMSK               | TBM            | TBM            | TBM            | XOPV 0x02 VNP not64 VL128  XMAP9 MOD[0b11] MOD=3 REG[0b001] RM[nnn]              | REG0=VGPR32_N():w:d REG1=GPR32_B():r:d
BLCMSK               | TBM            | TBM            | TBM            | XOPV 0x02 VNP mode64 VL128  XMAP9 MOD[0b11] MOD=3 REG[0b001] RM[nnn]             | REG0=VGPRy_N():w:y REG1=GPRy_B():r:y
BLCS                 | TBM            | TBM            | TBM            | XOPV 0x01 VNP not64 VL128  XMAP9 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM()       | REG0=VGPR32_N():w:d MEM0:r:d
BLCS                 | TBM            | TBM            | TBM            | XOPV 0x01 VNP mode64 VL128  XMAP9 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM()      | REG0=VGPRy_N():w:y MEM0:r:y
BLCS                 | TBM            | TBM            | TBM            | XOPV 0x01 VNP not64 VL128  XMAP9 MOD[0b11] MOD=3 REG[0b011] RM[nnn]              | REG0=VGPR32_N():w:d REG1=GPR32_B():r:d
BLCS                 | TBM            | TBM            | TBM            | XOPV 0x01 VNP mode64 VL128  XMAP9 MOD[0b11] MOD=3 REG[0b011] RM[nnn]             | REG0=VGPRy_N():w:y REG1=GPRy_B():r:y
BLSFILL              | TBM            | TBM            | TBM            | XOPV 0x01 VNP not64 VL128  XMAP9 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM()       | REG0=VGPR32_N():w:d MEM0:r:d
BLSFILL              | TBM            | TBM            | TBM            | XOPV 0x01 VNP mode64 VL128  XMAP9 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM()      | REG0=VGPRy_N():w:y MEM0:r:y
BLSFILL              | TBM            | TBM            | TBM            | XOPV 0x01 VNP not64 VL128  XMAP9 MOD[0b11] MOD=3 REG[0b010] RM[nnn]              | REG0=VGPR32_N():w:d REG1=GPR32_B():r:d
BLSFILL              | TBM            | TBM            | TBM            | XOPV 0x01 VNP mode64 VL128  XMAP9 MOD[0b11] MOD=3 REG[0b010] RM[nnn]             | REG0=VGPRy_N():w:y REG1=GPRy_B():r:y
BLSIC                | TBM            | TBM            | TBM            | XOPV 0x01 VNP not64 VL128  XMAP9 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM()       | REG0=VGPR32_N():w:d MEM0:r:d
BLSIC                | TBM            | TBM            | TBM            | XOPV 0x01 VNP mode64 VL128  XMAP9 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM()      | REG0=VGPRy_N():w:y MEM0:r:y
BLSIC                | TBM            | TBM            | TBM            | XOPV 0x01 VNP not64 VL128  XMAP9 MOD[0b11] MOD=3 REG[0b110] RM[nnn]              | REG0=VGPR32_N():w:d REG1=GPR32_B():r:d
BLSIC                | TBM            | TBM            | TBM            | XOPV 0x01 VNP mode64 VL128  XMAP9 MOD[0b11] MOD=3 REG[0b110] RM[nnn]             | REG0=VGPRy_N():w:y REG1=GPRy_B():r:y
BNDCL                | MPX            | MPX            | MPX            | 0x0F 0x1A MPXMODE=1  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()  f3_refining_prefix | REG0=BND_R():r AGEN:r
BNDCL                | MPX            | MPX            | MPX            | 0x0F 0x1A MPXMODE=1 MOD[mm] MOD=3 REG[rrr] RM[nnn]   f3_refining_prefix  mode64  | REG0=BND_R():r REG1=GPR64_B():r
BNDCL                | MPX            | MPX            | MPX            | 0x0F 0x1A MPXMODE=1 MOD[mm] MOD=3 REG[rrr] RM[nnn]   f3_refining_prefix  not64   | REG0=BND_R():r REG1=GPR32_B():r
BNDCN                | MPX            | MPX            | MPX            | 0x0F 0x1B MPXMODE=1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() f2_refining_prefix   | REG0=BND_R():r AGEN:r
BNDCN                | MPX            | MPX            | MPX            | 0x0F 0x1B MPXMODE=1 MOD[mm] MOD=3 REG[rrr] RM[nnn]  f2_refining_prefix  mode64   | REG0=BND_R():r REG1=GPR64_B():r
BNDCN                | MPX            | MPX            | MPX            | 0x0F 0x1B MPXMODE=1 MOD[mm] MOD=3 REG[rrr] RM[nnn]  f2_refining_prefix  not64    | REG0=BND_R():r REG1=GPR32_B():r
BNDCU                | MPX            | MPX            | MPX            | 0x0F 0x1A MPXMODE=1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()  f2_refining_prefix  | REG0=BND_R():r AGEN:r
BNDCU                | MPX            | MPX            | MPX            | 0x0F 0x1A MPXMODE=1 MOD[mm] MOD=3 REG[rrr] RM[nnn]   f2_refining_prefix  mode64  | REG0=BND_R():r REG1=GPR64_B():r
BNDCU                | MPX            | MPX            | MPX            | 0x0F 0x1A MPXMODE=1 MOD[mm] MOD=3 REG[rrr] RM[nnn]   f2_refining_prefix  not64   | REG0=BND_R():r REG1=GPR32_B():r
BNDLDX               | MPX            | MPX            | MPX            | 0x0F 0x1A MPXMODE=1 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  no_refining_prefix not64 eamode32 | REG0=BND_R():w MEM0:r:bnd32
BNDLDX               | MPX            | MPX            | MPX            | 0x0F 0x1A MPXMODE=1 MOD[mm] MOD=0 REG[rrr] RM[nnn]   MODRM()  no_refining_prefix mode64  # RM!=5 | REG0=BND_R():w MEM0:r:bnd64
BNDLDX               | MPX            | MPX            | MPX            | 0x0F 0x1A MPXMODE=1 MOD[mm] MOD=1 REG[rrr] RM[nnn]   MODRM()  no_refining_prefix mode64 | REG0=BND_R():w MEM0:r:bnd64
BNDLDX               | MPX            | MPX            | MPX            | 0x0F 0x1A MPXMODE=1 MOD[mm] MOD=2 REG[rrr] RM[nnn]   MODRM()  no_refining_prefix mode64 | REG0=BND_R():w MEM0:r:bnd64
BNDMK                | MPX            | MPX            | MPX            | 0x0F 0x1B MPXMODE=1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()  f3_refining_prefix  | REG0=BND_R():w  AGEN:r
BNDMOV               | MPX            | MPX            | MPX            | 0x0F 0x1A MPXMODE=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]  osz_refining_prefix REFINING66() | REG0=BND_R():w REG1=BND_B():r
BNDMOV               | MPX            | MPX            | MPX            | 0x0F 0x1A MPXMODE=1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()  osz_refining_prefix REFINING66() mode16 eamode32 | REG0=BND_R():w MEM0:r:q:u32
BNDMOV               | MPX            | MPX            | MPX            | 0x0F 0x1A MPXMODE=1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()  osz_refining_prefix REFINING66() mode32 eamode32 | REG0=BND_R():w MEM0:r:q:u32
BNDMOV               | MPX            | MPX            | MPX            | 0x0F 0x1A MPXMODE=1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()  osz_refining_prefix REFINING66() mode64 | REG0=BND_R():w MEM0:r:dq:u64
BNDMOV               | MPX            | MPX            | MPX            | 0x0F 0x1B MPXMODE=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] osz_refining_prefix REFINING66() | REG0=BND_B():w   REG1=BND_R():r
BNDMOV               | MPX            | MPX            | MPX            | 0x0F 0x1B MPXMODE=1 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  osz_refining_prefix REFINING66() mode16 eamode32 | MEM0:w:q:u32 REG0=BND_R():r
BNDMOV               | MPX            | MPX            | MPX            | 0x0F 0x1B MPXMODE=1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()  osz_refining_prefix REFINING66() mode32 eamode32 | MEM0:w:q:u32 REG0=BND_R():r
BNDMOV               | MPX            | MPX            | MPX            | 0x0F 0x1B MPXMODE=1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()  osz_refining_prefix REFINING66() mode64 | MEM0:w:dq:u64 REG0=BND_R():r
BNDSTX               | MPX            | MPX            | MPX            | 0x0F 0x1B MPXMODE=1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()  no_refining_prefix not64 eamode32 | MEM0:w:bnd32 REG0=BND_R():r
BNDSTX               | MPX            | MPX            | MPX            | 0x0F 0x1B MPXMODE=1 MOD[mm] MOD=0 REG[rrr] RM[nnn]  MODRM()  no_refining_prefix mode64 # RM!=5 | MEM0:w:bnd64 REG0=BND_R():r
BNDSTX               | MPX            | MPX            | MPX            | 0x0F 0x1B MPXMODE=1 MOD[mm] MOD=1 REG[rrr] RM[nnn]  MODRM()  no_refining_prefix mode64 | MEM0:w:bnd64 REG0=BND_R():r
BNDSTX               | MPX            | MPX            | MPX            | 0x0F 0x1B MPXMODE=1 MOD[mm] MOD=2 REG[rrr] RM[nnn]  MODRM()  no_refining_prefix mode64 | MEM0:w:bnd64 REG0=BND_R():r
BOUND                | INTERRUPT      | BASE           | I186           | 0x62 mode16 no66_prefix  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                 | REG0=GPRv_R():r MEM0:r:a16
BOUND                | INTERRUPT      | BASE           | I186           | 0x62 mode32 66_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                    | REG0=GPRv_R():r MEM0:r:a16
BOUND                | INTERRUPT      | BASE           | I186           | 0x62 mode16 66_prefix  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                   | REG0=GPRv_R():r MEM0:r:a32
BOUND                | INTERRUPT      | BASE           | I186           | 0x62 mode32 no66_prefix  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                 | REG0=GPRv_R():r MEM0:r:a32
BSF                  | BITBYTE        | BASE           | I386           | 0x0F 0xBC MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=GPRv_R():cw MEM0:r:v
BSF                  | BITBYTE        | BASE           | I386           | 0x0F 0xBC MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_R():cw REG1=GPRv_B():r
BSF                  | BITBYTE        | BASE           | I386           | 0x0F 0xBC not_refining_f3 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                | REG0=GPRv_R():cw MEM0:r:v
BSF                  | BITBYTE        | BASE           | I386           | 0x0F 0xBC not_refining_f3 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                       | REG0=GPRv_R():cw REG1=GPRv_B():r
BSF                  | BITBYTE        | BASE           | I386           | 0x0F 0xBC refining_f3 TZCNT=0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()            | REG0=GPRv_R():cw MEM0:r:v
BSF                  | BITBYTE        | BASE           | I386           | 0x0F 0xBC refining_f3 TZCNT=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                   | REG0=GPRv_R():cw REG1=GPRv_B():r
BSR                  | BITBYTE        | BASE           | I386           | 0x0F 0xBD  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                               | REG0=GPRv_R():cw MEM0:r:v
BSR                  | BITBYTE        | BASE           | I386           | 0x0F 0xBD  MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                      | REG0=GPRv_R():cw REG1=GPRv_B():r
BSR                  | BITBYTE        | BASE           | I386           | 0x0F 0xBD not_refining_f3 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                | REG0=GPRv_R():cw MEM0:r:v
BSR                  | BITBYTE        | BASE           | I386           | 0x0F 0xBD not_refining_f3 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                       | REG0=GPRv_R():cw REG1=GPRv_B():r
BSR                  | BITBYTE        | BASE           | I386           | 0x0F 0xBD  refining_f3 LZCNT=0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()           | REG0=GPRv_R():cw MEM0:r:v
BSR                  | BITBYTE        | BASE           | I386           | 0x0F 0xBD  refining_f3 LZCNT=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                  | REG0=GPRv_R():cw REG1=GPRv_B():r
BSWAP                | DATAXFER       | BASE           | I486REAL       | 0x0F 0b1100_1 SRM[rrr]                                                           | REG0=GPRv_SB():rw
BT                   | BITBYTE        | BASE           | I386           | 0x0F 0xBA MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM() UIMM8()                      | MEM0:r:v IMM0:r:b
BT                   | BITBYTE        | BASE           | I386           | 0x0F 0xBA MOD[0b11] MOD=3 REG[0b100] RM[nnn] UIMM8()                             | REG0=GPRv_B():r IMM0:r:b
BT                   | BITBYTE        | BASE           | I386           | 0x0F 0xA3 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:r:v REG0=GPRv_R():r
BT                   | BITBYTE        | BASE           | I386           | 0x0F 0xA3 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_B():r REG1=GPRv_R():r
BTC                  | BITBYTE        | BASE           | I386           | 0x0F 0xBA MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM() UIMM8() nolock_prefix        | MEM0:rw:v IMM0:r:b
BTC                  | BITBYTE        | BASE           | I386           | 0x0F 0xBA MOD[0b11] MOD=3 REG[0b111] RM[nnn] UIMM8()                             | REG0=GPRv_B():rw IMM0:r:b
BTC                  | BITBYTE        | BASE           | I386           | 0x0F 0xBB MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                  | MEM0:rw:v REG0=GPRv_R():r
BTC                  | BITBYTE        | BASE           | I386           | 0x0F 0xBB MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_B():rw REG1=GPRv_R():r
BTC_LOCK             | BITBYTE        | BASE           | I386           | 0x0F 0xBA MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM() UIMM8() lock_prefix          | MEM0:rw:v IMM0:r:b
BTC_LOCK             | BITBYTE        | BASE           | I386           | 0x0F 0xBB MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                    | MEM0:rw:v REG0=GPRv_R():r
BTR                  | BITBYTE        | BASE           | I386           | 0x0F 0xBA MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM() UIMM8() nolock_prefix        | MEM0:rw:v IMM0:r:b
BTR                  | BITBYTE        | BASE           | I386           | 0x0F 0xBA MOD[0b11] MOD=3 REG[0b110] RM[nnn] UIMM8()                             | REG0=GPRv_B():rw IMM0:r:b
BTR                  | BITBYTE        | BASE           | I386           | 0x0F 0xB3 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                  | MEM0:rw:v REG0=GPRv_R():r
BTR                  | BITBYTE        | BASE           | I386           | 0x0F 0xB3 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_B():rw REG1=GPRv_R():r
BTR_LOCK             | BITBYTE        | BASE           | I386           | 0x0F 0xBA MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM() UIMM8() lock_prefix          | MEM0:rw:v IMM0:r:b
BTR_LOCK             | BITBYTE        | BASE           | I386           | 0x0F 0xB3 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                    | MEM0:rw:v REG0=GPRv_R():r
BTS                  | BITBYTE        | BASE           | I386           | 0x0F 0xBA MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM() UIMM8() nolock_prefix        | MEM0:rw:v IMM0:r:b
BTS                  | BITBYTE        | BASE           | I386           | 0x0F 0xBA MOD[0b11] MOD=3 REG[0b101] RM[nnn] UIMM8()                             | REG0=GPRv_B():rw IMM0:r:b
BTS                  | BITBYTE        | BASE           | I386           | 0x0F 0xAB MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                  | MEM0:rw:v REG0=GPRv_R():r
BTS                  | BITBYTE        | BASE           | I386           | 0x0F 0xAB MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_B():rw REG1=GPRv_R():r
BTS_LOCK             | BITBYTE        | BASE           | I386           | 0x0F 0xBA MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM() UIMM8() lock_prefix          | MEM0:rw:v IMM0:r:b
BTS_LOCK             | BITBYTE        | BASE           | I386           | 0x0F 0xAB MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                    | MEM0:rw:v REG0=GPRv_R():r
CALL_FAR             | CALL           | BASE           | I86            | 0xFF MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM()                                   | MEM0:r:p2 REG0=XED_REG_STACKPUSH:w:spw2:SUPP REG1=rIP():w:SUPP
CALL_FAR             | CALL           | BASE           | I86            | 0x9A not64 BRDISPz() UIMM16()                                                    | PTR:r:p IMM0:r:w REG0=XED_REG_STACKPUSH:w:spw2:SUPP REG1=XED_REG_EIP:w:SUPP
CALL_NEAR            | CALL           | BASE           | I86            | 0xFF MOD[mm] MOD!=3 REG[0b010] RM[nnn]  DF64() IMMUNE66_LOOP64() MODRM()         | MEM0:r:v REG0=XED_REG_STACKPUSH:w:spw:SUPP REG1=rIP():rw:SUPP
CALL_NEAR            | CALL           | BASE           | I86            | 0xFF MOD[0b11] MOD=3 REG[0b010] RM[nnn]  DF64() IMMUNE66_LOOP64()                | REG0=GPRv_B():r REG1=XED_REG_STACKPUSH:w:spw:SUPP REG2=rIP():rw:SUPP
CALL_NEAR            | CALL           | BASE           | I86            | 0xE8 not64 BRDISPz()                                                             | RELBR:r:z REG0=XED_REG_STACKPUSH:w:spw:SUPP REG1=XED_REG_EIP:rw:SUPP
CALL_NEAR            | CALL           | BASE           | I86            | 0xE8 mode64  BRDISP32() DF64() FORCE64()                                         | RELBR:r:d REG0=XED_REG_STACKPUSH:w:spw:SUPP REG1=XED_REG_RIP:rw:SUPP
CBW                  | CONVERT        | BASE           | I86            | 0x98 mode16 no66_prefix                                                          | REG0=XED_REG_AX:w:SUPP REG1=XED_REG_AL:r:SUPP
CBW                  | CONVERT        | BASE           | I86            | 0x98 mode32 66_prefix                                                            | REG0=XED_REG_AX:w:SUPP REG1=XED_REG_AL:r:SUPP
CBW                  | CONVERT        | BASE           | I86            | 0x98 mode64 norexw_prefix 66_prefix                                              | REG0=XED_REG_AX:w:SUPP REG1=XED_REG_AL:r:SUPP
CDQ                  | CONVERT        | BASE           | I386           | 0x99 mode16 66_prefix                                                            | REG0=XED_REG_EDX:w:SUPP REG1=XED_REG_EAX:r:SUPP
CDQ                  | CONVERT        | BASE           | I386           | 0x99 mode32 no66_prefix                                                          | REG0=XED_REG_EDX:w:SUPP REG1=XED_REG_EAX:r:SUPP
CDQ                  | CONVERT        | BASE           | I386           | 0x99 mode64 norexw_prefix no66_prefix                                            | REG0=XED_REG_EDX:w:SUPP REG1=XED_REG_EAX:r:SUPP
CLC                  | FLAGOP         | BASE           | I86            | 0xF8                                                                             | 
CLD                  | FLAGOP         | BASE           | I86            | 0xFC                                                                             | 
CLDEMOTE             | CLDEMOTE       | CLDEMOTE       | CLDEMOTE       | 0x0F 0x1C MOD[mm] MOD!=3 REG[0b000] RM[nnn]  MODRM()  no_refining_prefix     CLDEMOTE=1 | MEM0:r:b:u8
CLEVICT0             | PREFETCH       | KNC            | KNCV           | VV1 0xAE  VL128 VF2 V0F  NOVSR MOD[mm] MOD!=3 REG[0b111] RM[nnn]  MODRM()        | MEM0:r:mprefetch  #FIXME: rw or r???
CLEVICT0_EVEX        | PREFETCH       | KNCE           | KNCE           | KVV 0xAE  VL512 VF2 V0F  NOEVSR MOD[mm] MOD!=3 REG[0b111] RM[nnn]  MODRM() NOSWIZD() | MEM0:r:mprefetch
CLEVICT1             | PREFETCH       | KNC            | KNCV           | VV1 0xAE  VL128 VF3 V0F  NOVSR MOD[mm] MOD!=3 REG[0b111] RM[nnn]  MODRM()        | MEM0:r:mprefetch  #FIXME: rw or r???
CLEVICT1_EVEX        | PREFETCH       | KNCE           | KNCE           | KVV 0xAE  VL512 VF3 V0F  NOEVSR MOD[mm] MOD!=3 REG[0b111] RM[nnn]  MODRM() NOSWIZD() | MEM0:r:mprefetch
CLFLUSH              | MISC           | CLFSH          | CLFSH          | 0x0F 0xAE  MOD[mm] MOD!=3 REG[0b111] RM[nnn]  no_refining_prefix MODRM()         | MEM0:r:mprefetch
CLFLUSHOPT           | CLFLUSHOPT     | CLFLUSHOPT     | CLFLUSHOPT     | 0x0F 0xAE  MOD[mm] MOD!=3 REG[0b111] RM[nnn]  osz_refining_prefix REFINING66() MODRM() | MEM0:r:mprefetch
CLI                  | FLAGOP         | BASE           | I86            | 0xFA                                                                             | 
CLRSSBSY             | CET            | CET            | CET            | 0x0F 0xAE MOD[mm] MOD!=3 REG[0b110] RM[nnn]  f3_refining_prefix     MODRM()      | MEM0:rw:q:u64
CLTS                 | SYSTEM         | BASE           | I286REAL       | 0x0F 0x06                                                                        | 
CLWB                 | CLWB           | CLWB           | CLWB           | 0x0F 0xAE  MOD[mm] MOD!=3 REG[0b110] RM[nnn]  osz_refining_prefix REFINING66() MODRM() | MEM0:r:mprefetch
CMC                  | FLAGOP         | BASE           | I86            | 0xF5                                                                             | 
CMOVB                | CMOV           | BASE           | CMOV           | 0x0F 0x42 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=GPRv_R():cw MEM0:r:v
CMOVB                | CMOV           | BASE           | CMOV           | 0x0F 0x42 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_R():cw REG1=GPRv_B():r
CMOVBE               | CMOV           | BASE           | CMOV           | 0x0F 0x46 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=GPRv_R():cw MEM0:r:v
CMOVBE               | CMOV           | BASE           | CMOV           | 0x0F 0x46 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_R():cw REG1=GPRv_B():r
CMOVL                | CMOV           | BASE           | CMOV           | 0x0F 0x4C MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=GPRv_R():cw MEM0:r:v
CMOVL                | CMOV           | BASE           | CMOV           | 0x0F 0x4C MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_R():cw REG1=GPRv_B():r
CMOVLE               | CMOV           | BASE           | CMOV           | 0x0F 0x4E MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=GPRv_R():cw MEM0:r:v
CMOVLE               | CMOV           | BASE           | CMOV           | 0x0F 0x4E MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_R():cw REG1=GPRv_B():r
CMOVNB               | CMOV           | BASE           | CMOV           | 0x0F 0x43 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=GPRv_R():cw MEM0:r:v
CMOVNB               | CMOV           | BASE           | CMOV           | 0x0F 0x43 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_R():cw REG1=GPRv_B():r
CMOVNBE              | CMOV           | BASE           | CMOV           | 0x0F 0x47 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=GPRv_R():cw MEM0:r:v
CMOVNBE              | CMOV           | BASE           | CMOV           | 0x0F 0x47 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_R():cw REG1=GPRv_B():r
CMOVNL               | CMOV           | BASE           | CMOV           | 0x0F 0x4D MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=GPRv_R():cw MEM0:r:v
CMOVNL               | CMOV           | BASE           | CMOV           | 0x0F 0x4D MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_R():cw REG1=GPRv_B():r
CMOVNLE              | CMOV           | BASE           | CMOV           | 0x0F 0x4F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=GPRv_R():cw MEM0:r:v
CMOVNLE              | CMOV           | BASE           | CMOV           | 0x0F 0x4F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_R():cw REG1=GPRv_B():r
CMOVNO               | CMOV           | BASE           | CMOV           | 0x0F 0x41 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=GPRv_R():cw MEM0:r:v
CMOVNO               | CMOV           | BASE           | CMOV           | 0x0F 0x41 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_R():cw REG1=GPRv_B():r
CMOVNP               | CMOV           | BASE           | CMOV           | 0x0F 0x4B MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=GPRv_R():cw MEM0:r:v
CMOVNP               | CMOV           | BASE           | CMOV           | 0x0F 0x4B MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_R():cw REG1=GPRv_B():r
CMOVNS               | CMOV           | BASE           | CMOV           | 0x0F 0x49 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=GPRv_R():cw MEM0:r:v
CMOVNS               | CMOV           | BASE           | CMOV           | 0x0F 0x49 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_R():cw REG1=GPRv_B():r
CMOVNZ               | CMOV           | BASE           | CMOV           | 0x0F 0x45 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=GPRv_R():cw MEM0:r:v
CMOVNZ               | CMOV           | BASE           | CMOV           | 0x0F 0x45 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_R():cw REG1=GPRv_B():r
CMOVO                | CMOV           | BASE           | CMOV           | 0x0F 0x40 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=GPRv_R():cw MEM0:r:v
CMOVO                | CMOV           | BASE           | CMOV           | 0x0F 0x40 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_R():cw REG1=GPRv_B():r
CMOVP                | CMOV           | BASE           | CMOV           | 0x0F 0x4A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=GPRv_R():cw MEM0:r:v
CMOVP                | CMOV           | BASE           | CMOV           | 0x0F 0x4A MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_R():cw REG1=GPRv_B():r
CMOVS                | CMOV           | BASE           | CMOV           | 0x0F 0x48 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=GPRv_R():cw MEM0:r:v
CMOVS                | CMOV           | BASE           | CMOV           | 0x0F 0x48 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_R():cw REG1=GPRv_B():r
CMOVZ                | CMOV           | BASE           | CMOV           | 0x0F 0x44 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=GPRv_R():cw MEM0:r:v
CMOVZ                | CMOV           | BASE           | CMOV           | 0x0F 0x44 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_R():cw REG1=GPRv_B():r
CMP                  | BINARY         | BASE           | I86            | 0x80 MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM() SIMM8()                           | MEM0:r:b IMM0:r:b:i8
CMP                  | BINARY         | BASE           | I86            | 0x80 MOD[0b11] MOD=3 REG[0b111] RM[nnn] SIMM8()                                  | REG0=GPR8_B():r IMM0:r:b:i8
CMP                  | BINARY         | BASE           | I86            | 0x81 MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM() SIMMz()                           | MEM0:r:v IMM0:r:z
CMP                  | BINARY         | BASE           | I86            | 0x81 MOD[0b11] MOD=3 REG[0b111] RM[nnn] SIMMz()                                  | REG0=GPRv_B():r IMM0:r:z
CMP                  | BINARY         | BASE           | I86            | 0x82 MOD[mm] MOD!=3 REG[0b111] RM[nnn] not64 MODRM() SIMM8()                     | MEM0:r:b IMM0:r:b:i8
CMP                  | BINARY         | BASE           | I86            | 0x82 MOD[0b11] MOD=3 REG[0b111] RM[nnn] not64 SIMM8()                            | REG0=GPR8_B():r IMM0:r:b:i8
CMP                  | BINARY         | BASE           | I86            | 0x83 MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM() SIMM8()                           | MEM0:r:v IMM0:r:b:i8
CMP                  | BINARY         | BASE           | I86            | 0x83 MOD[0b11] MOD=3 REG[0b111] RM[nnn] SIMM8()                                  | REG0=GPRv_B():r IMM0:r:b:i8
CMP                  | BINARY         | BASE           | I86            | 0x38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | MEM0:r:b REG0=GPR8_R():r
CMP                  | BINARY         | BASE           | I86            | 0x38 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPR8_B():r REG1=GPR8_R():r
CMP                  | BINARY         | BASE           | I86            | 0x39 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | MEM0:r:v REG0=GPRv_R():r
CMP                  | BINARY         | BASE           | I86            | 0x39 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPRv_B():r REG1=GPRv_R():r
CMP                  | BINARY         | BASE           | I86            | 0x3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=GPR8_R():r MEM0:r:b
CMP                  | BINARY         | BASE           | I86            | 0x3A MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPR8_R():r REG1=GPR8_B():r
CMP                  | BINARY         | BASE           | I86            | 0x3B MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=GPRv_R():r MEM0:r:v
CMP                  | BINARY         | BASE           | I86            | 0x3B MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPRv_R():r REG1=GPRv_B():r
CMP                  | BINARY         | BASE           | I86            | 0x3C SIMM8()                                                                     | REG0=XED_REG_AL:r:IMPL IMM0:r:b:i8
CMP                  | BINARY         | BASE           | I86            | 0x3D SIMMz()                                                                     | REG0=OrAX():r:IMPL IMM0:r:z
CMPSB                | STRINGOP       | BASE           | I86            | 0xA6 norep OVERRIDE_SEG0()                                                       | MEM0:r:SUPP:b BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:r:SUPP:b BASE1=ArDI():rw:SUPP SEG1=FINAL_ESEG1():r:SUPP
CMPSD                | STRINGOP       | BASE           | I386           | 0xA7 mode16 66_prefix  norep OVERRIDE_SEG0()                                     | MEM0:r:SUPP:d BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:r:SUPP:d BASE1=ArDI():rw:SUPP SEG1=FINAL_ESEG1():r:SUPP
CMPSD                | STRINGOP       | BASE           | I386           | 0xA7 mode32 no66_prefix  norep OVERRIDE_SEG0()                                   | MEM0:r:SUPP:d BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:r:SUPP:d BASE1=ArDI():rw:SUPP SEG1=FINAL_ESEG1():r:SUPP
CMPSD                | STRINGOP       | BASE           | I386           | 0xA7 mode64 norexw_prefix no66_prefix norep OVERRIDE_SEG0()                      | MEM0:r:SUPP:d BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:r:SUPP:d BASE1=ArDI():rw:SUPP SEG1=FINAL_ESEG1():r:SUPP
CMPSW                | STRINGOP       | BASE           | I86            | 0xA7 mode16 no66_prefix   norep OVERRIDE_SEG0()                                  | MEM0:r:SUPP:w BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:r:SUPP:w BASE1=ArDI():rw:SUPP SEG1=FINAL_ESEG1():r:SUPP
CMPSW                | STRINGOP       | BASE           | I86            | 0xA7 mode32 66_prefix  norep OVERRIDE_SEG0()                                     | MEM0:r:SUPP:w BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:r:SUPP:w BASE1=ArDI():rw:SUPP SEG1=FINAL_ESEG1():r:SUPP
CMPSW                | STRINGOP       | BASE           | I86            | 0xA7 mode64 norexw_prefix 66_prefix  norep OVERRIDE_SEG0()                       | MEM0:r:SUPP:w BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:r:SUPP:w BASE1=ArDI():rw:SUPP SEG1=FINAL_ESEG1():r:SUPP
CMPXCHG              | SEMAPHORE      | BASE           | I486REAL       | 0x0F 0xB0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                  | MEM0:rcw:b REG0=GPR8_R():r REG1=XED_REG_AL:rcw:SUPP
CMPXCHG              | SEMAPHORE      | BASE           | I486REAL       | 0x0F 0xB0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPR8_B():rcw REG1=GPR8_R():r REG2=XED_REG_AL:rcw:SUPP
CMPXCHG              | SEMAPHORE      | BASE           | I486REAL       | 0x0F 0xB1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                  | MEM0:rcw:v REG0=GPRv_R():r REG1=OrAX():rcw:SUPP
CMPXCHG              | SEMAPHORE      | BASE           | I486REAL       | 0x0F 0xB1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_B():rcw REG1=GPRv_R():r REG2=OrAX():rcw:SUPP
CMPXCHG_LOCK         | SEMAPHORE      | BASE           | I486REAL       | 0x0F 0xB0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                    | MEM0:rcw:b REG0=GPR8_R():r REG1=XED_REG_AL:rcw:SUPP
CMPXCHG_LOCK         | SEMAPHORE      | BASE           | I486REAL       | 0x0F 0xB1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                    | MEM0:rcw:v REG0=GPRv_R():r REG1=OrAX():rcw:SUPP
CMPXCHG16B           | SEMAPHORE      | LONGMODE       | CMPXCHG16B     | 0x0F 0xC7 MOD[mm] MOD!=3 REG[0b001] RM[nnn] mode64  rexw_prefix IMMUNE66() MODRM() nolock_prefix | MEM0:rcw:dq REG0=XED_REG_RDX:rcw:SUPP REG1=XED_REG_RAX:rcw:SUPP REG2=XED_REG_RCX:r:SUPP REG3=XED_REG_RBX:r:SUPP
CMPXCHG16B_LOCK      | SEMAPHORE      | LONGMODE       | CMPXCHG16B     | 0x0F 0xC7 MOD[mm] MOD!=3 REG[0b001] RM[nnn] mode64  rexw_prefix IMMUNE66() MODRM() lock_prefix | MEM0:rcw:dq REG0=XED_REG_RDX:rcw:SUPP REG1=XED_REG_RAX:rcw:SUPP REG2=XED_REG_RCX:r:SUPP REG3=XED_REG_RBX:r:SUPP
CMPXCHG8B            | SEMAPHORE      | BASE           | PENTIUMREAL    | 0x0F 0xC7 MOD[mm] MOD!=3 REG[0b001] RM[nnn] not64 IMMUNE66() MODRM() nolock_prefix | MEM0:rcw:q REG0=XED_REG_EDX:rcw:SUPP REG1=XED_REG_EAX:rcw:SUPP REG2=XED_REG_ECX:r:SUPP REG3=XED_REG_EBX:r:SUPP
CMPXCHG8B            | SEMAPHORE      | BASE           | PENTIUMREAL    | 0x0F 0xC7 MOD[mm] MOD!=3 REG[0b001] RM[nnn] mode64 norexw_prefix IMMUNE66() MODRM() nolock_prefix | MEM0:rcw:q REG0=XED_REG_EDX:rcw:SUPP REG1=XED_REG_EAX:rcw:SUPP REG2=XED_REG_ECX:r:SUPP REG3=XED_REG_EBX:r:SUPP
CMPXCHG8B_LOCK       | SEMAPHORE      | BASE           | PENTIUMREAL    | 0x0F 0xC7 MOD[mm] MOD!=3 REG[0b001] RM[nnn] not64 IMMUNE66() MODRM() lock_prefix | MEM0:rcw:q REG0=XED_REG_EDX:rcw:SUPP REG1=XED_REG_EAX:rcw:SUPP REG2=XED_REG_ECX:r:SUPP REG3=XED_REG_EBX:r:SUPP
CMPXCHG8B_LOCK       | SEMAPHORE      | BASE           | PENTIUMREAL    | 0x0F 0xC7 MOD[mm] MOD!=3 REG[0b001] RM[nnn] mode64 norexw_prefix IMMUNE66() MODRM() lock_prefix | MEM0:rcw:q REG0=XED_REG_EDX:rcw:SUPP REG1=XED_REG_EAX:rcw:SUPP REG2=XED_REG_ECX:r:SUPP REG3=XED_REG_EBX:r:SUPP
CPUID                | MISC           | BASE           | I486REAL       | 0x0F 0xA2                                                                        | REG0=XED_REG_EAX:rw:SUPP REG1=XED_REG_EBX:w:SUPP REG2=XED_REG_ECX:crw:SUPP REG3=XED_REG_EDX:w:SUPP
CRC32                | SSE            | SSE4           | SSE42          | 0x0F 0x38 0xF0  f2_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]    MODRM()    | REG0=GPRy_R():rw:y     MEM0:r:b
CRC32                | SSE            | SSE4           | SSE42          | 0x0F 0x38 0xF0  f2_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]              | REG0=GPRy_R():rw:y     REG1=GPR8_B():r:b
CRC32                | SSE            | SSE4           | SSE42          | 0x0F 0x38 0xF1  f2_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]   MODRM()     | REG0=GPRy_R():rw:y     MEM0:r:v
CRC32                | SSE            | SSE4           | SSE42          | 0x0F 0x38 0xF1  f2_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]              | REG0=GPRy_R():rw:y     REG1=GPRv_B():r:v
CWD                  | CONVERT        | BASE           | I86            | 0x99 mode16  no66_prefix                                                         | REG0=XED_REG_DX:w:SUPP REG1=XED_REG_AX:r:SUPP
CWD                  | CONVERT        | BASE           | I86            | 0x99 mode32 66_prefix                                                            | REG0=XED_REG_DX:w:SUPP REG1=XED_REG_AX:r:SUPP
CWD                  | CONVERT        | BASE           | I86            | 0x99 mode64 norexw_prefix 66_prefix                                              | REG0=XED_REG_DX:w:SUPP REG1=XED_REG_AX:r:SUPP
CWDE                 | CONVERT        | BASE           | I386           | 0x98 mode16 66_prefix                                                            | REG0=XED_REG_EAX:w:SUPP REG1=XED_REG_AX:r:SUPP
CWDE                 | CONVERT        | BASE           | I386           | 0x98 mode32 no66_prefix                                                          | REG0=XED_REG_EAX:w:SUPP REG1=XED_REG_AX:r:SUPP
CWDE                 | CONVERT        | BASE           | I386           | 0x98 mode64 norexw_prefix no66_prefix                                            | REG0=XED_REG_EAX:w:SUPP REG1=XED_REG_AX:r:SUPP
DAA                  | DECIMAL        | BASE           | I86            | 0x27 not64                                                                       | REG0=XED_REG_AL:rw:SUPP
DAS                  | DECIMAL        | BASE           | I86            | 0x2F not64                                                                       | REG0=XED_REG_AL:rw:SUPP
DEC                  | BINARY         | BASE           | I86            | 0xFE MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM() nolock_prefix                     | MEM0:rw:b
DEC                  | BINARY         | BASE           | I86            | 0xFE MOD[0b11] MOD=3 REG[0b001] RM[nnn]                                          | REG0=GPR8_B():rw
DEC                  | BINARY         | BASE           | I86            | 0xFF MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM() nolock_prefix                     | MEM0:rw:v
DEC                  | BINARY         | BASE           | I86            | 0xFF MOD[0b11] MOD=3 REG[0b001] RM[nnn]                                          | REG0=GPRv_B():rw
DEC                  | BINARY         | BASE           | I86            | 0b0100_1 SRM[rrr] not64                                                          | REG0=GPRv_SB():rw
DEC_LOCK             | BINARY         | BASE           | I86            | 0xFE MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM() lock_prefix                       | MEM0:rw:b
DEC_LOCK             | BINARY         | BASE           | I86            | 0xFF MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM() lock_prefix                       | MEM0:rw:v
DELAY                | KNCSCALAR      | KNC            | KNCV           | VV1 0xAE  VL128 VF3 V0F NOVSR MOD[0b11] MOD=3 REG[0b110] RM[nnn]  W0             | REG0=GPR32_B():r:d
DELAY                | KNCSCALAR      | KNC            | KNCV           | VV1 0xAE  VL128 VF3 V0F NOVSR MOD[0b11] MOD=3 REG[0b110] RM[nnn]  W1             | REG0=GPR64_B():r:q
DIV                  | BINARY         | BASE           | I86            | 0xF6 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM()                                   | MEM0:r:b REG0=XED_REG_AX:rw:SUPP
DIV                  | BINARY         | BASE           | I86            | 0xF6 MOD[0b11] MOD=3 REG[0b110] RM[nnn]                                          | REG0=GPR8_B():r REG1=XED_REG_AX:rw:SUPP
DIV                  | BINARY         | BASE           | I86            | 0xF7 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM()                                   | MEM0:r:v REG0=OrAX():rw:SUPP REG1=OrDX():rw:SUPP
DIV                  | BINARY         | BASE           | I86            | 0xF7 MOD[0b11] MOD=3 REG[0b110] RM[nnn]                                          | REG0=GPRv_B():r REG1=OrAX():rw:SUPP REG2=OrDX():rw:SUPP
EMMS                 | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x77 no_refining_prefix                                                     | 
ENCLS                | SGX            | SGX            | SGX            | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b001] RM[0b111] no_refining_prefix                | REG0=XED_REG_EAX:r:SUPP    \
ENCLU                | SGX            | SGX            | SGX            | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b010] RM[0b111] no_refining_prefix                | REG0=XED_REG_EAX:r:SUPP    \
ENCLV                | SGX            | SGX_ENCLV      | SGX_ENCLV      | 0x0F 0x01 MOD[0b11] MOD=3  REG[0b000] RM[0b000]  no_refining_prefix              | REG0=XED_REG_EAX:r:SUPP:d:u32 REG1=XED_REG_RBX:crw:SUPP:q:u64 REG2=XED_REG_RCX:crw:SUPP:q:u64 REG3=XED_REG_RDX:crw:SUPP:q:u64
ENDBR32              | CET            | CET            | CET            | 0x0F 0x1E MOD[0b11] MOD=3  REG[0b111] RM[0b011]  f3_refining_prefix     CET=1    | 
ENDBR64              | CET            | CET            | CET            | 0x0F 0x1E MOD[0b11] MOD=3  REG[0b111] RM[0b010]  f3_refining_prefix     CET=1    | 
ENQCMD               | ENQCMD         | ENQCMD         | ENQCMD         | 0x0F 0x38 0xF8 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  f2_refining_prefix      | REG0=A_GPR_R():r MEM0:r:zd:u32
ENQCMDS              | ENQCMD         | ENQCMD         | ENQCMD         | 0x0F 0x38 0xF8 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  f3_refining_prefix      | REG0=A_GPR_R():r MEM0:r:zd:u32
ENTER                | MISC           | BASE           | I186           | 0xC8 DF64() UIMM16() UIMM8_1()                                                   | IMM0:r:w IMM1:r:b REG0=XED_REG_STACKPUSH:w:spw:SUPP REG1=OrBP():rw:SUPP
FCMOVB               | FCMOV          | X87            | FCMOV          | 0xDA MOD[0b11] MOD=3 REG[0b000] RM[nnn]                                          | REG0=XED_REG_ST0:cw:IMPL:f80 REG1=X87():r:f80 REG2=XED_REG_X87STATUS:w:SUPP
FCMOVBE              | FCMOV          | X87            | FCMOV          | 0xDA MOD[0b11] MOD=3 REG[0b010] RM[nnn]                                          | REG0=XED_REG_ST0:cw:IMPL:f80 REG1=X87():r:f80 REG2=XED_REG_X87STATUS:w:SUPP
FCMOVE               | FCMOV          | X87            | FCMOV          | 0xDA MOD[0b11] MOD=3 REG[0b001] RM[nnn]                                          | REG0=XED_REG_ST0:cw:IMPL:f80 REG1=X87():r:f80 REG2=XED_REG_X87STATUS:w:SUPP
FCMOVNB              | FCMOV          | X87            | FCMOV          | 0xDB MOD[0b11] MOD=3 REG[0b000] RM[nnn]                                          | REG0=XED_REG_ST0:cw:IMPL:f80 REG1=X87():r:f80 REG2=XED_REG_X87STATUS:w:SUPP
FCMOVNBE             | FCMOV          | X87            | FCMOV          | 0xDB MOD[0b11] MOD=3 REG[0b010] RM[nnn]                                          | REG0=XED_REG_ST0:cw:IMPL:f80 REG1=X87():r:f80 REG2=XED_REG_X87STATUS:w:SUPP
FCMOVNE              | FCMOV          | X87            | FCMOV          | 0xDB MOD[0b11] MOD=3 REG[0b001] RM[nnn]                                          | REG0=XED_REG_ST0:cw:IMPL:f80 REG1=X87():r:f80 REG2=XED_REG_X87STATUS:w:SUPP
FCMOVNU              | FCMOV          | X87            | FCMOV          | 0xDB MOD[0b11] MOD=3 REG[0b011] RM[nnn]                                          | REG0=XED_REG_ST0:cw:IMPL:f80 REG1=X87():r:f80 REG2=XED_REG_X87STATUS:w:SUPP
FCMOVU               | FCMOV          | X87            | FCMOV          | 0xDA MOD[0b11] MOD=3 REG[0b011] RM[nnn]                                          | REG0=XED_REG_ST0:cw:IMPL:f80 REG1=X87():r:f80 REG2=XED_REG_X87STATUS:w:SUPP
FCOMI                | X87_ALU        | X87            | PPRO           | 0xDB MOD[0b11] MOD=3 REG[0b110] RM[nnn]                                          | REG0=XED_REG_ST0:r:IMPL:f80 REG1=X87():r:f80  REG2=XED_REG_X87STATUS:w:SUPP
FCOMIP               | X87_ALU        | X87            | PPRO           | 0xDF MOD[0b11] MOD=3 REG[0b110] RM[nnn]                                          | REG0=XED_REG_ST0:r:IMPL:f80 REG1=X87():r:f80  REG2=XED_REG_X87POP:r:SUPP REG3=XED_REG_X87STATUS:w:SUPP
FISTTP               | X87_ALU        | SSE3           | SSE3X87        | 0xDD MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM()                                   | MEM0:w:m64int REG0=XED_REG_ST0:r:IMPL:f80 REG1=XED_REG_X87POP:r:SUPP REG2=XED_REG_X87STATUS:w:SUPP
FISTTP               | X87_ALU        | SSE3           | SSE3X87        | 0xDF MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM()                                   | MEM0:w:mem16int REG0=XED_REG_ST0:r:IMPL:f80 REG1=XED_REG_X87POP:r:SUPP REG2=XED_REG_X87STATUS:w:SUPP
FUCOMI               | X87_ALU        | X87            | PPRO           | 0xDB MOD[0b11] MOD=3 REG[0b101] RM[nnn]                                          | REG0=XED_REG_ST0:r:IMPL:f80 REG1=X87():r:f80   REG2=XED_REG_X87STATUS:w:SUPP
FUCOMIP              | X87_ALU        | X87            | PPRO           | 0xDF MOD[0b11] MOD=3 REG[0b101] RM[nnn]                                          | REG0=XED_REG_ST0:r:IMPL:f80 REG1=X87():r:f80   REG2=XED_REG_X87POP:r:SUPP REG3=XED_REG_X87STATUS:w:SUPP
FXRSTOR              | SSE            | SSE            | FXSAVE         | 0x0F 0xAE MOD[mm] MOD!=3 REG[0b001] RM[nnn]   no_refining_prefix norexw_prefix MODRM() | MEM0:r:mfpxenv REG0=XED_REG_X87CONTROL:w:SUPP
FXRSTOR64            | SSE            | SSE            | FXSAVE64       | 0x0F 0xAE MOD[mm] MOD!=3 REG[0b001] RM[nnn]   no_refining_prefix rexw_prefix MODRM() | MEM0:r:mfpxenv REG0=XED_REG_X87CONTROL:w:SUPP
FXSAVE               | SSE            | SSE            | FXSAVE         | 0x0F 0xAE  MOD[mm] MOD!=3 REG[0b000] RM[nnn]  no_refining_prefix norexw_prefix MODRM() | MEM0:w:mfpxenv REG0=XED_REG_X87CONTROL:r:SUPP
FXSAVE64             | SSE            | SSE            | FXSAVE64       | 0x0F 0xAE  MOD[mm] MOD!=3 REG[0b000] RM[nnn]  no_refining_prefix rexw_prefix MODRM() | MEM0:w:mfpxenv REG0=XED_REG_X87CONTROL:r:SUPP
GF2P8AFFINEINVQB     | GFNI           | GFNI           | GFNI           | 0x0F 0x3A 0xCF MOD[0b11] MOD=3  REG[rrr] RM[nnn]  osz_refining_prefix     UIMM8() | REG0=XMM_R():rw:dq:u8 REG1=XMM_B():r:dq:u64 IMM0:r:b
GF2P8AFFINEINVQB     | GFNI           | GFNI           | GFNI           | 0x0F 0x3A 0xCF MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  osz_refining_prefix     UIMM8() | REG0=XMM_R():rw:dq:u8 MEM0:r:dq:u64 IMM0:r:b
GF2P8AFFINEQB        | GFNI           | GFNI           | GFNI           | 0x0F 0x3A 0xCE MOD[0b11] MOD=3  REG[rrr] RM[nnn]  osz_refining_prefix     UIMM8() | REG0=XMM_R():rw:dq:u8 REG1=XMM_B():r:dq:u64 IMM0:r:b
GF2P8AFFINEQB        | GFNI           | GFNI           | GFNI           | 0x0F 0x3A 0xCE MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  osz_refining_prefix     UIMM8() | REG0=XMM_R():rw:dq:u8 MEM0:r:dq:u64 IMM0:r:b
GF2P8MULB            | GFNI           | GFNI           | GFNI           | 0x0F 0x38 0xCF MOD[0b11] MOD=3  REG[rrr] RM[nnn]  osz_refining_prefix            | REG0=XMM_R():rw:dq:u8 REG1=XMM_B():r:dq:u8
GF2P8MULB            | GFNI           | GFNI           | GFNI           | 0x0F 0x38 0xCF MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  osz_refining_prefix     | REG0=XMM_R():rw:dq:u8 MEM0:r:dq:u8
HLT                  | SYSTEM         | BASE           | I86            | 0xF4                                                                             | 
IDIV                 | BINARY         | BASE           | I86            | 0xF6 MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM()                                   | MEM0:r:b REG0=XED_REG_AX:rw:SUPP
IDIV                 | BINARY         | BASE           | I86            | 0xF6 MOD[0b11] MOD=3 REG[0b111] RM[nnn]                                          | REG0=GPR8_B():r REG1=XED_REG_AX:rw:SUPP
IDIV                 | BINARY         | BASE           | I86            | 0xF7 MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM()                                   | MEM0:r:v REG0=OrAX():rw:SUPP REG1=OrDX():rw:SUPP
IDIV                 | BINARY         | BASE           | I86            | 0xF7 MOD[0b11] MOD=3 REG[0b111] RM[nnn]                                          | REG0=GPRv_B():r REG1=OrAX():rw:SUPP REG2=OrDX():rw:SUPP
IMUL                 | BINARY         | BASE           | I186           | 0x69 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SIMMz()                             | REG0=GPRv_R():w MEM0:r:v IMM0:r:z
IMUL                 | BINARY         | BASE           | I186           | 0x69 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SIMMz()                                    | REG0=GPRv_R():w REG1=GPRv_B():r IMM0:r:z
IMUL                 | BINARY         | BASE           | I186           | 0x6B MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SIMM8()                             | REG0=GPRv_R():w MEM0:r:v IMM0:r:b:i8
IMUL                 | BINARY         | BASE           | I186           | 0x6B MOD[0b11] MOD=3 REG[rrr] RM[nnn] SIMM8()                                    | REG0=GPRv_R():w REG1=GPRv_B():r IMM0:r:b:i8
IMUL                 | BINARY         | BASE           | I86            | 0xF6 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM()                                   | MEM0:r:b REG0=XED_REG_AL:r:SUPP REG1=XED_REG_AX:w:SUPP
IMUL                 | BINARY         | BASE           | I86            | 0xF6 MOD[0b11] MOD=3 REG[0b101] RM[nnn]                                          | REG0=GPR8_B():r REG1=XED_REG_AL:r:SUPP REG2=XED_REG_AX:w:SUPP
IMUL                 | BINARY         | BASE           | I86            | 0xF7 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM()                                   | MEM0:r:v REG0=OrAX():rw:SUPP REG1=OrDX():w:SUPP
IMUL                 | BINARY         | BASE           | I86            | 0xF7 MOD[0b11] MOD=3 REG[0b101] RM[nnn]                                          | REG0=GPRv_B():r REG1=OrAX():rw:SUPP REG2=OrDX():w:SUPP
IMUL                 | BINARY         | BASE           | I86            | 0x0F 0xAF MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=GPRv_R():rw MEM0:r:v
IMUL                 | BINARY         | BASE           | I86            | 0x0F 0xAF MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_R():rw REG1=GPRv_B():r
IN                   | IO             | BASE           | I86            | 0xE4 UIMM8()                                                                     | REG0=XED_REG_AL:w:IMPL IMM0:r:b
IN                   | IO             | BASE           | I86            | 0xE5 UIMM8() IMMUNE_REXW()                                                       | REG0=OeAX():w:IMPL IMM0:r:b
IN                   | IO             | BASE           | I86            | 0xEC                                                                             | REG0=XED_REG_AL:w:IMPL REG1=XED_REG_DX:r:IMPL
IN                   | IO             | BASE           | I86            | 0xED IMMUNE_REXW()                                                               | REG0=OeAX():w:IMPL REG1=XED_REG_DX:r:IMPL
INC                  | BINARY         | BASE           | I86            | 0xFE MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() nolock_prefix                     | MEM0:rw:b
INC                  | BINARY         | BASE           | I86            | 0xFE MOD[0b11] MOD=3 REG[0b000] RM[nnn]                                          | REG0=GPR8_B():rw
INC                  | BINARY         | BASE           | I86            | 0xFF MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() nolock_prefix                     | MEM0:rw:v
INC                  | BINARY         | BASE           | I86            | 0xFF MOD[0b11] MOD=3 REG[0b000] RM[nnn]                                          | REG0=GPRv_B():rw
INC                  | BINARY         | BASE           | I86            | 0b0100_0 SRM[rrr] not64                                                          | REG0=GPRv_SB():rw
INC_LOCK             | BINARY         | BASE           | I86            | 0xFE MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() lock_prefix                       | MEM0:rw:b
INC_LOCK             | BINARY         | BASE           | I86            | 0xFF MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() lock_prefix                       | MEM0:rw:v
INCSSPD              | CET            | CET            | CET            | 0x0F 0xAE MOD[0b11] MOD=3  REG[0b101] RM[nnn]  f3_refining_prefix    W0          | REG0=GPR32_B():r:d:u8 REG1=XED_REG_SSP:rw:SUPP:u64
INCSSPQ              | CET            | CET            | CET            | 0x0F 0xAE MOD[0b11] MOD=3  REG[0b101] RM[nnn]  f3_refining_prefix    W1  mode64  | REG0=GPR64_B():r:q:u8 REG1=XED_REG_SSP:rw:SUPP:u64
INSB                 | IOSTRINGOP     | BASE           | I186           | 0x6C norep                                                                       | MEM0:w:SUPP:b BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP
INSD                 | IOSTRINGOP     | BASE           | I386           | 0x6D mode16 66_prefix  norep                                                     | MEM0:w:SUPP:d BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP
INSD                 | IOSTRINGOP     | BASE           | I386           | 0x6D mode32 no66_prefix  norep                                                   | MEM0:w:SUPP:d BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP
INSD                 | IOSTRINGOP     | BASE           | I386           | 0x6D mode64 norexw_prefix no66_prefix  norep                                     | MEM0:w:SUPP:d BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP
INSD                 | IOSTRINGOP     | BASE           | I386           | 0x6D mode64 rexw_prefix norep                                                    | MEM0:w:SUPP:d BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP
INSW                 | IOSTRINGOP     | BASE           | I186           | 0x6D mode16 no66_prefix norep                                                    | MEM0:w:SUPP:w BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP
INSW                 | IOSTRINGOP     | BASE           | I186           | 0x6D mode32 66_prefix  norep                                                     | MEM0:w:SUPP:w BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP
INSW                 | IOSTRINGOP     | BASE           | I186           | 0x6D mode64 norexw_prefix 66_prefix  norep                                       | MEM0:w:SUPP:w BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP
INT                  | INTERRUPT      | BASE           | I86            | 0xCD UIMM8()                                                                     | IMM0:r:b REG0=rIP():w:SUPP
INT1                 | INTERRUPT      | BASE           | I86            | 0xF1                                                                             | REG0=rIP():w:SUPP
INT3                 | INTERRUPT      | BASE           | I86            | 0xCC                                                                             | REG0=rIP():w:SUPP
INTO                 | INTERRUPT      | BASE           | I86            | 0xCE not64                                                                       | REG0=XED_REG_EIP:w:SUPP
INVD                 | SYSTEM         | BASE           | I486REAL       | 0x0F 0x08                                                                        | 
INVLPG               | SYSTEM         | BASE           | I486REAL       | 0x0F 0x01 MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM()                              | MEM0:r:b
INVPCID              | MISC           | INVPCID        | INVPCID        | 0x0F 0x38 0x82 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() mode64 MODRM() CR_WIDTH() | REG0=GPR64_R():r MEM0:r:dq
INVPCID              | MISC           | INVPCID        | INVPCID        | 0x0F 0x38 0x82 osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() not64 MODRM() CR_WIDTH() | REG0=GPR32_R():r MEM0:r:dq
IRET                 | RET            | BASE           | I86            | 0xCF mode16 no66_prefix                                                          | REG0=XED_REG_STACKPOP:r:spw5:SUPP REG1=rIP():w:SUPP
IRET                 | RET            | BASE           | I86            | 0xCF mode32 66_prefix                                                            | REG0=XED_REG_STACKPOP:r:spw5:SUPP REG1=rIP():w:SUPP
IRET                 | RET            | BASE           | I86            | 0xCF mode64 norexw_prefix 66_prefix                                              | REG0=XED_REG_STACKPOP:r:spw5:SUPP REG1=rIP():w:SUPP
IRETD                | RET            | BASE           | I386           | 0xCF mode16 66_prefix                                                            | REG0=XED_REG_STACKPOP:r:spw5:SUPP REG1=rIP():w:SUPP
IRETD                | RET            | BASE           | I386           | 0xCF mode32 no66_prefix                                                          | REG0=XED_REG_STACKPOP:r:spw5:SUPP REG1=rIP():w:SUPP
IRETD                | RET            | BASE           | I386           | 0xCF mode64 norexw_prefix no66_prefix                                            | REG0=XED_REG_STACKPOP:r:spw5:SUPP REG1=rIP():w:SUPP
JB                   | COND_BR        | BASE           | I86            | 0x72 mode64 FORCE64() BRANCH_HINT() BRDISP8()                                    | RELBR:r:b:i8 REG0=XED_REG_RIP:rw:SUPP
JB                   | COND_BR        | BASE           | I86            | 0x72 not64 BRANCH_HINT() BRDISP8()                                               | RELBR:r:b:i8 REG0=XED_REG_EIP:rw:SUPP
JB                   | COND_BR        | BASE           | I86            | 0x0F 0x82 not64 BRANCH_HINT() BRDISPz()                                          | RELBR:r:z REG0=XED_REG_EIP:rw:SUPP
JB                   | COND_BR        | BASE           | I86            | 0x0F 0x82 mode64 FORCE64() BRANCH_HINT() BRDISP32()                              | RELBR:r:d REG0=XED_REG_RIP:rw:SUPP
JBE                  | COND_BR        | BASE           | I86            | 0x76 mode64 FORCE64() BRANCH_HINT() BRDISP8()                                    | RELBR:r:b:i8  REG0=XED_REG_RIP:rw:SUPP
JBE                  | COND_BR        | BASE           | I86            | 0x76 not64  BRANCH_HINT() BRDISP8()                                              | RELBR:r:b:i8 REG0=XED_REG_EIP:rw:SUPP
JBE                  | COND_BR        | BASE           | I86            | 0x0F 0x86 not64 BRANCH_HINT() BRDISPz()                                          | RELBR:r:z REG0=XED_REG_EIP:rw:SUPP
JBE                  | COND_BR        | BASE           | I86            | 0x0F 0x86 mode64 FORCE64() BRANCH_HINT() BRDISP32()                              | RELBR:r:d REG0=XED_REG_RIP:rw:SUPP
JCXZ                 | COND_BR        | BASE           | I386           | 0xE3 eamode16 BRDISP8()                                                          | RELBR:r:b:i8 REG0=XED_REG_CX:r:SUPP REG1=XED_REG_IP:rw:SUPP
JECXZ                | COND_BR        | BASE           | I386           | 0xE3 eamode32 not64 BRDISP8()                                                    | RELBR:r:b:i8 REG0=XED_REG_ECX:r:SUPP REG1=XED_REG_EIP:rw:SUPP
JECXZ                | COND_BR        | BASE           | I386           | 0xE3 eamode32 mode64 BRDISP8() FORCE64()                                         | RELBR:r:b:i8 REG0=XED_REG_ECX:r:SUPP REG1=XED_REG_RIP:rw:SUPP
JKNZD                | COND_BR        | KNCV           | KNCJKBR        | VV1 0x75 VNP VMAP0   W0 BRDISP8()                                                | REG0=MASK_N():w:mskw RELBR:r:b
JKNZD                | COND_BR        | KNCV           | KNCJKBR        | VV1 0x85 VNP not64 V0F   W0 BRDISPz()                                            | REG0=MASK_N():w:mskw RELBR:r:z
JKNZD                | COND_BR        | KNCV           | KNCJKBR        | VV1 0x85 VNP mode64 V0F   W0 BRDISP32()                                          | REG0=MASK_N():w:mskw RELBR:r:z
JKZD                 | COND_BR        | KNCV           | KNCJKBR        | VV1 0x74 VNP VMAP0   W0 BRDISP8()                                                | REG0=MASK_N():w:mskw RELBR:r:b
JKZD                 | COND_BR        | KNCV           | KNCJKBR        | VV1 0x84 not64 VNP V0F   W0 BRDISPz()                                            | REG0=MASK_N():w:mskw RELBR:r:z
JKZD                 | COND_BR        | KNCV           | KNCJKBR        | VV1 0x84 mode64 VNP V0F   W0 BRDISP32()                                          | REG0=MASK_N():w:mskw RELBR:r:z
JL                   | COND_BR        | BASE           | I86            | 0x7C mode64 FORCE64() BRANCH_HINT() BRDISP8()                                    | RELBR:r:b:i8 REG0=XED_REG_RIP:rw:SUPP
JL                   | COND_BR        | BASE           | I86            | 0x7C not64 BRANCH_HINT() BRDISP8()                                               | RELBR:r:b:i8 REG0=XED_REG_EIP:rw:SUPP
JL                   | COND_BR        | BASE           | I86            | 0x0F 0x8C not64 BRANCH_HINT() BRDISPz()                                          | RELBR:r:z REG0=XED_REG_EIP:rw:SUPP
JL                   | COND_BR        | BASE           | I86            | 0x0F 0x8C mode64 FORCE64() BRANCH_HINT() BRDISP32()                              | RELBR:r:d REG0=XED_REG_RIP:rw:SUPP
JLE                  | COND_BR        | BASE           | I86            | 0x7E mode64 FORCE64() BRANCH_HINT() BRDISP8()                                    | RELBR:r:b:i8 REG0=XED_REG_RIP:rw:SUPP
JLE                  | COND_BR        | BASE           | I86            | 0x7E not64 BRANCH_HINT() BRDISP8()                                               | RELBR:r:b:i8 REG0=XED_REG_EIP:rw:SUPP
JLE                  | COND_BR        | BASE           | I86            | 0x0F 0x8E not64 BRANCH_HINT() BRDISPz()                                          | RELBR:r:z REG0=XED_REG_EIP:rw:SUPP
JLE                  | COND_BR        | BASE           | I86            | 0x0F 0x8E mode64 FORCE64() BRANCH_HINT() BRDISP32()                              | RELBR:r:d REG0=XED_REG_RIP:rw:SUPP
JMP                  | UNCOND_BR      | BASE           | I86            | 0xFF MOD[mm] MOD!=3 REG[0b100] RM[nnn] DF64() IMMUNE66_LOOP64() MODRM()          | MEM0:r:v REG0=rIP():w:SUPP
JMP                  | UNCOND_BR      | BASE           | I86            | 0xFF MOD[0b11] MOD=3 REG[0b100] RM[nnn] DF64() IMMUNE66_LOOP64()                 | REG0=GPRv_B():r REG1=rIP():w:SUPP
JMP                  | UNCOND_BR      | BASE           | I86            | 0xE9 not64 BRDISPz()                                                             | RELBR:r:z REG0=XED_REG_EIP:rw:SUPP
JMP                  | UNCOND_BR      | BASE           | I86            | 0xE9 mode64 FORCE64() BRDISP32()                                                 | RELBR:r:d REG0=XED_REG_RIP:rw:SUPP
JMP                  | UNCOND_BR      | BASE           | I86            | 0xEB not64 BRDISP8()                                                             | RELBR:r:b:i8 REG0=XED_REG_EIP:rw:SUPP
JMP                  | UNCOND_BR      | BASE           | I86            | 0xEB mode64 FORCE64() BRDISP8()                                                  | RELBR:r:b:i8 REG0=XED_REG_RIP:rw:SUPP
JMP_FAR              | UNCOND_BR      | BASE           | I86            | 0xFF MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM()                                   | MEM0:r:p2 REG0=rIP():w:SUPP
JMP_FAR              | UNCOND_BR      | BASE           | I86            | 0xEA not64 BRDISPz() UIMM16()                                                    | PTR:r:p IMM0:r:w REG0=XED_REG_EIP:w:SUPP
JNB                  | COND_BR        | BASE           | I86            | 0x73 mode64 FORCE64() BRANCH_HINT() BRDISP8()                                    | RELBR:r:b:i8 REG0=XED_REG_RIP:rw:SUPP
JNB                  | COND_BR        | BASE           | I86            | 0x73 not64 BRANCH_HINT() BRDISP8()                                               | RELBR:r:b:i8 REG0=XED_REG_EIP:rw:SUPP
JNB                  | COND_BR        | BASE           | I86            | 0x0F 0x83 not64 BRANCH_HINT() BRDISPz()                                          | RELBR:r:z REG0=XED_REG_EIP:rw:SUPP
JNB                  | COND_BR        | BASE           | I86            | 0x0F 0x83 mode64 FORCE64() BRANCH_HINT() BRDISP32()                              | RELBR:r:d REG0=XED_REG_RIP:rw:SUPP
JNBE                 | COND_BR        | BASE           | I86            | 0x77 mode64 FORCE64() BRANCH_HINT() BRDISP8()                                    | RELBR:r:b:i8 REG0=XED_REG_RIP:rw:SUPP
JNBE                 | COND_BR        | BASE           | I86            | 0x77 not64  BRANCH_HINT() BRDISP8()                                              | RELBR:r:b:i8 REG0=XED_REG_EIP:rw:SUPP
JNBE                 | COND_BR        | BASE           | I86            | 0x0F 0x87 not64 BRANCH_HINT() BRDISPz()                                          | RELBR:r:z REG0=XED_REG_EIP:rw:SUPP
JNBE                 | COND_BR        | BASE           | I86            | 0x0F 0x87 mode64 FORCE64() BRANCH_HINT() BRDISP32()                              | RELBR:r:d REG0=XED_REG_RIP:rw:SUPP
JNL                  | COND_BR        | BASE           | I86            | 0x7D mode64 FORCE64() BRANCH_HINT() BRDISP8()                                    | RELBR:r:b:i8 REG0=XED_REG_RIP:rw:SUPP
JNL                  | COND_BR        | BASE           | I86            | 0x7D not64 BRANCH_HINT() BRDISP8()                                               | RELBR:r:b:i8 REG0=XED_REG_EIP:rw:SUPP
JNL                  | COND_BR        | BASE           | I86            | 0x0F 0x8D not64 BRANCH_HINT() BRDISPz()                                          | RELBR:r:z REG0=XED_REG_EIP:rw:SUPP
JNL                  | COND_BR        | BASE           | I86            | 0x0F 0x8D mode64 FORCE64() BRANCH_HINT() BRDISP32()                              | RELBR:r:d REG0=XED_REG_RIP:rw:SUPP
JNLE                 | COND_BR        | BASE           | I86            | 0x7F mode64 FORCE64() BRANCH_HINT() BRDISP8()                                    | RELBR:r:b:i8 REG0=XED_REG_RIP:rw:SUPP
JNLE                 | COND_BR        | BASE           | I86            | 0x7F not64 BRANCH_HINT() BRDISP8()                                               | RELBR:r:b:i8 REG0=XED_REG_EIP:rw:SUPP
JNLE                 | COND_BR        | BASE           | I86            | 0x0F 0x8F not64 BRANCH_HINT() BRDISPz()                                          | RELBR:r:z REG0=XED_REG_EIP:rw:SUPP
JNLE                 | COND_BR        | BASE           | I86            | 0x0F 0x8F mode64 FORCE64() BRANCH_HINT() BRDISP32()                              | RELBR:r:d REG0=XED_REG_RIP:rw:SUPP
JNO                  | COND_BR        | BASE           | I86            | 0x71 mode64 FORCE64() BRANCH_HINT() BRDISP8()                                    | RELBR:r:b:i8 REG0=XED_REG_RIP:rw:SUPP
JNO                  | COND_BR        | BASE           | I86            | 0x71 not64 BRANCH_HINT() BRDISP8()                                               | RELBR:r:b:i8 REG0=XED_REG_EIP:rw:SUPP
JNO                  | COND_BR        | BASE           | I86            | 0x0F 0x81 not64 BRANCH_HINT() BRDISPz()                                          | RELBR:r:z REG0=XED_REG_EIP:rw:SUPP
JNO                  | COND_BR        | BASE           | I86            | 0x0F 0x81 mode64 FORCE64() BRANCH_HINT() BRDISP32()                              | RELBR:r:d REG0=XED_REG_RIP:rw:SUPP
JNP                  | COND_BR        | BASE           | I86            | 0x7B mode64 FORCE64() BRANCH_HINT() BRDISP8()                                    | RELBR:r:b:i8 REG0=XED_REG_RIP:rw:SUPP
JNP                  | COND_BR        | BASE           | I86            | 0x7B not64  BRANCH_HINT() BRDISP8()                                              | RELBR:r:b:i8 REG0=XED_REG_EIP:rw:SUPP
JNP                  | COND_BR        | BASE           | I86            | 0x0F 0x8B not64 BRANCH_HINT() BRDISPz()                                          | RELBR:r:z REG0=XED_REG_EIP:rw:SUPP
JNP                  | COND_BR        | BASE           | I86            | 0x0F 0x8B mode64 FORCE64() BRANCH_HINT() BRDISP32()                              | RELBR:r:d REG0=XED_REG_RIP:rw:SUPP
JNS                  | COND_BR        | BASE           | I86            | 0x79 mode64 FORCE64() BRANCH_HINT() BRDISP8()                                    | RELBR:r:b:i8 REG0=XED_REG_RIP:rw:SUPP
JNS                  | COND_BR        | BASE           | I86            | 0x79 not64  BRANCH_HINT() BRDISP8()                                              | RELBR:r:b:i8 REG0=XED_REG_EIP:rw:SUPP
JNS                  | COND_BR        | BASE           | I86            | 0x0F 0x89 not64 BRANCH_HINT() BRDISPz()                                          | RELBR:r:z REG0=XED_REG_EIP:rw:SUPP
JNS                  | COND_BR        | BASE           | I86            | 0x0F 0x89 mode64 FORCE64() BRANCH_HINT() BRDISP32()                              | RELBR:r:d REG0=XED_REG_RIP:rw:SUPP
JNZ                  | COND_BR        | BASE           | I86            | 0x75 mode64 FORCE64() BRANCH_HINT() BRDISP8()                                    | RELBR:r:b:i8 REG0=XED_REG_RIP:rw:SUPP
JNZ                  | COND_BR        | BASE           | I86            | 0x75 not64  BRANCH_HINT() BRDISP8()                                              | RELBR:r:b:i8 REG0=XED_REG_EIP:rw:SUPP
JNZ                  | COND_BR        | BASE           | I86            | 0x0F 0x85 not64 BRANCH_HINT() BRDISPz()                                          | RELBR:r:z REG0=XED_REG_EIP:rw:SUPP
JNZ                  | COND_BR        | BASE           | I86            | 0x0F 0x85 mode64 FORCE64() BRANCH_HINT() BRDISP32()                              | RELBR:r:d REG0=XED_REG_RIP:rw:SUPP
JO                   | COND_BR        | BASE           | I86            | 0x70 mode64 FORCE64() BRANCH_HINT() BRDISP8()                                    | RELBR:r:b:i8 REG0=XED_REG_RIP:rw:SUPP
JO                   | COND_BR        | BASE           | I86            | 0x70 not64 BRANCH_HINT() BRDISP8()                                               | RELBR:r:b:i8 REG0=XED_REG_EIP:rw:SUPP
JO                   | COND_BR        | BASE           | I86            | 0x0F 0x80 mode64 FORCE64() BRANCH_HINT() BRDISP32()                              | RELBR:r:d REG0=XED_REG_RIP:rw:SUPP
JO                   | COND_BR        | BASE           | I86            | 0x0F 0x80 not64 BRANCH_HINT() BRDISPz()                                          | RELBR:r:z REG0=XED_REG_EIP:rw:SUPP
JP                   | COND_BR        | BASE           | I86            | 0x7A mode64 FORCE64() BRANCH_HINT() BRDISP8()                                    | RELBR:r:b:i8 REG0=XED_REG_RIP:rw:SUPP
JP                   | COND_BR        | BASE           | I86            | 0x7A not64  BRANCH_HINT() BRDISP8()                                              | RELBR:r:b:i8 REG0=XED_REG_EIP:rw:SUPP
JP                   | COND_BR        | BASE           | I86            | 0x0F 0x8A not64 BRANCH_HINT() BRDISPz()                                          | RELBR:r:z REG0=XED_REG_EIP:rw:SUPP
JP                   | COND_BR        | BASE           | I86            | 0x0F 0x8A mode64 FORCE64() BRANCH_HINT() BRDISP32()                              | RELBR:r:d REG0=XED_REG_RIP:rw:SUPP
JRCXZ                | COND_BR        | BASE           | LONGMODE       | 0xE3 eamode64 BRDISP8() FORCE64()                                                | RELBR:r:b:i8 REG0=XED_REG_RCX:r:SUPP REG1=XED_REG_RIP:rw:SUPP
JS                   | COND_BR        | BASE           | I86            | 0x78 mode64 FORCE64() BRANCH_HINT() BRDISP8()                                    | RELBR:r:b:i8 REG0=XED_REG_RIP:rw:SUPP
JS                   | COND_BR        | BASE           | I86            | 0x78 not64  BRANCH_HINT() BRDISP8()                                              | RELBR:r:b:i8 REG0=XED_REG_EIP:rw:SUPP
JS                   | COND_BR        | BASE           | I86            | 0x0F 0x88 not64 BRANCH_HINT() BRDISPz()                                          | RELBR:r:z REG0=XED_REG_EIP:rw:SUPP
JS                   | COND_BR        | BASE           | I86            | 0x0F 0x88 mode64 FORCE64() BRANCH_HINT() BRDISP32()                              | RELBR:r:d REG0=XED_REG_RIP:rw:SUPP
JZ                   | COND_BR        | BASE           | I86            | 0x74 mode64 FORCE64() BRANCH_HINT() BRDISP8()                                    | RELBR:r:b:i8 REG0=XED_REG_RIP:rw:SUPP
JZ                   | COND_BR        | BASE           | I86            | 0x74 not64 BRANCH_HINT() BRDISP8()                                               | RELBR:r:b:i8 REG0=XED_REG_EIP:rw:SUPP
JZ                   | COND_BR        | BASE           | I86            | 0x0F 0x84 not64 BRANCH_HINT() BRDISPz()                                          | RELBR:r:z REG0=XED_REG_EIP:rw:SUPP
JZ                   | COND_BR        | BASE           | I86            | 0x0F 0x84 mode64 FORCE64() BRANCH_HINT() BRDISP32()                              | RELBR:r:d REG0=XED_REG_RIP:rw:SUPP
KADDB                | KMASK          | AVX512VEX      | AVX512DQ_KOP   | VV1 0x4A V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W0                    | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
KADDD                | KMASK          | AVX512VEX      | AVX512BW_KOP   | VV1 0x4A V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W1                    | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
KADDQ                | KMASK          | AVX512VEX      | AVX512BW_KOP   | VV1 0x4A VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W1                    | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
KADDW                | KMASK          | AVX512VEX      | AVX512DQ_KOP   | VV1 0x4A VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W0                    | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
KAND                 | KNCMASK        | KNC            | KNCV           | VV1 0x41  VL128 VNP V0F REXW=0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]            | REG0=MASK_R():w:mskw    REG1=MASK_B():r:mskw
KANDB                | KMASK          | AVX512VEX      | AVX512DQ_KOP   | VV1 0x41 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W0                    | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
KANDD                | KMASK          | AVX512VEX      | AVX512BW_KOP   | VV1 0x41 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W1                    | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
KANDN                | KNCMASK        | KNC            | KNCV           | VV1 0x42  VL128 VNP V0F REXW=0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]            | REG0=MASK_R():w:mskw    REG1=MASK_B():r:mskw
KANDNB               | KMASK          | AVX512VEX      | AVX512DQ_KOP   | VV1 0x42 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W0                    | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
KANDND               | KMASK          | AVX512VEX      | AVX512BW_KOP   | VV1 0x42 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W1                    | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
KANDNQ               | KMASK          | AVX512VEX      | AVX512BW_KOP   | VV1 0x42 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W1                    | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
KANDNR               | KNCMASK        | KNC            | KNCV           | VV1 0x43  VL128 VNP V0F REXW=0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]            | REG0=MASK_R():w:mskw    REG1=MASK_B():r:mskw
KANDNW               | KMASK          | AVX512VEX      | AVX512F_KOP    | VV1 0x42 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W0                    | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
KANDQ                | KMASK          | AVX512VEX      | AVX512BW_KOP   | VV1 0x41 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W1                    | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
KANDW                | KMASK          | AVX512VEX      | AVX512F_KOP    | VV1 0x41 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W0                    | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
KCONCATH             | KNCMASK        | KNC            | KNCV           | VV1 0x95  VL128 VNP V0F REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                  | REG0=GPR64_R():w:q    REG1=MASK_N():r:mskw    REG2=MASK_B():r:mskw
KCONCATL             | KNCMASK        | KNC            | KNCV           | VV1 0x97  VL128 VNP V0F REXW=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                  | REG0=GPR64_R():w:q    REG1=MASK_N():r:mskw    REG2=MASK_B():r:mskw
KEXTRACT             | KNCMASK        | KNC            | KNCV           | VV1 0x3E  VL128 V66 V0F3A REXW=0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]  UIMM8() | REG0=MASK_R():w:mskw REG1=GPR64_B():r:q IMM0:r:b
KMERGE2L1H           | KNCMASK        | KNC            | KNCV           | VV1 0x48  VL128 VNP V0F REXW=0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]            | REG0=MASK_R():w:mskw    REG1=MASK_B():r:mskw
KMERGE2L1L           | KNCMASK        | KNC            | KNCV           | VV1 0x49  VL128 VNP V0F REXW=0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]            | REG0=MASK_R():w:mskw    REG1=MASK_B():r:mskw
KMOV                 | KNCMASK        | KNC            | KNCV           | VV1 0x90  VL128 VNP V0F REXW=0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]            | REG0=MASK_R():w:mskw    REG1=MASK_B():r:mskw
KMOV                 | KNCMASK        | KNC            | KNCV           | VV1 0x92  VL128 VNP V0F REXW=0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] mode64     | REG0=MASK_R():w:mskw    REG1=GPR32_B():r:mskw
KMOV                 | KNCMASK        | KNC            | KNCV           | VV1 0x93  VL128 VNP V0F REXW=0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn] mode64     | REG0=GPR32_R():w:mskw   REG1=MASK_B():r:mskw
KMOVB                | KMASK          | AVX512VEX      | AVX512DQ_KOP   | VV1 0x90 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0  NOVSR             | REG0=MASK_R():w:mskw REG1=MASK_B():r:mskw:u8
KMOVB                | KMASK          | AVX512VEX      | AVX512DQ_KOP   | VV1 0x90 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOVSR      | REG0=MASK_R():w:mskw MEM0:r:b:u8
KMOVB                | KMASK          | AVX512VEX      | AVX512DQ_KOP   | VV1 0x91 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOVSR      | MEM0:w:b:u8 REG0=MASK_R():r:mskw
KMOVB                | KMASK          | AVX512VEX      | AVX512DQ_KOP   | VV1 0x92 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0  NOVSR             | REG0=MASK_R():w:mskw REG1=GPR32_B():r:d:u32
KMOVB                | KMASK          | AVX512VEX      | AVX512DQ_KOP   | VV1 0x93 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0  NOVSR             | REG0=GPR32_R():w:d:u32 REG1=MASK_B():r:mskw
KMOVD                | KMASK          | AVX512VEX      | AVX512BW_KOP   | VV1 0x90 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W1  NOVSR             | REG0=MASK_R():w:mskw REG1=MASK_B():r:mskw:u32
KMOVD                | KMASK          | AVX512VEX      | AVX512BW_KOP   | VV1 0x90 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOVSR      | REG0=MASK_R():w:mskw MEM0:r:d:u32
KMOVD                | KMASK          | AVX512VEX      | AVX512BW_KOP   | VV1 0x91 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOVSR      | MEM0:w:d:u32 REG0=MASK_R():r:mskw
KMOVD                | KMASK          | AVX512VEX      | AVX512BW_KOP   | VV1 0x92 VF2 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0 mode64  NOVSR      | REG0=MASK_R():w:mskw REG1=GPR32_B():r:d:u32
KMOVD                | KMASK          | AVX512VEX      | AVX512BW_KOP   | VV1 0x92 VF2 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  not64 NOVSR           | REG0=MASK_R():w:mskw REG1=GPR32_B():r:d:u32
KMOVD                | KMASK          | AVX512VEX      | AVX512BW_KOP   | VV1 0x93 VF2 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0  mode64 NOVSR      | REG0=GPR32_R():w:d:u32 REG1=MASK_B():r:mskw
KMOVD                | KMASK          | AVX512VEX      | AVX512BW_KOP   | VV1 0x93 VF2 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  not64  NOVSR          | REG0=GPR32_R():w:d:u32 REG1=MASK_B():r:mskw
KMOVQ                | KMASK          | AVX512VEX      | AVX512BW_KOP   | VV1 0x90 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W1  NOVSR             | REG0=MASK_R():w:mskw REG1=MASK_B():r:mskw:u64
KMOVQ                | KMASK          | AVX512VEX      | AVX512BW_KOP   | VV1 0x90 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOVSR      | REG0=MASK_R():w:mskw MEM0:r:q:u64
KMOVQ                | KMASK          | AVX512VEX      | AVX512BW_KOP   | VV1 0x91 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOVSR      | MEM0:w:q:u64 REG0=MASK_R():r:mskw
KMOVQ                | KMASK          | AVX512VEX      | AVX512BW_KOP   | VV1 0x92 VF2 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W1  mode64  NOVSR     | REG0=MASK_R():w:mskw REG1=GPR64_B():r:q:u64
KMOVQ                | KMASK          | AVX512VEX      | AVX512BW_KOP   | VV1 0x93 VF2 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W1  mode64  NOVSR     | REG0=GPR64_R():w:q:u64 REG1=MASK_B():r:mskw
KMOVW                | KMASK          | AVX512VEX      | AVX512F_KOP    | VV1 0x90 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0  NOVSR             | REG0=MASK_R():w:mskw REG1=MASK_B():r:mskw:u16
KMOVW                | KMASK          | AVX512VEX      | AVX512F_KOP    | VV1 0x90 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOVSR      | REG0=MASK_R():w:mskw MEM0:r:wrd:u16
KMOVW                | KMASK          | AVX512VEX      | AVX512F_KOP    | VV1 0x91 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOVSR      | MEM0:w:wrd:u16 REG0=MASK_R():r:mskw
KMOVW                | KMASK          | AVX512VEX      | AVX512F_KOP    | VV1 0x92 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0  NOVSR             | REG0=MASK_R():w:mskw REG1=GPR32_B():r:d:u32
KMOVW                | KMASK          | AVX512VEX      | AVX512F_KOP    | VV1 0x93 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0  NOVSR             | REG0=GPR32_R():w:d:u32 REG1=MASK_B():r:mskw
KNOT                 | KNCMASK        | KNC            | KNCV           | VV1 0x44  VL128 VNP V0F REXW=0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]            | REG0=MASK_R():w:mskw    REG1=MASK_B():r:mskw
KNOTB                | KMASK          | AVX512VEX      | AVX512DQ_KOP   | VV1 0x44 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0  NOVSR             | REG0=MASK_R():w:mskw REG1=MASK_B():r:mskw
KNOTD                | KMASK          | AVX512VEX      | AVX512BW_KOP   | VV1 0x44 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W1  NOVSR             | REG0=MASK_R():w:mskw REG1=MASK_B():r:mskw
KNOTQ                | KMASK          | AVX512VEX      | AVX512BW_KOP   | VV1 0x44 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W1  NOVSR             | REG0=MASK_R():w:mskw REG1=MASK_B():r:mskw
KNOTW                | KMASK          | AVX512VEX      | AVX512F_KOP    | VV1 0x44 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0  NOVSR             | REG0=MASK_R():w:mskw REG1=MASK_B():r:mskw
KOR                  | KNCMASK        | KNC            | KNCV           | VV1 0x45  VL128 VNP V0F REXW=0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]            | REG0=MASK_R():w:mskw    REG1=MASK_B():r:mskw
KORB                 | KMASK          | AVX512VEX      | AVX512DQ_KOP   | VV1 0x45 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W0                    | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
KORD                 | KMASK          | AVX512VEX      | AVX512BW_KOP   | VV1 0x45 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W1                    | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
KORQ                 | KMASK          | AVX512VEX      | AVX512BW_KOP   | VV1 0x45 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W1                    | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
KORTESTB             | KMASK          | AVX512VEX      | AVX512DQ_KOP   | VV1 0x98 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0  NOVSR             | REG0=MASK_R():r:mskw REG1=MASK_B():r:mskw
KORTESTD             | KMASK          | AVX512VEX      | AVX512BW_KOP   | VV1 0x98 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W1  NOVSR             | REG0=MASK_R():r:mskw REG1=MASK_B():r:mskw
KORTESTD             | KNCMASK        | KNC            | KNCV           | VV1 0x98  VL128 VNP V0F REXW=0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]            | REG0=MASK_R():r:mskw    REG1=MASK_B():r:mskw
KORTESTQ             | KMASK          | AVX512VEX      | AVX512BW_KOP   | VV1 0x98 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W1  NOVSR             | REG0=MASK_R():r:mskw REG1=MASK_B():r:mskw
KORTESTW             | KMASK          | AVX512VEX      | AVX512F_KOP    | VV1 0x98 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0  NOVSR             | REG0=MASK_R():r:mskw REG1=MASK_B():r:mskw
KORW                 | KMASK          | AVX512VEX      | AVX512F_KOP    | VV1 0x45 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W0                    | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
KSHIFTLB             | KMASK          | AVX512VEX      | AVX512DQ_KOP   | VV1 0x32 V66 V0F3A MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0  NOVSR UIMM8()   | REG0=MASK_R():w:mskw REG1=MASK_B():r:mskw IMM0:r:b
KSHIFTLD             | KMASK          | AVX512VEX      | AVX512BW_KOP   | VV1 0x33 V66 V0F3A MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0  NOVSR UIMM8()   | REG0=MASK_R():w:mskw REG1=MASK_B():r:mskw IMM0:r:b
KSHIFTLQ             | KMASK          | AVX512VEX      | AVX512BW_KOP   | VV1 0x33 V66 V0F3A MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W1  NOVSR UIMM8()   | REG0=MASK_R():w:mskw REG1=MASK_B():r:mskw IMM0:r:b
KSHIFTLW             | KMASK          | AVX512VEX      | AVX512F_KOP    | VV1 0x32 V66 V0F3A MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W1  NOVSR UIMM8()   | REG0=MASK_R():w:mskw REG1=MASK_B():r:mskw IMM0:r:b
KSHIFTRB             | KMASK          | AVX512VEX      | AVX512DQ_KOP   | VV1 0x30 V66 V0F3A MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0  NOVSR UIMM8()   | REG0=MASK_R():w:mskw REG1=MASK_B():r:mskw IMM0:r:b
KSHIFTRD             | KMASK          | AVX512VEX      | AVX512BW_KOP   | VV1 0x31 V66 V0F3A MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0  NOVSR UIMM8()   | REG0=MASK_R():w:mskw REG1=MASK_B():r:mskw IMM0:r:b
KSHIFTRQ             | KMASK          | AVX512VEX      | AVX512BW_KOP   | VV1 0x31 V66 V0F3A MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W1  NOVSR UIMM8()   | REG0=MASK_R():w:mskw REG1=MASK_B():r:mskw IMM0:r:b
KSHIFTRW             | KMASK          | AVX512VEX      | AVX512F_KOP    | VV1 0x30 V66 V0F3A MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W1  NOVSR UIMM8()   | REG0=MASK_R():w:mskw REG1=MASK_B():r:mskw IMM0:r:b
KTESTB               | KMASK          | AVX512VEX      | AVX512DQ_KOP   | VV1 0x99 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0  NOVSR             | REG0=MASK_R():r:mskw REG1=MASK_B():r:mskw
KTESTD               | KMASK          | AVX512VEX      | AVX512BW_KOP   | VV1 0x99 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W1  NOVSR             | REG0=MASK_R():r:mskw REG1=MASK_B():r:mskw
KTESTQ               | KMASK          | AVX512VEX      | AVX512BW_KOP   | VV1 0x99 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W1  NOVSR             | REG0=MASK_R():r:mskw REG1=MASK_B():r:mskw
KTESTW               | KMASK          | AVX512VEX      | AVX512DQ_KOP   | VV1 0x99 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0  NOVSR             | REG0=MASK_R():r:mskw REG1=MASK_B():r:mskw
KUNPCKBW             | KMASK          | AVX512VEX      | AVX512F_KOP    | VV1 0x4B V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W0                    | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
KUNPCKDQ             | KMASK          | AVX512VEX      | AVX512BW_KOP   | VV1 0x4B VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W1                    | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
KUNPCKWD             | KMASK          | AVX512VEX      | AVX512BW_KOP   | VV1 0x4B VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W0                    | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
KXNOR                | KNCMASK        | KNC            | KNCV           | VV1 0x46  VL128 VNP V0F REXW=0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]            | REG0=MASK_R():w:mskw    REG1=MASK_B():r:mskw
KXNORB               | KMASK          | AVX512VEX      | AVX512DQ_KOP   | VV1 0x46 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W0                    | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
KXNORD               | KMASK          | AVX512VEX      | AVX512BW_KOP   | VV1 0x46 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W1                    | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
KXNORQ               | KMASK          | AVX512VEX      | AVX512BW_KOP   | VV1 0x46 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W1                    | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
KXNORW               | KMASK          | AVX512VEX      | AVX512F_KOP    | VV1 0x46 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W0                    | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
KXOR                 | KNCMASK        | KNC            | KNCV           | VV1 0x47  VL128 VNP V0F REXW=0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]            | REG0=MASK_R():w:mskw    REG1=MASK_B():r:mskw
KXORB                | KMASK          | AVX512VEX      | AVX512DQ_KOP   | VV1 0x47 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W0                    | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
KXORD                | KMASK          | AVX512VEX      | AVX512BW_KOP   | VV1 0x47 V66 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W1                    | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
KXORQ                | KMASK          | AVX512VEX      | AVX512BW_KOP   | VV1 0x47 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W1                    | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
KXORW                | KMASK          | AVX512VEX      | AVX512F_KOP    | VV1 0x47 VNP V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W0                    | REG0=MASK_R():w:mskw REG1=MASK_N():r:mskw REG2=MASK_B():r:mskw
LAHF                 | FLAGOP         | BASE           | LAHF           | 0x9F                                                                             | REG0=XED_REG_AH:w:SUPP
LAR                  | SYSTEM         | BASE           | I286PROTECTED  | 0x0F 0x02 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=GPRv_R():cw MEM0:r:w
LAR                  | SYSTEM         | BASE           | I286PROTECTED  | 0x0F 0x02 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_R():cw REG1=GPRv_B():r
LDMXCSR              | SSE            | SSE            | SSEMXCSR       | 0x0F 0xAE MOD[mm] MOD!=3 REG[0b010] RM[nnn]  no_refining_prefix MODRM()          | MEM0:r:d REG0=XED_REG_MXCSR:w:SUPP
LDS                  | SEGOP          | BASE           | I86            | 0xC5 MOD[mm] MOD!=3 REG[rrr] RM[nnn] not64 MODRM()                               | REG0=GPRz_R():w MEM0:r:p REG1=XED_REG_DS:w:SUPP
LEA                  | MISC           | BASE           | I86            | 0x8D MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() REMOVE_SEGMENT()                    | REG0=GPRv_R():w AGEN:r
LEAVE                | MISC           | BASE           | I186           | 0xC9 DF64()                                                                      | MEM0:r:SUPP:v BASE0=ArBP():r:SUPP SEG0=FINAL_SSEG0():r:SUPP REG0=OrBP():rw:SUPP REG1=OrSP():rw:SUPP
LES                  | SEGOP          | BASE           | I86            | 0xC4 MOD[mm] MOD!=3 REG[rrr] RM[nnn] not64 MODRM()                               | REG0=GPRz_R():w MEM0:r:p REG1=XED_REG_ES:w:SUPP
LFENCE               | MISC           | SSE2           | SSE2           | 0x0F 0xAE  MOD[0b11] MOD=3 REG[0b101] RM[nnn] no_refining_prefix                 | 
LFS                  | SEGOP          | BASE           | I386           | 0x0F 0xB4 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=GPRv_R():w MEM0:r:p2 REG1=XED_REG_FS:w:SUPP
LGDT                 | SYSTEM         | BASE           | I286REAL       | 0x0F 0x01 MOD[mm] MOD!=3 REG[0b010]  RM[nnn] mode64 FORCE64() MODRM()            | MEM0:r:s64 REG0=XED_REG_GDTR:w:SUPP
LGDT                 | SYSTEM         | BASE           | I286REAL       | 0x0F 0x01 MOD[mm] MOD!=3 REG[0b010]  RM[nnn] not64 MODRM()                       | MEM0:r:s REG0=XED_REG_GDTR:w:SUPP
LGS                  | SEGOP          | BASE           | I386           | 0x0F 0xB5 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=GPRv_R():w MEM0:r:p2 REG1=XED_REG_GS:w:SUPP
LIDT                 | SYSTEM         | BASE           | I286REAL       | 0x0F 0x01 MOD[mm] MOD!=3 REG[0b011] RM[nnn] mode64 FORCE64() MODRM()             | MEM0:r:s64 REG0=XED_REG_IDTR:w:SUPP
LIDT                 | SYSTEM         | BASE           | I286REAL       | 0x0F 0x01 MOD[mm] MOD!=3 REG[0b011] RM[nnn] not64 MODRM()                        | MEM0:r:s REG0=XED_REG_IDTR:w:SUPP
LLDT                 | SYSTEM         | BASE           | I286PROTECTED  | 0x0F 0x00 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM()                              | MEM0:r:w REG0=XED_REG_LDTR:w:SUPP
LLDT                 | SYSTEM         | BASE           | I286PROTECTED  | 0x0F 0x00 MOD[0b11] MOD=3 REG[0b010] RM[nnn]                                     | REG0=GPR16_B():r REG1=XED_REG_LDTR:w:SUPP
LLWPCB               | XOP            | XOP            | XOP            | XOPV 0x12 VNP VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[0b000] RM[nnn]               | REG0=GPRy_B():w:y
LMSW                 | SYSTEM         | BASE           | I286REAL       | 0x0F 0x01 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM()                              | MEM0:r:w REG0=XED_REG_CR0:w:SUPP
LMSW                 | SYSTEM         | BASE           | I286REAL       | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b110] RM[nnn]                                     | REG0=GPR16_B():r REG1=XED_REG_CR0:w:SUPP
LODSB                | STRINGOP       | BASE           | I86            | 0xAC norep OVERRIDE_SEG0()                                                       | REG0=XED_REG_AL:w:SUPP MEM0:r:SUPP:b BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP
LODSD                | STRINGOP       | BASE           | I386           | 0xAD mode16 66_prefix  norep OVERRIDE_SEG0()                                     | REG0=XED_REG_EAX:w:SUPP MEM0:r:SUPP:d BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP
LODSD                | STRINGOP       | BASE           | I386           | 0xAD mode32 no66_prefix  norep OVERRIDE_SEG0()                                   | REG0=XED_REG_EAX:w:SUPP MEM0:r:SUPP:d BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP
LODSD                | STRINGOP       | BASE           | I386           | 0xAD mode64 norexw_prefix no66_prefix  norep OVERRIDE_SEG0()                     | REG0=XED_REG_EAX:w:SUPP MEM0:r:SUPP:d BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP
LODSW                | STRINGOP       | BASE           | I86            | 0xAD mode16 no66_prefix  norep OVERRIDE_SEG0()                                   | REG0=XED_REG_AX:w:SUPP MEM0:r:SUPP:w BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP
LODSW                | STRINGOP       | BASE           | I86            | 0xAD mode32 66_prefix  norep OVERRIDE_SEG0()                                     | REG0=XED_REG_AX:w:SUPP MEM0:r:SUPP:w BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP
LODSW                | STRINGOP       | BASE           | I86            | 0xAD mode64 norexw_prefix 66_prefix  norep OVERRIDE_SEG0()                       | REG0=XED_REG_AX:w:SUPP MEM0:r:SUPP:w BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP
LOOP                 | COND_BR        | BASE           | I86            | 0xE2 DF64() BRDISP8() IMMUNE66_LOOP64()                                          | RELBR:r:b:i8 REG0=ArCX():rw:SUPP REG1=rIP():rw:SUPP
LOOPE                | COND_BR        | BASE           | I86            | 0xE1 MODEP5=1 REP=0 DF64() BRDISP8() IMMUNE66_LOOP64()                           | RELBR:r:b:i8 REG0=ArCX():rw:SUPP REG1=rIP():rw:SUPP
LOOPE                | COND_BR        | BASE           | I86            | 0xE1 MODEP5=1 REP=3 DF64() BRDISP8() IMMUNE66_LOOP64()                           | RELBR:r:b:i8 REG0=ArCX():rw:SUPP REG1=rIP():rw:SUPP
LOOPE                | COND_BR        | BASE           | I86            | 0xE1 MODEP5=0       DF64() BRDISP8() IMMUNE66_LOOP64()                           | RELBR:r:b:i8 REG0=ArCX():rw:SUPP REG1=rIP():rw:SUPP
LOOPE                | COND_BR        | BASE           | I86            | 0xE0 MODEP5=1 REP=3  DF64() BRDISP8() IMMUNE66_LOOP64()                          | RELBR:r:b:i8 REG0=ArCX():rw:SUPP REG1=rIP():rw:SUPP
LOOPNE               | COND_BR        | BASE           | I86            | 0xE0 MODEP5=1  REP=0  DF64() BRDISP8() IMMUNE66_LOOP64()                         | RELBR:r:b:i8 REG0=ArCX():rw:SUPP REG1=rIP():rw:SUPP
LOOPNE               | COND_BR        | BASE           | I86            | 0xE0 MODEP5=1  REP=2  DF64() BRDISP8() IMMUNE66_LOOP64()                         | RELBR:r:b:i8 REG0=ArCX():rw:SUPP REG1=rIP():rw:SUPP
LOOPNE               | COND_BR        | BASE           | I86            | 0xE0 MODEP5=0         DF64() BRDISP8() IMMUNE66_LOOP64()                         | RELBR:r:b:i8 REG0=ArCX():rw:SUPP REG1=rIP():rw:SUPP
LOOPNE               | COND_BR        | BASE           | I86            | 0xE1 MODEP5=1 REP=2  DF64() BRDISP8() IMMUNE66_LOOP64()                          | RELBR:r:b:i8 REG0=ArCX():rw:SUPP REG1=rIP():rw:SUPP
LSL                  | SYSTEM         | BASE           | I286PROTECTED  | 0x0F 0x03 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=GPRv_R():rw MEM0:r:w
LSL                  | SYSTEM         | BASE           | I286PROTECTED  | 0x0F 0x03 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_R():rw REG1=GPRz_B():r
LSS                  | SEGOP          | BASE           | I386           | 0x0F 0xB2 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=GPRv_R():w MEM0:r:p2 REG1=XED_REG_SS:w:SUPP
LTR                  | SYSTEM         | BASE           | I286PROTECTED  | 0x0F 0x00 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM()                              | MEM0:r:w REG0=XED_REG_TR:w:SUPP
LTR                  | SYSTEM         | BASE           | I286PROTECTED  | 0x0F 0x00 MOD[0b11] MOD=3 REG[0b011] RM[nnn]                                     | REG0=GPR16_B():r REG1=XED_REG_TR:w:SUPP
LWPINS               | XOP            | XOP            | XOP            | XOPV 0x12 VNP  VL128  XMAPA MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() UIMM32()   | REG0=VGPRy_N():w:y MEM0:r:d IMM0:r:d
LWPINS               | XOP            | XOP            | XOP            | XOPV 0x12 VNP  VL128  XMAPA MOD[0b11] MOD=3 REG[0b000] RM[nnn] UIMM32()          | REG0=VGPRy_N():w:y REG1=GPR32_B():r:y IMM0:r:d
LWPVAL               | XOP            | XOP            | XOP            | XOPV 0x12 VNP VL128  XMAPA MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM() UIMM32()    | REG0=VGPRy_N():w:y MEM0:r:d IMM0:r:d
LWPVAL               | XOP            | XOP            | XOP            | XOPV 0x12 VNP VL128  XMAPA MOD[0b11] MOD=3 REG[0b001] RM[nnn] UIMM32()           | REG0=VGPRy_N():w:y REG1=GPR32_B():r:y IMM0:r:d
LZCNT_VEX            | KNCSCALAR      | KNC            | KNCV           | VV1 0xBD  VL128 VF3 V0F W0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                | REG0=GPR32_R():w:d    REG1=GPR32_B():r:d
LZCNT_VEX            | KNCSCALAR      | KNC            | KNCV           | VV1 0xBD  VL128 VF3 V0F W1 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                | REG0=GPR64_R():w:q    REG1=GPR64_B():r:q
MASKMOVQ             | DATAXFER       | MMX            | PENTIUMMMX     | 0x0F 0xF7 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  OVERRIDE_SEG0()   | REG0=MMX_R():r:q:u8 REG1=MMX_B():r:q:i8 MEM0:w:q:SUPP BASE0=ArDI():r:SUPP SEG0=FINAL_DSEG():r:SUPP
MFENCE               | MISC           | SSE2           | SSE2           | 0x0F 0xAE  MOD[0b11] MOD=3 REG[0b110] RM[nnn] no_refining_prefix                 | 
MONITOR              | MISC           | MONITOR        | MONITOR        | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b001] RM[0b000] no_refining_prefix not64 eamode32 | REG0=XED_REG_EAX:r:SUPP REG1=XED_REG_ECX:r:SUPP REG2=XED_REG_EDX:r:SUPP
MONITOR              | MISC           | MONITOR        | MONITOR        | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b001] RM[0b000] no_refining_prefix not64 eamode16 | REG0=XED_REG_AX:r:SUPP REG1=XED_REG_ECX:r:SUPP REG2=XED_REG_EDX:r:SUPP
MONITOR              | MISC           | MONITOR        | MONITOR        | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b001] RM[0b000] no_refining_prefix mode64 eamode64 | REG0=XED_REG_RAX:r:SUPP REG1=XED_REG_ECX:r:SUPP REG2=XED_REG_EDX:r:SUPP
MONITOR              | MISC           | MONITOR        | MONITOR        | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b001] RM[0b000] no_refining_prefix mode64 eamode32 | REG0=XED_REG_RAX:r:SUPP REG1=XED_REG_ECX:r:SUPP REG2=XED_REG_EDX:r:SUPP
MOV                  | DATAXFER       | BASE           | I86            | 0xC6 MOD[0b11] MOD=3 REG[0b000] RM[nnn] UIMM8()                                  | REG0=GPR8_B():w IMM0:r:b
MOV                  | DATAXFER       | BASE           | I86            | 0xC6 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() UIMM8()                           | MEM0:w:b IMM0:r:b
MOV                  | DATAXFER       | BASE           | I86            | 0xC7 MOD[0b11] MOD=3 REG[0b000] RM[nnn] SIMMz()                                  | REG0=GPRv_B():w IMM0:r:z
MOV                  | DATAXFER       | BASE           | I86            | 0xC7 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() SIMMz()                           | MEM0:w:v IMM0:r:z
MOV                  | DATAXFER       | BASE           | I86            | 0x88 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPR8_B():w REG1=GPR8_R():r
MOV                  | DATAXFER       | BASE           | I86            | 0x88 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | MEM0:w:b REG0=GPR8_R():r
MOV                  | DATAXFER       | BASE           | I86            | 0x89 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | MEM0:w:v REG0=GPRv_R():r
MOV                  | DATAXFER       | BASE           | I86            | 0x89 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPRv_B():w REG1=GPRv_R():r
MOV                  | DATAXFER       | BASE           | I86            | 0x8A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=GPR8_R():w MEM0:r:b
MOV                  | DATAXFER       | BASE           | I86            | 0x8A MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPR8_R():w REG1=GPR8_B():r
MOV                  | DATAXFER       | BASE           | I86            | 0x8B MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=GPRv_R():w MEM0:r:v
MOV                  | DATAXFER       | BASE           | I86            | 0x8B MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPRv_R():w REG1=GPRv_B():r
MOV                  | DATAXFER       | BASE           | I86            | 0x8C MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | MEM0:w:w REG0=SEG():r
MOV                  | DATAXFER       | BASE           | I86            | 0x8C MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPRv_B():w REG1=SEG():r
MOV                  | DATAXFER       | BASE           | I86            | 0x8E MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=SEG_MOV():w MEM0:r:w
MOV                  | DATAXFER       | BASE           | I86            | 0x8E MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=SEG_MOV():w REG1=GPR16_B():r
MOV                  | DATAXFER       | BASE           | I86            | 0xA0 MEMDISPv()   OVERRIDE_SEG0()                                                | REG0=XED_REG_AL:w:IMPL MEM0:r:b SEG0=FINAL_DSEG():r:SUPP BASE0=XED_REG_INVALID:r:ECOND INDEX=XED_REG_INVALID:r:ECOND
MOV                  | DATAXFER       | BASE           | I86            | 0xA1 MEMDISPv() OVERRIDE_SEG0()                                                  | REG0=OrAX():w:IMPL MEM0:r:v SEG0=FINAL_DSEG():r:SUPP BASE0=XED_REG_INVALID:r:ECOND INDEX=XED_REG_INVALID:r:ECOND
MOV                  | DATAXFER       | BASE           | I86            | 0xA2 MEMDISPv()  OVERRIDE_SEG0()                                                 | MEM0:w:b REG0=XED_REG_AL:r:IMPL SEG0=FINAL_DSEG():r:SUPP BASE0=XED_REG_INVALID:r:ECOND INDEX=XED_REG_INVALID:r:ECOND
MOV                  | DATAXFER       | BASE           | I86            | 0xA3 MEMDISPv() OVERRIDE_SEG0()                                                  | MEM0:w:v REG0=OrAX():r:IMPL  SEG0=FINAL_DSEG():r:SUPP BASE0=XED_REG_INVALID:r:ECOND INDEX=XED_REG_INVALID:r:ECOND
MOV                  | DATAXFER       | BASE           | I86            | 0b1011_0 SRM[rrr] UIMM8()                                                        | REG0=GPR8_SB():w IMM0:r:b
MOV                  | DATAXFER       | BASE           | I86            | 0b1011_1 SRM[rrr] UIMMv()                                                        | REG0=GPRv_SB():w IMM0:r:v
MOV_CR               | DATAXFER       | BASE           | I86            | 0x0F 0x22 MOD[mm] REG[rrr] RM[nnn] CR_WIDTH() not64                              | REG0=CR_R():w REG1=GPR32_B():r
MOV_CR               | DATAXFER       | BASE           | I86            | 0x0F 0x22 MOD[mm] REG[rrr] RM[nnn] CR_WIDTH() mode64                             | REG0=CR_R():w REG1=GPR64_B():r
MOV_CR               | DATAXFER       | BASE           | I86            | 0x0F 0x20 MOD[mm] REG[rrr] RM[nnn] CR_WIDTH() not64                              | REG0=GPR32_B():w REG1=CR_R():r
MOV_CR               | DATAXFER       | BASE           | I86            | 0x0F 0x20 MOD[mm] REG[rrr] RM[nnn] CR_WIDTH() mode64                             | REG0=GPR64_B():w REG1=CR_R():r
MOV_DR               | DATAXFER       | BASE           | I86            | 0x0F 0x23 MOD[mm] REG[rrr] RM[nnn] CR_WIDTH() not64                              | REG0=DR_R():w REG1=GPR32_B():r
MOV_DR               | DATAXFER       | BASE           | I86            | 0x0F 0x23 MOD[mm] REG[rrr] RM[nnn] CR_WIDTH() mode64                             | REG0=DR_R():w REG1=GPR64_B():r
MOV_DR               | DATAXFER       | BASE           | I86            | 0x0F 0x21 MOD[mm] REG[rrr] RM[nnn] CR_WIDTH() not64                              | REG0=GPR32_B():w REG1=DR_R():r
MOV_DR               | DATAXFER       | BASE           | I86            | 0x0F 0x21 MOD[mm] REG[rrr] RM[nnn] CR_WIDTH() mode64                             | REG0=GPR64_B():w REG1=DR_R():r
MOVD                 | DATAXFER       | MMX            | PENTIUMMMX     | 0x0F 0x6E no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  mode64 norexw_prefix MODRM() | REG0=MMX_R():w:q MEM0:r:d
MOVD                 | DATAXFER       | MMX            | PENTIUMMMX     | 0x0F 0x6E no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  mode64 norexw_prefix | REG0=MMX_R():w:q REG1=GPR32_B():r
MOVD                 | DATAXFER       | MMX            | PENTIUMMMX     | 0x0F 0x6E no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  not64  MODRM()     | REG0=MMX_R():w:q MEM0:r:d
MOVD                 | DATAXFER       | MMX            | PENTIUMMMX     | 0x0F 0x6E no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  not64             | REG0=MMX_R():w:q REG1=GPR32_B():r
MOVD                 | DATAXFER       | MMX            | PENTIUMMMX     | 0x0F 0x7E no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  mode64 norexw_prefix MODRM() | MEM0:w:d REG0=MMX_R():r:d
MOVD                 | DATAXFER       | MMX            | PENTIUMMMX     | 0x0F 0x7E no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  mode64 norexw_prefix | REG0=GPR32_B():w REG1=MMX_R():r:d
MOVD                 | DATAXFER       | MMX            | PENTIUMMMX     | 0x0F 0x7E no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  not64 MODRM()      | MEM0:w:d REG0=MMX_R():r:d
MOVD                 | DATAXFER       | MMX            | PENTIUMMMX     | 0x0F 0x7E no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  not64             | REG0=GPR32_B():w REG1=MMX_R():r:d
MOVDIR64B            | MOVDIR         | MOVDIR         | MOVDIR         | 0x0F 0x38 0xF8 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  osz_refining_prefix   not64 | REG0=A_GPR_R():r MEM0:r:zd:u32 MEM1:w:zd:SUPP BASE1=A_GPR_R():r:SUPP  SEG1=XED_REG_ES:r:SUPP
MOVDIR64B            | MOVDIR         | MOVDIR         | MOVDIR         | 0x0F 0x38 0xF8 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  osz_refining_prefix   mode64 | REG0=A_GPR_R():r MEM0:r:zd:u32 MEM1:w:zd:SUPP BASE1=A_GPR_R():r:SUPP
MOVDIRI              | MOVDIR         | MOVDIR         | MOVDIR         | 0x0F 0x38 0xF9 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  no_refining_prefix      norexw_prefix | MEM0:w:d:u32 REG0=GPR32_R():r:d:u32
MOVDIRI              | MOVDIR         | MOVDIR         | MOVDIR         | 0x0F 0x38 0xF9 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  no_refining_prefix      mode64 rexw_prefix | MEM0:w:q:u64 REG0=GPR64_R():r:q:u64
MOVNTQ               | DATAXFER       | MMX            | PENTIUMMMX     | 0x0F 0xE7 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | MEM0:w:q REG0=MMX_R():r:q
MOVQ                 | DATAXFER       | MMX            | PENTIUMMMX     | 0x0F 0x6E no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  mode64 rexw_prefix MODRM() | REG0=MMX_R():w:q MEM0:r:q
MOVQ                 | DATAXFER       | MMX            | PENTIUMMMX     | 0x0F 0x6E no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  mode64 rexw_prefix | REG0=MMX_R():w:q REG1=GPR64_B():r
MOVQ                 | DATAXFER       | MMX            | PENTIUMMMX     | 0x0F 0x7E no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  mode64 rexw_prefix MODRM() | MEM0:w:q REG0=MMX_R():r:q
MOVQ                 | DATAXFER       | MMX            | PENTIUMMMX     | 0x0F 0x7E no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  mode64 rexw_prefix | REG0=GPR64_B():w REG1=MMX_R():r:q
MOVQ                 | DATAXFER       | MMX            | PENTIUMMMX     | 0x0F 0x6F no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():w:q MEM0:r:q
MOVQ                 | DATAXFER       | MMX            | PENTIUMMMX     | 0x0F 0x6F no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():w:q REG1=MMX_B():r:q
MOVQ                 | DATAXFER       | MMX            | PENTIUMMMX     | 0x0F 0x7F no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | MEM0:w:q REG0=MMX_R():r:q
MOVQ                 | DATAXFER       | MMX            | PENTIUMMMX     | 0x0F 0x7F no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_B():w:q REG1=MMX_R():r:q
MOVSB                | STRINGOP       | BASE           | I86            | 0xA4 norep OVERRIDE_SEG1()                                                       | MEM0:w:SUPP:b BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP  MEM1:r:SUPP:b BASE1=ArSI():rw:SUPP SEG1=FINAL_DSEG1():r:SUPP
MOVSD                | STRINGOP       | BASE           | I386           | 0xA5 mode16 66_prefix  norep OVERRIDE_SEG1()                                     | MEM0:w:SUPP:d BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:r:SUPP:d BASE1=ArSI():rw:SUPP SEG1=FINAL_DSEG1():r:SUPP
MOVSD                | STRINGOP       | BASE           | I386           | 0xA5 mode32 no66_prefix  norep OVERRIDE_SEG1()                                   | MEM0:w:SUPP:d BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:r:SUPP:d BASE1=ArSI():rw:SUPP SEG1=FINAL_DSEG1():r:SUPP
MOVSD                | STRINGOP       | BASE           | I386           | 0xA5 mode64 norexw_prefix no66_prefix  norep OVERRIDE_SEG1()                     | MEM0:w:SUPP:d BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:r:SUPP:d BASE1=ArSI():rw:SUPP SEG1=FINAL_DSEG1():r:SUPP
MOVSW                | STRINGOP       | BASE           | I86            | 0xA5 mode16 no66_prefix  norep  OVERRIDE_SEG1()                                  | MEM0:w:SUPP:w BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:r:SUPP:w BASE1=ArSI():rw:SUPP SEG1=FINAL_DSEG1():r:SUPP
MOVSW                | STRINGOP       | BASE           | I86            | 0xA5 mode32 66_prefix  norep  OVERRIDE_SEG1()                                    | MEM0:w:SUPP:w BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:r:SUPP:w BASE1=ArSI():rw:SUPP SEG1=FINAL_DSEG1():r:SUPP
MOVSW                | STRINGOP       | BASE           | I86            | 0xA5 mode64 norexw_prefix 66_prefix  norep  OVERRIDE_SEG1()                      | MEM0:w:SUPP:w BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:r:SUPP:w BASE1=ArSI():rw:SUPP SEG1=FINAL_DSEG1():r:SUPP
MOVSX                | DATAXFER       | BASE           | I386           | 0x0F 0xBE MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=GPRv_R():w MEM0:r:b
MOVSX                | DATAXFER       | BASE           | I386           | 0x0F 0xBE MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_R():w REG1=GPR8_B():r
MOVSX                | DATAXFER       | BASE           | I386           | 0x0F 0xBF MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=GPRv_R():w MEM0:r:w
MOVSX                | DATAXFER       | BASE           | I386           | 0x0F 0xBF MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_R():w REG1=GPR16_B():r
MOVZX                | DATAXFER       | BASE           | I386           | 0x0F 0xB6 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=GPRv_R():w MEM0:r:b
MOVZX                | DATAXFER       | BASE           | I386           | 0x0F 0xB6 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_R():w REG1=GPR8_B():r
MOVZX                | DATAXFER       | BASE           | I386           | 0x0F 0xB7 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | REG0=GPRv_R():w MEM0:r:w
MOVZX                | DATAXFER       | BASE           | I386           | 0x0F 0xB7 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_R():w REG1=GPR16_B():r
MUL                  | BINARY         | BASE           | I86            | 0xF6 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM()                                   | MEM0:r:b REG0=XED_REG_AL:r:SUPP REG1=XED_REG_AX:w:SUPP
MUL                  | BINARY         | BASE           | I86            | 0xF6 MOD[0b11] MOD=3 REG[0b100] RM[nnn]                                          | REG0=GPR8_B():r REG1=XED_REG_AL:r:SUPP REG2=XED_REG_AX:w:SUPP
MUL                  | BINARY         | BASE           | I86            | 0xF7 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM()                                   | MEM0:r:v REG0=OrAX():rw:SUPP REG1=OrDX():w:SUPP
MUL                  | BINARY         | BASE           | I86            | 0xF7 MOD[0b11] MOD=3 REG[0b100] RM[nnn]                                          | REG0=GPRv_B():r REG1=OrAX():rw:SUPP REG2=OrDX():w:SUPP
MWAIT                | MISC           | MONITOR        | MONITOR        | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b001] RM[0b001] no_refining_prefix                | REG0=XED_REG_EAX:r:SUPP REG1=XED_REG_ECX:r:SUPP
NEG                  | BINARY         | BASE           | I86            | 0xF6 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM() nolock_prefix                     | MEM0:rw:b
NEG                  | BINARY         | BASE           | I86            | 0xF6 MOD[0b11] MOD=3 REG[0b011] RM[nnn]                                          | REG0=GPR8_B():rw
NEG                  | BINARY         | BASE           | I86            | 0xF7 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM() nolock_prefix                     | MEM0:rw:v
NEG                  | BINARY         | BASE           | I86            | 0xF7 MOD[0b11] MOD=3 REG[0b011] RM[nnn]                                          | REG0=GPRv_B():rw
NEG_LOCK             | BINARY         | BASE           | I86            | 0xF6 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM() lock_prefix                       | MEM0:rw:b
NEG_LOCK             | BINARY         | BASE           | I86            | 0xF7 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM() lock_prefix                       | MEM0:rw:v
NOP                  | NOP            | BASE           | I86            | 0b1001_0 SRM[0b000] SRM=0  not_refining_f3 norexb_prefix                         | 
NOP                  | NOP            | BASE           | I86            | 0b1001_0 SRM[0b000] SRM=0  refining_f3 P4=0                                      | 
NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0x0F 0x18 MOD[0b11] MOD=3 REG[0b000] RM[nnn]                                     | REG0=GPRv_B():r
NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0x0F 0x18 MOD[0b11] MOD=3 REG[0b001] RM[nnn]                                     | REG0=GPRv_B():r
NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0x0F 0x18 MOD[0b11] MOD=3 REG[0b010] RM[nnn]                                     | REG0=GPRv_B():r
NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0x0F 0x18 MOD[0b11] MOD=3 REG[0b011] RM[nnn]                                     | REG0=GPRv_B():r
NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0x0F 0x18 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM()                              | MEM0:r:v
NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0x0F 0x18 MOD[0b11] MOD=3 REG[0b100] RM[nnn]                                     | REG0=GPRv_B():r
NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0x0F 0x18 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM()                              | MEM0:r:v
NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0x0F 0x18 MOD[0b11] MOD=3 REG[0b101] RM[nnn]                                     | REG0=GPRv_B():r
NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0x0F 0x18 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM()                              | MEM0:r:v
NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0x0F 0x18 MOD[0b11] MOD=3 REG[0b110] RM[nnn]                                     | REG0=GPRv_B():r
NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0x0F 0x18 MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM()                              | MEM0:r:v
NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0x0F 0x18 MOD[0b11] MOD=3 REG[0b111] RM[nnn]                                     | REG0=GPRv_B():r
NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0x0F 0x19 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:r:v REG0=GPRv_R():r
NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0x0F 0x19 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_B():r REG1=GPRv_R():r
NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0x0F 0x1A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:r:v REG0=GPRv_R():r
NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0x0F 0x1A MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_B():r REG1=GPRv_R():r
NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0x0F 0x1B MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:r:v REG0=GPRv_R():r
NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0x0F 0x1B MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_B():r REG1=GPRv_R():r
NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0x0F 0x1C MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:r:v REG0=GPRv_R():r
NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0x0F 0x1C MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_B():r REG1=GPRv_R():r
NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0x0F 0x1D MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:r:v REG0=GPRv_R():r
NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0x0F 0x1D MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_B():r REG1=GPRv_R():r
NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0x0F 0x1E MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:r:v REG0=GPRv_R():r
NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0x0F 0x1E MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_B():r REG1=GPRv_R():r
NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0x0F 0x1F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:r:v REG0=GPRv_R():r
NOP                  | WIDENOP        | BASE           | FAT_NOP        | 0x0F 0x1F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_B():r REG1=GPRv_R():r
NOP                  | WIDENOP        | BASE           | KNC_MISC       | 0x0F 0x1F MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM()                              | MEM0:r:v
NOP                  | WIDENOP        | BASE           | KNC_MISC       | 0x0F 0x1F MOD[0b11] MOD=3 REG[0b000] RM[nnn]                                     | REG0=GPRv_B():r
NOP                  | WIDENOP        | BASE           | PPRO           | 0x0F 0x1E MOD[0b11] MOD=3  REG[0b111] RM[0b010]  f3_refining_prefix CET=0        | REG0=GPRv_B():r REG1=GPRv_R():r
NOP                  | WIDENOP        | BASE           | PPRO           | 0x0F 0x1E MOD[0b11] MOD=3  REG[0b111] RM[0b011]  f3_refining_prefix CET=0        | REG0=GPRv_B():r REG1=GPRv_R():r
NOP                  | WIDENOP        | BASE           | PPRO           | 0x0F 0x1E MOD[0b11] MOD=3  REG[0b001] RM[nnn]  f3_refining_prefix W0 CET=0       | REG0=GPRv_B():r REG1=GPRv_R():r
NOP                  | WIDENOP        | BASE           | PPRO           | 0x0F 0x1E MOD[0b11] MOD=3  REG[0b001] RM[nnn]  f3_refining_prefix W1 mode64  CET=0 | REG0=GPRv_B():r REG1=GPRv_R():r
NOP                  | WIDENOP        | BASE           | PPRO           | 0x0F 0x1C MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() f2_refining_prefix           | MEM0:r:v REG0=GPRv_R():r
NOP                  | WIDENOP        | BASE           | PPRO           | 0x0F 0x1C MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() f3_refining_prefix           | MEM0:r:v REG0=GPRv_R():r
NOP                  | WIDENOP        | BASE           | PPRO           | 0x0F 0x1C MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() osz_refining_prefix          | MEM0:r:v REG0=GPRv_R():r
NOP                  | WIDENOP        | BASE           | PPRO           | 0x0F 0x1C MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM()                              | MEM0:r:v REG0=GPRv_R():r
NOP                  | WIDENOP        | BASE           | PPRO           | 0x0F 0x1C MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM()                              | MEM0:r:v REG0=GPRv_R():r
NOP                  | WIDENOP        | BASE           | PPRO           | 0x0F 0x1C MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM()                              | MEM0:r:v REG0=GPRv_R():r
NOP                  | WIDENOP        | BASE           | PPRO           | 0x0F 0x1C MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM()                              | MEM0:r:v REG0=GPRv_R():r
NOP                  | WIDENOP        | BASE           | PPRO           | 0x0F 0x1C MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM()                              | MEM0:r:v REG0=GPRv_R():r
NOP                  | WIDENOP        | BASE           | PPRO           | 0x0F 0x1C MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM()                              | MEM0:r:v REG0=GPRv_R():r
NOP                  | WIDENOP        | BASE           | PPRO           | 0x0F 0x1C MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM()                              | MEM0:r:v REG0=GPRv_R():r
NOP                  | WIDENOP        | BASE           | PPRO           | 0x0F 0x1C MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_B():r REG1=GPRv_R():r
NOP                  | WIDENOP        | BASE           | PPRO           | 0x0F 0x1C MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() no_refining_prefix CLDEMOTE=0 | MEM0:r:v REG0=GPRv_R():r
NOP                  | WIDENOP        | BASE           | PPRO           | 0x0F 0x1A MPXMODE=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] no_refining_prefix          | REG0=GPRv_B():r REG1=GPRv_R():r
NOP                  | WIDENOP        | BASE           | PPRO           | 0x0F 0x1B MPXMODE=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] no_refining_prefix          | REG0=GPRv_B():r REG1=GPRv_R():r
NOP                  | WIDENOP        | BASE           | PPRO           | 0x0F 0x1B MPXMODE=1 MOD[0b11] MOD=3 REG[rrr] RM[nnn] f3_refining_prefix          | REG0=GPRv_B():r REG1=GPRv_R():r
NOP                  | WIDENOP        | BASE           | PPRO           | 0x0F 0x1A MPXMODE=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                             | REG0=GPRv_B():r REG1=GPRv_R():r
NOP                  | WIDENOP        | BASE           | PPRO           | 0x0F 0x1B MPXMODE=0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                             | REG0=GPRv_B():r REG1=GPRv_R():r
NOP                  | WIDENOP        | BASE           | PPRO           | 0x0F 0x1A MPXMODE=0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                      | REG0=GPRv_B():r MEM0:r:v
NOP                  | WIDENOP        | BASE           | PPRO           | 0x0F 0x1B MPXMODE=0 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                     | REG0=GPRv_B():r MEM0:r:v
NOP                  | WIDENOP        | BASE           | PREFETCH_NOP   | 0x0F 0x0D MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_B():r REG1=GPRv_R():r
NOT                  | LOGICAL        | BASE           | I86            | 0xF6 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM() nolock_prefix                     | MEM0:rw:b
NOT                  | LOGICAL        | BASE           | I86            | 0xF6 MOD[0b11] MOD=3 REG[0b010] RM[nnn]                                          | REG0=GPR8_B():rw
NOT                  | LOGICAL        | BASE           | I86            | 0xF7 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM() nolock_prefix                     | MEM0:rw:v
NOT                  | LOGICAL        | BASE           | I86            | 0xF7 MOD[0b11] MOD=3 REG[0b010] RM[nnn]                                          | REG0=GPRv_B():rw
NOT_LOCK             | LOGICAL        | BASE           | I86            | 0xF6 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM() lock_prefix                       | MEM0:rw:b
NOT_LOCK             | LOGICAL        | BASE           | I86            | 0xF7 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM() lock_prefix                       | MEM0:rw:v
OR                   | LOGICAL        | BASE           | I86            | 0x80 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM() SIMM8() nolock_prefix             | MEM0:rw:b IMM0:r:b:i8
OR                   | LOGICAL        | BASE           | I86            | 0x80 MOD[0b11] MOD=3 REG[0b001] RM[nnn] SIMM8()                                  | REG0=GPR8_B():rw IMM0:r:b:i8
OR                   | LOGICAL        | BASE           | I86            | 0x81 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM() SIMMz() nolock_prefix             | MEM0:rw:v IMM0:r:z
OR                   | LOGICAL        | BASE           | I86            | 0x81 MOD[0b11] MOD=3 REG[0b001] RM[nnn] SIMMz()                                  | REG0=GPRv_B():rw IMM0:r:z
OR                   | LOGICAL        | BASE           | I86            | 0x82 MOD[mm] MOD!=3 REG[0b001] RM[nnn] not64 MODRM() SIMM8() nolock_prefix       | MEM0:rw:b IMM0:r:b:i8
OR                   | LOGICAL        | BASE           | I86            | 0x82 MOD[0b11] MOD=3 REG[0b001] RM[nnn] not64 SIMM8()                            | REG0=GPR8_B():rw IMM0:r:b:i8
OR                   | LOGICAL        | BASE           | I86            | 0x83 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM() SIMM8() nolock_prefix             | MEM0:rw:v IMM0:r:b:i8
OR                   | LOGICAL        | BASE           | I86            | 0x83 MOD[0b11] MOD=3 REG[0b001] RM[nnn] SIMM8()                                  | REG0=GPRv_B():rw IMM0:r:b:i8
OR                   | LOGICAL        | BASE           | I86            | 0x08 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                       | MEM0:rw:b REG0=GPR8_R():r
OR                   | LOGICAL        | BASE           | I86            | 0x08 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPR8_B():rw REG1=GPR8_R():r
OR                   | LOGICAL        | BASE           | I86            | 0x09 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                       | MEM0:rw:v REG0=GPRv_R():r
OR                   | LOGICAL        | BASE           | I86            | 0x09 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPRv_B():rw REG1=GPRv_R():r
OR                   | LOGICAL        | BASE           | I86            | 0x0A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=GPR8_R():rw MEM0:r:b
OR                   | LOGICAL        | BASE           | I86            | 0x0A MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPR8_R():rw REG1=GPR8_B():r
OR                   | LOGICAL        | BASE           | I86            | 0x0B MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=GPRv_R():rw MEM0:r:v
OR                   | LOGICAL        | BASE           | I86            | 0x0B MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPRv_R():rw REG1=GPRv_B():r
OR                   | LOGICAL        | BASE           | I86            | 0x0C UIMM8()                                                                     | REG0=XED_REG_AL:rw:IMPL IMM0:r:b
OR                   | LOGICAL        | BASE           | I86            | 0x0D SIMMz()                                                                     | REG0=OrAX():rw:IMPL IMM0:r:z
OR_LOCK              | LOGICAL        | BASE           | I86            | 0x80 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM() SIMM8() lock_prefix               | MEM0:rw:b IMM0:r:b:i8
OR_LOCK              | LOGICAL        | BASE           | I86            | 0x81 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM() SIMMz() lock_prefix               | MEM0:rw:v IMM0:r:z
OR_LOCK              | LOGICAL        | BASE           | I86            | 0x82 MOD[mm] MOD!=3 REG[0b001] RM[nnn] not64 MODRM() SIMM8() lock_prefix         | MEM0:rw:b IMM0:r:b:i8
OR_LOCK              | LOGICAL        | BASE           | I86            | 0x83 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM() SIMM8() lock_prefix               | MEM0:rw:v IMM0:r:b:i8
OR_LOCK              | LOGICAL        | BASE           | I86            | 0x08 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                         | MEM0:rw:b REG0=GPR8_R():r
OR_LOCK              | LOGICAL        | BASE           | I86            | 0x09 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                         | MEM0:rw:v REG0=GPRv_R():r
OUT                  | IO             | BASE           | I86            | 0xE6 UIMM8()                                                                     | IMM0:r:b REG0=XED_REG_AL:r:IMPL
OUT                  | IO             | BASE           | I86            | 0xE7 UIMM8() IMMUNE_REXW()                                                       | IMM0:r:b REG0=OeAX():r:IMPL
OUT                  | IO             | BASE           | I86            | 0xEE                                                                             | REG0=XED_REG_DX:r:IMPL REG1=XED_REG_AL:r:IMPL
OUT                  | IO             | BASE           | I86            | 0xEF IMMUNE_REXW()                                                               | REG0=XED_REG_DX:r:IMPL REG1=OeAX():r:IMPL
OUTSB                | IOSTRINGOP     | BASE           | I186           | 0x6E norep OVERRIDE_SEG0()                                                       | REG0=XED_REG_DX:r:SUPP MEM0:r:SUPP:b BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP
OUTSD                | IOSTRINGOP     | BASE           | I386           | 0x6F mode16 66_prefix  norep OVERRIDE_SEG0()                                     | REG0=XED_REG_DX:r:SUPP MEM0:r:SUPP:d BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP
OUTSD                | IOSTRINGOP     | BASE           | I386           | 0x6F mode32 no66_prefix  norep OVERRIDE_SEG0()                                   | REG0=XED_REG_DX:r:SUPP MEM0:r:SUPP:d BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP
OUTSD                | IOSTRINGOP     | BASE           | I386           | 0x6F mode64 norexw_prefix no66_prefix  norep OVERRIDE_SEG0()                     | REG0=XED_REG_DX:r:SUPP MEM0:r:SUPP:d BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP
OUTSD                | IOSTRINGOP     | BASE           | I386           | 0x6F mode64 rexw_prefix  norep OVERRIDE_SEG0()                                   | REG0=XED_REG_DX:r:SUPP MEM0:r:SUPP:d BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP
OUTSW                | IOSTRINGOP     | BASE           | I186           | 0x6F mode16 no66_prefix norep OVERRIDE_SEG0()                                    | REG0=XED_REG_DX:r:SUPP MEM0:r:SUPP:w BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP
OUTSW                | IOSTRINGOP     | BASE           | I186           | 0x6F mode32 66_prefix norep OVERRIDE_SEG0()                                      | REG0=XED_REG_DX:r:SUPP MEM0:r:SUPP:w BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP
OUTSW                | IOSTRINGOP     | BASE           | I186           | 0x6F mode64 norexw_prefix 66_prefix  norep OVERRIDE_SEG0()                       | REG0=XED_REG_DX:r:SUPP MEM0:r:SUPP:w BASE0=ArSI():rw:SUPP SEG0=FINAL_DSEG():r:SUPP
PABSB                | MMX            | SSSE3          | SSSE3MMX       | 0x0F 0x38 0x1C no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()       | REG0=MMX_R():w:q MEM0:r:q
PABSB                | MMX            | SSSE3          | SSSE3MMX       | 0x0F 0x38 0x1C no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]               | REG0=MMX_R():w:q REG1=MMX_B():r:q
PABSD                | MMX            | SSSE3          | SSSE3MMX       | 0x0F 0x38 0x1E no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()       | REG0=MMX_R():w:q MEM0:r:q
PABSD                | MMX            | SSSE3          | SSSE3MMX       | 0x0F 0x38 0x1E no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]               | REG0=MMX_R():w:q REG1=MMX_B():r:q
PABSW                | MMX            | SSSE3          | SSSE3MMX       | 0x0F 0x38 0x1D no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()       | REG0=MMX_R():w:q MEM0:r:q
PABSW                | MMX            | SSSE3          | SSSE3MMX       | 0x0F 0x38 0x1D no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]               | REG0=MMX_R():w:q REG1=MMX_B():r:q
PACKSSDW             | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x6B no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q:i32 MEM0:r:q:i32
PACKSSDW             | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x6B no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q:i32 REG1=MMX_B():r:q:i32
PACKSSWB             | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x63 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q:i16 MEM0:r:q:i16
PACKSSWB             | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x63 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q:i16 REG1=MMX_B():r:q:i16
PACKUSWB             | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x67 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q:i16 MEM0:r:q:i16
PACKUSWB             | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x67 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q:i16 REG1=MMX_B():r:q:i16
PADDB                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xFC no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q MEM0:r:q
PADDB                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xFC no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q REG1=MMX_B():r:q
PADDD                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xFE no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q MEM0:r:q
PADDD                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xFE no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q REG1=MMX_B():r:q
PADDQ                | MMX            | SSE2           | SSE2MMX        | 0x0F 0xD4 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q:u64 MEM0:r:q:u64
PADDQ                | MMX            | SSE2           | SSE2MMX        | 0x0F 0xD4 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q:u64 REG1=MMX_B():r:q:u64
PADDSB               | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xEC no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q MEM0:r:q
PADDSB               | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xEC no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q REG1=MMX_B():r:q
PADDSW               | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xED no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q MEM0:r:q
PADDSW               | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xED no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q REG1=MMX_B():r:q
PADDUSB              | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xDC no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q MEM0:r:q
PADDUSB              | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xDC no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q REG1=MMX_B():r:q
PADDUSW              | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xDD no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q MEM0:r:q
PADDUSW              | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xDD no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q REG1=MMX_B():r:q
PADDW                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xFD no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q MEM0:r:q
PADDW                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xFD no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q REG1=MMX_B():r:q
PALIGNR              | MMX            | SSSE3          | SSSE3MMX       | 0x0F 0x3A 0x0F no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM() UIMM8() | REG0=MMX_R():rw:q MEM0:r:q IMM0:r:b
PALIGNR              | MMX            | SSSE3          | SSSE3MMX       | 0x0F 0x3A 0x0F no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  UIMM8()      | REG0=MMX_R():rw:q REG1=MMX_B():r:q IMM0:r:b
PAND                 | LOGICAL        | MMX            | PENTIUMMMX     | 0x0F 0xDB no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q MEM0:r:q
PAND                 | LOGICAL        | MMX            | PENTIUMMMX     | 0x0F 0xDB no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q REG1=MMX_B():r:q
PANDN                | LOGICAL        | MMX            | PENTIUMMMX     | 0x0F 0xDF no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q MEM0:r:q
PANDN                | LOGICAL        | MMX            | PENTIUMMMX     | 0x0F 0xDF no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q REG1=MMX_B():r:q
PAUSE                | MISC           | PAUSE          | PAUSE          | 0b1001_0 SRM[0b000] SRM=0  refining_f3 P4=1                                      | 
PAVGB                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xE0 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q MEM0:r:q
PAVGB                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xE0 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q:i8 REG1=MMX_B():r:q:i8
PAVGW                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xE3 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q:i16 MEM0:r:q:i16
PAVGW                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xE3 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q:i16 REG1=MMX_B():r:q:i16
PCMPEQB              | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x74 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q:i8 MEM0:r:q:i8
PCMPEQB              | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x74 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q:i8 REG1=MMX_B():r:q:i8
PCMPEQD              | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x76 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q:i32 MEM0:r:q:i32
PCMPEQD              | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x76 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q:i32 REG1=MMX_B():r:q:i32
PCMPEQW              | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x75 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q:i16 MEM0:r:q:i16
PCMPEQW              | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x75 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q:i16 REG1=MMX_B():r:q:i16
PCMPESTRI            | SSE            | SSE4           | SSE42          | 0x0F 0x3A 0x61 osz_refining_prefix IMMUNE66() not64 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_EAX:r:SUPP REG2=XED_REG_EDX:r:SUPP REG3=XED_REG_ECX:w:SUPP
PCMPESTRI            | SSE            | SSE4           | SSE42          | 0x0F 0x3A 0x61 osz_refining_prefix IMMUNE66() not64  MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8() | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_EAX:r:SUPP REG3=XED_REG_EDX:r:SUPP REG4=XED_REG_ECX:w:SUPP
PCMPESTRI            | SSE            | SSE4           | SSE42          | 0x0F 0x3A 0x61 osz_refining_prefix IMMUNE66() mode64 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_EAX:r:SUPP REG2=XED_REG_EDX:r:SUPP REG3=XED_REG_ECX:w:SUPP
PCMPESTRI            | SSE            | SSE4           | SSE42          | 0x0F 0x3A 0x61 osz_refining_prefix IMMUNE66() mode64 norexw_prefix  MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8() | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_EAX:r:SUPP REG3=XED_REG_EDX:r:SUPP REG4=XED_REG_ECX:w:SUPP
PCMPESTRI64          | SSE            | SSE4           | SSE42          | 0x0F 0x3A 0x61 osz_refining_prefix IMMUNE66() mode64 rexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_RAX:r:SUPP REG2=XED_REG_RDX:r:SUPP REG3=XED_REG_RCX:w:SUPP
PCMPESTRI64          | SSE            | SSE4           | SSE42          | 0x0F 0x3A 0x61 osz_refining_prefix IMMUNE66() mode64 rexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8() | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_RAX:r:SUPP REG3=XED_REG_RDX:r:SUPP REG4=XED_REG_RCX:w:SUPP
PCMPESTRM            | SSE            | SSE4           | SSE42          | 0x0F 0x3A 0x60 osz_refining_prefix IMMUNE66() not64 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_EAX:r:SUPP REG2=XED_REG_EDX:r:SUPP REG3=XED_REG_XMM0:w:dq:SUPP
PCMPESTRM            | SSE            | SSE4           | SSE42          | 0x0F 0x3A 0x60 osz_refining_prefix IMMUNE66() not64 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8() | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_EAX:r:SUPP REG3=XED_REG_EDX:r:SUPP REG4=XED_REG_XMM0:w:dq:SUPP
PCMPESTRM            | SSE            | SSE4           | SSE42          | 0x0F 0x3A 0x60 osz_refining_prefix IMMUNE66() mode64 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_EAX:r:SUPP REG2=XED_REG_EDX:r:SUPP REG3=XED_REG_XMM0:w:dq:SUPP
PCMPESTRM            | SSE            | SSE4           | SSE42          | 0x0F 0x3A 0x60 osz_refining_prefix IMMUNE66() mode64 norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8() | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_EAX:r:SUPP REG3=XED_REG_EDX:r:SUPP REG4=XED_REG_XMM0:w:dq:SUPP
PCMPESTRM64          | SSE            | SSE4           | SSE42          | 0x0F 0x3A 0x60 osz_refining_prefix IMMUNE66() mode64 rexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_RAX:r:SUPP REG2=XED_REG_RDX:r:SUPP REG3=XED_REG_XMM0:w:dq:SUPP
PCMPESTRM64          | SSE            | SSE4           | SSE42          | 0x0F 0x3A 0x60 osz_refining_prefix IMMUNE66() mode64 rexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8() | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_RAX:r:SUPP REG3=XED_REG_RDX:r:SUPP REG4=XED_REG_XMM0:w:dq:SUPP
PCMPGTB              | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x64 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q:i8 MEM0:r:q:i8
PCMPGTB              | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x64 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q:i8 REG1=MMX_B():r:q:i8
PCMPGTD              | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x66 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q:i32 MEM0:r:q:i32
PCMPGTD              | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x66 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q:i32 REG1=MMX_B():r:q:i32
PCMPGTQ              | SSE            | SSE4           | SSE42          | 0x0F 0x38 0x37  osz_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  REFINING66() MODRM() | REG0=XMM_R():rw:dq     MEM0:r:dq
PCMPGTQ              | SSE            | SSE4           | SSE42          | 0x0F 0x38 0x37  osz_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  REFINING66() | REG0=XMM_R():rw:dq     REG1=XMM_B():r:dq
PCMPGTW              | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x65 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q:i16 MEM0:r:q:i16
PCMPGTW              | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x65 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q:i16 REG1=MMX_B():r:q:i16
PCMPISTRI            | SSE            | SSE4           | SSE42          | 0x0F 0x3A 0x63 osz_refining_prefix IMMUNE66() not64 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_ECX:w:SUPP
PCMPISTRI            | SSE            | SSE4           | SSE42          | 0x0F 0x3A 0x63 osz_refining_prefix IMMUNE66() not64 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8() | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_ECX:w:SUPP
PCMPISTRI            | SSE            | SSE4           | SSE42          | 0x0F 0x3A 0x63 osz_refining_prefix IMMUNE66() mode64 norexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_ECX:w:SUPP
PCMPISTRI            | SSE            | SSE4           | SSE42          | 0x0F 0x3A 0x63 osz_refining_prefix IMMUNE66() mode64 norexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8() | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_ECX:w:SUPP
PCMPISTRI64          | SSE            | SSE4           | SSE42          | 0x0F 0x3A 0x63 osz_refining_prefix IMMUNE66() mode64 rexw_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_RCX:w:SUPP
PCMPISTRI64          | SSE            | SSE4           | SSE42          | 0x0F 0x3A 0x63 osz_refining_prefix IMMUNE66() mode64 rexw_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8() | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_RCX:w:SUPP
PCMPISTRM            | SSE            | SSE4           | SSE42          | 0x0F 0x3A 0x62 osz_refining_prefix IMMUNE66() MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() | REG0=XMM_R():r:dq     MEM0:r:dq         IMM0:r:b REG1=XED_REG_XMM0:w:dq:SUPP
PCMPISTRM            | SSE            | SSE4           | SSE42          | 0x0F 0x3A 0x62 osz_refining_prefix IMMUNE66() MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8() | REG0=XMM_R():r:dq     REG1=XMM_B():r:dq IMM0:r:b REG2=XED_REG_XMM0:w:dq:SUPP
PCONFIG              | PCONFIG        | PCONFIG        | PCONFIG        | 0x0F 0x01 MOD[0b11] MOD=3  REG[0b000] RM[0b101]  no_refining_prefix              | REG0=XED_REG_EAX:rw:SUPP:d:u32 REG1=XED_REG_EBX:crw:SUPP:d:u32 REG2=XED_REG_ECX:crw:SUPP:d:u32 REG3=XED_REG_EDX:crw:SUPP:d:u32
PEXTRW               | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xC5 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  UIMM8()           | REG0=GPR32_R():w REG1=MMX_B():r:q:u16 IMM0:r:b
PHADDD               | MMX            | SSSE3          | SSSE3MMX       | 0x0F 0x38 0x02 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()       | REG0=MMX_R():rw:q MEM0:r:q
PHADDD               | MMX            | SSSE3          | SSSE3MMX       | 0x0F 0x38 0x02 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]               | REG0=MMX_R():rw:q REG1=MMX_B():r:q
PHADDSW              | MMX            | SSSE3          | SSSE3MMX       | 0x0F 0x38 0x03 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()       | REG0=MMX_R():rw:q MEM0:r:q
PHADDSW              | MMX            | SSSE3          | SSSE3MMX       | 0x0F 0x38 0x03 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]               | REG0=MMX_R():rw:q REG1=MMX_B():r:q
PHADDW               | MMX            | SSSE3          | SSSE3MMX       | 0x0F 0x38 0x01 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()       | REG0=MMX_R():rw:q MEM0:r:q
PHADDW               | MMX            | SSSE3          | SSSE3MMX       | 0x0F 0x38 0x01 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]               | REG0=MMX_R():rw:q REG1=MMX_B():r:q
PHSUBD               | MMX            | SSSE3          | SSSE3MMX       | 0x0F 0x38 0x06 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()       | REG0=MMX_R():rw:q MEM0:r:q
PHSUBD               | MMX            | SSSE3          | SSSE3MMX       | 0x0F 0x38 0x06 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]               | REG0=MMX_R():rw:q REG1=MMX_B():r:q
PHSUBSW              | MMX            | SSSE3          | SSSE3MMX       | 0x0F 0x38 0x07 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()       | REG0=MMX_R():rw:q MEM0:r:q
PHSUBSW              | MMX            | SSSE3          | SSSE3MMX       | 0x0F 0x38 0x07 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]               | REG0=MMX_R():rw:q REG1=MMX_B():r:q
PHSUBW               | MMX            | SSSE3          | SSSE3MMX       | 0x0F 0x38 0x05 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()       | REG0=MMX_R():rw:q MEM0:r:q
PHSUBW               | MMX            | SSSE3          | SSSE3MMX       | 0x0F 0x38 0x05 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]               | REG0=MMX_R():rw:q REG1=MMX_B():r:q
PINSRW               | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xC4 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM() UIMM8()    | REG0=MMX_R():rw:q:u16 MEM0:r:w:u16 IMM0:r:b
PINSRW               | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xC4 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  UIMM8()           | REG0=MMX_R():rw:q:u16 REG1=GPR32_B():r IMM0:r:b
PMADDUBSW            | MMX            | SSSE3          | SSSE3MMX       | 0x0F 0x38 0x04 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()       | REG0=MMX_R():rw:q:i8 MEM0:r:q:i8
PMADDUBSW            | MMX            | SSSE3          | SSSE3MMX       | 0x0F 0x38 0x04 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]               | REG0=MMX_R():rw:q:i8 REG1=MMX_B():r:q:i8
PMADDWD              | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xF5 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q:i16 MEM0:r:q:i16
PMADDWD              | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xF5 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q:i16 REG1=MMX_B():r:q:i16
PMAXSW               | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xEE no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q MEM0:r:q
PMAXSW               | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xEE no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q REG1=MMX_B():r:q
PMAXUB               | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xDE no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q MEM0:r:q
PMAXUB               | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xDE no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q REG1=MMX_B():r:q
PMINSW               | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xEA no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q MEM0:r:q
PMINSW               | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xEA no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q REG1=MMX_B():r:q
PMINUB               | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xDA no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q MEM0:r:q
PMINUB               | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xDA no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q REG1=MMX_B():r:q
PMOVMSKB             | MMX            | MMX            | SSE            | 0x0F 0xD7 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=GPR32_R():w REG1=MMX_B():r:q:i8
PMULHRSW             | MMX            | SSSE3          | SSSE3MMX       | 0x0F 0x38 0x0B no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()       | REG0=MMX_R():rw:q MEM0:r:q
PMULHRSW             | MMX            | SSSE3          | SSSE3MMX       | 0x0F 0x38 0x0B no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]               | REG0=MMX_R():rw:q REG1=MMX_B():r:q
PMULHUW              | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xE4 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q:u16 MEM0:r:q:u16
PMULHUW              | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xE4 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q:u16 REG1=MMX_B():r:q:u16
PMULHW               | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xE5 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q:i16 MEM0:r:q:i16
PMULHW               | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xE5 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q:i16 REG1=MMX_B():r:q:i16
PMULLW               | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xD5 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q:i16 MEM0:r:q:i16
PMULLW               | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xD5 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q:i16 REG1=MMX_B():r:q:i16
PMULUDQ              | MMX            | SSE2           | SSE2MMX        | 0x0F 0xF4 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q:u32 MEM0:r:q:u32
PMULUDQ              | MMX            | SSE2           | SSE2MMX        | 0x0F 0xF4 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q:u32 REG1=MMX_B():r:q:u32
POP                  | POP            | BASE           | I86            | 0x8F MOD[mm] MOD!=3 REG[0b000] RM[nnn] DF64() MODRM()                            | MEM0:w:v REG0=XED_REG_STACKPOP:r:spw:SUPP
POP                  | POP            | BASE           | I86            | 0x8F MOD[0b11] MOD=3 REG[0b000] RM[nnn] DF64()                                   | REG0=GPRv_B():w REG1=XED_REG_STACKPOP:r:spw:SUPP
POP                  | POP            | BASE           | I86            | 0x07 not64                                                                       | REG0=XED_REG_ES:w:IMPL REG1=XED_REG_STACKPOP:r:spw:SUPP
POP                  | POP            | BASE           | I86            | 0x1F not64                                                                       | REG0=XED_REG_DS:w:IMPL REG1=XED_REG_STACKPOP:r:spw:SUPP
POP                  | POP            | BASE           | I86            | 0b0101_1 SRM[rrr] DF64()                                                         | REG0=GPRv_SB():w REG1=XED_REG_STACKPOP:r:spw:SUPP
POP                  | POP            | BASE           | I86            | 0x0F 0xA1 DF64()                                                                 | REG0=XED_REG_FS:w:IMPL REG1=XED_REG_STACKPOP:r:spw:SUPP
POP                  | POP            | BASE           | I86            | 0x0F 0xA9 DF64()                                                                 | REG0=XED_REG_GS:w:IMPL REG1=XED_REG_STACKPOP:r:spw:SUPP
POPA                 | POP            | BASE           | I186           | 0x61 mode16 no66_prefix                                                          | REG0=XED_REG_STACKPOP:r:spw8:SUPP REG1=XED_REG_AX:w:SUPP REG2=XED_REG_CX:w:SUPP REG3=XED_REG_DX:w:SUPP REG4=XED_REG_BX:w:SUPP REG5=XED_REG_BP:w:SUPP REG6=XED_REG_SI:w:SUPP REG7=XED_REG_DI:w:SUPP
POPA                 | POP            | BASE           | I186           | 0x61 mode32 66_prefix                                                            | REG0=XED_REG_STACKPOP:r:spw8:SUPP REG1=XED_REG_AX:w:SUPP REG2=XED_REG_CX:w:SUPP REG3=XED_REG_DX:w:SUPP REG4=XED_REG_BX:w:SUPP REG5=XED_REG_BP:w:SUPP REG6=XED_REG_SI:w:SUPP REG7=XED_REG_DI:w:SUPP
POPAD                | POP            | BASE           | I386           | 0x61 mode16 66_prefix                                                            | REG0=XED_REG_STACKPOP:r:spw8:SUPP REG1=XED_REG_EAX:w:SUPP REG2=XED_REG_ECX:w:SUPP REG3=XED_REG_EDX:w:SUPP REG4=XED_REG_EBX:w:SUPP REG5=XED_REG_EBP:w:SUPP REG6=XED_REG_ESI:w:SUPP REG7=XED_REG_EDI:w:SUPP
POPAD                | POP            | BASE           | I386           | 0x61 mode32 no66_prefix                                                          | REG0=XED_REG_STACKPOP:r:spw8:SUPP REG1=XED_REG_EAX:w:SUPP REG2=XED_REG_ECX:w:SUPP REG3=XED_REG_EDX:w:SUPP REG4=XED_REG_EBX:w:SUPP REG5=XED_REG_EBP:w:SUPP REG6=XED_REG_ESI:w:SUPP REG7=XED_REG_EDI:w:SUPP
POPCNT               | SSE            | SSE4           | POPCNT         | 0x0F 0xB8  f3_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]   MODRM()          | REG0=GPRv_R():w:v     MEM0:r:v
POPCNT               | SSE            | SSE4           | POPCNT         | 0x0F 0xB8  f3_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                   | REG0=GPRv_R():w:v     REG1=GPRv_B():r:v
POPCNT_VEX           | KNCSCALAR      | KNC            | KNCV           | VV1 0xB8  VL128 VF3 V0F W0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                | REG0=GPR32_R():w:d   REG1=GPR32_B():r:d
POPCNT_VEX           | KNCSCALAR      | KNC            | KNCV           | VV1 0xB8  VL128 VF3 V0F W1 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                | REG0=GPR64_R():w:q  REG1=GPR64_B():r:q
POPF                 | POP            | BASE           | I86            | 0x9D  mode16 no66_prefix                                                         | REG0=XED_REG_STACKPOP:r:w:SUPP
POPF                 | POP            | BASE           | I86            | 0x9D mode32 66_prefix                                                            | REG0=XED_REG_STACKPOP:r:w:SUPP
POPF                 | POP            | BASE           | I86            | 0x9D mode64 norexw_prefix 66_prefix                                              | REG0=XED_REG_STACKPOP:r:w:SUPP
POPFD                | POP            | BASE           | I386           | 0x9D mode16 66_prefix                                                            | REG0=XED_REG_STACKPOP:r:d:SUPP
POPFD                | POP            | BASE           | I386           | 0x9D mode32 no66_prefix                                                          | REG0=XED_REG_STACKPOP:r:d:SUPP
POR                  | LOGICAL        | MMX            | PENTIUMMMX     | 0x0F 0xEB no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q MEM0:r:q
POR                  | LOGICAL        | MMX            | PENTIUMMMX     | 0x0F 0xEB no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q REG1=MMX_B():r:q
PREFETCH_EXCLUSIVE   | PREFETCH       | 3DNOW          | PREFETCH_NOP   | 0x0F 0x0D MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM()                              | MEM0:r:mprefetch
PREFETCH_RESERVED    | PREFETCH       | 3DNOW          | PREFETCH_NOP   | 0x0F 0x0D MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM()                              | MEM0:r:mprefetch
PREFETCH_RESERVED    | PREFETCH       | 3DNOW          | PREFETCH_NOP   | 0x0F 0x0D MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM()                              | MEM0:r:mprefetch
PREFETCH_RESERVED    | PREFETCH       | 3DNOW          | PREFETCH_NOP   | 0x0F 0x0D MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM()                              | MEM0:r:mprefetch
PREFETCH_RESERVED    | PREFETCH       | 3DNOW          | PREFETCH_NOP   | 0x0F 0x0D MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM()                              | MEM0:r:mprefetch
PREFETCH_RESERVED    | PREFETCH       | 3DNOW          | PREFETCH_NOP   | 0x0F 0x0D MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM()                              | MEM0:r:mprefetch
PREFETCHNTA          | PREFETCH       | SSE            | SSE_PREFETCH   | 0x0F 0x18 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM()                              | MEM0:r:mprefetch
PREFETCHT0           | PREFETCH       | SSE            | SSE_PREFETCH   | 0x0F 0x18 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM()                              | MEM0:r:mprefetch
PREFETCHT1           | PREFETCH       | SSE            | SSE_PREFETCH   | 0x0F 0x18 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM()                              | MEM0:r:mprefetch
PREFETCHT2           | PREFETCH       | SSE            | SSE_PREFETCH   | 0x0F 0x18 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM()                              | MEM0:r:mprefetch
PREFETCHW            | PREFETCH       | 3DNOW          | PREFETCH_NOP   | 0x0F 0x0D MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM()                              | MEM0:r:mprefetch
PREFETCHW            | PREFETCH       | 3DNOW          | PREFETCH_NOP   | 0x0F 0x0D MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM()                              | MEM0:r:mprefetch
PREFETCHWT1          | PREFETCHWT1    | PREFETCHWT1    | PREFETCHWT1    | 0x0F 0x0D MOD[mm] MOD!=3 REG[0b010] RM[nnn]  MODRM()                             | MEM0:r:b:u8
PSADBW               | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xF6 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q MEM0:r:q
PSADBW               | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xF6 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q REG1=MMX_B():r:q
PSHUFB               | MMX            | SSSE3          | SSSE3MMX       | 0x0F 0x38 0x00 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()       | REG0=MMX_R():rw:q MEM0:r:q
PSHUFB               | MMX            | SSSE3          | SSSE3MMX       | 0x0F 0x38 0x00 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]               | REG0=MMX_R():rw:q REG1=MMX_B():r:q
PSHUFW               | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x70 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM() UIMM8()    | REG0=MMX_R():w:q:u16 MEM0:r:q:u16 IMM0:r:b
PSHUFW               | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x70 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]  UIMM8()           | REG0=MMX_R():w:q:u16 REG1=MMX_B():r:q:u16 IMM0:r:b
PSIGNB               | MMX            | SSSE3          | SSSE3MMX       | 0x0F 0x38 0x08 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()       | REG0=MMX_R():rw:q MEM0:r:q
PSIGNB               | MMX            | SSSE3          | SSSE3MMX       | 0x0F 0x38 0x08 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]               | REG0=MMX_R():rw:q REG1=MMX_B():r:q
PSIGND               | MMX            | SSSE3          | SSSE3MMX       | 0x0F 0x38 0x0A no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()       | REG0=MMX_R():rw:q MEM0:r:q
PSIGND               | MMX            | SSSE3          | SSSE3MMX       | 0x0F 0x38 0x0A no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]               | REG0=MMX_R():rw:q REG1=MMX_B():r:q
PSIGNW               | MMX            | SSSE3          | SSSE3MMX       | 0x0F 0x38 0x09 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()       | REG0=MMX_R():rw:q MEM0:r:q
PSIGNW               | MMX            | SSSE3          | SSSE3MMX       | 0x0F 0x38 0x09 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]               | REG0=MMX_R():rw:q REG1=MMX_B():r:q
PSLLD                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x72 no_refining_prefix MOD[0b11] MOD=3 REG[0b110] RM[nnn]  UIMM8()         | REG0=MMX_B():rw:q:u32 IMM0:r:b
PSLLD                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xF2 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q:u32 MEM0:r:q
PSLLD                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xF2 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q:u32 REG1=MMX_B():r:q
PSLLQ                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x73 no_refining_prefix MOD[0b11] MOD=3 REG[0b110] RM[nnn]  UIMM8()         | REG0=MMX_B():rw:q:u64 IMM0:r:b
PSLLQ                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xF3 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q:u64 MEM0:r:q
PSLLQ                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xF3 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q:u64 REG1=MMX_B():r:q
PSLLW                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x71 no_refining_prefix MOD[0b11] MOD=3 REG[0b110] RM[nnn]  UIMM8()         | REG0=MMX_B():rw:q:u16 IMM0:r:b
PSLLW                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xF1 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q:u16 MEM0:r:q
PSLLW                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xF1 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q:u16 REG1=MMX_B():r:q
PSRAD                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x72 no_refining_prefix MOD[0b11] MOD=3 REG[0b100] RM[nnn]  UIMM8()         | REG0=MMX_B():rw:q:i32 IMM0:r:b
PSRAD                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xE2 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q:i32 MEM0:r:q
PSRAD                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xE2 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q:i32 REG1=MMX_B():r:q
PSRAW                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x71 no_refining_prefix MOD[0b11] MOD=3 REG[0b100] RM[nnn]  UIMM8()         | REG0=MMX_B():rw:q:i16 IMM0:r:b
PSRAW                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xE1 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q:i16 MEM0:r:q
PSRAW                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xE1 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q:i16 REG1=MMX_B():r:q
PSRLD                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x72 no_refining_prefix MOD[0b11] MOD=3 REG[0b010] RM[nnn]  UIMM8()         | REG0=MMX_B():rw:q:u32 IMM0:r:b
PSRLD                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xD2 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q:u32 MEM0:r:q
PSRLD                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xD2 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q:u32 REG1=MMX_B():r:q
PSRLQ                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x73 no_refining_prefix MOD[0b11] MOD=3 REG[0b010] RM[nnn]  UIMM8()         | REG0=MMX_B():rw:q:u64 IMM0:r:b
PSRLQ                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xD3 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q:u64 MEM0:r:q
PSRLQ                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xD3 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q:u64 REG1=MMX_B():r:q
PSRLW                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x71 no_refining_prefix MOD[0b11] MOD=3 REG[0b010] RM[nnn]  UIMM8()         | REG0=MMX_B():rw:q:u16 IMM0:r:b
PSRLW                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xD1 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q:u16 MEM0:r:q
PSRLW                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xD1 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q:u16 REG1=MMX_B():r:q
PSUBB                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xF8 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q MEM0:r:q
PSUBB                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xF8 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q REG1=MMX_B():r:q
PSUBD                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xFA no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q MEM0:r:q
PSUBD                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xFA no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q REG1=MMX_B():r:q
PSUBQ                | MMX            | SSE2           | SSE2MMX        | 0x0F 0xFB no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q MEM0:r:q
PSUBQ                | MMX            | SSE2           | SSE2MMX        | 0x0F 0xFB no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q REG1=MMX_B():r:q
PSUBSB               | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xE8 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q MEM0:r:q
PSUBSB               | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xE8 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q REG1=MMX_B():r:q
PSUBSW               | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xE9 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q MEM0:r:q
PSUBSW               | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xE9 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q REG1=MMX_B():r:q
PSUBUSB              | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xD8 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q MEM0:r:q
PSUBUSB              | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xD8 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q REG1=MMX_B():r:q
PSUBUSW              | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xD9 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q MEM0:r:q
PSUBUSW              | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xD9 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q REG1=MMX_B():r:q
PSUBW                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xF9 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q MEM0:r:q
PSUBW                | MMX            | MMX            | PENTIUMMMX     | 0x0F 0xF9 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q REG1=MMX_B():r:q
PUNPCKHBW            | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x68 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q MEM0:r:q
PUNPCKHBW            | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x68 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q REG1=MMX_B():r:d
PUNPCKHDQ            | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x6A no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q MEM0:r:q
PUNPCKHDQ            | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x6A no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q REG1=MMX_B():r:d
PUNPCKHWD            | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x69 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q MEM0:r:q
PUNPCKHWD            | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x69 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q REG1=MMX_B():r:d
PUNPCKLBW            | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x60 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q:u8 MEM0:r:d:u8
PUNPCKLBW            | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x60 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q:u8 REG1=MMX_B():r:d:u8
PUNPCKLDQ            | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x62 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q:u32 MEM0:r:d:u32
PUNPCKLDQ            | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x62 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q:u32 REG1=MMX_B():r:d:u32
PUNPCKLWD            | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x61 no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q:u16 MEM0:r:d:u16
PUNPCKLWD            | MMX            | MMX            | PENTIUMMMX     | 0x0F 0x61 no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q:u16 REG1=MMX_B():r:d:u16
PUSH                 | PUSH           | BASE           | I186           | 0x68 DF64() SIMMz()                                                              | IMM0:r:z REG0=XED_REG_STACKPUSH:w:spw:SUPP
PUSH                 | PUSH           | BASE           | I186           | 0x6A DF64() SIMM8()                                                              | IMM0:r:b:i8 REG0=XED_REG_STACKPUSH:w:spw:SUPP
PUSH                 | PUSH           | BASE           | I86            | 0xFF MOD[mm] MOD!=3 REG[0b110] RM[nnn] DF64() MODRM()                            | MEM0:r:v REG0=XED_REG_STACKPUSH:w:spw:SUPP
PUSH                 | PUSH           | BASE           | I86            | 0xFF MOD[0b11] MOD=3 REG[0b110] RM[nnn] DF64()                                   | REG0=GPRv_B():r REG1=XED_REG_STACKPUSH:w:spw:SUPP
PUSH                 | PUSH           | BASE           | I86            | 0x06 not64                                                                       | REG0=XED_REG_ES:r:IMPL REG1=XED_REG_STACKPUSH:w:spw:SUPP
PUSH                 | PUSH           | BASE           | I86            | 0x0E not64                                                                       | REG0=XED_REG_CS:r:IMPL REG1=XED_REG_STACKPUSH:w:spw:SUPP
PUSH                 | PUSH           | BASE           | I86            | 0x16 not64                                                                       | REG0=XED_REG_SS:r:IMPL REG1=XED_REG_STACKPUSH:w:spw:SUPP
PUSH                 | PUSH           | BASE           | I86            | 0x1E not64                                                                       | REG0=XED_REG_DS:r:IMPL REG1=XED_REG_STACKPUSH:w:spw:SUPP
PUSH                 | PUSH           | BASE           | I86            | 0b0101_0 SRM[rrr] DF64()                                                         | REG0=GPRv_SB():r REG1=XED_REG_STACKPUSH:w:spw:SUPP
PUSH                 | PUSH           | BASE           | I86            | 0x0F 0xA0 DF64()                                                                 | REG0=XED_REG_FS:r:IMPL REG1=XED_REG_STACKPUSH:w:spw:SUPP
PUSH                 | PUSH           | BASE           | I86            | 0x0F 0xA8 DF64()                                                                 | REG0=XED_REG_GS:r:IMPL REG1=XED_REG_STACKPUSH:w:spw:SUPP
PUSHA                | PUSH           | BASE           | I186           | 0x60 mode16 no66_prefix                                                          | REG0=XED_REG_STACKPUSH:w:spw8:SUPP REG1=XED_REG_AX:r:SUPP REG2=XED_REG_CX:r:SUPP REG3=XED_REG_DX:r:SUPP REG4=XED_REG_BX:r:SUPP REG5=XED_REG_SP:r:SUPP REG6=XED_REG_BP:r:SUPP REG7=XED_REG_SI:r:SUPP REG8=XED_REG_DI:r:SUPP
PUSHA                | PUSH           | BASE           | I186           | 0x60 mode32 66_prefix                                                            | REG0=XED_REG_STACKPUSH:w:spw8:SUPP REG1=XED_REG_AX:r:SUPP REG2=XED_REG_CX:r:SUPP REG3=XED_REG_DX:r:SUPP REG4=XED_REG_BX:r:SUPP REG5=XED_REG_SP:r:SUPP REG6=XED_REG_BP:r:SUPP REG7=XED_REG_SI:r:SUPP REG8=XED_REG_DI:r:SUPP
PUSHAD               | PUSH           | BASE           | I386           | 0x60 mode16 66_prefix                                                            | REG0=XED_REG_STACKPUSH:w:spw8:SUPP REG1=XED_REG_EAX:r:SUPP REG2=XED_REG_ECX:r:SUPP REG3=XED_REG_EDX:r:SUPP REG4=XED_REG_EBX:r:SUPP REG5=XED_REG_ESP:r:SUPP REG6=XED_REG_EBP:r:SUPP REG7=XED_REG_ESI:r:SUPP REG8=XED_REG_EDI:r:SUPP
PUSHAD               | PUSH           | BASE           | I386           | 0x60 mode32 no66_prefix                                                          | REG0=XED_REG_STACKPUSH:w:spw8:SUPP REG1=XED_REG_EAX:r:SUPP REG2=XED_REG_ECX:r:SUPP REG3=XED_REG_EDX:r:SUPP REG4=XED_REG_EBX:r:SUPP REG5=XED_REG_ESP:r:SUPP REG6=XED_REG_EBP:r:SUPP REG7=XED_REG_ESI:r:SUPP REG8=XED_REG_EDI:r:SUPP
PUSHF                | PUSH           | BASE           | I86            | 0x9C mode16 no66_prefix                                                          | REG0=XED_REG_STACKPUSH:w:w:SUPP
PUSHF                | PUSH           | BASE           | I86            | 0x9C mode32 66_prefix                                                            | REG0=XED_REG_STACKPUSH:w:w:SUPP
PUSHF                | PUSH           | BASE           | I86            | 0x9C mode64 norexw_prefix 66_prefix                                              | REG0=XED_REG_STACKPUSH:w:w:SUPP
PUSHFD               | PUSH           | BASE           | I386           | 0x9C mode32 no66_prefix                                                          | REG0=XED_REG_STACKPUSH:w:d:SUPP
PUSHFD               | PUSH           | BASE           | I386           | 0x9C mode16 66_prefix                                                            | REG0=XED_REG_STACKPUSH:w:d:SUPP
PXOR                 | LOGICAL        | MMX            | PENTIUMMMX     | 0x0F 0xEF no_refining_prefix MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()            | REG0=MMX_R():rw:q MEM0:r:q
PXOR                 | LOGICAL        | MMX            | PENTIUMMMX     | 0x0F 0xEF no_refining_prefix MOD[0b11] MOD=3 REG[rrr] RM[nnn]                    | REG0=MMX_R():rw:q REG1=MMX_B():r:q
RCL                  | ROTATE         | BASE           | I186           | 0xC0 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM() UIMM8()                           | MEM0:rw:b IMM0:r:b
RCL                  | ROTATE         | BASE           | I186           | 0xC0 MOD[0b11] MOD=3 REG[0b010] RM[nnn] UIMM8()                                  | REG0=GPR8_B():rw IMM0:r:b
RCL                  | ROTATE         | BASE           | I186           | 0xC1 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM() UIMM8()                           | MEM0:rw:v IMM0:r:b
RCL                  | ROTATE         | BASE           | I186           | 0xC1 MOD[0b11] MOD=3 REG[0b010] RM[nnn] UIMM8()                                  | REG0=GPRv_B():rw IMM0:r:b
RCL                  | ROTATE         | BASE           | I86            | 0xD0 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM() ONE()                             | MEM0:rw:b IMM0:r:b:IMPL
RCL                  | ROTATE         | BASE           | I86            | 0xD0 MOD[0b11] MOD=3 REG[0b010] RM[nnn] ONE()                                    | REG0=GPR8_B():rw IMM0:r:b:IMPL
RCL                  | ROTATE         | BASE           | I86            | 0xD1 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM() ONE()                             | MEM0:rw:v IMM0:r:b:IMPL
RCL                  | ROTATE         | BASE           | I86            | 0xD1 MOD[0b11] MOD=3 REG[0b010] RM[nnn] ONE()                                    | REG0=GPRv_B():rw IMM0:r:b:IMPL
RCL                  | ROTATE         | BASE           | I86            | 0xD2 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM()                                   | MEM0:rw:b REG0=XED_REG_CL:r:IMPL
RCL                  | ROTATE         | BASE           | I86            | 0xD2 MOD[0b11] MOD=3 REG[0b010] RM[nnn]                                          | REG0=GPR8_B():rw REG1=XED_REG_CL:r:IMPL
RCL                  | ROTATE         | BASE           | I86            | 0xD3 MOD[mm] MOD!=3 REG[0b010] RM[nnn] MODRM()                                   | MEM0:rw:v REG0=XED_REG_CL:r:IMPL
RCL                  | ROTATE         | BASE           | I86            | 0xD3 MOD[0b11] MOD=3 REG[0b010] RM[nnn]                                          | REG0=GPRv_B():rw REG1=XED_REG_CL:r:IMPL
RCR                  | ROTATE         | BASE           | I186           | 0xC0 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM() UIMM8()                           | MEM0:rw:b IMM0:r:b
RCR                  | ROTATE         | BASE           | I186           | 0xC0 MOD[0b11] MOD=3 REG[0b011] RM[nnn] UIMM8()                                  | REG0=GPR8_B():rw IMM0:r:b
RCR                  | ROTATE         | BASE           | I186           | 0xC1 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM() UIMM8()                           | MEM0:rw:v IMM0:r:b
RCR                  | ROTATE         | BASE           | I186           | 0xC1 MOD[0b11] MOD=3 REG[0b011] RM[nnn] UIMM8()                                  | REG0=GPRv_B():rw IMM0:r:b
RCR                  | ROTATE         | BASE           | I86            | 0xD0 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM() ONE()                             | MEM0:rw:b IMM0:r:b:IMPL
RCR                  | ROTATE         | BASE           | I86            | 0xD0 MOD[0b11] MOD=3 REG[0b011] RM[nnn] ONE()                                    | REG0=GPR8_B():rw IMM0:r:b:IMPL
RCR                  | ROTATE         | BASE           | I86            | 0xD1 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM() ONE()                             | MEM0:rw:v IMM0:r:b:IMPL
RCR                  | ROTATE         | BASE           | I86            | 0xD1 MOD[0b11] MOD=3 REG[0b011] RM[nnn] ONE()                                    | REG0=GPRv_B():rw IMM0:r:b:IMPL
RCR                  | ROTATE         | BASE           | I86            | 0xD2 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM()                                   | MEM0:rw:b REG0=XED_REG_CL:r:IMPL
RCR                  | ROTATE         | BASE           | I86            | 0xD2 MOD[0b11] MOD=3 REG[0b011] RM[nnn]                                          | REG0=GPR8_B():rw REG1=XED_REG_CL:r:IMPL
RCR                  | ROTATE         | BASE           | I86            | 0xD3 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM()                                   | MEM0:rw:v REG0=XED_REG_CL:r:IMPL
RCR                  | ROTATE         | BASE           | I86            | 0xD3 MOD[0b11] MOD=3 REG[0b011] RM[nnn]                                          | REG0=GPRv_B():rw REG1=XED_REG_CL:r:IMPL
RDMSR                | SYSTEM         | BASE           | PENTIUMREAL    | 0x0F 0x32                                                                        | REG0=XED_REG_EAX:w:SUPP REG1=XED_REG_EDX:w:SUPP REG2=XED_REG_ECX:r:SUPP REG3=XED_REG_MSRS:r:SUPP
RDPID                | RDPID          | RDPID          | RDPID          | 0x0F 0xC7 MOD[0b11] MOD=3  REG[0b111] RM[nnn]  f3_refining_prefix    not64       | REG0=GPR32_B():w:d:u32 REG1=XED_REG_TSCAUX:r:SUPP:d:u32
RDPID                | RDPID          | RDPID          | RDPID          | 0x0F 0xC7 MOD[0b11] MOD=3  REG[0b111] RM[nnn]  f3_refining_prefix   mode64       | REG0=GPR64_B():w:q:u64 REG1=XED_REG_TSCAUX:r:SUPP:d:u32
RDPKRU               | PKU            | PKU            | PKU            | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b101] RM[0b110]  no_refining_prefix               | REG0=XED_REG_EDX:w:SUPP REG1=XED_REG_EAX:w:SUPP REG2=XED_REG_ECX:r:SUPP
RDPMC                | SYSTEM         | BASE           | RDPMC          | 0x0F 0x33                                                                        | REG0=XED_REG_EAX:w:SUPP REG1=XED_REG_EDX:w:SUPP REG2=XED_REG_ECX:r:SUPP REG3=XED_REG_MSRS:r:SUPP
RDPRU                | RDPRU          | RDPRU          | RDPRU          | 0x0F 0x01 MOD[0b11] MOD=3  REG[0b111] RM[0b101]                                  | REG0=XED_REG_EDX:w:SUPP:d  REG1=XED_REG_EAX:w:SUPP:d REG2=XED_REG_ECX:r:SUPP:d
RDRAND               | RDRAND         | RDRAND         | RDRAND         | 0x0F 0xC7  MOD[0b11] MOD=3 REG[0b110] RM[nnn] not_refining                       | REG0=GPRv_B():w
RDSEED               | RDSEED         | RDSEED         | RDSEED         | 0x0F 0xC7  MOD[0b11] MOD=3 REG[0b111] RM[nnn] not_refining                       | REG0=GPRv_B():w
RDSSPD               | CET            | CET            | CET            | 0x0F 0x1E MOD[0b11] MOD=3  REG[0b001] RM[nnn]  f3_refining_prefix    W0 CET=1    | REG0=GPR32_B():w:d:u32 REG1=XED_REG_SSP:r:SUPP:u64
RDSSPQ               | CET            | CET            | CET            | 0x0F 0x1E MOD[0b11] MOD=3  REG[0b001] RM[nnn]  f3_refining_prefix    W1  mode64 CET=1 | REG0=GPR64_B():w:q:u64 REG1=XED_REG_SSP:r:SUPP:u64
RDTSC                | SYSTEM         | BASE           | PENTIUMREAL    | 0x0F 0x31                                                                        | REG0=XED_REG_EAX:w:SUPP REG1=XED_REG_EDX:w:SUPP REG2=XED_REG_TSC:r:SUPP
REP_INSB             | IOSTRINGOP     | BASE           | I186           | 0x6C repe                                                                        | MEM0:cw:SUPP:b BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP REG1=ArCX():rcw:SUPP
REP_INSB             | IOSTRINGOP     | BASE           | I186           | 0x6C repne                                                                       | MEM0:cw:SUPP:b BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP REG1=ArCX():rcw:SUPP
REP_INSD             | IOSTRINGOP     | BASE           | I386           | 0x6D mode16 66_prefix  repe                                                      | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP REG1=ArCX():rcw:SUPP
REP_INSD             | IOSTRINGOP     | BASE           | I386           | 0x6D mode32 no66_prefix  repe                                                    | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP REG1=ArCX():rcw:SUPP
REP_INSD             | IOSTRINGOP     | BASE           | I386           | 0x6D mode64 norexw_prefix no66_prefix  repe                                      | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP REG1=ArCX():rcw:SUPP
REP_INSD             | IOSTRINGOP     | BASE           | I386           | 0x6D mode64 rexw_prefix  repe                                                    | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP REG1=ArCX():rcw:SUPP
REP_INSD             | IOSTRINGOP     | BASE           | I386           | 0x6D mode16 66_prefix  repne                                                     | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP REG1=ArCX():rcw:SUPP
REP_INSD             | IOSTRINGOP     | BASE           | I386           | 0x6D mode32 no66_prefix  repne                                                   | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP REG1=ArCX():rcw:SUPP
REP_INSD             | IOSTRINGOP     | BASE           | I386           | 0x6D mode64 norexw_prefix no66_prefix  repne                                     | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP REG1=ArCX():rcw:SUPP
REP_INSD             | IOSTRINGOP     | BASE           | I386           | 0x6D mode64 rexw_prefix  repne                                                   | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP REG1=ArCX():rcw:SUPP
REP_INSW             | IOSTRINGOP     | BASE           | I186           | 0x6D mode16 no66_prefix  repe                                                    | MEM0:cw:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP REG1=ArCX():rcw:SUPP
REP_INSW             | IOSTRINGOP     | BASE           | I186           | 0x6D mode32 66_prefix  repe                                                      | MEM0:cw:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP REG1=ArCX():rcw:SUPP
REP_INSW             | IOSTRINGOP     | BASE           | I186           | 0x6D mode64 norexw_prefix 66_prefix  repe                                        | MEM0:cw:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP REG1=ArCX():rcw:SUPP
REP_INSW             | IOSTRINGOP     | BASE           | I186           | 0x6D mode16 no66_prefix  repne                                                   | MEM0:cw:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP REG1=ArCX():rcw:SUPP
REP_INSW             | IOSTRINGOP     | BASE           | I186           | 0x6D mode32 66_prefix   repne                                                    | MEM0:cw:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP REG1=ArCX():rcw:SUPP
REP_INSW             | IOSTRINGOP     | BASE           | I186           | 0x6D mode64 norexw_prefix 66_prefix  repne                                       | MEM0:cw:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_DX:r:SUPP REG1=ArCX():rcw:SUPP
REP_LODSB            | STRINGOP       | BASE           | I86            | 0xAC repe OVERRIDE_SEG0()                                                        | REG0=XED_REG_AL:cw:SUPP MEM0:cr:SUPP:b BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
REP_LODSB            | STRINGOP       | BASE           | I86            | 0xAC repne OVERRIDE_SEG0()                                                       | REG0=XED_REG_AL:cw:SUPP MEM0:cr:SUPP:b BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
REP_LODSD            | STRINGOP       | BASE           | I386           | 0xAD mode16 66_prefix  repe OVERRIDE_SEG0()                                      | REG0=XED_REG_EAX:cw:SUPP MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
REP_LODSD            | STRINGOP       | BASE           | I386           | 0xAD mode32 no66_prefix  repe OVERRIDE_SEG0()                                    | REG0=XED_REG_EAX:cw:SUPP MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
REP_LODSD            | STRINGOP       | BASE           | I386           | 0xAD mode64 norexw_prefix no66_prefix  repe OVERRIDE_SEG0()                      | REG0=XED_REG_EAX:cw:SUPP MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
REP_LODSD            | STRINGOP       | BASE           | I386           | 0xAD mode16 66_prefix  repne OVERRIDE_SEG0()                                     | REG0=XED_REG_EAX:cw:SUPP MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
REP_LODSD            | STRINGOP       | BASE           | I386           | 0xAD mode32 no66_prefix  repne OVERRIDE_SEG0()                                   | REG0=XED_REG_EAX:cw:SUPP MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
REP_LODSD            | STRINGOP       | BASE           | I386           | 0xAD mode64 norexw_prefix no66_prefix  repne OVERRIDE_SEG0()                     | REG0=XED_REG_EAX:cw:SUPP MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
REP_LODSW            | STRINGOP       | BASE           | I86            | 0xAD mode16 no66_prefix   repe OVERRIDE_SEG0()                                   | REG0=XED_REG_AX:cw:SUPP MEM0:cr:SUPP:w BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
REP_LODSW            | STRINGOP       | BASE           | I86            | 0xAD mode32 66_prefix  repe OVERRIDE_SEG0()                                      | REG0=XED_REG_AX:cw:SUPP MEM0:cr:SUPP:w BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
REP_LODSW            | STRINGOP       | BASE           | I86            | 0xAD mode64 norexw_prefix 66_prefix  repe OVERRIDE_SEG0()                        | REG0=XED_REG_AX:cw:SUPP MEM0:cr:SUPP:w BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
REP_LODSW            | STRINGOP       | BASE           | I86            | 0xAD mode16 no66_prefix   repne OVERRIDE_SEG0()                                  | REG0=XED_REG_AX:cw:SUPP MEM0:cr:SUPP:w BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
REP_LODSW            | STRINGOP       | BASE           | I86            | 0xAD mode32 66_prefix  repne OVERRIDE_SEG0()                                     | REG0=XED_REG_AX:cw:SUPP MEM0:cr:SUPP:w BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
REP_LODSW            | STRINGOP       | BASE           | I86            | 0xAD mode64 norexw_prefix 66_prefix  repne OVERRIDE_SEG0()                       | REG0=XED_REG_AX:cw:SUPP MEM0:cr:SUPP:w BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
REP_MOVSB            | STRINGOP       | BASE           | I86            | 0xA4 repe OVERRIDE_SEG1()                                                        | MEM0:cw:SUPP:b BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP  MEM1:cr:SUPP:b BASE1=ArSI():rcw:SUPP SEG1=FINAL_DSEG1():r:SUPP REG0=ArCX():rcw:SUPP
REP_MOVSB            | STRINGOP       | BASE           | I86            | 0xA4 repne OVERRIDE_SEG1()                                                       | MEM0:cw:SUPP:b BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP  MEM1:cr:SUPP:b BASE1=ArSI():rcw:SUPP SEG1=FINAL_DSEG1():r:SUPP REG0=ArCX():rcw:SUPP
REP_MOVSD            | STRINGOP       | BASE           | I386           | 0xA5 mode16 66_prefix  repe OVERRIDE_SEG1()                                      | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:cr:SUPP:d BASE1=ArSI():rcw:SUPP SEG1=FINAL_DSEG1():r:SUPP REG0=ArCX():rcw:SUPP
REP_MOVSD            | STRINGOP       | BASE           | I386           | 0xA5 mode32 no66_prefix  repe OVERRIDE_SEG1()                                    | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:cr:SUPP:d BASE1=ArSI():rcw:SUPP SEG1=FINAL_DSEG1():r:SUPP REG0=ArCX():rcw:SUPP
REP_MOVSD            | STRINGOP       | BASE           | I386           | 0xA5 mode64 norexw_prefix no66_prefix  repe OVERRIDE_SEG1()                      | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:cr:SUPP:d BASE1=ArSI():rcw:SUPP SEG1=FINAL_DSEG1():r:SUPP REG0=ArCX():rcw:SUPP
REP_MOVSD            | STRINGOP       | BASE           | I386           | 0xA5 mode16 66_prefix  repne OVERRIDE_SEG1()                                     | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:cr:SUPP:d BASE1=ArSI():rcw:SUPP SEG1=FINAL_DSEG1():r:SUPP REG0=ArCX():rcw:SUPP
REP_MOVSD            | STRINGOP       | BASE           | I386           | 0xA5 mode32 no66_prefix   repne OVERRIDE_SEG1()                                  | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:cr:SUPP:d BASE1=ArSI():rcw:SUPP SEG1=FINAL_DSEG1():r:SUPP REG0=ArCX():rcw:SUPP
REP_MOVSD            | STRINGOP       | BASE           | I386           | 0xA5 mode64 norexw_prefix no66_prefix  repne OVERRIDE_SEG1()                     | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:cr:SUPP:d BASE1=ArSI():rcw:SUPP SEG1=FINAL_DSEG1():r:SUPP REG0=ArCX():rcw:SUPP
REP_MOVSW            | STRINGOP       | BASE           | I86            | 0xA5 mode16 no66_prefix  repe OVERRIDE_SEG1()                                    | MEM0:cw:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:cr:SUPP:w BASE1=ArSI():rcw:SUPP SEG1=FINAL_DSEG1():r:SUPP REG0=ArCX():rcw:SUPP
REP_MOVSW            | STRINGOP       | BASE           | I86            | 0xA5 mode32 66_prefix  repe OVERRIDE_SEG1()                                      | MEM0:cw:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:cr:SUPP:w BASE1=ArSI():rcw:SUPP SEG1=FINAL_DSEG1():r:SUPP REG0=ArCX():rcw:SUPP
REP_MOVSW            | STRINGOP       | BASE           | I86            | 0xA5 mode64 norexw_prefix 66_prefix  repe OVERRIDE_SEG1()                        | MEM0:cw:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:cr:SUPP:w BASE1=ArSI():rcw:SUPP SEG1=FINAL_DSEG1():r:SUPP REG0=ArCX():rcw:SUPP
REP_MOVSW            | STRINGOP       | BASE           | I86            | 0xA5 mode16 no66_prefix  repne OVERRIDE_SEG1()                                   | MEM0:cw:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:cr:SUPP:w BASE1=ArSI():rcw:SUPP SEG1=FINAL_DSEG1():r:SUPP REG0=ArCX():rcw:SUPP
REP_MOVSW            | STRINGOP       | BASE           | I86            | 0xA5 mode32 66_prefix  repne OVERRIDE_SEG1()                                     | MEM0:cw:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:cr:SUPP:w BASE1=ArSI():rcw:SUPP SEG1=FINAL_DSEG1():r:SUPP REG0=ArCX():rcw:SUPP
REP_MOVSW            | STRINGOP       | BASE           | I86            | 0xA5 mode64 norexw_prefix 66_prefix  repne OVERRIDE_SEG1()                       | MEM0:cw:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP MEM1:cr:SUPP:w BASE1=ArSI():rcw:SUPP SEG1=FINAL_DSEG1():r:SUPP REG0=ArCX():rcw:SUPP
REP_OUTSB            | IOSTRINGOP     | BASE           | I186           | 0x6E repe OVERRIDE_SEG0()                                                        | REG0=XED_REG_DX:r:SUPP MEM0:cr:SUPP:b BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
REP_OUTSB            | IOSTRINGOP     | BASE           | I186           | 0x6E repne OVERRIDE_SEG0()                                                       | REG0=XED_REG_DX:r:SUPP MEM0:cr:SUPP:b BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
REP_OUTSD            | IOSTRINGOP     | BASE           | I386           | 0x6F mode16 66_prefix  repe OVERRIDE_SEG0()                                      | REG0=XED_REG_DX:r:SUPP MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
REP_OUTSD            | IOSTRINGOP     | BASE           | I386           | 0x6F mode32 no66_prefix  repe OVERRIDE_SEG0()                                    | REG0=XED_REG_DX:r:SUPP MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
REP_OUTSD            | IOSTRINGOP     | BASE           | I386           | 0x6F mode64 norexw_prefix no66_prefix  repe OVERRIDE_SEG0()                      | REG0=XED_REG_DX:r:SUPP MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
REP_OUTSD            | IOSTRINGOP     | BASE           | I386           | 0x6F mode64 rexw_prefix  repe OVERRIDE_SEG0()                                    | REG0=XED_REG_DX:r:SUPP MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
REP_OUTSD            | IOSTRINGOP     | BASE           | I386           | 0x6F mode16 66_prefix  repne OVERRIDE_SEG0()                                     | REG0=XED_REG_DX:r:SUPP MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
REP_OUTSD            | IOSTRINGOP     | BASE           | I386           | 0x6F mode32 no66_prefix  repne OVERRIDE_SEG0()                                   | REG0=XED_REG_DX:r:SUPP MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
REP_OUTSD            | IOSTRINGOP     | BASE           | I386           | 0x6F mode64 norexw_prefix no66_prefix  repne OVERRIDE_SEG0()                     | REG0=XED_REG_DX:r:SUPP MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
REP_OUTSD            | IOSTRINGOP     | BASE           | I386           | 0x6F mode64 rexw_prefix   repne OVERRIDE_SEG0()                                  | REG0=XED_REG_DX:r:SUPP MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
REP_OUTSW            | IOSTRINGOP     | BASE           | I186           | 0x6F mode16 no66_prefix  repe OVERRIDE_SEG0()                                    | REG0=XED_REG_DX:r:SUPP MEM0:cr:SUPP:w BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
REP_OUTSW            | IOSTRINGOP     | BASE           | I186           | 0x6F mode32 66_prefix  repe OVERRIDE_SEG0()                                      | REG0=XED_REG_DX:r:SUPP MEM0:cr:SUPP:w BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
REP_OUTSW            | IOSTRINGOP     | BASE           | I186           | 0x6F mode64 norexw_prefix 66_prefix  repe OVERRIDE_SEG0()                        | REG0=XED_REG_DX:r:SUPP MEM0:cr:SUPP:w BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
REP_OUTSW            | IOSTRINGOP     | BASE           | I186           | 0x6F mode16 no66_prefix  repne OVERRIDE_SEG0()                                   | REG0=XED_REG_DX:r:SUPP MEM0:cr:SUPP:w BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
REP_OUTSW            | IOSTRINGOP     | BASE           | I186           | 0x6F mode32 66_prefix  repne OVERRIDE_SEG0()                                     | REG0=XED_REG_DX:r:SUPP MEM0:cr:SUPP:w BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
REP_OUTSW            | IOSTRINGOP     | BASE           | I186           | 0x6F mode64 norexw_prefix 66_prefix  repne OVERRIDE_SEG0()                       | REG0=XED_REG_DX:r:SUPP MEM0:cr:SUPP:w BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP REG1=ArCX():rcw:SUPP
REP_STOSB            | STRINGOP       | BASE           | I86            | 0xAA repe                                                                        | MEM0:cw:SUPP:b BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_AL:r:SUPP REG1=ArCX():rcw:SUPP
REP_STOSB            | STRINGOP       | BASE           | I86            | 0xAA repne                                                                       | MEM0:cw:SUPP:b BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_AL:r:SUPP REG1=ArCX():rcw:SUPP
REP_STOSD            | STRINGOP       | BASE           | I386           | 0xAB mode16 66_prefix  repe                                                      | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_EAX:r:SUPP REG1=ArCX():rcw:SUPP
REP_STOSD            | STRINGOP       | BASE           | I386           | 0xAB mode32 no66_prefix  repe                                                    | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_EAX:r:SUPP REG1=ArCX():rcw:SUPP
REP_STOSD            | STRINGOP       | BASE           | I386           | 0xAB mode64 norexw_prefix no66_prefix  repe                                      | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_EAX:r:SUPP REG1=ArCX():rcw:SUPP
REP_STOSD            | STRINGOP       | BASE           | I386           | 0xAB mode16 66_prefix  repne                                                     | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_EAX:r:SUPP REG1=ArCX():rcw:SUPP
REP_STOSD            | STRINGOP       | BASE           | I386           | 0xAB mode32 no66_prefix  repne                                                   | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_EAX:r:SUPP REG1=ArCX():rcw:SUPP
REP_STOSD            | STRINGOP       | BASE           | I386           | 0xAB mode64 norexw_prefix no66_prefix  repne                                     | MEM0:cw:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_EAX:r:SUPP REG1=ArCX():rcw:SUPP
REP_STOSW            | STRINGOP       | BASE           | I86            | 0xAB mode16 no66_prefix  repe                                                    | MEM0:cw:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_AX:r:SUPP REG1=ArCX():rcw:SUPP
REP_STOSW            | STRINGOP       | BASE           | I86            | 0xAB mode32 66_prefix  repe                                                      | MEM0:cw:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_AX:r:SUPP REG1=ArCX():rcw:SUPP
REP_STOSW            | STRINGOP       | BASE           | I86            | 0xAB mode64 norexw_prefix 66_prefix repe                                         | MEM0:cw:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_AX:r:SUPP REG1=ArCX():rcw:SUPP
REP_STOSW            | STRINGOP       | BASE           | I86            | 0xAB mode16 no66_prefix  repne                                                   | MEM0:cw:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_AX:r:SUPP REG1=ArCX():rcw:SUPP
REP_STOSW            | STRINGOP       | BASE           | I86            | 0xAB mode32 66_prefix  repne                                                     | MEM0:cw:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_AX:r:SUPP REG1=ArCX():rcw:SUPP
REP_STOSW            | STRINGOP       | BASE           | I86            | 0xAB mode64 norexw_prefix 66_prefix  repne                                       | MEM0:cw:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_AX:r:SUPP REG1=ArCX():rcw:SUPP
REPE_CMPSB           | STRINGOP       | BASE           | I86            | 0xA6 repe OVERRIDE_SEG0()                                                        | MEM0:cr:SUPP:b BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:cr:SUPP:b BASE1=ArDI():rcw:SUPP SEG1=FINAL_ESEG1():r:SUPP REG0=ArCX():rcw:SUPP
REPE_CMPSD           | STRINGOP       | BASE           | I386           | 0xA7 mode16 66_prefix  repe OVERRIDE_SEG0()                                      | MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:cr:SUPP:d BASE1=ArDI():rcw:SUPP SEG1=FINAL_ESEG1():r:SUPP REG0=ArCX():rcw:SUPP
REPE_CMPSD           | STRINGOP       | BASE           | I386           | 0xA7 mode32 no66_prefix  repe OVERRIDE_SEG0()                                    | MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:cr:SUPP:d BASE1=ArDI():rcw:SUPP SEG1=FINAL_ESEG1():r:SUPP REG0=ArCX():rcw:SUPP
REPE_CMPSD           | STRINGOP       | BASE           | I386           | 0xA7 mode64 norexw_prefix no66_prefix  repe OVERRIDE_SEG0()                      | MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:cr:SUPP:d BASE1=ArDI():rcw:SUPP SEG1=FINAL_ESEG1():r:SUPP REG0=ArCX():rcw:SUPP
REPE_CMPSW           | STRINGOP       | BASE           | I86            | 0xA7 mode16 no66_prefix repe OVERRIDE_SEG0()                                     | MEM0:cr:SUPP:w BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:cr:SUPP:w BASE1=ArDI():rcw:SUPP SEG1=FINAL_ESEG1():r:SUPP REG0=ArCX():rcw:SUPP
REPE_CMPSW           | STRINGOP       | BASE           | I86            | 0xA7 mode32 66_prefix repe OVERRIDE_SEG0()                                       | MEM0:cr:SUPP:w BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:cr:SUPP:w BASE1=ArDI():rcw:SUPP SEG1=FINAL_ESEG1():r:SUPP REG0=ArCX():rcw:SUPP
REPE_CMPSW           | STRINGOP       | BASE           | I86            | 0xA7 mode64 norexw_prefix 66_prefix repe OVERRIDE_SEG0()                         | MEM0:cr:SUPP:w BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:cr:SUPP:w BASE1=ArDI():rcw:SUPP SEG1=FINAL_ESEG1():r:SUPP REG0=ArCX():rcw:SUPP
REPE_SCASB           | STRINGOP       | BASE           | I86            | 0xAE repe                                                                        | REG0=XED_REG_AL:r:SUPP MEM0:cr:SUPP:b BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG1=ArCX():rcw:SUPP
REPE_SCASD           | STRINGOP       | BASE           | I386           | 0xAF mode16 66_prefix  repe                                                      | REG0=XED_REG_EAX:r:SUPP MEM0:cr:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG1=ArCX():rcw:SUPP
REPE_SCASD           | STRINGOP       | BASE           | I386           | 0xAF mode32 no66_prefix  repe                                                    | REG0=XED_REG_EAX:r:SUPP MEM0:cr:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG1=ArCX():rcw:SUPP
REPE_SCASD           | STRINGOP       | BASE           | I386           | 0xAF mode64 norexw_prefix no66_prefix repe                                       | REG0=XED_REG_EAX:r:SUPP MEM0:cr:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG1=ArCX():rcw:SUPP
REPE_SCASW           | STRINGOP       | BASE           | I86            | 0xAF mode16 no66_prefix  repe                                                    | REG0=XED_REG_AX:r:SUPP MEM0:cr:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG1=ArCX():rcw:SUPP
REPE_SCASW           | STRINGOP       | BASE           | I86            | 0xAF mode32 66_prefix  repe                                                      | REG0=XED_REG_AX:r:SUPP MEM0:cr:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG1=ArCX():rcw:SUPP
REPE_SCASW           | STRINGOP       | BASE           | I86            | 0xAF mode64 norexw_prefix 66_prefix  repe                                        | REG0=XED_REG_AX:r:SUPP MEM0:cr:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG1=ArCX():rcw:SUPP
REPNE_CMPSB          | STRINGOP       | BASE           | I86            | 0xA6 repne OVERRIDE_SEG0()                                                       | MEM0:cr:SUPP:b BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:cr:SUPP:b BASE1=ArDI():rcw:SUPP SEG1=FINAL_ESEG1():r:SUPP REG0=ArCX():rcw:SUPP
REPNE_CMPSD          | STRINGOP       | BASE           | I386           | 0xA7 mode16 66_prefix  repne OVERRIDE_SEG0()                                     | MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:cr:SUPP:d BASE1=ArDI():rcw:SUPP SEG1=FINAL_ESEG1():r:SUPP REG0=ArCX():rcw:SUPP
REPNE_CMPSD          | STRINGOP       | BASE           | I386           | 0xA7 mode32 no66_prefix  repne OVERRIDE_SEG0()                                   | MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:cr:SUPP:d BASE1=ArDI():rcw:SUPP SEG1=FINAL_ESEG1():r:SUPP REG0=ArCX():rcw:SUPP
REPNE_CMPSD          | STRINGOP       | BASE           | I386           | 0xA7 mode64 norexw_prefix no66_prefix  repne OVERRIDE_SEG0()                     | MEM0:cr:SUPP:d BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:cr:SUPP:d BASE1=ArDI():rcw:SUPP SEG1=FINAL_ESEG1():r:SUPP REG0=ArCX():rcw:SUPP
REPNE_CMPSW          | STRINGOP       | BASE           | I86            | 0xA7 mode16 no66_prefix  repne OVERRIDE_SEG0()                                   | MEM0:cr:SUPP:w BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:cr:SUPP:w BASE1=ArDI():rcw:SUPP SEG1=FINAL_ESEG1():r:SUPP REG0=ArCX():rcw:SUPP
REPNE_CMPSW          | STRINGOP       | BASE           | I86            | 0xA7 mode32 66_prefix  repne OVERRIDE_SEG0()                                     | MEM0:cr:SUPP:w BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:cr:SUPP:w BASE1=ArDI():rcw:SUPP SEG1=FINAL_ESEG1():r:SUPP REG0=ArCX():rcw:SUPP
REPNE_CMPSW          | STRINGOP       | BASE           | I86            | 0xA7 mode64 norexw_prefix 66_prefix  repne OVERRIDE_SEG0()                       | MEM0:cr:SUPP:w BASE0=ArSI():rcw:SUPP SEG0=FINAL_DSEG():r:SUPP MEM1:cr:SUPP:w BASE1=ArDI():rcw:SUPP SEG1=FINAL_ESEG1():r:SUPP REG0=ArCX():rcw:SUPP
REPNE_SCASB          | STRINGOP       | BASE           | I86            | 0xAE repne                                                                       | REG0=XED_REG_AL:r:SUPP MEM0:cr:SUPP:b BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG1=ArCX():rcw:SUPP
REPNE_SCASD          | STRINGOP       | BASE           | I386           | 0xAF mode16 66_prefix  repne                                                     | REG0=XED_REG_EAX:r:SUPP MEM0:cr:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG1=ArCX():rcw:SUPP
REPNE_SCASD          | STRINGOP       | BASE           | I386           | 0xAF mode32 no66_prefix  repne                                                   | REG0=XED_REG_EAX:r:SUPP MEM0:cr:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG1=ArCX():rcw:SUPP
REPNE_SCASD          | STRINGOP       | BASE           | I386           | 0xAF mode64 norexw_prefix no66_prefix  repne                                     | REG0=XED_REG_EAX:r:SUPP MEM0:cr:SUPP:d BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG1=ArCX():rcw:SUPP
REPNE_SCASW          | STRINGOP       | BASE           | I86            | 0xAF mode16 no66_prefix  repne                                                   | REG0=XED_REG_AX:r:SUPP MEM0:cr:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG1=ArCX():rcw:SUPP
REPNE_SCASW          | STRINGOP       | BASE           | I86            | 0xAF mode32 66_prefix  repne                                                     | REG0=XED_REG_AX:r:SUPP MEM0:cr:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG1=ArCX():rcw:SUPP
REPNE_SCASW          | STRINGOP       | BASE           | I86            | 0xAF mode64 norexw_prefix 66_prefix  repne                                       | REG0=XED_REG_AX:r:SUPP MEM0:cr:SUPP:w BASE0=ArDI():rcw:SUPP SEG0=FINAL_ESEG():r:SUPP REG1=ArCX():rcw:SUPP
RET_FAR              | RET            | BASE           | I86            | 0xCA UIMM16()                                                                    | IMM0:r:w REG0=XED_REG_STACKPOP:r:spw2:SUPP REG1=rIP():w:SUPP
RET_FAR              | RET            | BASE           | I86            | 0xCB                                                                             | REG0=XED_REG_STACKPOP:r:spw2:SUPP REG1=rIP():w:SUPP
RET_NEAR             | RET            | BASE           | I86            | 0xC2 DF64() UIMM16() IMMUNE66_LOOP64()                                           | IMM0:r:w REG0=XED_REG_STACKPOP:r:spw:SUPP REG1=rIP():w:SUPP
RET_NEAR             | RET            | BASE           | I86            | 0xC3 DF64() IMMUNE66_LOOP64()                                                    | REG0=XED_REG_STACKPOP:r:spw:SUPP REG1=rIP():w:SUPP
ROL                  | ROTATE         | BASE           | I186           | 0xC0 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() UIMM8()                           | MEM0:rw:b IMM0:r:b
ROL                  | ROTATE         | BASE           | I186           | 0xC0 MOD[0b11] MOD=3 REG[0b000] RM[nnn] UIMM8()                                  | REG0=GPR8_B():rw IMM0:r:b
ROL                  | ROTATE         | BASE           | I186           | 0xC1 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() UIMM8()                           | MEM0:rw:v IMM0:r:b
ROL                  | ROTATE         | BASE           | I186           | 0xC1 MOD[0b11] MOD=3 REG[0b000] RM[nnn] UIMM8()                                  | REG0=GPRv_B():rw IMM0:r:b
ROL                  | ROTATE         | BASE           | I86            | 0xD0 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM()  ONE()                            | MEM0:rw:b IMM0:r:b:IMPL
ROL                  | ROTATE         | BASE           | I86            | 0xD0 MOD[0b11] MOD=3 REG[0b000] RM[nnn] ONE()                                    | REG0=GPR8_B():rw IMM0:r:b:IMPL
ROL                  | ROTATE         | BASE           | I86            | 0xD1 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() ONE()                             | MEM0:rw:v IMM0:r:b:IMPL
ROL                  | ROTATE         | BASE           | I86            | 0xD1 MOD[0b11] MOD=3 REG[0b000] RM[nnn] ONE()                                    | REG0=GPRv_B():rw IMM0:r:b:IMPL
ROL                  | ROTATE         | BASE           | I86            | 0xD2 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM()                                   | MEM0:rw:b REG0=XED_REG_CL:r:IMPL
ROL                  | ROTATE         | BASE           | I86            | 0xD2 MOD[0b11] MOD=3 REG[0b000] RM[nnn]                                          | REG0=GPR8_B():rw REG1=XED_REG_CL:r:IMPL
ROL                  | ROTATE         | BASE           | I86            | 0xD3 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM()                                   | MEM0:rw:v REG0=XED_REG_CL:r:IMPL
ROL                  | ROTATE         | BASE           | I86            | 0xD3 MOD[0b11] MOD=3 REG[0b000] RM[nnn]                                          | REG0=GPRv_B():rw REG1=XED_REG_CL:r:IMPL
ROR                  | ROTATE         | BASE           | I186           | 0xC0 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM() UIMM8()                           | MEM0:rw:b IMM0:r:b
ROR                  | ROTATE         | BASE           | I186           | 0xC0 MOD[0b11] MOD=3 REG[0b001] RM[nnn] UIMM8()                                  | REG0=GPR8_B():rw IMM0:r:b
ROR                  | ROTATE         | BASE           | I186           | 0xC1 MOD[0b11] MOD=3 REG[0b001] RM[nnn] UIMM8()                                  | REG0=GPRv_B():rw IMM0:r:b
ROR                  | ROTATE         | BASE           | I186           | 0xC1 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM() UIMM8()                           | MEM0:rw:v IMM0:r:b
ROR                  | ROTATE         | BASE           | I86            | 0xD0 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM() ONE()                             | MEM0:rw:b IMM0:r:b:IMPL
ROR                  | ROTATE         | BASE           | I86            | 0xD0 MOD[0b11] MOD=3 REG[0b001] RM[nnn] ONE()                                    | REG0=GPR8_B():rw IMM0:r:b:IMPL
ROR                  | ROTATE         | BASE           | I86            | 0xD1 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM() ONE()                             | MEM0:rw:v IMM0:r:b:IMPL
ROR                  | ROTATE         | BASE           | I86            | 0xD1 MOD[0b11] MOD=3 REG[0b001] RM[nnn] ONE()                                    | REG0=GPRv_B():rw IMM0:r:b:IMPL
ROR                  | ROTATE         | BASE           | I86            | 0xD2 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM()                                   | MEM0:rw:b REG0=XED_REG_CL:r:IMPL
ROR                  | ROTATE         | BASE           | I86            | 0xD2 MOD[0b11] MOD=3 REG[0b001] RM[nnn]                                          | REG0=GPR8_B():rw REG1=XED_REG_CL:r:IMPL
ROR                  | ROTATE         | BASE           | I86            | 0xD3 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM()                                   | MEM0:rw:v REG0=XED_REG_CL:r:IMPL
ROR                  | ROTATE         | BASE           | I86            | 0xD3 MOD[0b11] MOD=3 REG[0b001] RM[nnn]                                          | REG0=GPRv_B():rw REG1=XED_REG_CL:r:IMPL
RSM                  | SYSRET         | BASE           | I486           | 0x0F 0xAA                                                                        | REG0=rIP():w:SUPP
RSTORSSP             | CET            | CET            | CET            | 0x0F 0x01 MOD[mm] MOD!=3 REG[0b101] RM[nnn]  MODRM()  f3_refining_prefix         | MEM0:rw:q:u64 REG0=XED_REG_SSP:w:SUPP:u64
SAHF                 | FLAGOP         | BASE           | LAHF           | 0x9E                                                                             | REG0=XED_REG_AH:r:SUPP
SALC                 | FLAGOP         | BASE           | I86            | 0xD6 not64                                                                       | REG0=XED_REG_AL:w:SUPP
SAR                  | SHIFT          | BASE           | I186           | 0xC0 MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM() UIMM8()                           | MEM0:rw:b IMM0:r:b
SAR                  | SHIFT          | BASE           | I186           | 0xC0 MOD[0b11] MOD=3 REG[0b111] RM[nnn] UIMM8()                                  | REG0=GPR8_B():rw IMM0:r:b
SAR                  | SHIFT          | BASE           | I186           | 0xC1 MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM() UIMM8()                           | MEM0:rw:v IMM0:r:b
SAR                  | SHIFT          | BASE           | I186           | 0xC1 MOD[0b11] MOD=3 REG[0b111] RM[nnn] UIMM8()                                  | REG0=GPRv_B():rw IMM0:r:b
SAR                  | SHIFT          | BASE           | I86            | 0xD0 MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM() ONE()                             | MEM0:rw:b IMM0:r:b:IMPL
SAR                  | SHIFT          | BASE           | I86            | 0xD0 MOD[0b11] MOD=3 REG[0b111] RM[nnn] ONE()                                    | REG0=GPR8_B():rw IMM0:r:b:IMPL
SAR                  | SHIFT          | BASE           | I86            | 0xD1 MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM() ONE()                             | MEM0:rw:v IMM0:r:b:IMPL
SAR                  | SHIFT          | BASE           | I86            | 0xD1 MOD[0b11] MOD=3 REG[0b111] RM[nnn] ONE()                                    | REG0=GPRv_B():rw IMM0:r:b:IMPL
SAR                  | SHIFT          | BASE           | I86            | 0xD2 MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM()                                   | MEM0:rw:b REG0=XED_REG_CL:r:IMPL
SAR                  | SHIFT          | BASE           | I86            | 0xD2 MOD[0b11] MOD=3 REG[0b111] RM[nnn]                                          | REG0=GPR8_B():rw REG1=XED_REG_CL:r:IMPL
SAR                  | SHIFT          | BASE           | I86            | 0xD3 MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM()                                   | MEM0:rw:v REG0=XED_REG_CL:r:IMPL
SAR                  | SHIFT          | BASE           | I86            | 0xD3 MOD[0b11] MOD=3 REG[0b111] RM[nnn]                                          | REG0=GPRv_B():rw REG1=XED_REG_CL:r:IMPL
SAVEPREVSSP          | CET            | CET            | CET            | 0x0F 0x01 MOD[0b11] MOD=3  REG[0b101] RM[0b010]  f3_refining_prefix              | REG0=XED_REG_SSP:r:SUPP:u64
SBB                  | BINARY         | BASE           | I86            | 0x80 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM() SIMM8() nolock_prefix             | MEM0:rw:b IMM0:r:b:i8
SBB                  | BINARY         | BASE           | I86            | 0x80 MOD[0b11] MOD=3 REG[0b011] RM[nnn] SIMM8()                                  | REG0=GPR8_B():rw IMM0:r:b:i8
SBB                  | BINARY         | BASE           | I86            | 0x81 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM() SIMMz() nolock_prefix             | MEM0:rw:v IMM0:r:z
SBB                  | BINARY         | BASE           | I86            | 0x81 MOD[0b11] MOD=3 REG[0b011] RM[nnn] SIMMz()                                  | REG0=GPRv_B():rw IMM0:r:z
SBB                  | BINARY         | BASE           | I86            | 0x82 MOD[mm] MOD!=3 REG[0b011] RM[nnn] not64 MODRM() SIMM8() nolock_prefix       | MEM0:rw:b IMM0:r:b:i8
SBB                  | BINARY         | BASE           | I86            | 0x82 MOD[0b11] MOD=3 REG[0b011] RM[nnn] not64 SIMM8()                            | REG0=GPR8_B():rw IMM0:r:b:i8
SBB                  | BINARY         | BASE           | I86            | 0x83 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM() SIMM8() nolock_prefix             | MEM0:rw:v IMM0:r:b:i8
SBB                  | BINARY         | BASE           | I86            | 0x83 MOD[0b11] MOD=3 REG[0b011] RM[nnn] SIMM8()                                  | REG0=GPRv_B():rw IMM0:r:b:i8
SBB                  | BINARY         | BASE           | I86            | 0x18 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                       | MEM0:rw:b REG0=GPR8_R():r
SBB                  | BINARY         | BASE           | I86            | 0x18 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPR8_B():rw REG1=GPR8_R():r
SBB                  | BINARY         | BASE           | I86            | 0x19 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                       | MEM0:rw:v REG0=GPRv_R():r
SBB                  | BINARY         | BASE           | I86            | 0x19 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPRv_B():rw REG1=GPRv_R():r
SBB                  | BINARY         | BASE           | I86            | 0x1A MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPR8_R():rw REG1=GPR8_B():r
SBB                  | BINARY         | BASE           | I86            | 0x1A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=GPR8_R():rw MEM0:r:b
SBB                  | BINARY         | BASE           | I86            | 0x1B MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPRv_R():rw REG1=GPRv_B():r
SBB                  | BINARY         | BASE           | I86            | 0x1B MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=GPRv_R():rw MEM0:r:v
SBB                  | BINARY         | BASE           | I86            | 0x1C SIMM8()                                                                     | REG0=XED_REG_AL:rw:IMPL IMM0:r:b:i8
SBB                  | BINARY         | BASE           | I86            | 0x1D SIMMz()                                                                     | REG0=OrAX():rw:IMPL IMM0:r:z
SBB_LOCK             | BINARY         | BASE           | I86            | 0x80 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM() SIMM8() lock_prefix               | MEM0:rw:b IMM0:r:b:i8
SBB_LOCK             | BINARY         | BASE           | I86            | 0x81 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM() SIMMz() lock_prefix               | MEM0:rw:v IMM0:r:z
SBB_LOCK             | BINARY         | BASE           | I86            | 0x82 MOD[mm] MOD!=3 REG[0b011] RM[nnn] not64 MODRM() SIMM8() lock_prefix         | MEM0:rw:b IMM0:r:b:i8
SBB_LOCK             | BINARY         | BASE           | I86            | 0x83 MOD[mm] MOD!=3 REG[0b011] RM[nnn] MODRM() SIMM8() lock_prefix               | MEM0:rw:v IMM0:r:b:i8
SBB_LOCK             | BINARY         | BASE           | I86            | 0x18 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                         | MEM0:rw:b REG0=GPR8_R():r
SBB_LOCK             | BINARY         | BASE           | I86            | 0x19 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                         | MEM0:rw:v REG0=GPRv_R():r
SCASB                | STRINGOP       | BASE           | I86            | 0xAE norep                                                                       | REG0=XED_REG_AL:r:SUPP MEM0:r:SUPP:b BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP
SCASD                | STRINGOP       | BASE           | I386           | 0xAF mode16 66_prefix  norep                                                     | REG0=XED_REG_EAX:r:SUPP MEM0:r:SUPP:d BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP
SCASD                | STRINGOP       | BASE           | I386           | 0xAF mode32 no66_prefix  norep                                                   | REG0=XED_REG_EAX:r:SUPP MEM0:r:SUPP:d BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP
SCASD                | STRINGOP       | BASE           | I386           | 0xAF mode64 norexw_prefix no66_prefix norep                                      | REG0=XED_REG_EAX:r:SUPP MEM0:r:SUPP:d BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP
SCASW                | STRINGOP       | BASE           | I86            | 0xAF mode16 no66_prefix  norep                                                   | REG0=XED_REG_AX:r:SUPP MEM0:r:SUPP:w BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP
SCASW                | STRINGOP       | BASE           | I86            | 0xAF mode32 66_prefix  norep                                                     | REG0=XED_REG_AX:r:SUPP MEM0:r:SUPP:w BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP
SCASW                | STRINGOP       | BASE           | I86            | 0xAF mode64 norexw_prefix 66_prefix norep                                        | REG0=XED_REG_AX:r:SUPP MEM0:r:SUPP:w BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP
SERIALIZE            | SERIALIZE      | SERIALIZE      | SERIALIZE      | 0x0F 0x01 MOD[0b11] MOD=3  REG[0b101] RM[0b000]  no_refining_prefix              | 
SETB                 | SETCC          | BASE           | I386           | 0x0F 0x92 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:b
SETB                 | SETCC          | BASE           | I386           | 0x0F 0x92 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPR8_B():w
SETBE                | SETCC          | BASE           | I386           | 0x0F 0x96 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:b
SETBE                | SETCC          | BASE           | I386           | 0x0F 0x96 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPR8_B():w
SETL                 | SETCC          | BASE           | I386           | 0x0F 0x9C MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:b
SETL                 | SETCC          | BASE           | I386           | 0x0F 0x9C MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPR8_B():w
SETLE                | SETCC          | BASE           | I386           | 0x0F 0x9E MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:b
SETLE                | SETCC          | BASE           | I386           | 0x0F 0x9E MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPR8_B():w
SETNB                | SETCC          | BASE           | I386           | 0x0F 0x93 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:b
SETNB                | SETCC          | BASE           | I386           | 0x0F 0x93 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPR8_B():w
SETNBE               | SETCC          | BASE           | I386           | 0x0F 0x97 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:b
SETNBE               | SETCC          | BASE           | I386           | 0x0F 0x97 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPR8_B():w
SETNL                | SETCC          | BASE           | I386           | 0x0F 0x9D MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:b
SETNL                | SETCC          | BASE           | I386           | 0x0F 0x9D MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPR8_B():w
SETNLE               | SETCC          | BASE           | I386           | 0x0F 0x9F MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:b
SETNLE               | SETCC          | BASE           | I386           | 0x0F 0x9F MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPR8_B():w
SETNO                | SETCC          | BASE           | I386           | 0x0F 0x91 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:b
SETNO                | SETCC          | BASE           | I386           | 0x0F 0x91 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPR8_B():w
SETNP                | SETCC          | BASE           | I386           | 0x0F 0x9B MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:b
SETNP                | SETCC          | BASE           | I386           | 0x0F 0x9B MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPR8_B():w
SETNS                | SETCC          | BASE           | I386           | 0x0F 0x99 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:b
SETNS                | SETCC          | BASE           | I386           | 0x0F 0x99 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPR8_B():w
SETNZ                | SETCC          | BASE           | I386           | 0x0F 0x95 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:b
SETNZ                | SETCC          | BASE           | I386           | 0x0F 0x95 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPR8_B():w
SETO                 | SETCC          | BASE           | I386           | 0x0F 0x90 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:b
SETO                 | SETCC          | BASE           | I386           | 0x0F 0x90 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPR8_B():w
SETP                 | SETCC          | BASE           | I386           | 0x0F 0x9A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:b
SETP                 | SETCC          | BASE           | I386           | 0x0F 0x9A MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPR8_B():w
SETS                 | SETCC          | BASE           | I386           | 0x0F 0x98 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:b
SETS                 | SETCC          | BASE           | I386           | 0x0F 0x98 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPR8_B():w
SETSSBSY             | CET            | CET            | CET            | 0x0F 0x01 MOD[0b11] MOD=3  REG[0b101] RM[0b000]  f3_refining_prefix              | 
SETZ                 | SETCC          | BASE           | I386           | 0x0F 0x94 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:w:b
SETZ                 | SETCC          | BASE           | I386           | 0x0F 0x94 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPR8_B():w
SGDT                 | SYSTEM         | BASE           | I286REAL       | 0x0F 0x01 MOD[mm] MOD!=3 REG[0b000] RM[nnn] mode64 FORCE64() MODRM()             | MEM0:w:s64 REG0=XED_REG_GDTR:r:SUPP
SGDT                 | SYSTEM         | BASE           | I286REAL       | 0x0F 0x01 MOD[mm] MOD!=3 REG[0b000] RM[nnn] not64 MODRM()                        | MEM0:w:s REG0=XED_REG_GDTR:r:SUPP
SHA1MSG1             | SHA            | SHA            | SHA            | 0x0F 0x38 0xC9 MOD[0b11] MOD=3  REG[rrr] RM[nnn]  no_refining_prefix             | REG0=XMM_R():rw:dq:i32 REG1=XMM_B():r:dq:i32
SHA1MSG1             | SHA            | SHA            | SHA            | 0x0F 0x38 0xC9 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  no_refining_prefix      | REG0=XMM_R():rw:dq:i32 MEM0:r:dq:i32
SHA1MSG2             | SHA            | SHA            | SHA            | 0x0F 0x38 0xCA MOD[0b11] MOD=3  REG[rrr] RM[nnn]  no_refining_prefix             | REG0=XMM_R():rw:dq:i32 REG1=XMM_B():r:dq:i32
SHA1MSG2             | SHA            | SHA            | SHA            | 0x0F 0x38 0xCA MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  no_refining_prefix      | REG0=XMM_R():rw:dq:i32 MEM0:r:dq:i32
SHA1NEXTE            | SHA            | SHA            | SHA            | 0x0F 0x38 0xC8 MOD[0b11] MOD=3  REG[rrr] RM[nnn]  no_refining_prefix             | REG0=XMM_R():rw:dq:i32 REG1=XMM_B():r:dq:i32
SHA1NEXTE            | SHA            | SHA            | SHA            | 0x0F 0x38 0xC8 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  no_refining_prefix      | REG0=XMM_R():rw:dq:i32 MEM0:r:dq:i32
SHA1RNDS4            | SHA            | SHA            | SHA            | 0x0F 0x3A 0xCC MOD[0b11] MOD=3  REG[rrr] RM[nnn]  no_refining_prefix     UIMM8() | REG0=XMM_R():rw:dq:i32 REG1=XMM_B():r:dq:i32 IMM0:r:b
SHA1RNDS4            | SHA            | SHA            | SHA            | 0x0F 0x3A 0xCC MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  no_refining_prefix     UIMM8() | REG0=XMM_R():rw:dq:i32 MEM0:r:dq:i32 IMM0:r:b
SHA256MSG1           | SHA            | SHA            | SHA            | 0x0F 0x38 0xCC MOD[0b11] MOD=3  REG[rrr] RM[nnn]  no_refining_prefix             | REG0=XMM_R():rw:dq:i32 REG1=XMM_B():r:dq:i32
SHA256MSG1           | SHA            | SHA            | SHA            | 0x0F 0x38 0xCC MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  no_refining_prefix      | REG0=XMM_R():rw:dq:i32 MEM0:r:dq:i32
SHA256MSG2           | SHA            | SHA            | SHA            | 0x0F 0x38 0xCD MOD[0b11] MOD=3  REG[rrr] RM[nnn]  no_refining_prefix             | REG0=XMM_R():rw:dq:i32 REG1=XMM_B():r:dq:i32
SHA256MSG2           | SHA            | SHA            | SHA            | 0x0F 0x38 0xCD MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  no_refining_prefix      | REG0=XMM_R():rw:dq:i32 MEM0:r:dq:i32
SHA256RNDS2          | SHA            | SHA            | SHA            | 0x0F 0x38 0xCB MOD[0b11] MOD=3  REG[rrr] RM[nnn]  no_refining_prefix             | REG0=XMM_R():rw:dq:i32 REG1=XMM_B():r:dq:i32 REG2=XED_REG_XMM0:r:SUPP:dq:u8
SHA256RNDS2          | SHA            | SHA            | SHA            | 0x0F 0x38 0xCB MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  no_refining_prefix      | REG0=XMM_R():rw:dq:i32 MEM0:r:dq:i32 REG1=XED_REG_XMM0:r:SUPP:dq:u8
SHL                  | SHIFT          | BASE           | I186           | 0xC0 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM() UIMM8()                           | MEM0:rw:b IMM0:r:b
SHL                  | SHIFT          | BASE           | I186           | 0xC0 MOD[0b11] MOD=3 REG[0b100] RM[nnn] UIMM8()                                  | REG0=GPR8_B():rw IMM0:r:b
SHL                  | SHIFT          | BASE           | I186           | 0xC0 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM() UIMM8()                           | MEM0:rw:b IMM0:r:b
SHL                  | SHIFT          | BASE           | I186           | 0xC0 MOD[0b11] MOD=3 REG[0b110] RM[nnn] UIMM8()                                  | REG0=GPR8_B():rw IMM0:r:b
SHL                  | SHIFT          | BASE           | I186           | 0xC1 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM() UIMM8()                           | MEM0:rw:v IMM0:r:b
SHL                  | SHIFT          | BASE           | I186           | 0xC1 MOD[0b11] MOD=3 REG[0b100] RM[nnn] UIMM8()                                  | REG0=GPRv_B():rw IMM0:r:b
SHL                  | SHIFT          | BASE           | I186           | 0xC1 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM() UIMM8()                           | MEM0:rw:v IMM0:r:b
SHL                  | SHIFT          | BASE           | I186           | 0xC1 MOD[0b11] MOD=3 REG[0b110] RM[nnn] UIMM8()                                  | REG0=GPRv_B():rw IMM0:r:b
SHL                  | SHIFT          | BASE           | I86            | 0xD0 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM() ONE()                             | MEM0:rw:b IMM0:r:b:IMPL
SHL                  | SHIFT          | BASE           | I86            | 0xD0 MOD[0b11] MOD=3 REG[0b100] RM[nnn] ONE()                                    | REG0=GPR8_B():rw IMM0:r:b:IMPL
SHL                  | SHIFT          | BASE           | I86            | 0xD0 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM() ONE()                             | MEM0:rw:b IMM0:r:b:IMPL
SHL                  | SHIFT          | BASE           | I86            | 0xD0 MOD[0b11] MOD=3 REG[0b110] RM[nnn] ONE()                                    | REG0=GPR8_B():rw IMM0:r:b:IMPL
SHL                  | SHIFT          | BASE           | I86            | 0xD1 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM() ONE()                             | MEM0:rw:v IMM0:r:b:IMPL
SHL                  | SHIFT          | BASE           | I86            | 0xD1 MOD[0b11] MOD=3 REG[0b110] RM[nnn] ONE()                                    | REG0=GPRv_B():rw IMM0:r:b:IMPL
SHL                  | SHIFT          | BASE           | I86            | 0xD1 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM() ONE()                             | MEM0:rw:v IMM0:r:b:IMPL
SHL                  | SHIFT          | BASE           | I86            | 0xD1 MOD[0b11] MOD=3 REG[0b100] RM[nnn] ONE()                                    | REG0=GPRv_B():rw IMM0:r:b:IMPL
SHL                  | SHIFT          | BASE           | I86            | 0xD2 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM()                                   | MEM0:rw:b REG0=XED_REG_CL:r:IMPL
SHL                  | SHIFT          | BASE           | I86            | 0xD2 MOD[0b11] MOD=3 REG[0b100] RM[nnn]                                          | REG0=GPR8_B():rw REG1=XED_REG_CL:r:IMPL
SHL                  | SHIFT          | BASE           | I86            | 0xD2 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM()                                   | MEM0:rw:b REG0=XED_REG_CL:r:IMPL
SHL                  | SHIFT          | BASE           | I86            | 0xD2 MOD[0b11] MOD=3 REG[0b110] RM[nnn]                                          | REG0=GPR8_B():rw REG1=XED_REG_CL:r:IMPL
SHL                  | SHIFT          | BASE           | I86            | 0xD3 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM()                                   | MEM0:rw:v REG0=XED_REG_CL:r:IMPL
SHL                  | SHIFT          | BASE           | I86            | 0xD3 MOD[0b11] MOD=3 REG[0b100] RM[nnn]                                          | REG0=GPRv_B():rw REG1=XED_REG_CL:r:IMPL
SHL                  | SHIFT          | BASE           | I86            | 0xD3 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM()                                   | MEM0:rw:v REG0=XED_REG_CL:r:IMPL
SHL                  | SHIFT          | BASE           | I86            | 0xD3 MOD[0b11] MOD=3 REG[0b110] RM[nnn]                                          | REG0=GPRv_B():rw REG1=XED_REG_CL:r:IMPL
SHLD                 | SHIFT          | BASE           | I386           | 0x0F 0xA4 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                        | MEM0:rcw:v REG0=GPRv_R():r IMM0:r:b
SHLD                 | SHIFT          | BASE           | I386           | 0x0F 0xA4 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                               | REG0=GPRv_B():rcw REG1=GPRv_R():r IMM0:r:b
SHLD                 | SHIFT          | BASE           | I386           | 0x0F 0xA5 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:rcw:v REG0=GPRv_R():r REG1=XED_REG_CL:r:IMPL
SHLD                 | SHIFT          | BASE           | I386           | 0x0F 0xA5 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_B():rcw REG1=GPRv_R():r REG2=XED_REG_CL:r:IMPL
SHR                  | SHIFT          | BASE           | I186           | 0xC0 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM() UIMM8()                           | MEM0:rw:b IMM0:r:b
SHR                  | SHIFT          | BASE           | I186           | 0xC0 MOD[0b11] MOD=3 REG[0b101] RM[nnn] UIMM8()                                  | REG0=GPR8_B():rw IMM0:r:b
SHR                  | SHIFT          | BASE           | I186           | 0xC1 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM() UIMM8()                           | MEM0:rw:v IMM0:r:b
SHR                  | SHIFT          | BASE           | I186           | 0xC1 MOD[0b11] MOD=3 REG[0b101] RM[nnn] UIMM8()                                  | REG0=GPRv_B():rw IMM0:r:b
SHR                  | SHIFT          | BASE           | I86            | 0xD0 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM() ONE()                             | MEM0:rw:b IMM0:r:b:IMPL
SHR                  | SHIFT          | BASE           | I86            | 0xD0 MOD[0b11] MOD=3 REG[0b101] RM[nnn] ONE()                                    | REG0=GPR8_B():rw IMM0:r:b:IMPL
SHR                  | SHIFT          | BASE           | I86            | 0xD1 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM() ONE()                             | MEM0:rw:v IMM0:r:b:IMPL
SHR                  | SHIFT          | BASE           | I86            | 0xD1 MOD[0b11] MOD=3 REG[0b101] RM[nnn] ONE()                                    | REG0=GPRv_B():rw IMM0:r:b:IMPL
SHR                  | SHIFT          | BASE           | I86            | 0xD2 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM()                                   | MEM0:rw:b REG0=XED_REG_CL:r:IMPL
SHR                  | SHIFT          | BASE           | I86            | 0xD2 MOD[0b11] MOD=3 REG[0b101] RM[nnn]                                          | REG0=GPR8_B():rw REG1=XED_REG_CL:r:IMPL
SHR                  | SHIFT          | BASE           | I86            | 0xD3 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM()                                   | MEM0:rw:v REG0=XED_REG_CL:r:IMPL
SHR                  | SHIFT          | BASE           | I86            | 0xD3 MOD[0b11] MOD=3 REG[0b101] RM[nnn]                                          | REG0=GPRv_B():rw REG1=XED_REG_CL:r:IMPL
SHRD                 | SHIFT          | BASE           | I386           | 0x0F 0xAC MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()                        | MEM0:rcw:v REG0=GPRv_R():r IMM0:r:b
SHRD                 | SHIFT          | BASE           | I386           | 0x0F 0xAC MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()                               | REG0=GPRv_B():rcw REG1=GPRv_R():r IMM0:r:b
SHRD                 | SHIFT          | BASE           | I386           | 0x0F 0xAD MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                | MEM0:rcw:v REG0=GPRv_R():r REG1=XED_REG_CL:r:IMPL
SHRD                 | SHIFT          | BASE           | I386           | 0x0F 0xAD MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_B():rcw REG1=GPRv_R():r REG2=XED_REG_CL:r:IMPL
SIDT                 | SYSTEM         | BASE           | I286REAL       | 0x0F 0x01 MOD[mm] MOD!=3 REG[0b001] RM[nnn] not64 MODRM()                        | MEM0:w:s REG0=XED_REG_IDTR:r:SUPP
SIDT                 | SYSTEM         | BASE           | I286REAL       | 0x0F 0x01 MOD[mm] MOD!=3 REG[0b001] RM[nnn] mode64 FORCE64() MODRM()             | MEM0:w:s64 REG0=XED_REG_IDTR:r:SUPP
SLDT                 | SYSTEM         | BASE           | I286PROTECTED  | 0x0F 0x00 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM()                              | MEM0:w:w REG0=XED_REG_LDTR:r:SUPP
SLDT                 | SYSTEM         | BASE           | I286PROTECTED  | 0x0F 0x00 MOD[0b11] MOD=3 REG[0b000] RM[nnn]                                     | REG0=GPRv_B():w REG1=XED_REG_LDTR:r:SUPP
SLWPCB               | XOP            | XOP            | XOP            | XOPV 0x12 VNP VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[0b001] RM[nnn]               | REG0=GPRy_B():w:y
SMSW                 | SYSTEM         | BASE           | I286REAL       | 0x0F 0x01 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM()                              | MEM0:w:w REG0=XED_REG_CR0:r:SUPP
SMSW                 | SYSTEM         | BASE           | I286REAL       | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b100] RM[nnn]                                     | REG0=GPRv_B():w REG1=XED_REG_CR0:r:SUPP
SPFLT                | KNCSCALAR      | KNC            | KNCV           | VV1 0xAE  VL128 VF2 V0F NOVSR MOD[0b11] MOD=3 REG[0b110] RM[nnn]  W0             | REG0=GPR32_B():r:d
SPFLT                | KNCSCALAR      | KNC            | KNCV           | VV1 0xAE  VL128 VF2 V0F NOVSR MOD[0b11] MOD=3 REG[0b110] RM[nnn]  W1             | REG0=GPR64_B():r:q
STC                  | FLAGOP         | BASE           | I86            | 0xF9                                                                             | 
STD                  | FLAGOP         | BASE           | I86            | 0xFD                                                                             | 
STI                  | FLAGOP         | BASE           | I86            | 0xFB                                                                             | 
STMXCSR              | SSE            | SSE            | SSEMXCSR       | 0x0F 0xAE MOD[mm] MOD!=3 REG[0b011] RM[nnn] no_refining_prefix  MODRM()          | MEM0:w:d REG0=XED_REG_MXCSR:r:SUPP
STOSB                | STRINGOP       | BASE           | I86            | 0xAA norep                                                                       | MEM0:w:SUPP:b BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_AL:r:SUPP
STOSD                | STRINGOP       | BASE           | I386           | 0xAB mode16 66_prefix  norep                                                     | MEM0:w:SUPP:d BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_EAX:r:SUPP
STOSD                | STRINGOP       | BASE           | I386           | 0xAB mode32 no66_prefix  norep                                                   | MEM0:w:SUPP:d BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_EAX:r:SUPP
STOSD                | STRINGOP       | BASE           | I386           | 0xAB mode64 norexw_prefix no66_prefix  norep                                     | MEM0:w:SUPP:d BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_EAX:r:SUPP
STOSW                | STRINGOP       | BASE           | I86            | 0xAB mode16 no66_prefix  norep                                                   | MEM0:w:SUPP:w BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_AX:r:SUPP
STOSW                | STRINGOP       | BASE           | I86            | 0xAB mode32 66_prefix  norep                                                     | MEM0:w:SUPP:w BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_AX:r:SUPP
STOSW                | STRINGOP       | BASE           | I86            | 0xAB mode64 norexw_prefix 66_prefix  norep                                       | MEM0:w:SUPP:w BASE0=ArDI():rw:SUPP SEG0=FINAL_ESEG():r:SUPP REG0=XED_REG_AX:r:SUPP
STR                  | SYSTEM         | BASE           | I286PROTECTED  | 0x0F 0x00 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM()                              | MEM0:w:w REG0=XED_REG_TR:r:SUPP
STR                  | SYSTEM         | BASE           | I286PROTECTED  | 0x0F 0x00 MOD[0b11] MOD=3 REG[0b001] RM[nnn]                                     | REG0=GPRv_B():w REG1=XED_REG_TR:r:SUPP
SUB                  | BINARY         | BASE           | I86            | 0x80 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM() SIMM8() nolock_prefix             | MEM0:rw:b IMM0:r:b:i8
SUB                  | BINARY         | BASE           | I86            | 0x80 MOD[0b11] MOD=3 REG[0b101] RM[nnn] SIMM8()                                  | REG0=GPR8_B():rw IMM0:r:b:i8
SUB                  | BINARY         | BASE           | I86            | 0x81 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM() SIMMz() nolock_prefix             | MEM0:rw:v IMM0:r:z
SUB                  | BINARY         | BASE           | I86            | 0x81 MOD[0b11] MOD=3 REG[0b101] RM[nnn] SIMMz()                                  | REG0=GPRv_B():rw IMM0:r:z
SUB                  | BINARY         | BASE           | I86            | 0x82 MOD[mm] MOD!=3 REG[0b101] RM[nnn] not64 MODRM() SIMM8() nolock_prefix       | MEM0:rw:b IMM0:r:b:i8
SUB                  | BINARY         | BASE           | I86            | 0x82 MOD[0b11] MOD=3 REG[0b101] RM[nnn] not64 SIMM8()                            | REG0=GPR8_B():rw IMM0:r:b:i8
SUB                  | BINARY         | BASE           | I86            | 0x83 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM() SIMM8() nolock_prefix             | MEM0:rw:v IMM0:r:b:i8
SUB                  | BINARY         | BASE           | I86            | 0x83 MOD[0b11] MOD=3 REG[0b101] RM[nnn] SIMM8()                                  | REG0=GPRv_B():rw IMM0:r:b:i8
SUB                  | BINARY         | BASE           | I86            | 0x28 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                       | MEM0:rw:b REG0=GPR8_R():r
SUB                  | BINARY         | BASE           | I86            | 0x28 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPR8_B():rw REG1=GPR8_R():r
SUB                  | BINARY         | BASE           | I86            | 0x29 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                       | MEM0:rw:v REG0=GPRv_R():r
SUB                  | BINARY         | BASE           | I86            | 0x29 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPRv_B():rw REG1=GPRv_R():r
SUB                  | BINARY         | BASE           | I86            | 0x2A MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPR8_R():rw REG1=GPR8_B():r
SUB                  | BINARY         | BASE           | I86            | 0x2A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=GPR8_R():rw MEM0:r:b
SUB                  | BINARY         | BASE           | I86            | 0x2B MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPRv_R():rw REG1=GPRv_B():r
SUB                  | BINARY         | BASE           | I86            | 0x2B MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=GPRv_R():rw MEM0:r:v
SUB                  | BINARY         | BASE           | I86            | 0x2C SIMM8()                                                                     | REG0=XED_REG_AL:rw:IMPL IMM0:r:b:i8
SUB                  | BINARY         | BASE           | I86            | 0x2D SIMMz()                                                                     | REG0=OrAX():rw:IMPL IMM0:r:z
SUB_LOCK             | BINARY         | BASE           | I86            | 0x80 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM() SIMM8() lock_prefix               | MEM0:rw:b IMM0:r:b:i8
SUB_LOCK             | BINARY         | BASE           | I86            | 0x81 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM() SIMMz() lock_prefix               | MEM0:rw:v IMM0:r:z
SUB_LOCK             | BINARY         | BASE           | I86            | 0x82 MOD[mm] MOD!=3 REG[0b101] RM[nnn] not64 MODRM() SIMM8() lock_prefix         | MEM0:rw:b IMM0:r:b:i8
SUB_LOCK             | BINARY         | BASE           | I86            | 0x83 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM() SIMM8() lock_prefix               | MEM0:rw:v IMM0:r:b:i8
SUB_LOCK             | BINARY         | BASE           | I86            | 0x28 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                         | MEM0:rw:b REG0=GPR8_R():r
SUB_LOCK             | BINARY         | BASE           | I86            | 0x29 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                         | MEM0:rw:v REG0=GPRv_R():r
SYSCALL              | SYSCALL        | LONGMODE       | LONGMODE       | 0x0F 0x05 mode64 FORCE64()                                                       | REG0=XED_REG_RIP:w:SUPP REG1=XED_REG_RCX:w:SUPP REG2=XED_REG_R11:w:SUPP
SYSCALL_AMD          | SYSCALL        | BASE           | AMD            | 0x0F 0x05 not64 IGNORE66()                                                       | REG0=rIP():w:SUPP
SYSENTER             | SYSCALL        | BASE           | PPRO           | 0x0F 0x34 not64                                                                  | REG0=XED_REG_EIP:w:SUPP REG1=XED_REG_ESP:w:SUPP
SYSENTER             | SYSCALL        | BASE           | PPRO           | 0x0F 0x34 mode64                                                                 | REG0=XED_REG_RIP:w:SUPP REG1=XED_REG_RSP:w:SUPP
SYSEXIT              | SYSRET         | BASE           | PPRO           | 0x0F 0x35 not64                                                                  | REG0=XED_REG_EIP:w:SUPP  REG1=XED_REG_ESP:w:SUPP REG2=XED_REG_ECX:r:SUPP REG3=XED_REG_EDX:r:SUPP
SYSEXIT              | SYSRET         | BASE           | PPRO           | 0x0F 0x35 mode64                                                                 | REG0=XED_REG_RIP:w:SUPP  REG1=XED_REG_RSP:w:SUPP REG2=XED_REG_RCX:r:SUPP REG3=XED_REG_RDX:r:SUPP
SYSRET               | SYSRET         | LONGMODE       | LONGMODE       | 0x0F 0x07 mode64 norexw_prefix                                                   | REG0=XED_REG_EIP:w:SUPP  REG1=XED_REG_ECX:r:SUPP
SYSRET_AMD           | SYSRET         | BASE           | AMD            | 0x0F 0x07 not64                                                                  | REG0=XED_REG_EIP:w:SUPP
SYSRET64             | SYSRET         | LONGMODE       | LONGMODE       | 0x0F 0x07 mode64 rexw_prefix                                                     | REG0=XED_REG_RIP:w:SUPP  REG1=XED_REG_RCX:r:SUPP  REG2=XED_REG_R11:r:SUPP
T1MSKC               | TBM            | TBM            | TBM            | XOPV 0x01 VNP not64 VL128  XMAP9 MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM()       | REG0=VGPR32_N():w:d MEM0:r:d
T1MSKC               | TBM            | TBM            | TBM            | XOPV 0x01 VNP mode64 VL128  XMAP9 MOD[mm] MOD!=3 REG[0b111] RM[nnn] MODRM()      | REG0=VGPRy_N():w:y MEM0:r:y
T1MSKC               | TBM            | TBM            | TBM            | XOPV 0x01 VNP not64 VL128  XMAP9 MOD[0b11] MOD=3 REG[0b111] RM[nnn]              | REG0=VGPR32_N():w:d REG1=GPR32_B():r:d
T1MSKC               | TBM            | TBM            | TBM            | XOPV 0x01 VNP mode64 VL128  XMAP9 MOD[0b11] MOD=3 REG[0b111] RM[nnn]             | REG0=VGPRy_N():w:y REG1=GPRy_B():r:y
TEST                 | LOGICAL        | BASE           | I86            | 0xF6 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() SIMM8()                           | MEM0:r:b IMM0:r:b:i8
TEST                 | LOGICAL        | BASE           | I86            | 0xF6 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM() SIMM8()                           | MEM0:r:b IMM0:r:b:i8
TEST                 | LOGICAL        | BASE           | I86            | 0xF6 MOD[0b11] MOD=3 REG[0b000] RM[nnn] SIMM8()                                  | REG0=GPR8_B():r IMM0:r:b:i8
TEST                 | LOGICAL        | BASE           | I86            | 0xF6 MOD[0b11] MOD=3 REG[0b001] RM[nnn] SIMM8()                                  | REG0=GPR8_B():r IMM0:r:b:i8
TEST                 | LOGICAL        | BASE           | I86            | 0xF7 MOD[mm] MOD!=3 REG[0b000] RM[nnn] MODRM() SIMMz()                           | MEM0:r:v IMM0:r:z
TEST                 | LOGICAL        | BASE           | I86            | 0xF7 MOD[mm] MOD!=3 REG[0b001] RM[nnn] MODRM() SIMMz()                           | MEM0:r:v IMM0:r:z
TEST                 | LOGICAL        | BASE           | I86            | 0xF7 MOD[0b11] MOD=3 REG[0b000] RM[nnn] SIMMz()                                  | REG0=GPRv_B():r IMM0:r:z
TEST                 | LOGICAL        | BASE           | I86            | 0xF7 MOD[0b11] MOD=3 REG[0b001] RM[nnn] SIMMz()                                  | REG0=GPRv_B():r IMM0:r:z
TEST                 | LOGICAL        | BASE           | I86            | 0x84 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | MEM0:r:b REG0=GPR8_R():r
TEST                 | LOGICAL        | BASE           | I86            | 0x84 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPR8_B():r REG1=GPR8_R():r
TEST                 | LOGICAL        | BASE           | I86            | 0x85 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | MEM0:r:v REG0=GPRv_R():r
TEST                 | LOGICAL        | BASE           | I86            | 0x85 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPRv_B():r REG1=GPRv_R():r
TEST                 | LOGICAL        | BASE           | I86            | 0xA8 SIMM8()                                                                     | REG0=XED_REG_AL:r:IMPL IMM0:r:b:i8
TEST                 | LOGICAL        | BASE           | I86            | 0xA9 SIMMz()                                                                     | REG0=OrAX():r:IMPL IMM0:r:z
TPAUSE               | WAITPKG        | WAITPKG        | WAITPKG        | 0x0F 0xAE MOD[0b11] MOD=3  REG[0b110] RM[nnn]  osz_refining_prefix      norexw_prefix | REG0=GPR32_B():r:d:u32 REG1=XED_REG_EDX:r:SUPP:d:u32 REG2=XED_REG_EAX:r:SUPP:d:u32
TPAUSE               | WAITPKG        | WAITPKG        | WAITPKG        | 0x0F 0xAE MOD[0b11] MOD=3  REG[0b110] RM[nnn]  osz_refining_prefix      mode64 rexw_prefix | REG0=GPR64_B():r:q:u64 REG1=XED_REG_EDX:r:SUPP:d:u32 REG2=XED_REG_EAX:r:SUPP:d:u32
TZCNT_VEX            | KNCSCALAR      | KNC            | KNCV           | VV1 0xBC  VL128 VF3 V0F W0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                | REG0=GPR32_R():w:d   REG1=GPR32_B():r:d
TZCNT_VEX            | KNCSCALAR      | KNC            | KNCV           | VV1 0xBC  VL128 VF3 V0F W1 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                | REG0=GPR64_R():w:q  REG1=GPR64_B():r:q
TZCNTI               | KNCSCALAR      | KNC            | KNCV           | VV1 0xBC  VL128 VF2 V0F W0 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                | REG0=GPR32_R():rw:d   REG1=GPR32_B():r:d
TZCNTI               | KNCSCALAR      | KNC            | KNCV           | VV1 0xBC  VL128 VF2 V0F W1 NOVSR MOD[0b11] MOD=3 REG[rrr] RM[nnn]                | REG0=GPR64_R():rw:q  REG1=GPR64_B():r:q
TZMSK                | TBM            | TBM            | TBM            | XOPV 0x01 VNP not64 VL128  XMAP9 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM()       | REG0=VGPR32_N():w:d MEM0:r:d
TZMSK                | TBM            | TBM            | TBM            | XOPV 0x01 VNP mode64 VL128  XMAP9 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM()      | REG0=VGPRy_N():w:y MEM0:r:y
TZMSK                | TBM            | TBM            | TBM            | XOPV 0x01 VNP not64 VL128  XMAP9 MOD[0b11] MOD=3 REG[0b100] RM[nnn]              | REG0=VGPR32_N():w:d REG1=GPR32_B():r:d
TZMSK                | TBM            | TBM            | TBM            | XOPV 0x01 VNP mode64 VL128  XMAP9 MOD[0b11] MOD=3 REG[0b100] RM[nnn]             | REG0=VGPRy_N():w:y REG1=GPRy_B():r:y
UD0                  | MISC           | BASE           | PPRO           | 0x0F 0xFF MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                               | REG0=GPR32_R():r MEM0:r:d
UD0                  | MISC           | BASE           | PPRO           | 0x0F 0xFF MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPR32_R():r REG1=GPR32_B():r
UD1                  | MISC           | BASE           | PPRO           | 0x0F 0xB9 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()                               | REG0=GPR32_R():r MEM0:r:d
UD1                  | MISC           | BASE           | PPRO           | 0x0F 0xB9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPR32_R():r REG1=GPR32_B():r
UD2                  | MISC           | BASE           | PPRO           | 0x0F 0x0B                                                                        | 
UMONITOR             | WAITPKG        | WAITPKG        | WAITPKG        | 0x0F 0xAE MOD[0b11] MOD=3  REG[0b110] RM[nnn]  f3_refining_prefix                | REG0=A_GPR_B():r
UMWAIT               | WAITPKG        | WAITPKG        | WAITPKG        | 0x0F 0xAE MOD[0b11] MOD=3  REG[0b110] RM[nnn]  f2_refining_prefix      norexw_prefix | REG0=GPR32_B():r:d:u32 REG1=XED_REG_EDX:r:SUPP:d:u32 REG2=XED_REG_EAX:r:SUPP:d:u32
UMWAIT               | WAITPKG        | WAITPKG        | WAITPKG        | 0x0F 0xAE MOD[0b11] MOD=3  REG[0b110] RM[nnn]  f2_refining_prefix      mode64 rexw_prefix | REG0=GPR64_B():r:q:u64 REG1=XED_REG_EDX:r:SUPP:d:u32 REG2=XED_REG_EAX:r:SUPP:d:u32
V4FMADDPS            | AVX512_4FMAPS  | AVX512EVEX     | AVX512_4FMAPS_512 | EVV 0x9A VF2 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_TUPLE1_4X() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32:MULTISOURCE4 MEM0:r:dq:f32
V4FMADDSS            | AVX512_4FMAPS  | AVX512EVEX     | AVX512_4FMAPS_SCALAR | EVV 0x9B VF2 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_TUPLE1_4X() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32:MULTISOURCE4 MEM0:r:dq:f32
V4FNMADDPS           | AVX512_4FMAPS  | AVX512EVEX     | AVX512_4FMAPS_512 | EVV 0xAA VF2 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_TUPLE1_4X() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32:MULTISOURCE4 MEM0:r:dq:f32
V4FNMADDSS           | AVX512_4FMAPS  | AVX512EVEX     | AVX512_4FMAPS_SCALAR | EVV 0xAB VF2 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_TUPLE1_4X() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32:MULTISOURCE4 MEM0:r:dq:f32
VADDPD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x58 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1              | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VADDPD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x58 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VADDPD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x58 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1              | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
VADDPD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x58 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VADDPD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x58 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1              | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VADDPD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x58 V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1 | REG0=ZMM_R3():w:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VADDPD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x58 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
VADDPS               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x58 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0              | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VADDPS               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x58 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VADDPS               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x58 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0              | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
VADDPS               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x58 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VADDPS               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x58 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0              | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VADDPS               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x58 VNP V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0 | REG0=ZMM_R3():w:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VADDPS               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x58 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
VADDSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x58 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                     | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VADDSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x58 VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1 | REG0=XMM_R3():w:dq:f64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VADDSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x58 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
VADDSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x58 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                     | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VADDSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x58 VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W0 | REG0=XMM_R3():w:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VADDSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x58 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
VAESDEC              | VAES           | AVX512EVEX     | AVX512_VAES_128 | EVV 0xDE V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128      ZEROING=0 MASK=0 | REG0=XMM_R3():w:dq:u128 REG1=XMM_N3():r:dq:u128 REG2=XMM_B3():r:dq:u128
VAESDEC              | VAES           | AVX512EVEX     | AVX512_VAES_128 | EVV 0xDE V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ZEROING=0 MASK=0  ESIZE_128_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u128 REG1=XMM_N3():r:dq:u128 MEM0:r:dq:u128
VAESDEC              | VAES           | AVX512EVEX     | AVX512_VAES_256 | EVV 0xDE V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256      ZEROING=0 MASK=0 | REG0=YMM_R3():w:qq:u128 REG1=YMM_N3():r:qq:u128 REG2=YMM_B3():r:qq:u128
VAESDEC              | VAES           | AVX512EVEX     | AVX512_VAES_256 | EVV 0xDE V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ZEROING=0 MASK=0  ESIZE_128_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u128 REG1=YMM_N3():r:qq:u128 MEM0:r:qq:u128
VAESDEC              | VAES           | AVX512EVEX     | AVX512_VAES_512 | EVV 0xDE V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512      ZEROING=0 MASK=0 | REG0=ZMM_R3():w:zu128 REG1=ZMM_N3():r:zu128 REG2=ZMM_B3():r:zu128
VAESDEC              | VAES           | AVX512EVEX     | AVX512_VAES_512 | EVV 0xDE V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ZEROING=0 MASK=0  ESIZE_128_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu128 REG1=ZMM_N3():r:zu128 MEM0:r:zd:u128
VAESDEC              | VAES           | VAES           | VAES           | VV1 0xDE V66 V0F38 MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256                      | REG0=YMM_R():w:qq:u128 REG1=YMM_N():r:qq:u128 REG2=YMM_B():r:qq:u128
VAESDEC              | VAES           | VAES           | VAES           | VV1 0xDE V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256               | REG0=YMM_R():w:qq:u128 REG1=YMM_N():r:qq:u128 MEM0:r:qq:u128
VAESDECLAST          | VAES           | AVX512EVEX     | AVX512_VAES_128 | EVV 0xDF V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128      ZEROING=0 MASK=0 | REG0=XMM_R3():w:dq:u128 REG1=XMM_N3():r:dq:u128 REG2=XMM_B3():r:dq:u128
VAESDECLAST          | VAES           | AVX512EVEX     | AVX512_VAES_128 | EVV 0xDF V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ZEROING=0 MASK=0  ESIZE_128_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u128 REG1=XMM_N3():r:dq:u128 MEM0:r:dq:u128
VAESDECLAST          | VAES           | AVX512EVEX     | AVX512_VAES_256 | EVV 0xDF V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256      ZEROING=0 MASK=0 | REG0=YMM_R3():w:qq:u128 REG1=YMM_N3():r:qq:u128 REG2=YMM_B3():r:qq:u128
VAESDECLAST          | VAES           | AVX512EVEX     | AVX512_VAES_256 | EVV 0xDF V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ZEROING=0 MASK=0  ESIZE_128_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u128 REG1=YMM_N3():r:qq:u128 MEM0:r:qq:u128
VAESDECLAST          | VAES           | AVX512EVEX     | AVX512_VAES_512 | EVV 0xDF V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512      ZEROING=0 MASK=0 | REG0=ZMM_R3():w:zu128 REG1=ZMM_N3():r:zu128 REG2=ZMM_B3():r:zu128
VAESDECLAST          | VAES           | AVX512EVEX     | AVX512_VAES_512 | EVV 0xDF V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ZEROING=0 MASK=0  ESIZE_128_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu128 REG1=ZMM_N3():r:zu128 MEM0:r:zd:u128
VAESDECLAST          | VAES           | VAES           | VAES           | VV1 0xDF V66 V0F38 MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256                      | REG0=YMM_R():w:qq:u128 REG1=YMM_N():r:qq:u128 REG2=YMM_B():r:qq:u128
VAESDECLAST          | VAES           | VAES           | VAES           | VV1 0xDF V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256               | REG0=YMM_R():w:qq:u128 REG1=YMM_N():r:qq:u128 MEM0:r:qq:u128
VAESENC              | VAES           | AVX512EVEX     | AVX512_VAES_128 | EVV 0xDC V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128      ZEROING=0 MASK=0 | REG0=XMM_R3():w:dq:u128 REG1=XMM_N3():r:dq:u128 REG2=XMM_B3():r:dq:u128
VAESENC              | VAES           | AVX512EVEX     | AVX512_VAES_128 | EVV 0xDC V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ZEROING=0 MASK=0  ESIZE_128_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u128 REG1=XMM_N3():r:dq:u128 MEM0:r:dq:u128
VAESENC              | VAES           | AVX512EVEX     | AVX512_VAES_256 | EVV 0xDC V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256      ZEROING=0 MASK=0 | REG0=YMM_R3():w:qq:u128 REG1=YMM_N3():r:qq:u128 REG2=YMM_B3():r:qq:u128
VAESENC              | VAES           | AVX512EVEX     | AVX512_VAES_256 | EVV 0xDC V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ZEROING=0 MASK=0  ESIZE_128_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u128 REG1=YMM_N3():r:qq:u128 MEM0:r:qq:u128
VAESENC              | VAES           | AVX512EVEX     | AVX512_VAES_512 | EVV 0xDC V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512      ZEROING=0 MASK=0 | REG0=ZMM_R3():w:zu128 REG1=ZMM_N3():r:zu128 REG2=ZMM_B3():r:zu128
VAESENC              | VAES           | AVX512EVEX     | AVX512_VAES_512 | EVV 0xDC V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ZEROING=0 MASK=0  ESIZE_128_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu128 REG1=ZMM_N3():r:zu128 MEM0:r:zd:u128
VAESENC              | VAES           | VAES           | VAES           | VV1 0xDC V66 V0F38 MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256                      | REG0=YMM_R():w:qq:u128 REG1=YMM_N():r:qq:u128 REG2=YMM_B():r:qq:u128
VAESENC              | VAES           | VAES           | VAES           | VV1 0xDC V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256               | REG0=YMM_R():w:qq:u128 REG1=YMM_N():r:qq:u128 MEM0:r:qq:u128
VAESENCLAST          | VAES           | AVX512EVEX     | AVX512_VAES_128 | EVV 0xDD V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128      ZEROING=0 MASK=0 | REG0=XMM_R3():w:dq:u128 REG1=XMM_N3():r:dq:u128 REG2=XMM_B3():r:dq:u128
VAESENCLAST          | VAES           | AVX512EVEX     | AVX512_VAES_128 | EVV 0xDD V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ZEROING=0 MASK=0  ESIZE_128_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u128 REG1=XMM_N3():r:dq:u128 MEM0:r:dq:u128
VAESENCLAST          | VAES           | AVX512EVEX     | AVX512_VAES_256 | EVV 0xDD V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256      ZEROING=0 MASK=0 | REG0=YMM_R3():w:qq:u128 REG1=YMM_N3():r:qq:u128 REG2=YMM_B3():r:qq:u128
VAESENCLAST          | VAES           | AVX512EVEX     | AVX512_VAES_256 | EVV 0xDD V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ZEROING=0 MASK=0  ESIZE_128_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u128 REG1=YMM_N3():r:qq:u128 MEM0:r:qq:u128
VAESENCLAST          | VAES           | AVX512EVEX     | AVX512_VAES_512 | EVV 0xDD V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512      ZEROING=0 MASK=0 | REG0=ZMM_R3():w:zu128 REG1=ZMM_N3():r:zu128 REG2=ZMM_B3():r:zu128
VAESENCLAST          | VAES           | AVX512EVEX     | AVX512_VAES_512 | EVV 0xDD V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ZEROING=0 MASK=0  ESIZE_128_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu128 REG1=ZMM_N3():r:zu128 MEM0:r:zd:u128
VAESENCLAST          | VAES           | VAES           | VAES           | VV1 0xDD V66 V0F38 MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256                      | REG0=YMM_R():w:qq:u128 REG1=YMM_N():r:qq:u128 REG2=YMM_B():r:qq:u128
VAESENCLAST          | VAES           | VAES           | VAES           | VV1 0xDD V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256               | REG0=YMM_R():w:qq:u128 REG1=YMM_N():r:qq:u128 MEM0:r:qq:u128
VALIGND              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x03 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0   UIMM8()  | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32 IMM0:r:b
VALIGND              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x03 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
VALIGND              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x03 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0   UIMM8()  | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32 IMM0:r:b
VALIGND              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x03 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
VALIGND              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x03 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0   UIMM8()  | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32 IMM0:r:b
VALIGND              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x03 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
VALIGNQ              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x03 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1   UIMM8()  | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64 IMM0:r:b
VALIGNQ              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x03 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VALIGNQ              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x03 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1   UIMM8()  | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64 IMM0:r:b
VALIGNQ              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x03 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VALIGNQ              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x03 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1   UIMM8()  | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64 IMM0:r:b
VALIGNQ              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x03 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VANDNPD              | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_128   | EVV 0x55 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1              | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
VANDNPD              | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_128   | EVV 0x55 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VANDNPD              | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_256   | EVV 0x55 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1              | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
VANDNPD              | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_256   | EVV 0x55 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VANDNPD              | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_512   | EVV 0x55 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1              | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
VANDNPD              | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_512   | EVV 0x55 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
VANDNPS              | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_128   | EVV 0x55 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0              | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
VANDNPS              | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_128   | EVV 0x55 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VANDNPS              | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_256   | EVV 0x55 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0              | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
VANDNPS              | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_256   | EVV 0x55 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VANDNPS              | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_512   | EVV 0x55 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0              | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
VANDNPS              | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_512   | EVV 0x55 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
VANDPD               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_128   | EVV 0x54 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1              | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
VANDPD               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_128   | EVV 0x54 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VANDPD               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_256   | EVV 0x54 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1              | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
VANDPD               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_256   | EVV 0x54 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VANDPD               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_512   | EVV 0x54 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1              | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
VANDPD               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_512   | EVV 0x54 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
VANDPS               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_128   | EVV 0x54 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0              | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
VANDPS               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_128   | EVV 0x54 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VANDPS               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_256   | EVV 0x54 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0              | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
VANDPS               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_256   | EVV 0x54 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VANDPS               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_512   | EVV 0x54 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0              | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
VANDPS               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_512   | EVV 0x54 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
VBLENDMPD            | BLEND          | AVX512EVEX     | AVX512F_128    | EVV 0x65 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VBLENDMPD            | BLEND          | AVX512EVEX     | AVX512F_128    | EVV 0x65 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VBLENDMPD            | BLEND          | AVX512EVEX     | AVX512F_256    | EVV 0x65 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
VBLENDMPD            | BLEND          | AVX512EVEX     | AVX512F_256    | EVV 0x65 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VBLENDMPD            | BLEND          | AVX512EVEX     | AVX512F_512    | EVV 0x65 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VBLENDMPD            | BLEND          | AVX512EVEX     | AVX512F_512    | EVV 0x65 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
VBLENDMPS            | BLEND          | AVX512EVEX     | AVX512F_128    | EVV 0x65 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VBLENDMPS            | BLEND          | AVX512EVEX     | AVX512F_128    | EVV 0x65 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VBLENDMPS            | BLEND          | AVX512EVEX     | AVX512F_256    | EVV 0x65 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
VBLENDMPS            | BLEND          | AVX512EVEX     | AVX512F_256    | EVV 0x65 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VBLENDMPS            | BLEND          | AVX512EVEX     | AVX512F_512    | EVV 0x65 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VBLENDMPS            | BLEND          | AVX512EVEX     | AVX512F_512    | EVV 0x65 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
VBROADCASTF32X2      | BROADCAST      | AVX512EVEX     | AVX512DQ_256   | EVV 0x19 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32 EMX_BROADCAST_2TO8_32
VBROADCASTF32X2      | BROADCAST      | AVX512EVEX     | AVX512DQ_256   | EVV 0x19 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_TUPLE2() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:f32 EMX_BROADCAST_2TO8_32
VBROADCASTF32X2      | BROADCAST      | AVX512EVEX     | AVX512DQ_512   | EVV 0x19 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32 EMX_BROADCAST_2TO16_32
VBROADCASTF32X2      | BROADCAST      | AVX512EVEX     | AVX512DQ_512   | EVV 0x19 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_TUPLE2() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:f32 EMX_BROADCAST_2TO16_32
VBROADCASTF32X4      | BROADCAST      | AVX512EVEX     | AVX512F_256    | EVV 0x1A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_TUPLE4() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:f32 EMX_BROADCAST_4TO8_32
VBROADCASTF32X4      | BROADCAST      | AVX512EVEX     | AVX512F_512    | EVV 0x1A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_TUPLE4() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:f32 EMX_BROADCAST_4TO16_32
VBROADCASTF32X8      | BROADCAST      | AVX512EVEX     | AVX512DQ_512   | EVV 0x1B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_TUPLE8() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:f32 EMX_BROADCAST_8TO16_32
VBROADCASTF64X2      | BROADCAST      | AVX512EVEX     | AVX512DQ_256   | EVV 0x1A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_TUPLE2() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:f64 EMX_BROADCAST_2TO4_64
VBROADCASTF64X2      | BROADCAST      | AVX512EVEX     | AVX512DQ_512   | EVV 0x1A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_TUPLE2() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:f64 EMX_BROADCAST_2TO8_64
VBROADCASTF64X4      | BROADCAST      | AVX512EVEX     | AVX512F_512    | EVV 0x1B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_TUPLE4() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:f64 EMX_BROADCAST_4TO8_64
VBROADCASTI32X2      | BROADCAST      | AVX512EVEX     | AVX512DQ_128   | EVV 0x59 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32 EMX_BROADCAST_2TO4_32
VBROADCASTI32X2      | BROADCAST      | AVX512EVEX     | AVX512DQ_128   | EVV 0x59 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_TUPLE2() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:u32 EMX_BROADCAST_2TO4_32
VBROADCASTI32X2      | BROADCAST      | AVX512EVEX     | AVX512DQ_256   | EVV 0x59 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32 EMX_BROADCAST_2TO8_32
VBROADCASTI32X2      | BROADCAST      | AVX512EVEX     | AVX512DQ_256   | EVV 0x59 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_TUPLE2() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:u32 EMX_BROADCAST_2TO8_32
VBROADCASTI32X2      | BROADCAST      | AVX512EVEX     | AVX512DQ_512   | EVV 0x59 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32 EMX_BROADCAST_2TO16_32
VBROADCASTI32X2      | BROADCAST      | AVX512EVEX     | AVX512DQ_512   | EVV 0x59 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_TUPLE2() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:u32 EMX_BROADCAST_2TO16_32
VBROADCASTI32X4      | BROADCAST      | AVX512EVEX     | AVX512F_256    | EVV 0x5A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_TUPLE4() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u32 EMX_BROADCAST_4TO8_32
VBROADCASTI32X4      | BROADCAST      | AVX512EVEX     | AVX512F_512    | EVV 0x5A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_TUPLE4() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u32 EMX_BROADCAST_4TO16_32
VBROADCASTI32X8      | BROADCAST      | AVX512EVEX     | AVX512DQ_512   | EVV 0x5B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_TUPLE8() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u32 EMX_BROADCAST_8TO16_32
VBROADCASTI64X2      | BROADCAST      | AVX512EVEX     | AVX512DQ_256   | EVV 0x5A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_TUPLE2() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u64 EMX_BROADCAST_2TO4_64
VBROADCASTI64X2      | BROADCAST      | AVX512EVEX     | AVX512DQ_512   | EVV 0x5A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_TUPLE2() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u64 EMX_BROADCAST_2TO8_64
VBROADCASTI64X4      | BROADCAST      | AVX512EVEX     | AVX512F_512    | EVV 0x5B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_TUPLE4() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u64 EMX_BROADCAST_4TO8_64
VBROADCASTSD         | BROADCAST      | AVX512EVEX     | AVX512F_256    | EVV 0x19 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_TUPLE1() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:f64 EMX_BROADCAST_1TO4_64
VBROADCASTSD         | BROADCAST      | AVX512EVEX     | AVX512F_256    | EVV 0x19 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR    | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64 EMX_BROADCAST_1TO4_64
VBROADCASTSD         | BROADCAST      | AVX512EVEX     | AVX512F_512    | EVV 0x19 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_TUPLE1() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:f64 EMX_BROADCAST_1TO8_64
VBROADCASTSD         | BROADCAST      | AVX512EVEX     | AVX512F_512    | EVV 0x19 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR    | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64 EMX_BROADCAST_1TO8_64
VBROADCASTSS         | BROADCAST      | AVX512EVEX     | AVX512F_128    | EVV 0x18 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_TUPLE1() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:d:f32 EMX_BROADCAST_1TO4_32
VBROADCASTSS         | BROADCAST      | AVX512EVEX     | AVX512F_128    | EVV 0x18 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32 EMX_BROADCAST_1TO4_32
VBROADCASTSS         | BROADCAST      | AVX512EVEX     | AVX512F_256    | EVV 0x18 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_TUPLE1() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:d:f32 EMX_BROADCAST_1TO8_32
VBROADCASTSS         | BROADCAST      | AVX512EVEX     | AVX512F_256    | EVV 0x18 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32 EMX_BROADCAST_1TO8_32
VBROADCASTSS         | BROADCAST      | AVX512EVEX     | AVX512F_512    | EVV 0x18 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_TUPLE1() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:d:f32 EMX_BROADCAST_1TO16_32
VBROADCASTSS         | BROADCAST      | AVX512EVEX     | AVX512F_512    | EVV 0x18 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32 EMX_BROADCAST_1TO16_32
VCMPPD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0xC2 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64 IMM0:r:b
VCMPPD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0xC2 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
VCMPPD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0xC2 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64 IMM0:r:b
VCMPPD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0xC2 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
VCMPPD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0xC2 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64 IMM0:r:b
VCMPPD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0xC2 V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W1    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw:TXT=SAESTR REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64 IMM0:r:b
VCMPPD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0xC2 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
VCMPPS               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0xC2 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32 IMM0:r:b
VCMPPS               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0xC2 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
VCMPPS               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0xC2 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32 IMM0:r:b
VCMPPS               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0xC2 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
VCMPPS               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0xC2 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32 IMM0:r:b
VCMPPS               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0xC2 VNP V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw:TXT=SAESTR REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32 IMM0:r:b
VCMPPS               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0xC2 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
VCMPSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0xC2 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64 IMM0:r:b
VCMPSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0xC2 VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W1    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw:TXT=SAESTR REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64 IMM0:r:b
VCMPSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0xC2 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_SCALAR() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64 IMM0:r:b
VCMPSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0xC2 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32 IMM0:r:b
VCMPSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0xC2 VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W0    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw:TXT=SAESTR REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32 IMM0:r:b
VCMPSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0xC2 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_SCALAR() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32 IMM0:r:b
VCOMISD              | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2F V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1  NOEVSR  ZEROING=0 MASK=0  FIX_ROUND_LEN128() | REG0=XMM_R3():r:dq:f64 REG1=XMM_B3():r:dq:f64
VCOMISD              | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2F V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W1  NOEVSR  ZEROING=0 MASK=0 | REG0=XMM_R3():r:dq:f64:TXT=SAESTR REG1=XMM_B3():r:dq:f64
VCOMISD              | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2F V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_SCALAR() FIX_ROUND_LEN128() | REG0=XMM_R3():r:dq:f64 MEM0:r:q:f64
VCOMISS              | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2F VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0  NOEVSR  ZEROING=0 MASK=0 FIX_ROUND_LEN128() | REG0=XMM_R3():r:dq:f32 REG1=XMM_B3():r:dq:f32
VCOMISS              | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2F VNP V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W0  NOEVSR  ZEROING=0 MASK=0 | REG0=XMM_R3():r:dq:f32:TXT=SAESTR REG1=XMM_B3():r:dq:f32
VCOMISS              | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2F VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_SCALAR() FIX_ROUND_LEN128() | REG0=XMM_R3():r:dq:f32 MEM0:r:d:f32
VCOMPRESSPD          | COMPRESS       | AVX512EVEX     | AVX512F_128    | EVV 0x8A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:w:dq:f64 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:f64
VCOMPRESSPD          | COMPRESS       | AVX512EVEX     | AVX512F_128    | EVV 0x8A V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR    | REG0=XMM_B3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:f64
VCOMPRESSPD          | COMPRESS       | AVX512EVEX     | AVX512F_256    | EVV 0x8A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:w:qq:f64 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:f64
VCOMPRESSPD          | COMPRESS       | AVX512EVEX     | AVX512F_256    | EVV 0x8A V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR    | REG0=YMM_B3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:f64
VCOMPRESSPD          | COMPRESS       | AVX512EVEX     | AVX512F_512    | EVV 0x8A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:w:zd:f64 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zf64
VCOMPRESSPD          | COMPRESS       | AVX512EVEX     | AVX512F_512    | EVV 0x8A V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR    | REG0=ZMM_B3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zf64
VCOMPRESSPS          | COMPRESS       | AVX512EVEX     | AVX512F_128    | EVV 0x8A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:w:dq:f32 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:f32
VCOMPRESSPS          | COMPRESS       | AVX512EVEX     | AVX512F_128    | EVV 0x8A V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_B3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:f32
VCOMPRESSPS          | COMPRESS       | AVX512EVEX     | AVX512F_256    | EVV 0x8A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:w:qq:f32 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:f32
VCOMPRESSPS          | COMPRESS       | AVX512EVEX     | AVX512F_256    | EVV 0x8A V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=YMM_B3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:f32
VCOMPRESSPS          | COMPRESS       | AVX512EVEX     | AVX512F_512    | EVV 0x8A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:w:zd:f32 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zf32
VCOMPRESSPS          | COMPRESS       | AVX512EVEX     | AVX512F_512    | EVV 0x8A V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=ZMM_B3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zf32
VCVTDQ2PD            | CONVERT        | AVX512EVEX     | AVX512F_128    | EVV 0xE6 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR      | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i32
VCVTDQ2PD            | CONVERT        | AVX512EVEX     | AVX512F_128    | EVV 0xE6 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:i32:TXT=BCASTSTR
VCVTDQ2PD            | CONVERT        | AVX512EVEX     | AVX512F_256    | EVV 0xE6 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR      | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i32
VCVTDQ2PD            | CONVERT        | AVX512EVEX     | AVX512F_256    | EVV 0xE6 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:i32:TXT=BCASTSTR
VCVTDQ2PD            | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0xE6 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR      | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:i32
VCVTDQ2PD            | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0xE6 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:i32:TXT=BCASTSTR
VCVTDQ2PS            | CONVERT        | AVX512EVEX     | AVX512F_128    | EVV 0x5B VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR      | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i32
VCVTDQ2PS            | CONVERT        | AVX512EVEX     | AVX512F_128    | EVV 0x5B VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:i32:TXT=BCASTSTR
VCVTDQ2PS            | CONVERT        | AVX512EVEX     | AVX512F_256    | EVV 0x5B VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR      | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:i32
VCVTDQ2PS            | CONVERT        | AVX512EVEX     | AVX512F_256    | EVV 0x5B VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:i32:TXT=BCASTSTR
VCVTDQ2PS            | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x5B VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR      | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zi32
VCVTDQ2PS            | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x5B VNP V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0  NOEVSR | REG0=ZMM_R3():w:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zi32
VCVTDQ2PS            | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x5B VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:i32:TXT=BCASTSTR
VCVTFXPNTPS2DQ       | CONVERT        | KNCE           | KNCE           | KVV 0xCB V0F3A V66  W0  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() UPCONVERT_FLT32() | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  MEM0:r:zv:TXT=CONVERT:TXT=NT IMM0:r:b
VCVTFXPNTPS2DQ       | CONVERT        | KNCE           | KNCE           | KVV 0xCB V0F3A V66  W0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] UIMM8() NR=1 KNC_SAE() | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  REG2=ZMM_B3():r:zf32:TXT=SAEC  IMM0:r:b
VCVTFXPNTPS2DQ       | CONVERT        | KNCE           | KNCE           | KVV 0xCB V0F3A V66  W0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] UIMM8() NR=0 REG_SWIZZLE32() | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  REG2=ZMM_B3():r:zf32:TXT=REGSWIZ IMM0:r:b
VCVTFXPNTPS2UDQ      | CONVERT        | KNCE           | KNCE           | KVV 0xCA V0F3A V66  W0  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() UPCONVERT_FLT32() | REG0=ZMM_R3():rw:zud REG1=MASK1():r:mskw  MEM0:r:zv:TXT=CONVERT:TXT=NT IMM0:r:b
VCVTFXPNTPS2UDQ      | CONVERT        | KNCE           | KNCE           | KVV 0xCA V0F3A V66  W0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] UIMM8() NR=1 KNC_SAE() | REG0=ZMM_R3():rw:zud REG1=MASK1():r:mskw  REG2=ZMM_B3():r:zf32:TXT=SAEC  IMM0:r:b
VCVTFXPNTPS2UDQ      | CONVERT        | KNCE           | KNCE           | KVV 0xCA V0F3A V66  W0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] UIMM8() NR=0 REG_SWIZZLE32() | REG0=ZMM_R3():rw:zud REG1=MASK1():r:mskw  REG2=ZMM_B3():r:zf32:TXT=REGSWIZ IMM0:r:b
VCVTNE2PS2BF16       | CONVERT        | AVX512EVEX     | AVX512_BF16_128 | EVV 0x72 VF2 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():w:dq:bf16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VCVTNE2PS2BF16       | CONVERT        | AVX512EVEX     | AVX512_BF16_128 | EVV 0x72 VF2 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:bf16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VCVTNE2PS2BF16       | CONVERT        | AVX512EVEX     | AVX512_BF16_256 | EVV 0x72 VF2 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():w:qq:bf16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
VCVTNE2PS2BF16       | CONVERT        | AVX512EVEX     | AVX512_BF16_256 | EVV 0x72 VF2 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:bf16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VCVTNE2PS2BF16       | CONVERT        | AVX512EVEX     | AVX512_BF16_512 | EVV 0x72 VF2 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():w:zbf16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VCVTNE2PS2BF16       | CONVERT        | AVX512EVEX     | AVX512_BF16_512 | EVV 0x72 VF2 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zbf16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
VCVTNEPS2BF16        | CONVERT        | AVX512EVEX     | AVX512_BF16_128 | EVV 0x72 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_R3():w:dq:bf16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
VCVTNEPS2BF16        | CONVERT        | AVX512EVEX     | AVX512_BF16_128 | EVV 0x72 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:bf16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VCVTNEPS2BF16        | CONVERT        | AVX512EVEX     | AVX512_BF16_256 | EVV 0x72 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=XMM_R3():w:dq:bf16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
VCVTNEPS2BF16        | CONVERT        | AVX512EVEX     | AVX512_BF16_256 | EVV 0x72 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:bf16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VCVTNEPS2BF16        | CONVERT        | AVX512EVEX     | AVX512_BF16_512 | EVV 0x72 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=YMM_R3():w:qq:bf16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
VCVTNEPS2BF16        | CONVERT        | AVX512EVEX     | AVX512_BF16_512 | EVV 0x72 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:bf16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VCVTPD2DQ            | CONVERT        | AVX512EVEX     | AVX512F_128    | EVV 0xE6 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR      | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64
VCVTPD2DQ            | CONVERT        | AVX512EVEX     | AVX512F_128    | EVV 0xE6 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VCVTPD2DQ            | CONVERT        | AVX512EVEX     | AVX512F_256    | EVV 0xE6 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR      | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64
VCVTPD2DQ            | CONVERT        | AVX512EVEX     | AVX512F_256    | EVV 0xE6 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VCVTPD2DQ            | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0xE6 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR      | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
VCVTPD2DQ            | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0xE6 VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1  NOEVSR | REG0=YMM_R3():w:qq:i32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
VCVTPD2DQ            | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0xE6 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VCVTPD2PS            | CONVERT        | AVX512EVEX     | AVX512F_128    | EVV 0x5A V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR      | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64
VCVTPD2PS            | CONVERT        | AVX512EVEX     | AVX512F_128    | EVV 0x5A V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VCVTPD2PS            | CONVERT        | AVX512EVEX     | AVX512F_256    | EVV 0x5A V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR      | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64
VCVTPD2PS            | CONVERT        | AVX512EVEX     | AVX512F_256    | EVV 0x5A V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VCVTPD2PS            | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x5A V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR      | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
VCVTPD2PS            | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x5A V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1  NOEVSR | REG0=YMM_R3():w:qq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
VCVTPD2PS            | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x5A V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VCVTPD2QQ            | CONVERT        | AVX512EVEX     | AVX512DQ_128   | EVV 0x7B V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR      | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64
VCVTPD2QQ            | CONVERT        | AVX512EVEX     | AVX512DQ_128   | EVV 0x7B V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VCVTPD2QQ            | CONVERT        | AVX512EVEX     | AVX512DQ_256   | EVV 0x7B V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR      | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64
VCVTPD2QQ            | CONVERT        | AVX512EVEX     | AVX512DQ_256   | EVV 0x7B V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VCVTPD2QQ            | CONVERT        | AVX512EVEX     | AVX512DQ_512   | EVV 0x7B V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR      | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
VCVTPD2QQ            | CONVERT        | AVX512EVEX     | AVX512DQ_512   | EVV 0x7B V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1  NOEVSR | REG0=ZMM_R3():w:zi64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
VCVTPD2QQ            | CONVERT        | AVX512EVEX     | AVX512DQ_512   | EVV 0x7B V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VCVTPD2UDQ           | CONVERT        | AVX512EVEX     | AVX512F_128    | EVV 0x79 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR      | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64
VCVTPD2UDQ           | CONVERT        | AVX512EVEX     | AVX512F_128    | EVV 0x79 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VCVTPD2UDQ           | CONVERT        | AVX512EVEX     | AVX512F_256    | EVV 0x79 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR      | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64
VCVTPD2UDQ           | CONVERT        | AVX512EVEX     | AVX512F_256    | EVV 0x79 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VCVTPD2UDQ           | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x79 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR      | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
VCVTPD2UDQ           | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x79 VNP V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1  NOEVSR | REG0=YMM_R3():w:qq:u32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
VCVTPD2UDQ           | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x79 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VCVTPD2UQQ           | CONVERT        | AVX512EVEX     | AVX512DQ_128   | EVV 0x79 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR      | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64
VCVTPD2UQQ           | CONVERT        | AVX512EVEX     | AVX512DQ_128   | EVV 0x79 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VCVTPD2UQQ           | CONVERT        | AVX512EVEX     | AVX512DQ_256   | EVV 0x79 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR      | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64
VCVTPD2UQQ           | CONVERT        | AVX512EVEX     | AVX512DQ_256   | EVV 0x79 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VCVTPD2UQQ           | CONVERT        | AVX512EVEX     | AVX512DQ_512   | EVV 0x79 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR      | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
VCVTPD2UQQ           | CONVERT        | AVX512EVEX     | AVX512DQ_512   | EVV 0x79 V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1  NOEVSR | REG0=ZMM_R3():w:zu64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
VCVTPD2UQQ           | CONVERT        | AVX512EVEX     | AVX512DQ_512   | EVV 0x79 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VCVTPH2PS            | CONVERT        | AVX512EVEX     | AVX512F_128    | EVV 0x13 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f16
VCVTPH2PS            | CONVERT        | AVX512EVEX     | AVX512F_128    | EVV 0x13 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_16_BITS() NELEM_HALFMEM() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:f16
VCVTPH2PS            | CONVERT        | AVX512EVEX     | AVX512F_256    | EVV 0x13 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f16
VCVTPH2PS            | CONVERT        | AVX512EVEX     | AVX512F_256    | EVV 0x13 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_16_BITS() NELEM_HALFMEM() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:f16
VCVTPH2PS            | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x13 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f16
VCVTPH2PS            | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x13 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0  NOEVSR | REG0=ZMM_R3():w:zf32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f16
VCVTPH2PS            | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x13 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_16_BITS() NELEM_HALFMEM() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:f16
VCVTPS2DQ            | CONVERT        | AVX512EVEX     | AVX512F_128    | EVV 0x5B V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR      | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
VCVTPS2DQ            | CONVERT        | AVX512EVEX     | AVX512F_128    | EVV 0x5B V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VCVTPS2DQ            | CONVERT        | AVX512EVEX     | AVX512F_256    | EVV 0x5B V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR      | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
VCVTPS2DQ            | CONVERT        | AVX512EVEX     | AVX512F_256    | EVV 0x5B V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VCVTPS2DQ            | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x5B V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR      | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
VCVTPS2DQ            | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x5B V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0  NOEVSR | REG0=ZMM_R3():w:zi32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
VCVTPS2DQ            | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x5B V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VCVTPS2PD            | CONVERT        | AVX512EVEX     | AVX512F_128    | EVV 0x5A VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR      | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
VCVTPS2PD            | CONVERT        | AVX512EVEX     | AVX512F_128    | EVV 0x5A VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VCVTPS2PD            | CONVERT        | AVX512EVEX     | AVX512F_256    | EVV 0x5A VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR      | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
VCVTPS2PD            | CONVERT        | AVX512EVEX     | AVX512F_256    | EVV 0x5A VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VCVTPS2PD            | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x5A VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR      | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
VCVTPS2PD            | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x5A VNP V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0  NOEVSR | REG0=ZMM_R3():w:zf64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
VCVTPS2PD            | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x5A VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VCVTPS2PH            | CONVERT        | AVX512EVEX     | AVX512F_128    | EVV 0x1D V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR UIMM8() | REG0=XMM_B3():w:dq:f16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:f32 IMM0:r:b
VCVTPS2PH            | CONVERT        | AVX512EVEX     | AVX512F_128    | EVV 0x1D V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0 UIMM8()  ESIZE_16_BITS() NELEM_HALFMEM() | MEM0:w:q:f16 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:f32 IMM0:r:b
VCVTPS2PH            | CONVERT        | AVX512EVEX     | AVX512F_256    | EVV 0x1D V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR UIMM8() | REG0=XMM_B3():w:dq:f16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:f32 IMM0:r:b
VCVTPS2PH            | CONVERT        | AVX512EVEX     | AVX512F_256    | EVV 0x1D V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0 UIMM8()  ESIZE_16_BITS() NELEM_HALFMEM() | MEM0:w:dq:f16 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:f32 IMM0:r:b
VCVTPS2PH            | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x1D V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR UIMM8() | REG0=YMM_B3():w:qq:f16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zf32 IMM0:r:b
VCVTPS2PH            | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x1D V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0  NOEVSR UIMM8() | REG0=YMM_B3():w:qq:f16:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zf32 IMM0:r:b
VCVTPS2PH            | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x1D V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0 UIMM8()  ESIZE_16_BITS() NELEM_HALFMEM() | MEM0:w:qq:f16 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zf32 IMM0:r:b
VCVTPS2QQ            | CONVERT        | AVX512EVEX     | AVX512DQ_128   | EVV 0x7B V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR      | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
VCVTPS2QQ            | CONVERT        | AVX512EVEX     | AVX512DQ_128   | EVV 0x7B V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VCVTPS2QQ            | CONVERT        | AVX512EVEX     | AVX512DQ_256   | EVV 0x7B V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR      | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
VCVTPS2QQ            | CONVERT        | AVX512EVEX     | AVX512DQ_256   | EVV 0x7B V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VCVTPS2QQ            | CONVERT        | AVX512EVEX     | AVX512DQ_512   | EVV 0x7B V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR      | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
VCVTPS2QQ            | CONVERT        | AVX512EVEX     | AVX512DQ_512   | EVV 0x7B V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0  NOEVSR | REG0=ZMM_R3():w:zi64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
VCVTPS2QQ            | CONVERT        | AVX512EVEX     | AVX512DQ_512   | EVV 0x7B V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VCVTPS2UDQ           | CONVERT        | AVX512EVEX     | AVX512F_128    | EVV 0x79 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR      | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
VCVTPS2UDQ           | CONVERT        | AVX512EVEX     | AVX512F_128    | EVV 0x79 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VCVTPS2UDQ           | CONVERT        | AVX512EVEX     | AVX512F_256    | EVV 0x79 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR      | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
VCVTPS2UDQ           | CONVERT        | AVX512EVEX     | AVX512F_256    | EVV 0x79 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VCVTPS2UDQ           | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x79 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR      | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
VCVTPS2UDQ           | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x79 VNP V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0  NOEVSR | REG0=ZMM_R3():w:zu32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
VCVTPS2UDQ           | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x79 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VCVTPS2UQQ           | CONVERT        | AVX512EVEX     | AVX512DQ_128   | EVV 0x79 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR      | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
VCVTPS2UQQ           | CONVERT        | AVX512EVEX     | AVX512DQ_128   | EVV 0x79 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VCVTPS2UQQ           | CONVERT        | AVX512EVEX     | AVX512DQ_256   | EVV 0x79 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR      | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
VCVTPS2UQQ           | CONVERT        | AVX512EVEX     | AVX512DQ_256   | EVV 0x79 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VCVTPS2UQQ           | CONVERT        | AVX512EVEX     | AVX512DQ_512   | EVV 0x79 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR      | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
VCVTPS2UQQ           | CONVERT        | AVX512EVEX     | AVX512DQ_512   | EVV 0x79 V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0  NOEVSR | REG0=ZMM_R3():w:zu64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
VCVTPS2UQQ           | CONVERT        | AVX512EVEX     | AVX512DQ_512   | EVV 0x79 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VCVTQQ2PD            | CONVERT        | AVX512EVEX     | AVX512DQ_128   | EVV 0xE6 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR      | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64
VCVTQQ2PD            | CONVERT        | AVX512EVEX     | AVX512DQ_128   | EVV 0xE6 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VCVTQQ2PD            | CONVERT        | AVX512EVEX     | AVX512DQ_256   | EVV 0xE6 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR      | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64
VCVTQQ2PD            | CONVERT        | AVX512EVEX     | AVX512DQ_256   | EVV 0xE6 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VCVTQQ2PD            | CONVERT        | AVX512EVEX     | AVX512DQ_512   | EVV 0xE6 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR      | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
VCVTQQ2PD            | CONVERT        | AVX512EVEX     | AVX512DQ_512   | EVV 0xE6 VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1  NOEVSR | REG0=ZMM_R3():w:zi64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
VCVTQQ2PD            | CONVERT        | AVX512EVEX     | AVX512DQ_512   | EVV 0xE6 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VCVTQQ2PS            | CONVERT        | AVX512EVEX     | AVX512DQ_128   | EVV 0x5B VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR      | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u64
VCVTQQ2PS            | CONVERT        | AVX512EVEX     | AVX512DQ_128   | EVV 0x5B VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR
VCVTQQ2PS            | CONVERT        | AVX512EVEX     | AVX512DQ_256   | EVV 0x5B VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR      | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u64
VCVTQQ2PS            | CONVERT        | AVX512EVEX     | AVX512DQ_256   | EVV 0x5B VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR
VCVTQQ2PS            | CONVERT        | AVX512EVEX     | AVX512DQ_512   | EVV 0x5B VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR      | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu64
VCVTQQ2PS            | CONVERT        | AVX512EVEX     | AVX512DQ_512   | EVV 0x5B VNP V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1  NOEVSR | REG0=YMM_R3():w:qq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu64
VCVTQQ2PS            | CONVERT        | AVX512EVEX     | AVX512DQ_512   | EVV 0x5B VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR
VCVTSD2SI            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2D VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  not64  NOEVSR  ZEROING=0 MASK=0  FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:i32 REG1=XMM_B3():r:dq:f64
VCVTSD2SI            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2D VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64 W0  NOEVSR  ZEROING=0 MASK=0 EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:i32 REG1=XMM_B3():r:dq:f64
VCVTSD2SI            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2D VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  not64  NOEVSR  ZEROING=0 MASK=0 | REG0=GPR32_R():w:d:i32:TXT=ROUNDC REG1=XMM_B3():r:dq:f64
VCVTSD2SI            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2D VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  mode64 W0  NOEVSR  ZEROING=0 MASK=0 EVEXRR_ONE | REG0=GPR32_R():w:d:i32:TXT=ROUNDC REG1=XMM_B3():r:dq:f64
VCVTSD2SI            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2D VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  not64  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_GPR_WRITER_LDOP_Q()  FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:i32 MEM0:r:q:f64
VCVTSD2SI            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2D VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64 W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_GPR_WRITER_LDOP_Q() EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:i32 MEM0:r:q:f64
VCVTSD2SI            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2D VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64  W1  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR64_R():w:q:i64 REG1=XMM_B3():r:dq:f64
VCVTSD2SI            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2D VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  mode64  W1  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE | REG0=GPR64_R():w:q:i64:TXT=ROUNDC REG1=XMM_B3():r:dq:f64
VCVTSD2SI            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2D VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64  W1  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_GPR_WRITER_LDOP_Q() EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR64_R():w:q:i64 MEM0:r:q:f64
VCVTSD2SS            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x5A VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                     | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VCVTSD2SS            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x5A VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1 | REG0=XMM_R3():w:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VCVTSD2SS            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x5A VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
VCVTSD2USI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x79 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  not64  NOEVSR  ZEROING=0 MASK=0  FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:u32 REG1=XMM_B3():r:dq:f64
VCVTSD2USI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x79 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64 W0  NOEVSR  ZEROING=0 MASK=0 EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:u32 REG1=XMM_B3():r:dq:f64
VCVTSD2USI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x79 VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  not64  NOEVSR  ZEROING=0 MASK=0 | REG0=GPR32_R():w:d:u32:TXT=ROUNDC REG1=XMM_B3():r:dq:f64
VCVTSD2USI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x79 VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  mode64 W0  NOEVSR  ZEROING=0 MASK=0 EVEXRR_ONE | REG0=GPR32_R():w:d:u32:TXT=ROUNDC REG1=XMM_B3():r:dq:f64
VCVTSD2USI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x79 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  not64  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_GPR_WRITER_LDOP_Q() FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:u32 MEM0:r:q:f64
VCVTSD2USI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x79 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64 W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_GPR_WRITER_LDOP_Q() EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:u32 MEM0:r:q:f64
VCVTSD2USI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x79 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64  W1  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR64_R():w:q:u64 REG1=XMM_B3():r:dq:f64
VCVTSD2USI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x79 VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  mode64  W1  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE | REG0=GPR64_R():w:q:u64:TXT=ROUNDC REG1=XMM_B3():r:dq:f64
VCVTSD2USI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x79 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64  W1  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_GPR_WRITER_LDOP_Q() EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR64_R():w:q:u64 MEM0:r:q:f64
VCVTSI2SD            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2A VF2 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  not64    ZEROING=0 MASK=0 FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f64 REG1=XMM_N3():r:dq:f64 REG2=GPR32_B():r:d:i32
VCVTSI2SD            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2A VF2 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  mode64 W0    ZEROING=0 MASK=0  FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f64 REG1=XMM_N3():r:dq:f64 REG2=GPR32_B():r:d:i32
VCVTSI2SD            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2A VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  not64 ZEROING=0 MASK=0 BCRC=0  ESIZE_32_BITS() NELEM_GPR_READER() FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f64 REG1=XMM_N3():r:dq:f64 MEM0:r:d:i32
VCVTSI2SD            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2A VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  mode64 W0 ZEROING=0 MASK=0 BCRC=0  ESIZE_32_BITS() NELEM_GPR_READER() FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f64 REG1=XMM_N3():r:dq:f64 MEM0:r:d:i32
VCVTSI2SD            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2A VF2 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] mode64 W1 ZEROING=0 MASK=0 BCRC=0  FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f64 REG1=XMM_N3():r:dq:f64 REG2=GPR64_B():r:q:i64
VCVTSI2SD            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2A VF2 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn] mode64 W1 ZEROING=0 MASK=0 BCRC=1 FIX_ROUND_LEN128() AVX512_ROUND() | REG0=XMM_R3():w:dq:f64:TXT=ROUNDC REG1=XMM_N3():r:dq:f64 REG2=GPR64_B():r:q:i64
VCVTSI2SD            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2A VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM() mode64 W1 ZEROING=0 MASK=0 BCRC=0  ESIZE_64_BITS() NELEM_GPR_READER() FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f64 REG1=XMM_N3():r:dq:f64 MEM0:r:q:i64
VCVTSI2SS            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2A VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  not64    ZEROING=0 MASK=0 FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f32 REG1=XMM_N3():r:dq:f32 REG2=GPR32_B():r:d:i32
VCVTSI2SS            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2A VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64 W0    ZEROING=0 MASK=0 FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f32 REG1=XMM_N3():r:dq:f32 REG2=GPR32_B():r:d:i32
VCVTSI2SS            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2A VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  not64    ZEROING=0 MASK=0 | REG0=XMM_R3():w:dq:f32:TXT=ROUNDC REG1=XMM_N3():r:dq:f32 REG2=GPR32_B():r:d:i32
VCVTSI2SS            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2A VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  mode64 W0    ZEROING=0 MASK=0 | REG0=XMM_R3():w:dq:f32:TXT=ROUNDC REG1=XMM_N3():r:dq:f32 REG2=GPR32_B():r:d:i32
VCVTSI2SS            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2A VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  not64    ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_READER() FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f32 REG1=XMM_N3():r:dq:f32 MEM0:r:d:i32
VCVTSI2SS            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2A VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64 W0    ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_READER() FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f32 REG1=XMM_N3():r:dq:f32 MEM0:r:d:i32
VCVTSI2SS            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2A VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64  W1    ZEROING=0 MASK=0  FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f32 REG1=XMM_N3():r:dq:f32 REG2=GPR64_B():r:q:i64
VCVTSI2SS            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2A VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  mode64  W1    ZEROING=0 MASK=0 | REG0=XMM_R3():w:dq:f32:TXT=ROUNDC REG1=XMM_N3():r:dq:f32 REG2=GPR64_B():r:q:i64
VCVTSI2SS            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2A VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64  W1    ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_GPR_READER() FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f32 REG1=XMM_N3():r:dq:f32 MEM0:r:q:i64
VCVTSS2SD            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x5A VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                     | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VCVTSS2SD            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x5A VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W0 | REG0=XMM_R3():w:dq:f64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VCVTSS2SD            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x5A VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
VCVTSS2SI            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2D VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  not64  NOEVSR  ZEROING=0 MASK=0 FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:i32 REG1=XMM_B3():r:dq:f32
VCVTSS2SI            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2D VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64 W0  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:i32 REG1=XMM_B3():r:dq:f32
VCVTSS2SI            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2D VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  not64  NOEVSR  ZEROING=0 MASK=0 | REG0=GPR32_R():w:d:i32:TXT=ROUNDC REG1=XMM_B3():r:dq:f32
VCVTSS2SI            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2D VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  mode64 W0  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE | REG0=GPR32_R():w:d:i32:TXT=ROUNDC REG1=XMM_B3():r:dq:f32
VCVTSS2SI            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2D VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  not64  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_WRITER_LDOP_D() FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:i32 MEM0:r:d:f32
VCVTSS2SI            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2D VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64 W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_WRITER_LDOP_D() EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:i32 MEM0:r:d:f32
VCVTSS2SI            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2D VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64  W1  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR64_R():w:q:i64 REG1=XMM_B3():r:dq:f32
VCVTSS2SI            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2D VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  mode64  W1  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE | REG0=GPR64_R():w:q:i64:TXT=ROUNDC REG1=XMM_B3():r:dq:f32
VCVTSS2SI            | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2D VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64  W1  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_WRITER_LDOP_D() EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR64_R():w:q:i64 MEM0:r:d:f32
VCVTSS2USI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x79 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  not64  NOEVSR  ZEROING=0 MASK=0  FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:u32 REG1=XMM_B3():r:dq:f32
VCVTSS2USI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x79 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64 W0  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:u32 REG1=XMM_B3():r:dq:f32
VCVTSS2USI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x79 VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  not64  NOEVSR  ZEROING=0 MASK=0 | REG0=GPR32_R():w:d:u32:TXT=ROUNDC REG1=XMM_B3():r:dq:f32
VCVTSS2USI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x79 VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  mode64 W0  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE | REG0=GPR32_R():w:d:u32:TXT=ROUNDC REG1=XMM_B3():r:dq:f32
VCVTSS2USI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x79 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  not64  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_WRITER_LDOP_D() FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:u32 MEM0:r:d:f32
VCVTSS2USI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x79 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64 W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_WRITER_LDOP_D() EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:u32 MEM0:r:d:f32
VCVTSS2USI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x79 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64  W1  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR64_R():w:q:u64 REG1=XMM_B3():r:dq:f32
VCVTSS2USI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x79 VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  mode64  W1  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE | REG0=GPR64_R():w:q:u64:TXT=ROUNDC REG1=XMM_B3():r:dq:f32
VCVTSS2USI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x79 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64  W1  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_WRITER_LDOP_D() EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR64_R():w:q:u64 MEM0:r:d:f32
VCVTTPD2DQ           | CONVERT        | AVX512EVEX     | AVX512F_128    | EVV 0xE6 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR      | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64
VCVTTPD2DQ           | CONVERT        | AVX512EVEX     | AVX512F_128    | EVV 0xE6 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VCVTTPD2DQ           | CONVERT        | AVX512EVEX     | AVX512F_256    | EVV 0xE6 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR      | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64
VCVTTPD2DQ           | CONVERT        | AVX512EVEX     | AVX512F_256    | EVV 0xE6 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VCVTTPD2DQ           | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0xE6 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR      | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
VCVTTPD2DQ           | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0xE6 V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W1  NOEVSR | REG0=YMM_R3():w:qq:i32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
VCVTTPD2DQ           | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0xE6 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VCVTTPD2QQ           | CONVERT        | AVX512EVEX     | AVX512DQ_128   | EVV 0x7A V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR      | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64
VCVTTPD2QQ           | CONVERT        | AVX512EVEX     | AVX512DQ_128   | EVV 0x7A V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VCVTTPD2QQ           | CONVERT        | AVX512EVEX     | AVX512DQ_256   | EVV 0x7A V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR      | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64
VCVTTPD2QQ           | CONVERT        | AVX512EVEX     | AVX512DQ_256   | EVV 0x7A V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VCVTTPD2QQ           | CONVERT        | AVX512EVEX     | AVX512DQ_512   | EVV 0x7A V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR      | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
VCVTTPD2QQ           | CONVERT        | AVX512EVEX     | AVX512DQ_512   | EVV 0x7A V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W1  NOEVSR | REG0=ZMM_R3():w:zi64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
VCVTTPD2QQ           | CONVERT        | AVX512EVEX     | AVX512DQ_512   | EVV 0x7A V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VCVTTPD2UDQ          | CONVERT        | AVX512EVEX     | AVX512F_128    | EVV 0x78 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR      | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64
VCVTTPD2UDQ          | CONVERT        | AVX512EVEX     | AVX512F_128    | EVV 0x78 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VCVTTPD2UDQ          | CONVERT        | AVX512EVEX     | AVX512F_256    | EVV 0x78 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR      | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64
VCVTTPD2UDQ          | CONVERT        | AVX512EVEX     | AVX512F_256    | EVV 0x78 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VCVTTPD2UDQ          | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x78 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR      | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
VCVTTPD2UDQ          | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x78 VNP V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W1  NOEVSR | REG0=YMM_R3():w:qq:u32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
VCVTTPD2UDQ          | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x78 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VCVTTPD2UQQ          | CONVERT        | AVX512EVEX     | AVX512DQ_128   | EVV 0x78 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR      | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64
VCVTTPD2UQQ          | CONVERT        | AVX512EVEX     | AVX512DQ_128   | EVV 0x78 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VCVTTPD2UQQ          | CONVERT        | AVX512EVEX     | AVX512DQ_256   | EVV 0x78 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR      | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64
VCVTTPD2UQQ          | CONVERT        | AVX512EVEX     | AVX512DQ_256   | EVV 0x78 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VCVTTPD2UQQ          | CONVERT        | AVX512EVEX     | AVX512DQ_512   | EVV 0x78 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR      | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
VCVTTPD2UQQ          | CONVERT        | AVX512EVEX     | AVX512DQ_512   | EVV 0x78 V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W1  NOEVSR | REG0=ZMM_R3():w:zu64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
VCVTTPD2UQQ          | CONVERT        | AVX512EVEX     | AVX512DQ_512   | EVV 0x78 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VCVTTPS2DQ           | CONVERT        | AVX512EVEX     | AVX512F_128    | EVV 0x5B VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR      | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
VCVTTPS2DQ           | CONVERT        | AVX512EVEX     | AVX512F_128    | EVV 0x5B VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VCVTTPS2DQ           | CONVERT        | AVX512EVEX     | AVX512F_256    | EVV 0x5B VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR      | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
VCVTTPS2DQ           | CONVERT        | AVX512EVEX     | AVX512F_256    | EVV 0x5B VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VCVTTPS2DQ           | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x5B VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR      | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
VCVTTPS2DQ           | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x5B VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0  NOEVSR | REG0=ZMM_R3():w:zi32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
VCVTTPS2DQ           | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x5B VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VCVTTPS2QQ           | CONVERT        | AVX512EVEX     | AVX512DQ_128   | EVV 0x7A V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR      | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
VCVTTPS2QQ           | CONVERT        | AVX512EVEX     | AVX512DQ_128   | EVV 0x7A V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VCVTTPS2QQ           | CONVERT        | AVX512EVEX     | AVX512DQ_256   | EVV 0x7A V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR      | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
VCVTTPS2QQ           | CONVERT        | AVX512EVEX     | AVX512DQ_256   | EVV 0x7A V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VCVTTPS2QQ           | CONVERT        | AVX512EVEX     | AVX512DQ_512   | EVV 0x7A V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR      | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
VCVTTPS2QQ           | CONVERT        | AVX512EVEX     | AVX512DQ_512   | EVV 0x7A V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0  NOEVSR | REG0=ZMM_R3():w:zi64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
VCVTTPS2QQ           | CONVERT        | AVX512EVEX     | AVX512DQ_512   | EVV 0x7A V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VCVTTPS2UDQ          | CONVERT        | AVX512EVEX     | AVX512F_128    | EVV 0x78 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR      | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
VCVTTPS2UDQ          | CONVERT        | AVX512EVEX     | AVX512F_128    | EVV 0x78 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VCVTTPS2UDQ          | CONVERT        | AVX512EVEX     | AVX512F_256    | EVV 0x78 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR      | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
VCVTTPS2UDQ          | CONVERT        | AVX512EVEX     | AVX512F_256    | EVV 0x78 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VCVTTPS2UDQ          | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x78 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR      | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
VCVTTPS2UDQ          | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x78 VNP V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0  NOEVSR | REG0=ZMM_R3():w:zu32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
VCVTTPS2UDQ          | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x78 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VCVTTPS2UQQ          | CONVERT        | AVX512EVEX     | AVX512DQ_128   | EVV 0x78 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR      | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
VCVTTPS2UQQ          | CONVERT        | AVX512EVEX     | AVX512DQ_128   | EVV 0x78 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VCVTTPS2UQQ          | CONVERT        | AVX512EVEX     | AVX512DQ_256   | EVV 0x78 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR      | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
VCVTTPS2UQQ          | CONVERT        | AVX512EVEX     | AVX512DQ_256   | EVV 0x78 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VCVTTPS2UQQ          | CONVERT        | AVX512EVEX     | AVX512DQ_512   | EVV 0x78 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR      | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
VCVTTPS2UQQ          | CONVERT        | AVX512EVEX     | AVX512DQ_512   | EVV 0x78 V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0  NOEVSR | REG0=ZMM_R3():w:zu64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
VCVTTPS2UQQ          | CONVERT        | AVX512EVEX     | AVX512DQ_512   | EVV 0x78 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VCVTTSD2SI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2C VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  not64  NOEVSR  ZEROING=0 MASK=0 FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:i32 REG1=XMM_B3():r:dq:f64
VCVTTSD2SI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2C VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64 W0  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:i32 REG1=XMM_B3():r:dq:f64
VCVTTSD2SI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2C VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  not64  NOEVSR  ZEROING=0 MASK=0 | REG0=GPR32_R():w:d:i32:TXT=SAESTR REG1=XMM_B3():r:dq:f64
VCVTTSD2SI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2C VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  mode64 W0  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE | REG0=GPR32_R():w:d:i32:TXT=SAESTR REG1=XMM_B3():r:dq:f64
VCVTTSD2SI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2C VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  not64  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_GPR_WRITER_LDOP_Q() FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:i32 MEM0:r:q:f64
VCVTTSD2SI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2C VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64 W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_GPR_WRITER_LDOP_Q() EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:i32 MEM0:r:q:f64
VCVTTSD2SI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2C VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64  W1  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR64_R():w:q:i64 REG1=XMM_B3():r:dq:f64
VCVTTSD2SI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2C VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  mode64  W1  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE | REG0=GPR64_R():w:q:i64:TXT=SAESTR REG1=XMM_B3():r:dq:f64
VCVTTSD2SI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2C VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64  W1  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_GPR_WRITER_LDOP_Q() EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR64_R():w:q:i64 MEM0:r:q:f64
VCVTTSD2USI          | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x78 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  not64  NOEVSR  ZEROING=0 MASK=0 FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:u32 REG1=XMM_B3():r:dq:f64
VCVTTSD2USI          | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x78 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64 W0  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:u32 REG1=XMM_B3():r:dq:f64
VCVTTSD2USI          | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x78 VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  not64  NOEVSR  ZEROING=0 MASK=0 | REG0=GPR32_R():w:d:u32:TXT=SAESTR REG1=XMM_B3():r:dq:f64
VCVTTSD2USI          | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x78 VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  mode64 W0  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE | REG0=GPR32_R():w:d:u32:TXT=SAESTR REG1=XMM_B3():r:dq:f64
VCVTTSD2USI          | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x78 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  not64  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_GPR_WRITER_LDOP_Q() FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:u32 MEM0:r:q:f64
VCVTTSD2USI          | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x78 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64 W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_GPR_WRITER_LDOP_Q() EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:u32 MEM0:r:q:f64
VCVTTSD2USI          | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x78 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64  W1  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR64_R():w:q:u64 REG1=XMM_B3():r:dq:f64
VCVTTSD2USI          | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x78 VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  mode64  W1  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE | REG0=GPR64_R():w:q:u64:TXT=SAESTR REG1=XMM_B3():r:dq:f64
VCVTTSD2USI          | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x78 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64  W1  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_GPR_WRITER_LDOP_Q() EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR64_R():w:q:u64 MEM0:r:q:f64
VCVTTSS2SI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2C VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  not64  NOEVSR  ZEROING=0 MASK=0 FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:i32 REG1=XMM_B3():r:dq:f32
VCVTTSS2SI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2C VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64 W0  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:i32 REG1=XMM_B3():r:dq:f32
VCVTTSS2SI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2C VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  not64  NOEVSR  ZEROING=0 MASK=0 | REG0=GPR32_R():w:d:i32:TXT=SAESTR REG1=XMM_B3():r:dq:f32
VCVTTSS2SI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2C VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  mode64 W0  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE | REG0=GPR32_R():w:d:i32:TXT=SAESTR REG1=XMM_B3():r:dq:f32
VCVTTSS2SI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2C VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  not64  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_WRITER_LDOP_D() FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:i32 MEM0:r:d:f32
VCVTTSS2SI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2C VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64 W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_WRITER_LDOP_D() EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:i32 MEM0:r:d:f32
VCVTTSS2SI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2C VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64  W1  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR64_R():w:q:i64 REG1=XMM_B3():r:dq:f32
VCVTTSS2SI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2C VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  mode64  W1  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE | REG0=GPR64_R():w:q:i64:TXT=SAESTR REG1=XMM_B3():r:dq:f32
VCVTTSS2SI           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2C VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64  W1  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_WRITER_LDOP_D() EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR64_R():w:q:i64 MEM0:r:d:f32
VCVTTSS2USI          | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x78 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  not64  NOEVSR  ZEROING=0 MASK=0  FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:u32 REG1=XMM_B3():r:dq:f32
VCVTTSS2USI          | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x78 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64 W0  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:u32 REG1=XMM_B3():r:dq:f32
VCVTTSS2USI          | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x78 VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  not64  NOEVSR  ZEROING=0 MASK=0 | REG0=GPR32_R():w:d:u32:TXT=SAESTR REG1=XMM_B3():r:dq:f32
VCVTTSS2USI          | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x78 VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  mode64 W0  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE | REG0=GPR32_R():w:d:u32:TXT=SAESTR REG1=XMM_B3():r:dq:f32
VCVTTSS2USI          | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x78 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  not64  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_WRITER_LDOP_D() FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:u32 MEM0:r:d:f32
VCVTTSS2USI          | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x78 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64 W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_WRITER_LDOP_D() EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR32_R():w:d:u32 MEM0:r:d:f32
VCVTTSS2USI          | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x78 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64  W1  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR64_R():w:q:u64 REG1=XMM_B3():r:dq:f32
VCVTTSS2USI          | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x78 VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  mode64  W1  NOEVSR  ZEROING=0 MASK=0  EVEXRR_ONE | REG0=GPR64_R():w:q:u64:TXT=SAESTR REG1=XMM_B3():r:dq:f32
VCVTTSS2USI          | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x78 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64  W1  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_WRITER_LDOP_D() EVEXRR_ONE FIX_ROUND_LEN128() | REG0=GPR64_R():w:q:u64 MEM0:r:d:f32
VCVTUDQ2PD           | CONVERT        | AVX512EVEX     | AVX512F_128    | EVV 0x7A VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR      | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32
VCVTUDQ2PD           | CONVERT        | AVX512EVEX     | AVX512F_128    | EVV 0x7A VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR
VCVTUDQ2PD           | CONVERT        | AVX512EVEX     | AVX512F_256    | EVV 0x7A VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR      | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32
VCVTUDQ2PD           | CONVERT        | AVX512EVEX     | AVX512F_256    | EVV 0x7A VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR
VCVTUDQ2PD           | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x7A VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR      | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u32
VCVTUDQ2PD           | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x7A VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALF() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR
VCVTUDQ2PS           | CONVERT        | AVX512EVEX     | AVX512F_128    | EVV 0x7A VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR      | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32
VCVTUDQ2PS           | CONVERT        | AVX512EVEX     | AVX512F_128    | EVV 0x7A VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR
VCVTUDQ2PS           | CONVERT        | AVX512EVEX     | AVX512F_256    | EVV 0x7A VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR      | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u32
VCVTUDQ2PS           | CONVERT        | AVX512EVEX     | AVX512F_256    | EVV 0x7A VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR
VCVTUDQ2PS           | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x7A VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR      | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu32
VCVTUDQ2PS           | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x7A VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0  NOEVSR | REG0=ZMM_R3():w:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu32
VCVTUDQ2PS           | CONVERT        | AVX512EVEX     | AVX512F_512    | EVV 0x7A VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR
VCVTUQQ2PD           | CONVERT        | AVX512EVEX     | AVX512DQ_128   | EVV 0x7A VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR      | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u64
VCVTUQQ2PD           | CONVERT        | AVX512EVEX     | AVX512DQ_128   | EVV 0x7A VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR
VCVTUQQ2PD           | CONVERT        | AVX512EVEX     | AVX512DQ_256   | EVV 0x7A VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR      | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u64
VCVTUQQ2PD           | CONVERT        | AVX512EVEX     | AVX512DQ_256   | EVV 0x7A VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR
VCVTUQQ2PD           | CONVERT        | AVX512EVEX     | AVX512DQ_512   | EVV 0x7A VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR      | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu64
VCVTUQQ2PD           | CONVERT        | AVX512EVEX     | AVX512DQ_512   | EVV 0x7A VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1  NOEVSR | REG0=ZMM_R3():w:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu64
VCVTUQQ2PD           | CONVERT        | AVX512EVEX     | AVX512DQ_512   | EVV 0x7A VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR
VCVTUQQ2PS           | CONVERT        | AVX512EVEX     | AVX512DQ_128   | EVV 0x7A VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR      | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u64
VCVTUQQ2PS           | CONVERT        | AVX512EVEX     | AVX512DQ_128   | EVV 0x7A VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR
VCVTUQQ2PS           | CONVERT        | AVX512EVEX     | AVX512DQ_256   | EVV 0x7A VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR      | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u64
VCVTUQQ2PS           | CONVERT        | AVX512EVEX     | AVX512DQ_256   | EVV 0x7A VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR
VCVTUQQ2PS           | CONVERT        | AVX512EVEX     | AVX512DQ_512   | EVV 0x7A VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR      | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu64
VCVTUQQ2PS           | CONVERT        | AVX512EVEX     | AVX512DQ_512   | EVV 0x7A VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1  NOEVSR | REG0=YMM_R3():w:qq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu64
VCVTUQQ2PS           | CONVERT        | AVX512EVEX     | AVX512DQ_512   | EVV 0x7A VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR
VCVTUSI2SD           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x7B VF2 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] not64 ZEROING=0 MASK=0 FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f64 REG1=XMM_N3():r:dq:f64 REG2=GPR32_B():r:d:u32
VCVTUSI2SD           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x7B VF2 V0F MOD[0b11] MOD=3 REG[rrr] RM[nnn] mode64 W0 ZEROING=0 MASK=0  FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f64 REG1=XMM_N3():r:dq:f64 REG2=GPR32_B():r:d:u32
VCVTUSI2SD           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x7B VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  not64    ZEROING=0 MASK=0 BCRC=0  ESIZE_32_BITS() NELEM_GPR_READER() FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f64 REG1=XMM_N3():r:dq:f64 MEM0:r:d:u32
VCVTUSI2SD           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x7B VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  mode64 W0    ZEROING=0 MASK=0 BCRC=0 ESIZE_32_BITS() NELEM_GPR_READER() FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f64 REG1=XMM_N3():r:dq:f64 MEM0:r:d:u32
VCVTUSI2SD           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x7B VF2 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn]  mode64 W1 ZEROING=0 MASK=0 BCRC=0 FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f64 REG1=XMM_N3():r:dq:f64 REG2=GPR64_B():r:q:u64
VCVTUSI2SD           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x7B VF2 V0F MOD[0b11] MOD=3  REG[rrr] RM[nnn] mode64 W1 ZEROING=0 MASK=0 BCRC=1 FIX_ROUND_LEN128() AVX512_ROUND() | REG0=XMM_R3():w:dq:f64:TXT=ROUNDC REG1=XMM_N3():r:dq:f64 REG2=GPR64_B():r:q:u64
VCVTUSI2SD           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x7B VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM() mode64 W1 ZEROING=0 MASK=0 BCRC=0  ESIZE_64_BITS() NELEM_GPR_READER() FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f64 REG1=XMM_N3():r:dq:f64 MEM0:r:q:u64
VCVTUSI2SS           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x7B VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  not64    ZEROING=0 MASK=0 FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f32 REG1=XMM_N3():r:dq:f32 REG2=GPR32_B():r:d:u32
VCVTUSI2SS           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x7B VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64 W0    ZEROING=0 MASK=0  FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f32 REG1=XMM_N3():r:dq:f32 REG2=GPR32_B():r:d:u32
VCVTUSI2SS           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x7B VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  not64    ZEROING=0 MASK=0 | REG0=XMM_R3():w:dq:f32:TXT=ROUNDC REG1=XMM_N3():r:dq:f32 REG2=GPR32_B():r:d:u32
VCVTUSI2SS           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x7B VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  mode64 W0    ZEROING=0 MASK=0 | REG0=XMM_R3():w:dq:f32:TXT=ROUNDC REG1=XMM_N3():r:dq:f32 REG2=GPR32_B():r:d:u32
VCVTUSI2SS           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x7B VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  not64    ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_READER() FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f32 REG1=XMM_N3():r:dq:f32 MEM0:r:d:u32
VCVTUSI2SS           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x7B VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64 W0    ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_READER() FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f32 REG1=XMM_N3():r:dq:f32 MEM0:r:d:u32
VCVTUSI2SS           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x7B VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  mode64  W1    ZEROING=0 MASK=0  FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f32 REG1=XMM_N3():r:dq:f32 REG2=GPR64_B():r:q:u64
VCVTUSI2SS           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x7B VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  mode64  W1    ZEROING=0 MASK=0 | REG0=XMM_R3():w:dq:f32:TXT=ROUNDC REG1=XMM_N3():r:dq:f32 REG2=GPR64_B():r:q:u64
VCVTUSI2SS           | CONVERT        | AVX512EVEX     | AVX512F_SCALAR | EVV 0x7B VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  mode64  W1    ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_GPR_READER() FIX_ROUND_LEN128() | REG0=XMM_R3():w:dq:f32 REG1=XMM_N3():r:dq:f32 MEM0:r:q:u64
VDBPSADBW            | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x42 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0   UIMM8()  | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8 IMM0:r:b
VDBPSADBW            | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x42 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0   UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8 IMM0:r:b
VDBPSADBW            | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x42 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0   UIMM8()  | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8 IMM0:r:b
VDBPSADBW            | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x42 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0   UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8 IMM0:r:b
VDBPSADBW            | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x42 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0   UIMM8()  | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8 IMM0:r:b
VDBPSADBW            | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x42 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0   UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8 IMM0:r:b
VDIVPD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x5E V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1              | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VDIVPD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x5E V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VDIVPD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x5E V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1              | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
VDIVPD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x5E V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VDIVPD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x5E V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1              | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VDIVPD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x5E V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1 | REG0=ZMM_R3():w:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VDIVPD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x5E V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
VDIVPS               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x5E VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0              | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VDIVPS               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x5E VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VDIVPS               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x5E VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0              | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
VDIVPS               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x5E VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VDIVPS               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x5E VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0              | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VDIVPS               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x5E VNP V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0 | REG0=ZMM_R3():w:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VDIVPS               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x5E VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
VDIVSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x5E VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                     | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VDIVSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x5E VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1 | REG0=XMM_R3():w:dq:f64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VDIVSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x5E VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
VDIVSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x5E VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                     | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VDIVSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x5E VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W0 | REG0=XMM_R3():w:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VDIVSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x5E VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
VDPBF16PS            | AVX512         | AVX512EVEX     | AVX512_BF16_128 | EVV 0x52 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
VDPBF16PS            | AVX512         | AVX512EVEX     | AVX512_BF16_128 | EVV 0x52 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VDPBF16PS            | AVX512         | AVX512EVEX     | AVX512_BF16_256 | EVV 0x52 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
VDPBF16PS            | AVX512         | AVX512EVEX     | AVX512_BF16_256 | EVV 0x52 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VDPBF16PS            | AVX512         | AVX512EVEX     | AVX512_BF16_512 | EVV 0x52 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
VDPBF16PS            | AVX512         | AVX512EVEX     | AVX512_BF16_512 | EVV 0x52 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
VERR                 | SYSTEM         | BASE           | I286PROTECTED  | 0x0F 0x00 MOD[mm] MOD!=3 REG[0b100] RM[nnn] MODRM()                              | MEM0:r:w
VERR                 | SYSTEM         | BASE           | I286PROTECTED  | 0x0F 0x00 MOD[0b11] MOD=3 REG[0b100] RM[nnn]                                     | REG0=GPR16_B():r
VERW                 | SYSTEM         | BASE           | I286PROTECTED  | 0x0F 0x00 MOD[mm] MOD!=3 REG[0b101] RM[nnn] MODRM()                              | MEM0:r:w
VERW                 | SYSTEM         | BASE           | I286PROTECTED  | 0x0F 0x00 MOD[0b11] MOD=3 REG[0b101] RM[nnn]                                     | REG0=GPR16_B():r
VEXP223PS            | KNC            | KNCE           | KNCE           | KVV 0xC8 V0F38 V66  W0  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() NOSWIZF32() | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  MEM0:r:zd:TXT=NT
VEXP223PS            | KNC            | KNCE           | KNCE           | KVV 0xC8 V0F38 V66  W0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] NR=1 KNC_SAE()  | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw REG2=ZMM_B3():r:zd:TXT=SAEC
VEXP223PS            | KNC            | KNCE           | KNCE           | KVV 0xC8 V0F38 V66  W0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] NR=0 SWIZ=0     | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw REG2=ZMM_B3():r:zd
VEXP2PD              | AVX512         | AVX512EVEX     | AVX512ER_512   | EVV 0xC8 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR    | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
VEXP2PD              | AVX512         | AVX512EVEX     | AVX512ER_512   | EVV 0xC8 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W1  NOEVSR | REG0=ZMM_R3():w:zf64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
VEXP2PD              | AVX512         | AVX512EVEX     | AVX512ER_512   | EVV 0xC8 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VEXP2PS              | AVX512         | AVX512EVEX     | AVX512ER_512   | EVV 0xC8 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
VEXP2PS              | AVX512         | AVX512EVEX     | AVX512ER_512   | EVV 0xC8 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0  NOEVSR | REG0=ZMM_R3():w:zf32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
VEXP2PS              | AVX512         | AVX512EVEX     | AVX512ER_512   | EVV 0xC8 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VEXPANDPD            | EXPAND         | AVX512EVEX     | AVX512F_128    | EVV 0x88 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_GSCAT() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:f64
VEXPANDPD            | EXPAND         | AVX512EVEX     | AVX512F_128    | EVV 0x88 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR    | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64
VEXPANDPD            | EXPAND         | AVX512EVEX     | AVX512F_256    | EVV 0x88 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_GSCAT() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:f64
VEXPANDPD            | EXPAND         | AVX512EVEX     | AVX512F_256    | EVV 0x88 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR    | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64
VEXPANDPD            | EXPAND         | AVX512EVEX     | AVX512F_512    | EVV 0x88 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_GSCAT() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:f64
VEXPANDPD            | EXPAND         | AVX512EVEX     | AVX512F_512    | EVV 0x88 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR    | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
VEXPANDPS            | EXPAND         | AVX512EVEX     | AVX512F_128    | EVV 0x88 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_GSCAT() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:f32
VEXPANDPS            | EXPAND         | AVX512EVEX     | AVX512F_128    | EVV 0x88 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
VEXPANDPS            | EXPAND         | AVX512EVEX     | AVX512F_256    | EVV 0x88 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_GSCAT() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:f32
VEXPANDPS            | EXPAND         | AVX512EVEX     | AVX512F_256    | EVV 0x88 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
VEXPANDPS            | EXPAND         | AVX512EVEX     | AVX512F_512    | EVV 0x88 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_GSCAT() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:f32
VEXPANDPS            | EXPAND         | AVX512EVEX     | AVX512F_512    | EVV 0x88 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
VEXTRACTF32X4        | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x19 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR UIMM8() | REG0=XMM_B3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:f32 IMM0:r:b
VEXTRACTF32X4        | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x19 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_TUPLE4() | MEM0:w:dq:f32 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:f32 IMM0:r:b
VEXTRACTF32X4        | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x19 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR UIMM8() | REG0=XMM_B3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zf32 IMM0:r:b
VEXTRACTF32X4        | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x19 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_TUPLE4() | MEM0:w:dq:f32 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zf32 IMM0:r:b
VEXTRACTF32X8        | AVX512         | AVX512EVEX     | AVX512DQ_512   | EVV 0x1B V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR UIMM8() | REG0=YMM_B3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zf32 IMM0:r:b
VEXTRACTF32X8        | AVX512         | AVX512EVEX     | AVX512DQ_512   | EVV 0x1B V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_TUPLE8() | MEM0:w:qq:f32 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zf32 IMM0:r:b
VEXTRACTF64X2        | AVX512         | AVX512EVEX     | AVX512DQ_256   | EVV 0x19 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR UIMM8() | REG0=XMM_B3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:f64 IMM0:r:b
VEXTRACTF64X2        | AVX512         | AVX512EVEX     | AVX512DQ_256   | EVV 0x19 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_TUPLE2() | MEM0:w:dq:f64 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:f64 IMM0:r:b
VEXTRACTF64X2        | AVX512         | AVX512EVEX     | AVX512DQ_512   | EVV 0x19 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR UIMM8() | REG0=XMM_B3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zf64 IMM0:r:b
VEXTRACTF64X2        | AVX512         | AVX512EVEX     | AVX512DQ_512   | EVV 0x19 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_TUPLE2() | MEM0:w:dq:f64 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zf64 IMM0:r:b
VEXTRACTF64X4        | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x1B V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR UIMM8() | REG0=YMM_B3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zf64 IMM0:r:b
VEXTRACTF64X4        | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x1B V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_TUPLE4() | MEM0:w:qq:f64 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zf64 IMM0:r:b
VEXTRACTI32X4        | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x39 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR UIMM8() | REG0=XMM_B3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u32 IMM0:r:b
VEXTRACTI32X4        | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x39 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_TUPLE4() | MEM0:w:dq:u32 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u32 IMM0:r:b
VEXTRACTI32X4        | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x39 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR UIMM8() | REG0=XMM_B3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu32 IMM0:r:b
VEXTRACTI32X4        | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x39 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_TUPLE4() | MEM0:w:dq:u32 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu32 IMM0:r:b
VEXTRACTI32X8        | AVX512         | AVX512EVEX     | AVX512DQ_512   | EVV 0x3B V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR UIMM8() | REG0=YMM_B3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu32 IMM0:r:b
VEXTRACTI32X8        | AVX512         | AVX512EVEX     | AVX512DQ_512   | EVV 0x3B V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_TUPLE8() | MEM0:w:qq:u32 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu32 IMM0:r:b
VEXTRACTI64X2        | AVX512         | AVX512EVEX     | AVX512DQ_256   | EVV 0x39 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR UIMM8() | REG0=XMM_B3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u64 IMM0:r:b
VEXTRACTI64X2        | AVX512         | AVX512EVEX     | AVX512DQ_256   | EVV 0x39 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_TUPLE2() | MEM0:w:dq:u64 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u64 IMM0:r:b
VEXTRACTI64X2        | AVX512         | AVX512EVEX     | AVX512DQ_512   | EVV 0x39 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR UIMM8() | REG0=XMM_B3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu64 IMM0:r:b
VEXTRACTI64X2        | AVX512         | AVX512EVEX     | AVX512DQ_512   | EVV 0x39 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_TUPLE2() | MEM0:w:dq:u64 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu64 IMM0:r:b
VEXTRACTI64X4        | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x3B V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR UIMM8() | REG0=YMM_B3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu64 IMM0:r:b
VEXTRACTI64X4        | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x3B V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_TUPLE4() | MEM0:w:qq:u64 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu64 IMM0:r:b
VEXTRACTPS           | AVX512         | AVX512EVEX     | AVX512F_128N   | EVV 0x17 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR  ZEROING=0 MASK=0 UIMM8() | REG0=GPR32_B():w:d:f32 REG1=XMM_R3():r:dq:f32 IMM0:r:b
VEXTRACTPS           | AVX512         | AVX512EVEX     | AVX512F_128N   | EVV 0x17 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128    NOEVSR  ZEROING=0 MASK=0 UIMM8()  ESIZE_32_BITS() NELEM_GPR_WRITER_STORE() | MEM0:w:d:f32 REG0=XMM_R3():r:dq:f32 IMM0:r:b
VFIXUPIMMPD          | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x54 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1   UIMM8()  | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64 IMM0:r:b
VFIXUPIMMPD          | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x54 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
VFIXUPIMMPD          | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x54 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1   UIMM8()  | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64 IMM0:r:b
VFIXUPIMMPD          | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x54 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
VFIXUPIMMPD          | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x54 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1   UIMM8()  | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64 IMM0:r:b
VFIXUPIMMPD          | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x54 V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W1   UIMM8() | REG0=ZMM_R3():rw:zf64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64 IMM0:r:b
VFIXUPIMMPD          | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x54 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
VFIXUPIMMPS          | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x54 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0   UIMM8()  | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32 IMM0:r:b
VFIXUPIMMPS          | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x54 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
VFIXUPIMMPS          | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x54 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0   UIMM8()  | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32 IMM0:r:b
VFIXUPIMMPS          | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x54 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
VFIXUPIMMPS          | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x54 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0   UIMM8()  | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32 IMM0:r:b
VFIXUPIMMPS          | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x54 V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0   UIMM8() | REG0=ZMM_R3():rw:zf32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32 IMM0:r:b
VFIXUPIMMPS          | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x54 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
VFIXUPIMMSD          | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x55 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1   UIMM8()         | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64 IMM0:r:b
VFIXUPIMMSD          | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x55 V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W1   UIMM8() | REG0=XMM_R3():rw:dq:f64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64 IMM0:r:b
VFIXUPIMMSD          | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x55 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1   UIMM8()  ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64 IMM0:r:b
VFIXUPIMMSS          | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x55 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0   UIMM8()         | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32 IMM0:r:b
VFIXUPIMMSS          | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x55 V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W0   UIMM8() | REG0=XMM_R3():rw:dq:f32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32 IMM0:r:b
VFIXUPIMMSS          | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x55 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0   UIMM8()  ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32 IMM0:r:b
VFMADD132PD          | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0x98 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFMADD132PD          | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0x98 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VFMADD132PD          | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0x98 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
VFMADD132PD          | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0x98 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VFMADD132PD          | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0x98 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VFMADD132PD          | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0x98 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1 | REG0=ZMM_R3():rw:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VFMADD132PD          | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0x98 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
VFMADD132PS          | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0x98 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFMADD132PS          | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0x98 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VFMADD132PS          | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0x98 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
VFMADD132PS          | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0x98 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VFMADD132PS          | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0x98 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VFMADD132PS          | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0x98 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0 | REG0=ZMM_R3():rw:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VFMADD132PS          | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0x98 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
VFMADD132SD          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0x99 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                   | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFMADD132SD          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0x99 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1 | REG0=XMM_R3():rw:dq:f64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFMADD132SD          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0x99 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
VFMADD132SS          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0x99 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                   | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFMADD132SS          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0x99 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W0 | REG0=XMM_R3():rw:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFMADD132SS          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0x99 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
VFMADD213PD          | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xA8 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFMADD213PD          | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xA8 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VFMADD213PD          | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xA8 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
VFMADD213PD          | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xA8 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VFMADD213PD          | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xA8 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VFMADD213PD          | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xA8 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1 | REG0=ZMM_R3():rw:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VFMADD213PD          | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xA8 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
VFMADD213PS          | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xA8 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFMADD213PS          | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xA8 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VFMADD213PS          | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xA8 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
VFMADD213PS          | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xA8 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VFMADD213PS          | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xA8 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VFMADD213PS          | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xA8 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0 | REG0=ZMM_R3():rw:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VFMADD213PS          | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xA8 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
VFMADD213SD          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xA9 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                   | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFMADD213SD          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xA9 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1 | REG0=XMM_R3():rw:dq:f64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFMADD213SD          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xA9 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
VFMADD213SS          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xA9 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                   | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFMADD213SS          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xA9 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W0 | REG0=XMM_R3():rw:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFMADD213SS          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xA9 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
VFMADD231PD          | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xB8 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFMADD231PD          | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xB8 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VFMADD231PD          | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xB8 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
VFMADD231PD          | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xB8 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VFMADD231PD          | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xB8 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VFMADD231PD          | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xB8 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1 | REG0=ZMM_R3():rw:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VFMADD231PD          | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xB8 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
VFMADD231PS          | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xB8 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFMADD231PS          | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xB8 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VFMADD231PS          | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xB8 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
VFMADD231PS          | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xB8 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VFMADD231PS          | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xB8 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VFMADD231PS          | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xB8 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0 | REG0=ZMM_R3():rw:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VFMADD231PS          | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xB8 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
VFMADD231SD          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xB9 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                   | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFMADD231SD          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xB9 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1 | REG0=XMM_R3():rw:dq:f64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFMADD231SD          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xB9 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
VFMADD231SS          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xB9 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                   | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFMADD231SS          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xB9 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W0 | REG0=XMM_R3():rw:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFMADD231SS          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xB9 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
VFMADDPD             | FMA4           | FMA4           | FMA4           | VV1 0x69 V66 W0 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64 REG2=XMM_SE():r:dq:f64
VFMADDPD             | FMA4           | FMA4           | FMA4           | VV1 0x69 V66 W0 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64 REG3=XMM_SE():r:dq:f64
VFMADDPD             | FMA4           | FMA4           | FMA4           | VV1 0x69 V66 W1 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_SE():r:dq:f64 MEM0:r:dq:f64
VFMADDPD             | FMA4           | FMA4           | FMA4           | VV1 0x69 V66 W1 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_SE():r:dq:f64 REG3=XMM_B():r:dq:f64
VFMADDPD             | FMA4           | FMA4           | FMA4           | VV1 0x69 V66 W0 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64 REG2=YMM_SE():r:qq:f64
VFMADDPD             | FMA4           | FMA4           | FMA4           | VV1 0x69 V66 W0 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64 REG3=YMM_SE():r:qq:f64
VFMADDPD             | FMA4           | FMA4           | FMA4           | VV1 0x69 V66 W1 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_SE():r:qq:f64 MEM0:r:qq:f64
VFMADDPD             | FMA4           | FMA4           | FMA4           | VV1 0x69 V66 W1 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_SE():r:qq:f64 REG3=YMM_B():r:qq:f64
VFMADDPS             | FMA4           | FMA4           | FMA4           | VV1 0x68 V66 W0 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32 REG2=XMM_SE():r:dq:f32
VFMADDPS             | FMA4           | FMA4           | FMA4           | VV1 0x68 V66 W0 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32 REG3=XMM_SE():r:dq:f32
VFMADDPS             | FMA4           | FMA4           | FMA4           | VV1 0x68 V66 W1 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_SE():r:dq:f32 MEM0:r:dq:f32
VFMADDPS             | FMA4           | FMA4           | FMA4           | VV1 0x68 V66 W1 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_SE():r:dq:f32 REG3=XMM_B():r:dq:f32
VFMADDPS             | FMA4           | FMA4           | FMA4           | VV1 0x68 V66 W0 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32 REG2=YMM_SE():r:qq:f32
VFMADDPS             | FMA4           | FMA4           | FMA4           | VV1 0x68 V66 W0 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32 REG3=YMM_SE():r:qq:f32
VFMADDPS             | FMA4           | FMA4           | FMA4           | VV1 0x68 V66 W1 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_SE():r:qq:f32 MEM0:r:qq:f32
VFMADDPS             | FMA4           | FMA4           | FMA4           | VV1 0x68 V66 W1 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_SE():r:qq:f32 REG3=YMM_B():r:qq:f32
VFMADDSD             | FMA4           | FMA4           | FMA4           | VV1 0x6B V66 W0  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()         | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:q:f64 MEM0:r:q:f64 REG2=XMM_SE():r:q:f64
VFMADDSD             | FMA4           | FMA4           | FMA4           | VV1 0x6B V66 W0  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_B():r:q:f64 REG3=XMM_SE():r:q:f64
VFMADDSD             | FMA4           | FMA4           | FMA4           | VV1 0x6B V66 W1  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()         | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_SE():r:q:f64 MEM0:r:q:f64
VFMADDSD             | FMA4           | FMA4           | FMA4           | VV1 0x6B V66 W1  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_SE():r:q:f64 REG3=XMM_B():r:q:f64
VFMADDSS             | FMA4           | FMA4           | FMA4           | VV1 0x6A V66 W0  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()         | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:d:f32 MEM0:r:d:f32 REG2=XMM_SE():r:d:f32
VFMADDSS             | FMA4           | FMA4           | FMA4           | VV1 0x6A V66 W0  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_B():r:d:f32 REG3=XMM_SE():r:d:f32
VFMADDSS             | FMA4           | FMA4           | FMA4           | VV1 0x6A V66 W1  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()         | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_SE():r:d:f32 MEM0:r:d:f32
VFMADDSS             | FMA4           | FMA4           | FMA4           | VV1 0x6A V66 W1  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_SE():r:d:f32 REG3=XMM_B():r:d:f32
VFMADDSUB132PD       | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0x96 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFMADDSUB132PD       | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0x96 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VFMADDSUB132PD       | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0x96 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
VFMADDSUB132PD       | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0x96 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VFMADDSUB132PD       | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0x96 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VFMADDSUB132PD       | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0x96 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1 | REG0=ZMM_R3():rw:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VFMADDSUB132PD       | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0x96 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
VFMADDSUB132PS       | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0x96 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFMADDSUB132PS       | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0x96 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VFMADDSUB132PS       | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0x96 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
VFMADDSUB132PS       | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0x96 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VFMADDSUB132PS       | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0x96 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VFMADDSUB132PS       | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0x96 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0 | REG0=ZMM_R3():rw:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VFMADDSUB132PS       | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0x96 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
VFMADDSUB213PD       | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xA6 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFMADDSUB213PD       | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xA6 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VFMADDSUB213PD       | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xA6 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
VFMADDSUB213PD       | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xA6 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VFMADDSUB213PD       | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xA6 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VFMADDSUB213PD       | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xA6 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1 | REG0=ZMM_R3():rw:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VFMADDSUB213PD       | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xA6 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
VFMADDSUB213PS       | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xA6 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFMADDSUB213PS       | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xA6 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VFMADDSUB213PS       | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xA6 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
VFMADDSUB213PS       | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xA6 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VFMADDSUB213PS       | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xA6 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VFMADDSUB213PS       | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xA6 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0 | REG0=ZMM_R3():rw:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VFMADDSUB213PS       | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xA6 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
VFMADDSUB231PD       | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xB6 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFMADDSUB231PD       | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xB6 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VFMADDSUB231PD       | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xB6 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
VFMADDSUB231PD       | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xB6 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VFMADDSUB231PD       | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xB6 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VFMADDSUB231PD       | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xB6 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1 | REG0=ZMM_R3():rw:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VFMADDSUB231PD       | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xB6 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
VFMADDSUB231PS       | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xB6 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFMADDSUB231PS       | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xB6 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VFMADDSUB231PS       | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xB6 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
VFMADDSUB231PS       | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xB6 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VFMADDSUB231PS       | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xB6 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VFMADDSUB231PS       | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xB6 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0 | REG0=ZMM_R3():rw:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VFMADDSUB231PS       | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xB6 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
VFMADDSUBPD          | FMA4           | FMA4           | FMA4           | VV1 0x5D V66 W0 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64 REG2=XMM_SE():r:dq:f64
VFMADDSUBPD          | FMA4           | FMA4           | FMA4           | VV1 0x5D V66 W0 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64 REG3=XMM_SE():r:dq:f64
VFMADDSUBPD          | FMA4           | FMA4           | FMA4           | VV1 0x5D V66 W1 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_SE():r:dq:f64 MEM0:r:dq:f64
VFMADDSUBPD          | FMA4           | FMA4           | FMA4           | VV1 0x5D V66 W1 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_SE():r:dq:f64 REG3=XMM_B():r:dq:f64
VFMADDSUBPD          | FMA4           | FMA4           | FMA4           | VV1 0x5D V66 W0 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64 REG2=YMM_SE():r:qq:f64
VFMADDSUBPD          | FMA4           | FMA4           | FMA4           | VV1 0x5D V66 W0 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64 REG3=YMM_SE():r:qq:f64
VFMADDSUBPD          | FMA4           | FMA4           | FMA4           | VV1 0x5D V66 W1 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_SE():r:qq:f64 MEM0:r:qq:f64
VFMADDSUBPD          | FMA4           | FMA4           | FMA4           | VV1 0x5D V66 W1 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_SE():r:qq:f64 REG3=YMM_B():r:qq:f64
VFMADDSUBPS          | FMA4           | FMA4           | FMA4           | VV1 0x5C V66 W0 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32 REG2=XMM_SE():r:dq:f32
VFMADDSUBPS          | FMA4           | FMA4           | FMA4           | VV1 0x5C V66 W0 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32 REG3=XMM_SE():r:dq:f32
VFMADDSUBPS          | FMA4           | FMA4           | FMA4           | VV1 0x5C V66 W1 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_SE():r:dq:f32 MEM0:r:dq:f32
VFMADDSUBPS          | FMA4           | FMA4           | FMA4           | VV1 0x5C V66 W1 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_SE():r:dq:f32 REG3=XMM_B():r:dq:f32
VFMADDSUBPS          | FMA4           | FMA4           | FMA4           | VV1 0x5C V66 W0 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32 REG2=YMM_SE():r:qq:f32
VFMADDSUBPS          | FMA4           | FMA4           | FMA4           | VV1 0x5C V66 W0 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32 REG3=YMM_SE():r:qq:f32
VFMADDSUBPS          | FMA4           | FMA4           | FMA4           | VV1 0x5C V66 W1 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_SE():r:qq:f32 MEM0:r:qq:f32
VFMADDSUBPS          | FMA4           | FMA4           | FMA4           | VV1 0x5C V66 W1 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_SE():r:qq:f32 REG3=YMM_B():r:qq:f32
VFMSUB132PD          | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0x9A V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFMSUB132PD          | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0x9A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VFMSUB132PD          | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0x9A V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
VFMSUB132PD          | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0x9A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VFMSUB132PD          | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0x9A V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VFMSUB132PD          | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0x9A V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1 | REG0=ZMM_R3():rw:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VFMSUB132PD          | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0x9A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
VFMSUB132PS          | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0x9A V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFMSUB132PS          | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0x9A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VFMSUB132PS          | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0x9A V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
VFMSUB132PS          | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0x9A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VFMSUB132PS          | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0x9A V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VFMSUB132PS          | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0x9A V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0 | REG0=ZMM_R3():rw:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VFMSUB132PS          | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0x9A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
VFMSUB132SD          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0x9B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                   | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFMSUB132SD          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0x9B V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1 | REG0=XMM_R3():rw:dq:f64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFMSUB132SD          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0x9B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
VFMSUB132SS          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0x9B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                   | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFMSUB132SS          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0x9B V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W0 | REG0=XMM_R3():rw:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFMSUB132SS          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0x9B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
VFMSUB213PD          | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xAA V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFMSUB213PD          | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xAA V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VFMSUB213PD          | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xAA V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
VFMSUB213PD          | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xAA V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VFMSUB213PD          | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xAA V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VFMSUB213PD          | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xAA V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1 | REG0=ZMM_R3():rw:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VFMSUB213PD          | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xAA V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
VFMSUB213PS          | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xAA V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFMSUB213PS          | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xAA V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VFMSUB213PS          | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xAA V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
VFMSUB213PS          | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xAA V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VFMSUB213PS          | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xAA V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VFMSUB213PS          | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xAA V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0 | REG0=ZMM_R3():rw:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VFMSUB213PS          | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xAA V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
VFMSUB213SD          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xAB V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                   | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFMSUB213SD          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xAB V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1 | REG0=XMM_R3():rw:dq:f64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFMSUB213SD          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xAB V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
VFMSUB213SS          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xAB V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                   | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFMSUB213SS          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xAB V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W0 | REG0=XMM_R3():rw:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFMSUB213SS          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xAB V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
VFMSUB231PD          | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xBA V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFMSUB231PD          | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xBA V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VFMSUB231PD          | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xBA V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
VFMSUB231PD          | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xBA V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VFMSUB231PD          | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xBA V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VFMSUB231PD          | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xBA V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1 | REG0=ZMM_R3():rw:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VFMSUB231PD          | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xBA V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
VFMSUB231PS          | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xBA V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFMSUB231PS          | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xBA V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VFMSUB231PS          | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xBA V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
VFMSUB231PS          | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xBA V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VFMSUB231PS          | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xBA V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VFMSUB231PS          | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xBA V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0 | REG0=ZMM_R3():rw:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VFMSUB231PS          | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xBA V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
VFMSUB231SD          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xBB V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                   | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFMSUB231SD          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xBB V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1 | REG0=XMM_R3():rw:dq:f64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFMSUB231SD          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xBB V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
VFMSUB231SS          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xBB V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                   | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFMSUB231SS          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xBB V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W0 | REG0=XMM_R3():rw:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFMSUB231SS          | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xBB V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
VFMSUBADD132PD       | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0x97 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFMSUBADD132PD       | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0x97 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VFMSUBADD132PD       | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0x97 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
VFMSUBADD132PD       | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0x97 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VFMSUBADD132PD       | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0x97 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VFMSUBADD132PD       | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0x97 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1 | REG0=ZMM_R3():rw:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VFMSUBADD132PD       | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0x97 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
VFMSUBADD132PS       | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0x97 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFMSUBADD132PS       | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0x97 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VFMSUBADD132PS       | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0x97 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
VFMSUBADD132PS       | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0x97 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VFMSUBADD132PS       | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0x97 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VFMSUBADD132PS       | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0x97 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0 | REG0=ZMM_R3():rw:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VFMSUBADD132PS       | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0x97 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
VFMSUBADD213PD       | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xA7 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFMSUBADD213PD       | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xA7 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VFMSUBADD213PD       | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xA7 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
VFMSUBADD213PD       | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xA7 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VFMSUBADD213PD       | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xA7 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VFMSUBADD213PD       | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xA7 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1 | REG0=ZMM_R3():rw:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VFMSUBADD213PD       | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xA7 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
VFMSUBADD213PS       | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xA7 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFMSUBADD213PS       | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xA7 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VFMSUBADD213PS       | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xA7 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
VFMSUBADD213PS       | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xA7 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VFMSUBADD213PS       | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xA7 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VFMSUBADD213PS       | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xA7 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0 | REG0=ZMM_R3():rw:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VFMSUBADD213PS       | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xA7 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
VFMSUBADD231PD       | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xB7 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFMSUBADD231PD       | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xB7 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VFMSUBADD231PD       | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xB7 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
VFMSUBADD231PD       | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xB7 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VFMSUBADD231PD       | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xB7 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VFMSUBADD231PD       | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xB7 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1 | REG0=ZMM_R3():rw:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VFMSUBADD231PD       | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xB7 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
VFMSUBADD231PS       | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xB7 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFMSUBADD231PS       | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xB7 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VFMSUBADD231PS       | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xB7 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
VFMSUBADD231PS       | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xB7 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VFMSUBADD231PS       | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xB7 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VFMSUBADD231PS       | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xB7 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0 | REG0=ZMM_R3():rw:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VFMSUBADD231PS       | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xB7 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
VFMSUBADDPD          | FMA4           | FMA4           | FMA4           | VV1 0x5F V66 W0 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64 REG2=XMM_SE():r:dq:f64
VFMSUBADDPD          | FMA4           | FMA4           | FMA4           | VV1 0x5F V66 W0 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64 REG3=XMM_SE():r:dq:f64
VFMSUBADDPD          | FMA4           | FMA4           | FMA4           | VV1 0x5F V66 W1 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_SE():r:dq:f64 MEM0:r:dq:f64
VFMSUBADDPD          | FMA4           | FMA4           | FMA4           | VV1 0x5F V66 W1 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_SE():r:dq:f64 REG3=XMM_B():r:dq:f64
VFMSUBADDPD          | FMA4           | FMA4           | FMA4           | VV1 0x5F V66 W0 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64 REG2=YMM_SE():r:qq:f64
VFMSUBADDPD          | FMA4           | FMA4           | FMA4           | VV1 0x5F V66 W0 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64 REG3=YMM_SE():r:qq:f64
VFMSUBADDPD          | FMA4           | FMA4           | FMA4           | VV1 0x5F V66 W1 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_SE():r:qq:f64 MEM0:r:qq:f64
VFMSUBADDPD          | FMA4           | FMA4           | FMA4           | VV1 0x5F V66 W1 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_SE():r:qq:f64 REG3=YMM_B():r:qq:f64
VFMSUBADDPS          | FMA4           | FMA4           | FMA4           | VV1 0x5E V66 W0 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32 REG2=XMM_SE():r:dq:f32
VFMSUBADDPS          | FMA4           | FMA4           | FMA4           | VV1 0x5E V66 W0 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32 REG3=XMM_SE():r:dq:f32
VFMSUBADDPS          | FMA4           | FMA4           | FMA4           | VV1 0x5E V66 W1 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_SE():r:dq:f32 MEM0:r:dq:f32
VFMSUBADDPS          | FMA4           | FMA4           | FMA4           | VV1 0x5E V66 W1 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_SE():r:dq:f32 REG3=XMM_B():r:dq:f32
VFMSUBADDPS          | FMA4           | FMA4           | FMA4           | VV1 0x5E V66 W0 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32 REG2=YMM_SE():r:qq:f32
VFMSUBADDPS          | FMA4           | FMA4           | FMA4           | VV1 0x5E V66 W0 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32 REG3=YMM_SE():r:qq:f32
VFMSUBADDPS          | FMA4           | FMA4           | FMA4           | VV1 0x5E V66 W1 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_SE():r:qq:f32 MEM0:r:qq:f32
VFMSUBADDPS          | FMA4           | FMA4           | FMA4           | VV1 0x5E V66 W1 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_SE():r:qq:f32 REG3=YMM_B():r:qq:f32
VFMSUBPD             | FMA4           | FMA4           | FMA4           | VV1 0x6D V66 W0 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64 REG2=XMM_SE():r:dq:f64
VFMSUBPD             | FMA4           | FMA4           | FMA4           | VV1 0x6D V66 W0 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64 REG3=XMM_SE():r:dq:f64
VFMSUBPD             | FMA4           | FMA4           | FMA4           | VV1 0x6D V66 W1 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_SE():r:dq:f64 MEM0:r:dq:f64
VFMSUBPD             | FMA4           | FMA4           | FMA4           | VV1 0x6D V66 W1 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_SE():r:dq:f64 REG3=XMM_B():r:dq:f64
VFMSUBPD             | FMA4           | FMA4           | FMA4           | VV1 0x6D V66 W0 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64 REG2=YMM_SE():r:qq:f64
VFMSUBPD             | FMA4           | FMA4           | FMA4           | VV1 0x6D V66 W0 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64 REG3=YMM_SE():r:qq:f64
VFMSUBPD             | FMA4           | FMA4           | FMA4           | VV1 0x6D V66 W1 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_SE():r:qq:f64 MEM0:r:qq:f64
VFMSUBPD             | FMA4           | FMA4           | FMA4           | VV1 0x6D V66 W1 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_SE():r:qq:f64 REG3=YMM_B():r:qq:f64
VFMSUBPS             | FMA4           | FMA4           | FMA4           | VV1 0x6C V66 W0 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32 REG2=XMM_SE():r:dq:f32
VFMSUBPS             | FMA4           | FMA4           | FMA4           | VV1 0x6C V66 W0 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32 REG3=XMM_SE():r:dq:f32
VFMSUBPS             | FMA4           | FMA4           | FMA4           | VV1 0x6C V66 W1 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_SE():r:dq:f32 MEM0:r:dq:f32
VFMSUBPS             | FMA4           | FMA4           | FMA4           | VV1 0x6C V66 W1 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_SE():r:dq:f32 REG3=XMM_B():r:dq:f32
VFMSUBPS             | FMA4           | FMA4           | FMA4           | VV1 0x6C V66 W0 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32 REG2=YMM_SE():r:qq:f32
VFMSUBPS             | FMA4           | FMA4           | FMA4           | VV1 0x6C V66 W0 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32 REG3=YMM_SE():r:qq:f32
VFMSUBPS             | FMA4           | FMA4           | FMA4           | VV1 0x6C V66 W1 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_SE():r:qq:f32 MEM0:r:qq:f32
VFMSUBPS             | FMA4           | FMA4           | FMA4           | VV1 0x6C V66 W1 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_SE():r:qq:f32 REG3=YMM_B():r:qq:f32
VFMSUBSD             | FMA4           | FMA4           | FMA4           | VV1 0x6F V66 W0  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()         | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:q:f64 MEM0:r:q:f64 REG2=XMM_SE():r:q:f64
VFMSUBSD             | FMA4           | FMA4           | FMA4           | VV1 0x6F V66 W0  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_B():r:q:f64 REG3=XMM_SE():r:q:f64
VFMSUBSD             | FMA4           | FMA4           | FMA4           | VV1 0x6F V66 W1  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()         | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_SE():r:q:f64 MEM0:r:q:f64
VFMSUBSD             | FMA4           | FMA4           | FMA4           | VV1 0x6F V66 W1  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_SE():r:q:f64 REG3=XMM_B():r:q:f64
VFMSUBSS             | FMA4           | FMA4           | FMA4           | VV1 0x6E V66 W0  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()         | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:d:f32 MEM0:r:d:f32 REG2=XMM_SE():r:d:f32
VFMSUBSS             | FMA4           | FMA4           | FMA4           | VV1 0x6E V66 W0  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_B():r:d:f32 REG3=XMM_SE():r:d:f32
VFMSUBSS             | FMA4           | FMA4           | FMA4           | VV1 0x6E V66 W1  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()         | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_SE():r:d:f32 MEM0:r:d:f32
VFMSUBSS             | FMA4           | FMA4           | FMA4           | VV1 0x6E V66 W1  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_SE():r:d:f32 REG3=XMM_B():r:d:f32
VFNMADD132PD         | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0x9C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFNMADD132PD         | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0x9C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VFNMADD132PD         | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0x9C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
VFNMADD132PD         | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0x9C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VFNMADD132PD         | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0x9C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VFNMADD132PD         | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0x9C V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1 | REG0=ZMM_R3():rw:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VFNMADD132PD         | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0x9C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
VFNMADD132PS         | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0x9C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFNMADD132PS         | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0x9C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VFNMADD132PS         | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0x9C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
VFNMADD132PS         | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0x9C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VFNMADD132PS         | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0x9C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VFNMADD132PS         | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0x9C V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0 | REG0=ZMM_R3():rw:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VFNMADD132PS         | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0x9C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
VFNMADD132SD         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0x9D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                   | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFNMADD132SD         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0x9D V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1 | REG0=XMM_R3():rw:dq:f64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFNMADD132SD         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0x9D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
VFNMADD132SS         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0x9D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                   | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFNMADD132SS         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0x9D V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W0 | REG0=XMM_R3():rw:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFNMADD132SS         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0x9D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
VFNMADD213PD         | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xAC V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFNMADD213PD         | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xAC V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VFNMADD213PD         | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xAC V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
VFNMADD213PD         | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xAC V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VFNMADD213PD         | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xAC V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VFNMADD213PD         | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xAC V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1 | REG0=ZMM_R3():rw:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VFNMADD213PD         | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xAC V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
VFNMADD213PS         | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xAC V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFNMADD213PS         | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xAC V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VFNMADD213PS         | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xAC V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
VFNMADD213PS         | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xAC V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VFNMADD213PS         | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xAC V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VFNMADD213PS         | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xAC V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0 | REG0=ZMM_R3():rw:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VFNMADD213PS         | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xAC V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
VFNMADD213SD         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xAD V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                   | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFNMADD213SD         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xAD V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1 | REG0=XMM_R3():rw:dq:f64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFNMADD213SD         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xAD V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
VFNMADD213SS         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xAD V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                   | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFNMADD213SS         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xAD V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W0 | REG0=XMM_R3():rw:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFNMADD213SS         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xAD V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
VFNMADD231PD         | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xBC V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFNMADD231PD         | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xBC V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VFNMADD231PD         | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xBC V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
VFNMADD231PD         | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xBC V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VFNMADD231PD         | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xBC V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VFNMADD231PD         | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xBC V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1 | REG0=ZMM_R3():rw:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VFNMADD231PD         | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xBC V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
VFNMADD231PS         | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xBC V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFNMADD231PS         | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xBC V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VFNMADD231PS         | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xBC V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
VFNMADD231PS         | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xBC V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VFNMADD231PS         | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xBC V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VFNMADD231PS         | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xBC V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0 | REG0=ZMM_R3():rw:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VFNMADD231PS         | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xBC V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
VFNMADD231SD         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xBD V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                   | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFNMADD231SD         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xBD V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1 | REG0=XMM_R3():rw:dq:f64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFNMADD231SD         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xBD V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
VFNMADD231SS         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xBD V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                   | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFNMADD231SS         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xBD V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W0 | REG0=XMM_R3():rw:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFNMADD231SS         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xBD V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
VFNMADDPD            | FMA4           | FMA4           | FMA4           | VV1 0x79 V66 W0 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64 REG2=XMM_SE():r:dq:f64
VFNMADDPD            | FMA4           | FMA4           | FMA4           | VV1 0x79 V66 W0 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64 REG3=XMM_SE():r:dq:f64
VFNMADDPD            | FMA4           | FMA4           | FMA4           | VV1 0x79 V66 W1 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_SE():r:dq:f64 MEM0:r:dq:f64
VFNMADDPD            | FMA4           | FMA4           | FMA4           | VV1 0x79 V66 W1 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_SE():r:dq:f64 REG3=XMM_B():r:dq:f64
VFNMADDPD            | FMA4           | FMA4           | FMA4           | VV1 0x79 V66 W0 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64 REG2=YMM_SE():r:qq:f64
VFNMADDPD            | FMA4           | FMA4           | FMA4           | VV1 0x79 V66 W0 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64 REG3=YMM_SE():r:qq:f64
VFNMADDPD            | FMA4           | FMA4           | FMA4           | VV1 0x79 V66 W1 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_SE():r:qq:f64 MEM0:r:qq:f64
VFNMADDPD            | FMA4           | FMA4           | FMA4           | VV1 0x79 V66 W1 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_SE():r:qq:f64 REG3=YMM_B():r:qq:f64
VFNMADDPS            | FMA4           | FMA4           | FMA4           | VV1 0x78 V66 W0 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32 REG2=XMM_SE():r:dq:f32
VFNMADDPS            | FMA4           | FMA4           | FMA4           | VV1 0x78 V66 W0 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32 REG3=XMM_SE():r:dq:f32
VFNMADDPS            | FMA4           | FMA4           | FMA4           | VV1 0x78 V66 W1 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_SE():r:dq:f32 MEM0:r:dq:f32
VFNMADDPS            | FMA4           | FMA4           | FMA4           | VV1 0x78 V66 W1 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_SE():r:dq:f32 REG3=XMM_B():r:dq:f32
VFNMADDPS            | FMA4           | FMA4           | FMA4           | VV1 0x78 V66 W0 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32 REG2=YMM_SE():r:qq:f32
VFNMADDPS            | FMA4           | FMA4           | FMA4           | VV1 0x78 V66 W0 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32 REG3=YMM_SE():r:qq:f32
VFNMADDPS            | FMA4           | FMA4           | FMA4           | VV1 0x78 V66 W1 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_SE():r:qq:f32 MEM0:r:qq:f32
VFNMADDPS            | FMA4           | FMA4           | FMA4           | VV1 0x78 V66 W1 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_SE():r:qq:f32 REG3=YMM_B():r:qq:f32
VFNMADDSD            | FMA4           | FMA4           | FMA4           | VV1 0x7B V66 W0  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()         | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:q:f64 MEM0:r:q:f64 REG2=XMM_SE():r:q:f64
VFNMADDSD            | FMA4           | FMA4           | FMA4           | VV1 0x7B V66 W0  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_B():r:q:f64 REG3=XMM_SE():r:q:f64
VFNMADDSD            | FMA4           | FMA4           | FMA4           | VV1 0x7B V66 W1  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()         | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_SE():r:q:f64 MEM0:r:q:f64
VFNMADDSD            | FMA4           | FMA4           | FMA4           | VV1 0x7B V66 W1  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_SE():r:q:f64 REG3=XMM_B():r:q:f64
VFNMADDSS            | FMA4           | FMA4           | FMA4           | VV1 0x7A V66 W0  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()         | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:d:f32 MEM0:r:d:f32 REG2=XMM_SE():r:d:f32
VFNMADDSS            | FMA4           | FMA4           | FMA4           | VV1 0x7A V66 W0  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_B():r:d:f32 REG3=XMM_SE():r:d:f32
VFNMADDSS            | FMA4           | FMA4           | FMA4           | VV1 0x7A V66 W1  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()         | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_SE():r:d:f32 MEM0:r:d:f32
VFNMADDSS            | FMA4           | FMA4           | FMA4           | VV1 0x7A V66 W1  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_SE():r:d:f32 REG3=XMM_B():r:d:f32
VFNMSUB132PD         | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0x9E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFNMSUB132PD         | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0x9E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VFNMSUB132PD         | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0x9E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
VFNMSUB132PD         | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0x9E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VFNMSUB132PD         | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0x9E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VFNMSUB132PD         | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0x9E V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1 | REG0=ZMM_R3():rw:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VFNMSUB132PD         | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0x9E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
VFNMSUB132PS         | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0x9E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFNMSUB132PS         | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0x9E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VFNMSUB132PS         | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0x9E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
VFNMSUB132PS         | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0x9E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VFNMSUB132PS         | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0x9E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VFNMSUB132PS         | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0x9E V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0 | REG0=ZMM_R3():rw:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VFNMSUB132PS         | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0x9E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
VFNMSUB132SD         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0x9F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                   | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFNMSUB132SD         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0x9F V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1 | REG0=XMM_R3():rw:dq:f64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFNMSUB132SD         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0x9F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
VFNMSUB132SS         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0x9F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                   | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFNMSUB132SS         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0x9F V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W0 | REG0=XMM_R3():rw:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFNMSUB132SS         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0x9F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
VFNMSUB213PD         | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xAE V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFNMSUB213PD         | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xAE V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VFNMSUB213PD         | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xAE V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
VFNMSUB213PD         | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xAE V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VFNMSUB213PD         | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xAE V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VFNMSUB213PD         | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xAE V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1 | REG0=ZMM_R3():rw:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VFNMSUB213PD         | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xAE V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
VFNMSUB213PS         | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xAE V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFNMSUB213PS         | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xAE V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VFNMSUB213PS         | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xAE V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
VFNMSUB213PS         | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xAE V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VFNMSUB213PS         | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xAE V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VFNMSUB213PS         | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xAE V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0 | REG0=ZMM_R3():rw:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VFNMSUB213PS         | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xAE V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
VFNMSUB213SD         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xAF V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                   | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFNMSUB213SD         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xAF V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1 | REG0=XMM_R3():rw:dq:f64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFNMSUB213SD         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xAF V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
VFNMSUB213SS         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xAF V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                   | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFNMSUB213SS         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xAF V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W0 | REG0=XMM_R3():rw:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFNMSUB213SS         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xAF V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
VFNMSUB231PD         | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xBE V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFNMSUB231PD         | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xBE V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VFNMSUB231PD         | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xBE V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
VFNMSUB231PD         | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xBE V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VFNMSUB231PD         | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xBE V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VFNMSUB231PD         | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xBE V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1 | REG0=ZMM_R3():rw:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VFNMSUB231PD         | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xBE V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
VFNMSUB231PS         | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xBE V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFNMSUB231PS         | VFMA           | AVX512EVEX     | AVX512F_128    | EVV 0xBE V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VFNMSUB231PS         | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xBE V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
VFNMSUB231PS         | VFMA           | AVX512EVEX     | AVX512F_256    | EVV 0xBE V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VFNMSUB231PS         | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xBE V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VFNMSUB231PS         | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xBE V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0 | REG0=ZMM_R3():rw:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VFNMSUB231PS         | VFMA           | AVX512EVEX     | AVX512F_512    | EVV 0xBE V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
VFNMSUB231SD         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xBF V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                   | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFNMSUB231SD         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xBF V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1 | REG0=XMM_R3():rw:dq:f64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VFNMSUB231SD         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xBF V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
VFNMSUB231SS         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xBF V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                   | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFNMSUB231SS         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xBF V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W0 | REG0=XMM_R3():rw:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VFNMSUB231SS         | VFMA           | AVX512EVEX     | AVX512F_SCALAR | EVV 0xBF V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
VFNMSUBPD            | FMA4           | FMA4           | FMA4           | VV1 0x7D V66 W0 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64 REG2=XMM_SE():r:dq:f64
VFNMSUBPD            | FMA4           | FMA4           | FMA4           | VV1 0x7D V66 W0 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64 REG3=XMM_SE():r:dq:f64
VFNMSUBPD            | FMA4           | FMA4           | FMA4           | VV1 0x7D V66 W1 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_SE():r:dq:f64 MEM0:r:dq:f64
VFNMSUBPD            | FMA4           | FMA4           | FMA4           | VV1 0x7D V66 W1 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_SE():r:dq:f64 REG3=XMM_B():r:dq:f64
VFNMSUBPD            | FMA4           | FMA4           | FMA4           | VV1 0x7D V66 W0 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64 REG2=YMM_SE():r:qq:f64
VFNMSUBPD            | FMA4           | FMA4           | FMA4           | VV1 0x7D V66 W0 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64 REG3=YMM_SE():r:qq:f64
VFNMSUBPD            | FMA4           | FMA4           | FMA4           | VV1 0x7D V66 W1 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_SE():r:qq:f64 MEM0:r:qq:f64
VFNMSUBPD            | FMA4           | FMA4           | FMA4           | VV1 0x7D V66 W1 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_SE():r:qq:f64 REG3=YMM_B():r:qq:f64
VFNMSUBPS            | FMA4           | FMA4           | FMA4           | VV1 0x7C V66 W0 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32 REG2=XMM_SE():r:dq:f32
VFNMSUBPS            | FMA4           | FMA4           | FMA4           | VV1 0x7C V66 W0 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32 REG3=XMM_SE():r:dq:f32
VFNMSUBPS            | FMA4           | FMA4           | FMA4           | VV1 0x7C V66 W1 VL128  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_SE():r:dq:f32 MEM0:r:dq:f32
VFNMSUBPS            | FMA4           | FMA4           | FMA4           | VV1 0x7C V66 W1 VL128  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_SE():r:dq:f32 REG3=XMM_B():r:dq:f32
VFNMSUBPS            | FMA4           | FMA4           | FMA4           | VV1 0x7C V66 W0 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32 REG2=YMM_SE():r:qq:f32
VFNMSUBPS            | FMA4           | FMA4           | FMA4           | VV1 0x7C V66 W0 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32 REG3=YMM_SE():r:qq:f32
VFNMSUBPS            | FMA4           | FMA4           | FMA4           | VV1 0x7C V66 W1 VL256  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_SE():r:qq:f32 MEM0:r:qq:f32
VFNMSUBPS            | FMA4           | FMA4           | FMA4           | VV1 0x7C V66 W1 VL256  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_SE():r:qq:f32 REG3=YMM_B():r:qq:f32
VFNMSUBSD            | FMA4           | FMA4           | FMA4           | VV1 0x7F V66 W0  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()         | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:q:f64 MEM0:r:q:f64 REG2=XMM_SE():r:q:f64
VFNMSUBSD            | FMA4           | FMA4           | FMA4           | VV1 0x7F V66 W0  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_B():r:q:f64 REG3=XMM_SE():r:q:f64
VFNMSUBSD            | FMA4           | FMA4           | FMA4           | VV1 0x7F V66 W1  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()         | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_SE():r:q:f64 MEM0:r:q:f64
VFNMSUBSD            | FMA4           | FMA4           | FMA4           | VV1 0x7F V66 W1  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_SE():r:q:f64 REG3=XMM_B():r:q:f64
VFNMSUBSS            | FMA4           | FMA4           | FMA4           | VV1 0x7E V66 W0  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()         | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:d:f32 MEM0:r:d:f32 REG2=XMM_SE():r:d:f32
VFNMSUBSS            | FMA4           | FMA4           | FMA4           | VV1 0x7E V66 W0  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_B():r:d:f32 REG3=XMM_SE():r:d:f32
VFNMSUBSS            | FMA4           | FMA4           | FMA4           | VV1 0x7E V66 W1  V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()         | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_SE():r:d:f32 MEM0:r:d:f32
VFNMSUBSS            | FMA4           | FMA4           | FMA4           | VV1 0x7E V66 W1  V0F3A MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()                | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_SE():r:d:f32 REG3=XMM_B():r:d:f32
VFPCLASSPD           | AVX512         | AVX512EVEX     | AVX512DQ_128   | EVV 0x66 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR  ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_B3():r:dq:f64 IMM0:r:b
VFPCLASSPD           | AVX512         | AVX512EVEX     | AVX512DQ_128   | EVV 0x66 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
VFPCLASSPD           | AVX512         | AVX512EVEX     | AVX512DQ_256   | EVV 0x66 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR  ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_B3():r:qq:f64 IMM0:r:b
VFPCLASSPD           | AVX512         | AVX512EVEX     | AVX512DQ_256   | EVV 0x66 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
VFPCLASSPD           | AVX512         | AVX512EVEX     | AVX512DQ_512   | EVV 0x66 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR  ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_B3():r:zf64 IMM0:r:b
VFPCLASSPD           | AVX512         | AVX512EVEX     | AVX512DQ_512   | EVV 0x66 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
VFPCLASSPS           | AVX512         | AVX512EVEX     | AVX512DQ_128   | EVV 0x66 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR  ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_B3():r:dq:f32 IMM0:r:b
VFPCLASSPS           | AVX512         | AVX512EVEX     | AVX512DQ_128   | EVV 0x66 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
VFPCLASSPS           | AVX512         | AVX512EVEX     | AVX512DQ_256   | EVV 0x66 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR  ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_B3():r:qq:f32 IMM0:r:b
VFPCLASSPS           | AVX512         | AVX512EVEX     | AVX512DQ_256   | EVV 0x66 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
VFPCLASSPS           | AVX512         | AVX512EVEX     | AVX512DQ_512   | EVV 0x66 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR  ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_B3():r:zf32 IMM0:r:b
VFPCLASSPS           | AVX512         | AVX512EVEX     | AVX512DQ_512   | EVV 0x66 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
VFPCLASSSD           | AVX512         | AVX512EVEX     | AVX512DQ_SCALAR | EVV 0x67 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1  NOEVSR  ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_B3():r:dq:f64 IMM0:r:b
VFPCLASSSD           | AVX512         | AVX512EVEX     | AVX512DQ_SCALAR | EVV 0x67 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1  NOEVSR  ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_SCALAR() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw MEM0:r:q:f64 IMM0:r:b
VFPCLASSSS           | AVX512         | AVX512EVEX     | AVX512DQ_SCALAR | EVV 0x67 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0  NOEVSR  ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_B3():r:dq:f32 IMM0:r:b
VFPCLASSSS           | AVX512         | AVX512EVEX     | AVX512DQ_SCALAR | EVV 0x67 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0  NOEVSR  ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_SCALAR() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw MEM0:r:d:f32 IMM0:r:b
VFRCZPD              | XOP            | XOP            | XOP            | XOPV 0x81 VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()       | REG0=XMM_R():w:dq:f64 MEM0:r:dq:f64
VFRCZPD              | XOP            | XOP            | XOP            | XOPV 0x81 VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]              | REG0=XMM_R():w:dq:f64 REG1=XMM_B():r:dq:f64
VFRCZPD              | XOP            | XOP            | XOP            | XOPV 0x81 VNP W0 VL256 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()       | REG0=YMM_R():w:qq:f64 MEM0:r:qq:f64
VFRCZPD              | XOP            | XOP            | XOP            | XOPV 0x81 VNP W0 VL256 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]              | REG0=YMM_R():w:qq:f64 REG1=YMM_B():r:qq:f64
VFRCZPS              | XOP            | XOP            | XOP            | XOPV 0x80 VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()       | REG0=XMM_R():w:dq:f32 MEM0:r:dq:f32
VFRCZPS              | XOP            | XOP            | XOP            | XOPV 0x80 VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]              | REG0=XMM_R():w:dq:f32 REG1=XMM_B():r:dq:f32
VFRCZPS              | XOP            | XOP            | XOP            | XOPV 0x80 VNP W0 VL256 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()       | REG0=YMM_R():w:qq:f32 MEM0:r:qq:f32
VFRCZPS              | XOP            | XOP            | XOP            | XOPV 0x80 VNP W0 VL256 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]              | REG0=YMM_R():w:qq:f32 REG1=YMM_B():r:qq:f32
VFRCZSD              | XOP            | XOP            | XOP            | XOPV 0x83 VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()       | REG0=XMM_R():w:dq:f64 MEM0:r:q:f64
VFRCZSD              | XOP            | XOP            | XOP            | XOPV 0x83 VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]              | REG0=XMM_R():w:dq:f64 REG1=XMM_B():r:q:f64
VFRCZSS              | XOP            | XOP            | XOP            | XOPV 0x82 VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()       | REG0=XMM_R():w:dq:f32 MEM0:r:d:f32
VFRCZSS              | XOP            | XOP            | XOP            | XOPV 0x82 VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]              | REG0=XMM_R():w:dq:f32 REG1=XMM_B():r:d:f32
VGATHERDPD           | GATHER         | AVX512EVEX     | AVX512F_128    | EVV 0x92 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL128  W1 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | REG0=XMM_R3():w:dq:f64 REG1=MASKNOT0():rw:mskw MEM0:r:q:f64
VGATHERDPD           | GATHER         | AVX512EVEX     | AVX512F_256    | EVV 0x92 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL256  W1 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | REG0=YMM_R3():w:qq:f64 REG1=MASKNOT0():rw:mskw MEM0:r:q:f64
VGATHERDPD           | GATHER         | AVX512EVEX     | AVX512F_512    | EVV 0x92 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL512  W1 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | REG0=ZMM_R3():w:zf64 REG1=MASKNOT0():rw:mskw MEM0:r:q:f64
VGATHERDPD           | KNC            | KNCE           | KNCE           | KVV 0x92 V66 V0F38  W1 NO_SPARSE_EVSR MOD[mm] MOD!=3 REG[rrr] RM[0b100] KNC_VMODRM() UPCONVERT_FLT64_LOAD() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():rw:mskw  MEM0:r:zv:TXT=NT:TXT=CONVERT NELEM=1:SUPP
VGATHERDPS           | GATHER         | AVX512EVEX     | AVX512F_128    | EVV 0x92 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL128  W0 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | REG0=XMM_R3():w:dq:f32 REG1=MASKNOT0():rw:mskw MEM0:r:d:f32
VGATHERDPS           | GATHER         | AVX512EVEX     | AVX512F_256    | EVV 0x92 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL256  W0 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | REG0=YMM_R3():w:qq:f32 REG1=MASKNOT0():rw:mskw MEM0:r:d:f32
VGATHERDPS           | GATHER         | AVX512EVEX     | AVX512F_512    | EVV 0x92 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL512  W0 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | REG0=ZMM_R3():w:zf32 REG1=MASKNOT0():rw:mskw MEM0:r:d:f32
VGATHERDPS           | KNC            | KNCE           | KNCE           | KVV 0x92 V66 V0F38  W0 NO_SPARSE_EVSR MOD[mm] MOD!=3 REG[rrr] RM[0b100] KNC_VMODRM() UPCONVERT_FLT32_LOAD() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():rw:mskw  MEM0:r:zv:TXT=NT:TXT=CONVERT NELEM=1:SUPP
VGATHERPF0DPD        | GATHER         | AVX512EVEX     | AVX512PF_512   | EVV 0xC6 V66 V0F38 MOD[mm] MOD!=3 REG[0b001] RM[nnn] BCRC=0   VL512  W1 RM=4 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:r:q:f64 REG0=MASKNOT0():rw:mskw
VGATHERPF0DPS        | GATHER         | AVX512EVEX     | AVX512PF_512   | EVV 0xC6 V66 V0F38 MOD[mm] MOD!=3 REG[0b001] RM[nnn] BCRC=0   VL512  W0 RM=4 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:r:d:f32 REG0=MASKNOT0():rw:mskw
VGATHERPF0HINTDPD    | PREFETCH       | KNCE           | KNC_PF_HINT    | KVV 0xC6 V0F38 V66  REXW=1 NO_SPARSE_EVSR MOD[mm] MOD!=3 REG[0b000] RM[0b100] KNC_VMODRM() UPCONVERT_FLT32_LOAD() | MEM0:r:zv:TXT=NT REG0=MASK1():rw:mskw
VGATHERPF0HINTDPS    | PREFETCH       | KNCE           | KNC_PF_HINT    | KVV 0xC6 V0F38 V66  REXW=0 NO_SPARSE_EVSR MOD[mm] MOD!=3 REG[0b000] RM[0b100] KNC_VMODRM() UPCONVERT_FLT32_LOAD() | MEM0:r:zv:TXT=NT REG0=MASK1():rw:mskw
VGATHERPF0QPD        | GATHER         | AVX512EVEX     | AVX512PF_512   | EVV 0xC7 V66 V0F38 MOD[mm] MOD!=3 REG[0b001] RM[nnn] BCRC=0   VL512  W1 RM=4 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:r:q:f64 REG0=MASKNOT0():rw:mskw
VGATHERPF0QPS        | GATHER         | AVX512EVEX     | AVX512PF_512   | EVV 0xC7 V66 V0F38 MOD[mm] MOD!=3 REG[0b001] RM[nnn] BCRC=0   VL512  W0 RM=4 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:r:d:f32 REG0=MASKNOT0():rw:mskw
VGATHERPF1DPD        | GATHER         | AVX512EVEX     | AVX512PF_512   | EVV 0xC6 V66 V0F38 MOD[mm] MOD!=3 REG[0b010] RM[nnn] BCRC=0   VL512  W1 RM=4 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:r:q:f64 REG0=MASKNOT0():rw:mskw
VGATHERPF1DPS        | GATHER         | AVX512EVEX     | AVX512PF_512   | EVV 0xC6 V66 V0F38 MOD[mm] MOD!=3 REG[0b010] RM[nnn] BCRC=0   VL512  W0 RM=4 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:r:d:f32 REG0=MASKNOT0():rw:mskw
VGATHERPF1QPD        | GATHER         | AVX512EVEX     | AVX512PF_512   | EVV 0xC7 V66 V0F38 MOD[mm] MOD!=3 REG[0b010] RM[nnn] BCRC=0   VL512  W1 RM=4 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:r:q:f64 REG0=MASKNOT0():rw:mskw
VGATHERPF1QPS        | GATHER         | AVX512EVEX     | AVX512PF_512   | EVV 0xC7 V66 V0F38 MOD[mm] MOD!=3 REG[0b010] RM[nnn] BCRC=0   VL512  W0 RM=4 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:r:d:f32 REG0=MASKNOT0():rw:mskw
VGATHERQPD           | GATHER         | AVX512EVEX     | AVX512F_128    | EVV 0x93 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL128  W1 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | REG0=XMM_R3():w:dq:f64 REG1=MASKNOT0():rw:mskw MEM0:r:q:f64
VGATHERQPD           | GATHER         | AVX512EVEX     | AVX512F_256    | EVV 0x93 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL256  W1 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | REG0=YMM_R3():w:qq:f64 REG1=MASKNOT0():rw:mskw MEM0:r:q:f64
VGATHERQPD           | GATHER         | AVX512EVEX     | AVX512F_512    | EVV 0x93 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL512  W1 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | REG0=ZMM_R3():w:zf64 REG1=MASKNOT0():rw:mskw MEM0:r:q:f64
VGATHERQPS           | GATHER         | AVX512EVEX     | AVX512F_128    | EVV 0x93 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL128  W0 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | REG0=XMM_R3():w:dq:f32 REG1=MASKNOT0():rw:mskw MEM0:r:d:f32
VGATHERQPS           | GATHER         | AVX512EVEX     | AVX512F_256    | EVV 0x93 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL256  W0 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | REG0=XMM_R3():w:dq:f32 REG1=MASKNOT0():rw:mskw MEM0:r:d:f32
VGATHERQPS           | GATHER         | AVX512EVEX     | AVX512F_512    | EVV 0x93 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL512  W0 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | REG0=YMM_R3():w:qq:f32 REG1=MASKNOT0():rw:mskw MEM0:r:d:f32
VGETEXPPD            | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x42 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR    | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64
VGETEXPPD            | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x42 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VGETEXPPD            | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x42 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR    | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64
VGETEXPPD            | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x42 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VGETEXPPD            | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x42 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR    | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
VGETEXPPD            | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x42 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W1  NOEVSR | REG0=ZMM_R3():w:zf64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
VGETEXPPD            | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x42 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VGETEXPPS            | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x42 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
VGETEXPPS            | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x42 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VGETEXPPS            | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x42 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
VGETEXPPS            | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x42 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VGETEXPPS            | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x42 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
VGETEXPPS            | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x42 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0  NOEVSR | REG0=ZMM_R3():w:zf32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
VGETEXPPS            | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x42 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VGETEXPSD            | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x43 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                   | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VGETEXPSD            | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x43 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W1 | REG0=XMM_R3():w:dq:f64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VGETEXPSD            | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x43 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
VGETEXPSS            | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x43 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                   | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VGETEXPSS            | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x43 V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W0 | REG0=XMM_R3():w:dq:f32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VGETEXPSS            | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x43 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
VGETMANTPD           | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x26 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR UIMM8() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64 IMM0:r:b
VGETMANTPD           | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x26 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
VGETMANTPD           | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x26 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR UIMM8() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64 IMM0:r:b
VGETMANTPD           | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x26 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
VGETMANTPD           | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x26 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR UIMM8() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64 IMM0:r:b
VGETMANTPD           | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x26 V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W1  NOEVSR UIMM8() | REG0=ZMM_R3():w:zf64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64 IMM0:r:b
VGETMANTPD           | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x26 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
VGETMANTPS           | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x26 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR UIMM8() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32 IMM0:r:b
VGETMANTPS           | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x26 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
VGETMANTPS           | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x26 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR UIMM8() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32 IMM0:r:b
VGETMANTPS           | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x26 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
VGETMANTPS           | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x26 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR UIMM8() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32 IMM0:r:b
VGETMANTPS           | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x26 V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0  NOEVSR UIMM8() | REG0=ZMM_R3():w:zf32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32 IMM0:r:b
VGETMANTPS           | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x26 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
VGETMANTPS           | KNC            | KNCE           | KNCE           | KVV 0x26 V0F3A V66  W0  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() UPCONVERT_FLT32() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw  MEM0:r:zv:TXT=CONVERT:TXT=NT IMM0:r:b
VGETMANTPS           | KNC            | KNCE           | KNCE           | KVV 0x26 V0F3A V66  W0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] UIMM8() NR=1 KNC_SAE() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw  REG2=ZMM_B3():r:zf32:TXT=SAEC  IMM0:r:b
VGETMANTPS           | KNC            | KNCE           | KNCE           | KVV 0x26 V0F3A V66  W0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] UIMM8() NR=0 REG_SWIZZLE32() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw  REG2=ZMM_B3():r:zf32:TXT=REGSWIZ IMM0:r:b
VGETMANTSD           | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x27 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1   UIMM8()         | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64 IMM0:r:b
VGETMANTSD           | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x27 V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W1   UIMM8() | REG0=XMM_R3():w:dq:f64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64 IMM0:r:b
VGETMANTSD           | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x27 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1   UIMM8()  ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64 IMM0:r:b
VGETMANTSS           | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x27 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0   UIMM8()         | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32 IMM0:r:b
VGETMANTSS           | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x27 V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W0   UIMM8() | REG0=XMM_R3():w:dq:f32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32 IMM0:r:b
VGETMANTSS           | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x27 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0   UIMM8()  ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32 IMM0:r:b
VGF2P8AFFINEINVQB    | GFNI           | AVX512EVEX     | AVX512_GFNI_128 | EVV 0xCF V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1   UIMM8()  | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u64 IMM0:r:b
VGF2P8AFFINEINVQB    | GFNI           | AVX512EVEX     | AVX512_GFNI_128 | EVV 0xCF V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VGF2P8AFFINEINVQB    | GFNI           | AVX512EVEX     | AVX512_GFNI_256 | EVV 0xCF V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1   UIMM8()  | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u64 IMM0:r:b
VGF2P8AFFINEINVQB    | GFNI           | AVX512EVEX     | AVX512_GFNI_256 | EVV 0xCF V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VGF2P8AFFINEINVQB    | GFNI           | AVX512EVEX     | AVX512_GFNI_512 | EVV 0xCF V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1   UIMM8()  | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu64 IMM0:r:b
VGF2P8AFFINEINVQB    | GFNI           | AVX512EVEX     | AVX512_GFNI_512 | EVV 0xCF V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VGF2P8AFFINEINVQB    | GFNI           | GFNI           | AVX_GFNI       | VV1 0xCF V66 V0F3A MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W1   UIMM8()        | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:u64 IMM0:r:b
VGF2P8AFFINEINVQB    | GFNI           | GFNI           | AVX_GFNI       | VV1 0xCF V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1   UIMM8() | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 MEM0:r:dq:u64 IMM0:r:b
VGF2P8AFFINEINVQB    | GFNI           | GFNI           | AVX_GFNI       | VV1 0xCF V66 V0F3A MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W1   UIMM8()        | REG0=YMM_R():w:qq:u8 REG1=YMM_N():r:qq:u8 REG2=YMM_B():r:qq:u64 IMM0:r:b
VGF2P8AFFINEINVQB    | GFNI           | GFNI           | AVX_GFNI       | VV1 0xCF V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1   UIMM8() | REG0=YMM_R():w:qq:u8 REG1=YMM_N():r:qq:u8 MEM0:r:qq:u64 IMM0:r:b
VGF2P8AFFINEQB       | GFNI           | AVX512EVEX     | AVX512_GFNI_128 | EVV 0xCE V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1   UIMM8()  | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u64 IMM0:r:b
VGF2P8AFFINEQB       | GFNI           | AVX512EVEX     | AVX512_GFNI_128 | EVV 0xCE V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VGF2P8AFFINEQB       | GFNI           | AVX512EVEX     | AVX512_GFNI_256 | EVV 0xCE V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1   UIMM8()  | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u64 IMM0:r:b
VGF2P8AFFINEQB       | GFNI           | AVX512EVEX     | AVX512_GFNI_256 | EVV 0xCE V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VGF2P8AFFINEQB       | GFNI           | AVX512EVEX     | AVX512_GFNI_512 | EVV 0xCE V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1   UIMM8()  | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu64 IMM0:r:b
VGF2P8AFFINEQB       | GFNI           | AVX512EVEX     | AVX512_GFNI_512 | EVV 0xCE V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VGF2P8AFFINEQB       | GFNI           | GFNI           | AVX_GFNI       | VV1 0xCE V66 V0F3A MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W1   UIMM8()        | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:u64 IMM0:r:b
VGF2P8AFFINEQB       | GFNI           | GFNI           | AVX_GFNI       | VV1 0xCE V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1   UIMM8() | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 MEM0:r:dq:u64 IMM0:r:b
VGF2P8AFFINEQB       | GFNI           | GFNI           | AVX_GFNI       | VV1 0xCE V66 V0F3A MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W1   UIMM8()        | REG0=YMM_R():w:qq:u8 REG1=YMM_N():r:qq:u8 REG2=YMM_B():r:qq:u64 IMM0:r:b
VGF2P8AFFINEQB       | GFNI           | GFNI           | AVX_GFNI       | VV1 0xCE V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1   UIMM8() | REG0=YMM_R():w:qq:u8 REG1=YMM_N():r:qq:u8 MEM0:r:qq:u64 IMM0:r:b
VGF2P8MULB           | GFNI           | AVX512EVEX     | AVX512_GFNI_128 | EVV 0xCF V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
VGF2P8MULB           | GFNI           | AVX512EVEX     | AVX512_GFNI_128 | EVV 0xCF V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0    ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
VGF2P8MULB           | GFNI           | AVX512EVEX     | AVX512_GFNI_256 | EVV 0xCF V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
VGF2P8MULB           | GFNI           | AVX512EVEX     | AVX512_GFNI_256 | EVV 0xCF V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0    ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
VGF2P8MULB           | GFNI           | AVX512EVEX     | AVX512_GFNI_512 | EVV 0xCF V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
VGF2P8MULB           | GFNI           | AVX512EVEX     | AVX512_GFNI_512 | EVV 0xCF V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0    ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
VGF2P8MULB           | GFNI           | GFNI           | AVX_GFNI       | VV1 0xCF V66 V0F38 MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL128  W0                  | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:u8
VGF2P8MULB           | GFNI           | GFNI           | AVX_GFNI       | VV1 0xCF V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0           | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 MEM0:r:dq:u8
VGF2P8MULB           | GFNI           | GFNI           | AVX_GFNI       | VV1 0xCF V66 V0F38 MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256  W0                  | REG0=YMM_R():w:qq:u8 REG1=YMM_N():r:qq:u8 REG2=YMM_B():r:qq:u8
VGF2P8MULB           | GFNI           | GFNI           | AVX_GFNI       | VV1 0xCF V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0           | REG0=YMM_R():w:qq:u8 REG1=YMM_N():r:qq:u8 MEM0:r:qq:u8
VINSERTF32X4         | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x18 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0   UIMM8()  | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=XMM_B3():r:dq:f32 IMM0:r:b
VINSERTF32X4         | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x18 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0   UIMM8()  ESIZE_32_BITS() NELEM_TUPLE4() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:dq:f32 IMM0:r:b
VINSERTF32X4         | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x18 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0   UIMM8()  | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=XMM_B3():r:dq:f32 IMM0:r:b
VINSERTF32X4         | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x18 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0   UIMM8()  ESIZE_32_BITS() NELEM_TUPLE4() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:dq:f32 IMM0:r:b
VINSERTF32X8         | AVX512         | AVX512EVEX     | AVX512DQ_512   | EVV 0x1A V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0   UIMM8()  | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=YMM_B3():r:qq:f32 IMM0:r:b
VINSERTF32X8         | AVX512         | AVX512EVEX     | AVX512DQ_512   | EVV 0x1A V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0   UIMM8()  ESIZE_32_BITS() NELEM_TUPLE8() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:qq:f32 IMM0:r:b
VINSERTF64X2         | AVX512         | AVX512EVEX     | AVX512DQ_256   | EVV 0x18 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1   UIMM8()  | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=XMM_B3():r:dq:f64 IMM0:r:b
VINSERTF64X2         | AVX512         | AVX512EVEX     | AVX512DQ_256   | EVV 0x18 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1   UIMM8()  ESIZE_64_BITS() NELEM_TUPLE2() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:dq:f64 IMM0:r:b
VINSERTF64X2         | AVX512         | AVX512EVEX     | AVX512DQ_512   | EVV 0x18 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1   UIMM8()  | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=XMM_B3():r:dq:f64 IMM0:r:b
VINSERTF64X2         | AVX512         | AVX512EVEX     | AVX512DQ_512   | EVV 0x18 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_TUPLE2() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:dq:f64 IMM0:r:b
VINSERTF64X4         | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x1A V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1   UIMM8()  | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=YMM_B3():r:qq:f64 IMM0:r:b
VINSERTF64X4         | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x1A V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_TUPLE4() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:qq:f64 IMM0:r:b
VINSERTI32X4         | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x38 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0   UIMM8()  | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=XMM_B3():r:dq:u32 IMM0:r:b
VINSERTI32X4         | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x38 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0   UIMM8()  ESIZE_32_BITS() NELEM_TUPLE4() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:dq:u32 IMM0:r:b
VINSERTI32X4         | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x38 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0   UIMM8()  | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=XMM_B3():r:dq:u32 IMM0:r:b
VINSERTI32X4         | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x38 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0   UIMM8()  ESIZE_32_BITS() NELEM_TUPLE4() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:dq:u32 IMM0:r:b
VINSERTI32X8         | AVX512         | AVX512EVEX     | AVX512DQ_512   | EVV 0x3A V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0   UIMM8()  | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=YMM_B3():r:qq:u32 IMM0:r:b
VINSERTI32X8         | AVX512         | AVX512EVEX     | AVX512DQ_512   | EVV 0x3A V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0   UIMM8()  ESIZE_32_BITS() NELEM_TUPLE8() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:qq:u32 IMM0:r:b
VINSERTI64X2         | AVX512         | AVX512EVEX     | AVX512DQ_256   | EVV 0x38 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1   UIMM8()  | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=XMM_B3():r:dq:u64 IMM0:r:b
VINSERTI64X2         | AVX512         | AVX512EVEX     | AVX512DQ_256   | EVV 0x38 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1   UIMM8()  ESIZE_64_BITS() NELEM_TUPLE2() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:dq:u64 IMM0:r:b
VINSERTI64X2         | AVX512         | AVX512EVEX     | AVX512DQ_512   | EVV 0x38 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1   UIMM8()  | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=XMM_B3():r:dq:u64 IMM0:r:b
VINSERTI64X2         | AVX512         | AVX512EVEX     | AVX512DQ_512   | EVV 0x38 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_TUPLE2() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:dq:u64 IMM0:r:b
VINSERTI64X4         | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x3A V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1   UIMM8()  | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=YMM_B3():r:qq:u64 IMM0:r:b
VINSERTI64X4         | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x3A V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_TUPLE4() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:qq:u64 IMM0:r:b
VINSERTPS            | AVX512         | AVX512EVEX     | AVX512F_128N   | EVV 0x21 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0    ZEROING=0 MASK=0 UIMM8() | REG0=XMM_R3():w:dq:f32 REG1=XMM_N3():r:dq:f32 REG2=XMM_B3():r:dq:f32 IMM0:r:b
VINSERTPS            | AVX512         | AVX512EVEX     | AVX512F_128N   | EVV 0x21 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0    ZEROING=0 MASK=0 UIMM8()  ESIZE_32_BITS() NELEM_TUPLE1() | REG0=XMM_R3():w:dq:f32 REG1=XMM_N3():r:dq:f32 MEM0:r:d:f32 IMM0:r:b
VLOG2PS              | KNC            | KNCE           | KNCE           | KVV 0xC9 V0F38 V66  W0  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() NOSWIZF32() | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  MEM0:r:zd:TXT=NT
VLOG2PS              | KNC            | KNCE           | KNCE           | KVV 0xC9 V0F38 V66  W0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] NR=1 KNC_SAE()  | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw REG2=ZMM_B3():r:zd:TXT=SAEC
VLOG2PS              | KNC            | KNCE           | KNCE           | KVV 0xC9 V0F38 V66  W0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] NR=0 SWIZ=0     | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw REG2=ZMM_B3():r:zd
VMAXPD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x5F V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1              | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VMAXPD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x5F V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VMAXPD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x5F V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1              | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
VMAXPD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x5F V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VMAXPD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x5F V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1              | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VMAXPD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x5F V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W1 | REG0=ZMM_R3():w:zf64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VMAXPD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x5F V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
VMAXPS               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x5F VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0              | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VMAXPS               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x5F VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VMAXPS               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x5F VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0              | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
VMAXPS               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x5F VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VMAXPS               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x5F VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0              | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VMAXPS               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x5F VNP V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0 | REG0=ZMM_R3():w:zf32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VMAXPS               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x5F VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
VMAXSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x5F VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                     | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VMAXSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x5F VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W1 | REG0=XMM_R3():w:dq:f64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VMAXSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x5F VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
VMAXSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x5F VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                     | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VMAXSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x5F VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W0 | REG0=XMM_R3():w:dq:f32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VMAXSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x5F VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
VMFUNC               | VTX            | VMFUNC         | VMFUNC         | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b010] RM[0b100] no_refining_prefix                | REG0=XED_REG_EAX:r:SUPP
VMINPD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x5D V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1              | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VMINPD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x5D V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VMINPD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x5D V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1              | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
VMINPD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x5D V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VMINPD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x5D V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1              | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VMINPD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x5D V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W1 | REG0=ZMM_R3():w:zf64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VMINPD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x5D V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
VMINPS               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x5D VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0              | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VMINPS               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x5D VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VMINPS               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x5D VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0              | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
VMINPS               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x5D VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VMINPS               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x5D VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0              | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VMINPS               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x5D VNP V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0 | REG0=ZMM_R3():w:zf32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VMINPS               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x5D VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
VMINSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x5D VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                     | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VMINSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x5D VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W1 | REG0=XMM_R3():w:dq:f64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VMINSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x5D VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
VMINSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x5D VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                     | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VMINSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x5D VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W0 | REG0=XMM_R3():w:dq:f32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VMINSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x5D VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
VMOVAPD              | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x28 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR      | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64
VMOVAPD              | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x28 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:f64
VMOVAPD              | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x29 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR      | REG0=XMM_B3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:f64
VMOVAPD              | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x29 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_FULLMEM() | MEM0:w:dq:f64 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:f64
VMOVAPD              | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x28 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR      | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64
VMOVAPD              | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x28 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:f64
VMOVAPD              | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x29 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR      | REG0=YMM_B3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:f64
VMOVAPD              | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x29 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_FULLMEM() | MEM0:w:qq:f64 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:f64
VMOVAPD              | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x28 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR      | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
VMOVAPD              | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x28 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:f64
VMOVAPD              | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x29 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR      | REG0=ZMM_B3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zf64
VMOVAPD              | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x29 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_FULLMEM() | MEM0:w:zd:f64 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zf64
VMOVAPS              | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x28 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR      | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
VMOVAPS              | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x28 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:f32
VMOVAPS              | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x29 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR      | REG0=XMM_B3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:f32
VMOVAPS              | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x29 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_FULLMEM() | MEM0:w:dq:f32 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:f32
VMOVAPS              | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x28 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR      | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
VMOVAPS              | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x28 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:f32
VMOVAPS              | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x29 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR      | REG0=YMM_B3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:f32
VMOVAPS              | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x29 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_FULLMEM() | MEM0:w:qq:f32 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:f32
VMOVAPS              | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x28 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR      | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
VMOVAPS              | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x28 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:f32
VMOVAPS              | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x29 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR      | REG0=ZMM_B3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zf32
VMOVAPS              | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x29 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_FULLMEM() | MEM0:w:zd:f32 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zf32
VMOVD                | DATAXFER       | AVX512EVEX     | AVX512F_128N   | EVV 0x6E V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  not64  NOEVSR  ZEROING=0 MASK=0 | REG0=XMM_R3():w:dq:u32 REG1=GPR32_B():r:d:u32
VMOVD                | DATAXFER       | AVX512EVEX     | AVX512F_128N   | EVV 0x6E V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  mode64 W0  NOEVSR  ZEROING=0 MASK=0 | REG0=XMM_R3():w:dq:u32 REG1=GPR32_B():r:d:u32
VMOVD                | DATAXFER       | AVX512EVEX     | AVX512F_128N   | EVV 0x6E V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  not64  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_READER() | REG0=XMM_R3():w:dq:u32 MEM0:r:d:u32
VMOVD                | DATAXFER       | AVX512EVEX     | AVX512F_128N   | EVV 0x6E V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  mode64 W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_READER() | REG0=XMM_R3():w:dq:u32 MEM0:r:d:u32
VMOVD                | DATAXFER       | AVX512EVEX     | AVX512F_128N   | EVV 0x7E V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  not64  NOEVSR  ZEROING=0 MASK=0 | REG0=GPR32_B():w:d:u32 REG1=XMM_R3():r:dq:u32
VMOVD                | DATAXFER       | AVX512EVEX     | AVX512F_128N   | EVV 0x7E V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  mode64 W0  NOEVSR  ZEROING=0 MASK=0 | REG0=GPR32_B():w:d:u32 REG1=XMM_R3():r:dq:u32
VMOVD                | DATAXFER       | AVX512EVEX     | AVX512F_128N   | EVV 0x7E V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  not64  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_WRITER_STORE() | MEM0:w:d:u32 REG0=XMM_R3():r:dq:u32
VMOVD                | DATAXFER       | AVX512EVEX     | AVX512F_128N   | EVV 0x7E V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  mode64 W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_GPR_WRITER_STORE() | MEM0:w:d:u32 REG0=XMM_R3():r:dq:u32
VMOVDDUP             | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x12 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR      | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64
VMOVDDUP             | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x12 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_MOVDDUP() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:f64
VMOVDDUP             | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x12 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR      | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64
VMOVDDUP             | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x12 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_MOVDDUP() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:f64
VMOVDDUP             | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x12 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR      | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
VMOVDDUP             | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x12 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_MOVDDUP() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:f64
VMOVDQA32            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x6F V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR      | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32
VMOVDQA32            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x6F V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u32
VMOVDQA32            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x7F V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR      | REG0=XMM_B3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u32
VMOVDQA32            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x7F V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_FULLMEM() | MEM0:w:dq:u32 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u32
VMOVDQA32            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x6F V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR      | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u32
VMOVDQA32            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x6F V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u32
VMOVDQA32            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x7F V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR      | REG0=YMM_B3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u32
VMOVDQA32            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x7F V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_FULLMEM() | MEM0:w:qq:u32 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u32
VMOVDQA32            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x6F V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR      | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu32
VMOVDQA32            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x6F V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:u32
VMOVDQA32            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x7F V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR      | REG0=ZMM_B3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu32
VMOVDQA32            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x7F V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_FULLMEM() | MEM0:w:zd:u32 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu32
VMOVDQA64            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x6F V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR      | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u64
VMOVDQA64            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x6F V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u64
VMOVDQA64            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x7F V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR      | REG0=XMM_B3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u64
VMOVDQA64            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x7F V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_FULLMEM() | MEM0:w:dq:u64 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u64
VMOVDQA64            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x6F V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR      | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u64
VMOVDQA64            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x6F V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u64
VMOVDQA64            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x7F V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR      | REG0=YMM_B3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u64
VMOVDQA64            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x7F V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_FULLMEM() | MEM0:w:qq:u64 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u64
VMOVDQA64            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x6F V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR      | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu64
VMOVDQA64            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x6F V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:u64
VMOVDQA64            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x7F V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR      | REG0=ZMM_B3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu64
VMOVDQA64            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x7F V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_FULLMEM() | MEM0:w:zd:u64 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu64
VMOVDQU16            | DATAXFER       | AVX512EVEX     | AVX512BW_128   | EVV 0x6F VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR      | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u16
VMOVDQU16            | DATAXFER       | AVX512EVEX     | AVX512BW_128   | EVV 0x6F VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u16
VMOVDQU16            | DATAXFER       | AVX512EVEX     | AVX512BW_128   | EVV 0x7F VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR      | REG0=XMM_B3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u16
VMOVDQU16            | DATAXFER       | AVX512EVEX     | AVX512BW_128   | EVV 0x7F VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_FULLMEM() | MEM0:w:dq:u16 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u16
VMOVDQU16            | DATAXFER       | AVX512EVEX     | AVX512BW_256   | EVV 0x6F VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR      | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u16
VMOVDQU16            | DATAXFER       | AVX512EVEX     | AVX512BW_256   | EVV 0x6F VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u16
VMOVDQU16            | DATAXFER       | AVX512EVEX     | AVX512BW_256   | EVV 0x7F VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR      | REG0=YMM_B3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u16
VMOVDQU16            | DATAXFER       | AVX512EVEX     | AVX512BW_256   | EVV 0x7F VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_FULLMEM() | MEM0:w:qq:u16 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u16
VMOVDQU16            | DATAXFER       | AVX512EVEX     | AVX512BW_512   | EVV 0x6F VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR      | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu16
VMOVDQU16            | DATAXFER       | AVX512EVEX     | AVX512BW_512   | EVV 0x6F VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:u16
VMOVDQU16            | DATAXFER       | AVX512EVEX     | AVX512BW_512   | EVV 0x7F VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR      | REG0=ZMM_B3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu16
VMOVDQU16            | DATAXFER       | AVX512EVEX     | AVX512BW_512   | EVV 0x7F VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_FULLMEM() | MEM0:w:zd:u16 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu16
VMOVDQU32            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x6F VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR      | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32
VMOVDQU32            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x6F VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u32
VMOVDQU32            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x7F VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR      | REG0=XMM_B3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u32
VMOVDQU32            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x7F VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_FULLMEM() | MEM0:w:dq:u32 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u32
VMOVDQU32            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x6F VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR      | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u32
VMOVDQU32            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x6F VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u32
VMOVDQU32            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x7F VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR      | REG0=YMM_B3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u32
VMOVDQU32            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x7F VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_FULLMEM() | MEM0:w:qq:u32 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u32
VMOVDQU32            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x6F VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR      | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu32
VMOVDQU32            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x6F VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:u32
VMOVDQU32            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x7F VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR      | REG0=ZMM_B3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu32
VMOVDQU32            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x7F VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_FULLMEM() | MEM0:w:zd:u32 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu32
VMOVDQU64            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x6F VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR      | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u64
VMOVDQU64            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x6F VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u64
VMOVDQU64            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x7F VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR      | REG0=XMM_B3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u64
VMOVDQU64            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x7F VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_FULLMEM() | MEM0:w:dq:u64 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u64
VMOVDQU64            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x6F VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR      | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u64
VMOVDQU64            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x6F VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u64
VMOVDQU64            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x7F VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR      | REG0=YMM_B3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u64
VMOVDQU64            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x7F VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_FULLMEM() | MEM0:w:qq:u64 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u64
VMOVDQU64            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x6F VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR      | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu64
VMOVDQU64            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x6F VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:u64
VMOVDQU64            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x7F VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR      | REG0=ZMM_B3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu64
VMOVDQU64            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x7F VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_FULLMEM() | MEM0:w:zd:u64 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu64
VMOVDQU8             | DATAXFER       | AVX512EVEX     | AVX512BW_128   | EVV 0x6F VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR      | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u8
VMOVDQU8             | DATAXFER       | AVX512EVEX     | AVX512BW_128   | EVV 0x6F VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u8
VMOVDQU8             | DATAXFER       | AVX512EVEX     | AVX512BW_128   | EVV 0x7F VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR      | REG0=XMM_B3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u8
VMOVDQU8             | DATAXFER       | AVX512EVEX     | AVX512BW_128   | EVV 0x7F VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_FULLMEM() | MEM0:w:dq:u8 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u8
VMOVDQU8             | DATAXFER       | AVX512EVEX     | AVX512BW_256   | EVV 0x6F VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR      | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u8
VMOVDQU8             | DATAXFER       | AVX512EVEX     | AVX512BW_256   | EVV 0x6F VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u8
VMOVDQU8             | DATAXFER       | AVX512EVEX     | AVX512BW_256   | EVV 0x7F VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR      | REG0=YMM_B3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u8
VMOVDQU8             | DATAXFER       | AVX512EVEX     | AVX512BW_256   | EVV 0x7F VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_FULLMEM() | MEM0:w:qq:u8 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u8
VMOVDQU8             | DATAXFER       | AVX512EVEX     | AVX512BW_512   | EVV 0x6F VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR      | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu8
VMOVDQU8             | DATAXFER       | AVX512EVEX     | AVX512BW_512   | EVV 0x6F VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:u8
VMOVDQU8             | DATAXFER       | AVX512EVEX     | AVX512BW_512   | EVV 0x7F VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR      | REG0=ZMM_B3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu8
VMOVDQU8             | DATAXFER       | AVX512EVEX     | AVX512BW_512   | EVV 0x7F VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_FULLMEM() | MEM0:w:zd:u8 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu8
VMOVHLPS             | DATAXFER       | AVX512EVEX     | AVX512F_128N   | EVV 0x12 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0    ZEROING=0 MASK=0 | REG0=XMM_R3():w:dq:f32 REG1=XMM_N3():r:dq:f32 REG2=XMM_B3():r:dq:f32
VMOVHPD              | DATAXFER       | AVX512EVEX     | AVX512F_128N   | EVV 0x16 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128 W1    ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=XMM_N3():r:q:f64 MEM0:r:q:f64
VMOVHPD              | DATAXFER       | AVX512EVEX     | AVX512F_128N   | EVV 0x17 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM() VL128  W1  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_SCALAR() | MEM0:w:q:f64 REG0=XMM_R3():r:dq:f64
VMOVHPS              | DATAXFER       | AVX512EVEX     | AVX512F_128N   | EVV 0x16 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0    ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_TUPLE2() | REG0=XMM_R3():w:dq:f32 REG1=XMM_N3():r:q:f32 MEM0:r:q:f32
VMOVHPS              | DATAXFER       | AVX512EVEX     | AVX512F_128N   | EVV 0x17 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_TUPLE2() | MEM0:w:q:f32 REG0=XMM_R3():r:dq:f32
VMOVLHPS             | DATAXFER       | AVX512EVEX     | AVX512F_128N   | EVV 0x16 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0    ZEROING=0 MASK=0 | REG0=XMM_R3():w:dq:f32 REG1=XMM_N3():r:q:f32 REG2=XMM_B3():r:q:f32
VMOVLPD              | DATAXFER       | AVX512EVEX     | AVX512F_128N   | EVV 0x12 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM() VL128  W1    ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=XMM_N3():r:dq:f64 MEM0:r:q:f64
VMOVLPD              | DATAXFER       | AVX512EVEX     | AVX512F_128N   | EVV 0x13 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM() VL128 W1  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_SCALAR() | MEM0:w:q:f64 REG0=XMM_R3():r:q:f64
VMOVLPS              | DATAXFER       | AVX512EVEX     | AVX512F_128N   | EVV 0x12 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0    ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_TUPLE2() | REG0=XMM_R3():w:dq:f32 REG1=XMM_N3():r:dq:f32 MEM0:r:q:f32
VMOVLPS              | DATAXFER       | AVX512EVEX     | AVX512F_128N   | EVV 0x13 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_TUPLE2() | MEM0:w:q:f32 REG0=XMM_R3():r:q:f32
VMOVNRAPD            | DATAXFER       | KNCE           | KNCSTREAM      | KVV 0x29 V0F VF3  REXW=1 NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] NR=0 MODRM()  DNCONVERT_FLT64() | MEM0:rw:zf64:TXT=NT  REG0=MASK1():r:mskw REG1=ZMM_R3():r:zf64
VMOVNRAPS            | DATAXFER       | KNCE           | KNCSTREAM      | KVV 0x29 V0F VF2  REXW=0 NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] NR=0 MODRM() DNCONVERT_FLT32() | MEM0:rw:zv:TXT=CONVERT:TXT=NT  REG0=MASK1():r:mskw   REG1=ZMM_R3():r:zf32
VMOVNRNGOAPD         | DATAXFER       | KNCE           | KNCSTREAM      | KVV 0x29 V0F VF3  REXW=1 NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] NR=1 MODRM()  DNCONVERT_FLT64() | MEM0:rw:zf64:TXT=NT  REG0=MASK1():r:mskw REG1=ZMM_R3():r:zf64
VMOVNRNGOAPS         | DATAXFER       | KNCE           | KNCSTREAM      | KVV 0x29 V0F VF2  REXW=0 NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] NR=1 MODRM() DNCONVERT_FLT32() | MEM0:rw:zv:TXT=CONVERT:TXT=NT  REG0=MASK1():r:mskw   REG1=ZMM_R3():r:zf32
VMOVNTDQ             | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0xE7 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_FULLMEM() | MEM0:w:dq:u32 REG0=XMM_R3():r:dq:u32
VMOVNTDQ             | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0xE7 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_FULLMEM() | MEM0:w:qq:u32 REG0=YMM_R3():r:qq:u32
VMOVNTDQ             | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0xE7 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_FULLMEM() | MEM0:w:zd:u32 REG0=ZMM_R3():r:zu32
VMOVNTDQA            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x2A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u32 MEM0:r:dq:u32
VMOVNTDQA            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x2A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u32 MEM0:r:qq:u32
VMOVNTDQA            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x2A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu32 MEM0:r:zd:u32
VMOVNTPD             | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x2B V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_FULLMEM() | MEM0:w:dq:f64 REG0=XMM_R3():r:dq:f64
VMOVNTPD             | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x2B V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_FULLMEM() | MEM0:w:qq:f64 REG0=YMM_R3():r:qq:f64
VMOVNTPD             | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x2B V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_FULLMEM() | MEM0:w:zd:f64 REG0=ZMM_R3():r:zf64
VMOVNTPS             | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x2B VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_FULLMEM() | MEM0:w:dq:f32 REG0=XMM_R3():r:dq:f32
VMOVNTPS             | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x2B VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_FULLMEM() | MEM0:w:qq:f32 REG0=YMM_R3():r:qq:f32
VMOVNTPS             | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x2B VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_FULLMEM() | MEM0:w:zd:f32 REG0=ZMM_R3():r:zf32
VMOVQ                | DATAXFER       | AVX512EVEX     | AVX512F_128N   | EVV 0x6E V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  mode64  NOEVSR  ZEROING=0 MASK=0 | REG0=XMM_R3():w:dq:u64 REG1=GPR64_B():r:q:u64
VMOVQ                | DATAXFER       | AVX512EVEX     | AVX512F_128N   | EVV 0x6E V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  mode64  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_GPR_READER() | REG0=XMM_R3():w:dq:u64 MEM0:r:q:u64
VMOVQ                | DATAXFER       | AVX512EVEX     | AVX512F_128N   | EVV 0x7E V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  mode64  NOEVSR  ZEROING=0 MASK=0 | REG0=GPR64_B():w:q:u64 REG1=XMM_R3():r:dq:u64
VMOVQ                | DATAXFER       | AVX512EVEX     | AVX512F_128N   | EVV 0x7E V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  mode64  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_GPR_WRITER_STORE() | MEM0:w:q:u64 REG0=XMM_R3():r:dq:u64
VMOVQ                | DATAXFER       | AVX512EVEX     | AVX512F_128N   | EVV 0x7E VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn] VL128 W1  NOEVSR  ZEROING=0 MASK=0 | REG0=XMM_R3():w:dq:u64 REG1=XMM_B3():r:dq:u64
VMOVQ                | DATAXFER       | AVX512EVEX     | AVX512F_128N   | EVV 0x7E VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128 W1  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:u64 MEM0:r:q:u64
VMOVQ                | DATAXFER       | AVX512EVEX     | AVX512F_128N   | EVV 0xD6 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128 W1  NOEVSR  ZEROING=0 MASK=0 | REG0=XMM_B3():w:dq:u64 REG1=XMM_R3():r:dq:u64
VMOVQ                | DATAXFER       | AVX512EVEX     | AVX512F_128N   | EVV 0xD6 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128 W1  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_SCALAR() | MEM0:w:q:u64 REG0=XMM_R3():r:dq:u64
VMOVSD               | DATAXFER       | AVX512EVEX     | AVX512F_SCALAR | EVV 0x10 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1  NOEVSR  ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:f64
VMOVSD               | DATAXFER       | AVX512EVEX     | AVX512F_SCALAR | EVV 0x11 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_SCALAR() | MEM0:w:q:f64 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:f64
VMOVSD               | DATAXFER       | AVX512EVEX     | AVX512F_SCALAR | EVV 0x10 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                     | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VMOVSD               | DATAXFER       | AVX512EVEX     | AVX512F_SCALAR | EVV 0x11 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                     | REG0=XMM_B3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_R3():r:dq:f64
VMOVSHDUP            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x16 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR      | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
VMOVSHDUP            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x16 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:f32
VMOVSHDUP            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x16 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR      | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
VMOVSHDUP            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x16 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:f32
VMOVSHDUP            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x16 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR      | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
VMOVSHDUP            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x16 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:f32
VMOVSLDUP            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x12 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR      | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
VMOVSLDUP            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x12 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:f32
VMOVSLDUP            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x12 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR      | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
VMOVSLDUP            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x12 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:f32
VMOVSLDUP            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x12 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR      | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
VMOVSLDUP            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x12 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:f32
VMOVSS               | DATAXFER       | AVX512EVEX     | AVX512F_SCALAR | EVV 0x10 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0  NOEVSR  ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:d:f32
VMOVSS               | DATAXFER       | AVX512EVEX     | AVX512F_SCALAR | EVV 0x11 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_SCALAR() | MEM0:w:d:f32 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:f32
VMOVSS               | DATAXFER       | AVX512EVEX     | AVX512F_SCALAR | EVV 0x10 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                     | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VMOVSS               | DATAXFER       | AVX512EVEX     | AVX512F_SCALAR | EVV 0x11 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                     | REG0=XMM_B3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_R3():r:dq:f32
VMOVUPD              | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x10 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR      | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64
VMOVUPD              | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x10 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:f64
VMOVUPD              | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x11 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR      | REG0=XMM_B3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:f64
VMOVUPD              | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x11 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_FULLMEM() | MEM0:w:dq:f64 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:f64
VMOVUPD              | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x10 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR      | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64
VMOVUPD              | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x10 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:f64
VMOVUPD              | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x11 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR      | REG0=YMM_B3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:f64
VMOVUPD              | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x11 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_FULLMEM() | MEM0:w:qq:f64 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:f64
VMOVUPD              | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x10 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR      | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
VMOVUPD              | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x10 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:f64
VMOVUPD              | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x11 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR      | REG0=ZMM_B3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zf64
VMOVUPD              | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x11 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_FULLMEM() | MEM0:w:zd:f64 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zf64
VMOVUPS              | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x10 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR      | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
VMOVUPS              | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x10 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:f32
VMOVUPS              | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x11 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR      | REG0=XMM_B3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:f32
VMOVUPS              | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x11 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_FULLMEM() | MEM0:w:dq:f32 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:f32
VMOVUPS              | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x10 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR      | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
VMOVUPS              | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x10 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:f32
VMOVUPS              | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x11 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR      | REG0=YMM_B3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:f32
VMOVUPS              | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x11 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_FULLMEM() | MEM0:w:qq:f32 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:f32
VMOVUPS              | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x10 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR      | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
VMOVUPS              | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x10 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:f32
VMOVUPS              | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x11 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR      | REG0=ZMM_B3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zf32
VMOVUPS              | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x11 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_FULLMEM() | MEM0:w:zd:f32 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zf32
VMULPD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x59 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1              | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VMULPD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x59 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VMULPD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x59 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1              | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
VMULPD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x59 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VMULPD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x59 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1              | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VMULPD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x59 V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1 | REG0=ZMM_R3():w:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VMULPD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x59 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
VMULPS               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x59 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0              | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VMULPS               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x59 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VMULPS               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x59 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0              | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
VMULPS               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x59 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VMULPS               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x59 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0              | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VMULPS               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x59 VNP V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0 | REG0=ZMM_R3():w:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VMULPS               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x59 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
VMULSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x59 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                     | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VMULSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x59 VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1 | REG0=XMM_R3():w:dq:f64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VMULSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x59 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
VMULSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x59 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                     | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VMULSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x59 VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W0 | REG0=XMM_R3():w:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VMULSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x59 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
VORPD                | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_128   | EVV 0x56 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1              | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
VORPD                | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_128   | EVV 0x56 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VORPD                | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_256   | EVV 0x56 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1              | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
VORPD                | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_256   | EVV 0x56 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VORPD                | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_512   | EVV 0x56 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1              | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
VORPD                | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_512   | EVV 0x56 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
VORPS                | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_128   | EVV 0x56 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0              | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
VORPS                | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_128   | EVV 0x56 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VORPS                | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_256   | EVV 0x56 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0              | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
VORPS                | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_256   | EVV 0x56 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VORPS                | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_512   | EVV 0x56 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0              | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
VORPS                | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_512   | EVV 0x56 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
VP2INTERSECTD        | AVX512_VP2INTERSECT | AVX512EVEX     | AVX512_VP2INTERSECT_128 | EVV 0x68 VF2 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0    ZEROING=0 MASK=0 | REG0=MASK_R():w:mskw:MULTIDEST2 REG1=XMM_N3():r:dq:u32 REG2=XMM_B3():r:dq:u32
VP2INTERSECTD        | AVX512_VP2INTERSECT | AVX512EVEX     | AVX512_VP2INTERSECT_128 | EVV 0x68 VF2 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw:MULTIDEST2 REG1=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VP2INTERSECTD        | AVX512_VP2INTERSECT | AVX512EVEX     | AVX512_VP2INTERSECT_256 | EVV 0x68 VF2 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0    ZEROING=0 MASK=0 | REG0=MASK_R():w:mskw:MULTIDEST2 REG1=YMM_N3():r:qq:u32 REG2=YMM_B3():r:qq:u32
VP2INTERSECTD        | AVX512_VP2INTERSECT | AVX512EVEX     | AVX512_VP2INTERSECT_256 | EVV 0x68 VF2 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw:MULTIDEST2 REG1=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VP2INTERSECTD        | AVX512_VP2INTERSECT | AVX512EVEX     | AVX512_VP2INTERSECT_512 | EVV 0x68 VF2 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0    ZEROING=0 MASK=0 | REG0=MASK_R():w:mskw:MULTIDEST2 REG1=ZMM_N3():r:zu32 REG2=ZMM_B3():r:zu32
VP2INTERSECTD        | AVX512_VP2INTERSECT | AVX512EVEX     | AVX512_VP2INTERSECT_512 | EVV 0x68 VF2 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw:MULTIDEST2 REG1=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
VP2INTERSECTQ        | AVX512_VP2INTERSECT | AVX512EVEX     | AVX512_VP2INTERSECT_128 | EVV 0x68 VF2 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1    ZEROING=0 MASK=0 | REG0=MASK_R():w:mskw:MULTIDEST2 REG1=XMM_N3():r:dq:u64 REG2=XMM_B3():r:dq:u64
VP2INTERSECTQ        | AVX512_VP2INTERSECT | AVX512EVEX     | AVX512_VP2INTERSECT_128 | EVV 0x68 VF2 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw:MULTIDEST2 REG1=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VP2INTERSECTQ        | AVX512_VP2INTERSECT | AVX512EVEX     | AVX512_VP2INTERSECT_256 | EVV 0x68 VF2 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1    ZEROING=0 MASK=0 | REG0=MASK_R():w:mskw:MULTIDEST2 REG1=YMM_N3():r:qq:u64 REG2=YMM_B3():r:qq:u64
VP2INTERSECTQ        | AVX512_VP2INTERSECT | AVX512EVEX     | AVX512_VP2INTERSECT_256 | EVV 0x68 VF2 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw:MULTIDEST2 REG1=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VP2INTERSECTQ        | AVX512_VP2INTERSECT | AVX512EVEX     | AVX512_VP2INTERSECT_512 | EVV 0x68 VF2 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1    ZEROING=0 MASK=0 | REG0=MASK_R():w:mskw:MULTIDEST2 REG1=ZMM_N3():r:zu64 REG2=ZMM_B3():r:zu64
VP2INTERSECTQ        | AVX512_VP2INTERSECT | AVX512EVEX     | AVX512_VP2INTERSECT_512 | EVV 0x68 VF2 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw:MULTIDEST2 REG1=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
VP4DPWSSD            | AVX512_4VNNIW  | AVX512EVEX     | AVX512_4VNNIW_512 | EVV 0x52 VF2 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() BCRC=0  VL512  W0    ESIZE_32_BITS() NELEM_TUPLE1_4X() | REG0=ZMM_R3():rw:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16:MULTISOURCE4 MEM0:r:dq:u32
VP4DPWSSDS           | AVX512_4VNNIW  | AVX512EVEX     | AVX512_4VNNIW_512 | EVV 0x53 VF2 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() BCRC=0  VL512  W0    ESIZE_32_BITS() NELEM_TUPLE1_4X() | REG0=ZMM_R3():rw:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16:MULTISOURCE4 MEM0:r:dq:u32
VPABSB               | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x1C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR      | REG0=XMM_R3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i8
VPABSB               | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x1C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128    NOEVSR  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:i8
VPABSB               | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x1C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256    NOEVSR      | REG0=YMM_R3():w:qq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:i8
VPABSB               | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x1C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256    NOEVSR  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:i8
VPABSB               | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x1C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512    NOEVSR      | REG0=ZMM_R3():w:zi8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zi8
VPABSB               | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x1C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512    NOEVSR  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zi8 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:i8
VPABSD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x1E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i32
VPABSD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x1E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:i32:TXT=BCASTSTR
VPABSD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x1E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:i32
VPABSD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x1E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:i32:TXT=BCASTSTR
VPABSD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x1E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zi32
VPABSD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x1E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:i32:TXT=BCASTSTR
VPABSQ               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x1F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR    | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i64
VPABSQ               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x1F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:i64:TXT=BCASTSTR
VPABSQ               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x1F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR    | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:i64
VPABSQ               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x1F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:i64:TXT=BCASTSTR
VPABSQ               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x1F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR    | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zi64
VPABSQ               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x1F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:i64:TXT=BCASTSTR
VPABSW               | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x1D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR      | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i16
VPABSW               | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x1D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128    NOEVSR  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:i16
VPABSW               | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x1D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256    NOEVSR      | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:i16
VPABSW               | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x1D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256    NOEVSR  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:i16
VPABSW               | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x1D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512    NOEVSR      | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zi16
VPABSW               | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x1D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512    NOEVSR  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:i16
VPACKSSDW            | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x6B V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0              | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i32 REG3=XMM_B3():r:dq:i32
VPACKSSDW            | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x6B V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i32 MEM0:r:vv:i32:TXT=BCASTSTR
VPACKSSDW            | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x6B V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0              | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i32 REG3=YMM_B3():r:qq:i32
VPACKSSDW            | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x6B V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i32 MEM0:r:vv:i32:TXT=BCASTSTR
VPACKSSDW            | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x6B V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0              | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi32 REG3=ZMM_B3():r:zi32
VPACKSSDW            | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x6B V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi32 MEM0:r:vv:i32:TXT=BCASTSTR
VPACKSSWB            | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x63 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                  | REG0=XMM_R3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 REG3=XMM_B3():r:dq:i16
VPACKSSWB            | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x63 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 MEM0:r:dq:i16
VPACKSSWB            | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x63 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                  | REG0=YMM_R3():w:qq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 REG3=YMM_B3():r:qq:i16
VPACKSSWB            | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x63 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 MEM0:r:qq:i16
VPACKSSWB            | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x63 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                  | REG0=ZMM_R3():w:zi8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 REG3=ZMM_B3():r:zi16
VPACKSSWB            | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x63 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zi8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 MEM0:r:zd:i16
VPACKSTOREHD         | DATAXFER       | KNCE           | KNCE           | KVV 0xD4 V66 V0F38  W0 NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() DNCONVERT_INT32() | MEM0:w:zv:TXT=CONVERT:TXT=NT  REG0=MASK1():r:mskw   REG1=ZMM_R3():r:zd NO_SCALE_DISP8=1
VPACKSTOREHPD        | DATAXFER       | KNCE           | KNCE           | KVV 0xD5 V66 V0F38  W1 NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() DNCONVERT_FLT64() | MEM0:w:zv:TXT=CONVERT:TXT=NT  REG0=MASK1():r:mskw   REG1=ZMM_R3():r:zf64 NO_SCALE_DISP8=1
VPACKSTOREHPS        | DATAXFER       | KNCE           | KNCE           | KVV 0xD5 V66 V0F38  W0 NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() DNCONVERT_FLT32() | MEM0:w:zv:TXT=CONVERT:TXT=NT  REG0=MASK1():r:mskw   REG1=ZMM_R3():r:zf32 NO_SCALE_DISP8=1
VPACKSTOREHQ         | DATAXFER       | KNCE           | KNCE           | KVV 0xD4 V66 V0F38  W1 NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() DNCONVERT_INT64() | MEM0:w:zv:TXT=CONVERT:TXT=NT  REG0=MASK1():r:mskw   REG1=ZMM_R3():r:zq NO_SCALE_DISP8=1
VPACKSTORELD         | DATAXFER       | KNCE           | KNCE           | KVV 0xD0 V66 V0F38  W0 NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() DNCONVERT_INT32() | MEM0:w:zv:TXT=CONVERT:TXT=NT  REG0=MASK1():r:mskw   REG1=ZMM_R3():r:zd NO_SCALE_DISP8=1
VPACKSTORELPD        | DATAXFER       | KNCE           | KNCE           | KVV 0xD1 V66 V0F38  W1 NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() DNCONVERT_FLT64() | MEM0:w:zv:TXT=CONVERT:TXT=NT  REG0=MASK1():r:mskw   REG1=ZMM_R3():r:zf64 NO_SCALE_DISP8=1
VPACKSTORELPS        | DATAXFER       | KNCE           | KNCE           | KVV 0xD1 V66 V0F38  W0 NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() DNCONVERT_FLT32() | MEM0:w:zv:TXT=CONVERT:TXT=NT  REG0=MASK1():r:mskw   REG1=ZMM_R3():r:zf32 NO_SCALE_DISP8=1
VPACKSTORELQ         | DATAXFER       | KNCE           | KNCE           | KVV 0xD0 V66 V0F38  W1 NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() DNCONVERT_INT64() | MEM0:w:zv:TXT=CONVERT:TXT=NT  REG0=MASK1():r:mskw   REG1=ZMM_R3():r:zq NO_SCALE_DISP8=1
VPACKUSDW            | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x2B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
VPACKUSDW            | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x2B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPACKUSDW            | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x2B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
VPACKUSDW            | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x2B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPACKUSDW            | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x2B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
VPACKUSDW            | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x2B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
VPACKUSWB            | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x67 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                  | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
VPACKUSWB            | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x67 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
VPACKUSWB            | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x67 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                  | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
VPACKUSWB            | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x67 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
VPACKUSWB            | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x67 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                  | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
VPACKUSWB            | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x67 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
VPADDB               | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xFC V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                  | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
VPADDB               | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xFC V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
VPADDB               | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xFC V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                  | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
VPADDB               | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xFC V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
VPADDB               | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xFC V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                  | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
VPADDB               | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xFC V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
VPADDD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0xFE V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0              | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
VPADDD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0xFE V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPADDD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0xFE V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0              | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
VPADDD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0xFE V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPADDD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0xFE V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0              | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
VPADDD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0xFE V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
VPADDQ               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0xD4 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1              | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
VPADDQ               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0xD4 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPADDQ               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0xD4 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1              | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
VPADDQ               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0xD4 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPADDQ               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0xD4 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1              | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
VPADDQ               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0xD4 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
VPADDSB              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xEC V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                  | REG0=XMM_R3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i8 REG3=XMM_B3():r:dq:i8
VPADDSB              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xEC V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i8 MEM0:r:dq:i8
VPADDSB              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xEC V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                  | REG0=YMM_R3():w:qq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i8 REG3=YMM_B3():r:qq:i8
VPADDSB              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xEC V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i8 MEM0:r:qq:i8
VPADDSB              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xEC V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                  | REG0=ZMM_R3():w:zi8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi8 REG3=ZMM_B3():r:zi8
VPADDSB              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xEC V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zi8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi8 MEM0:r:zd:i8
VPADDSW              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xED V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                  | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 REG3=XMM_B3():r:dq:i16
VPADDSW              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xED V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 MEM0:r:dq:i16
VPADDSW              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xED V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                  | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 REG3=YMM_B3():r:qq:i16
VPADDSW              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xED V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 MEM0:r:qq:i16
VPADDSW              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xED V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                  | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 REG3=ZMM_B3():r:zi16
VPADDSW              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xED V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 MEM0:r:zd:i16
VPADDUSB             | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xDC V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                  | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
VPADDUSB             | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xDC V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
VPADDUSB             | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xDC V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                  | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
VPADDUSB             | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xDC V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
VPADDUSB             | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xDC V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                  | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
VPADDUSB             | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xDC V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
VPADDUSW             | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xDD V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                  | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
VPADDUSW             | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xDD V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
VPADDUSW             | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xDD V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                  | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
VPADDUSW             | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xDD V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
VPADDUSW             | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xDD V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                  | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
VPADDUSW             | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xDD V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
VPADDW               | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xFD V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                  | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
VPADDW               | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xFD V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
VPADDW               | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xFD V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                  | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
VPADDW               | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xFD V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
VPADDW               | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xFD V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                  | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
VPADDW               | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xFD V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
VPALIGNR             | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x0F V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128     UIMM8()    | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8 IMM0:r:b
VPALIGNR             | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x0F V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128     UIMM8()  ESIZE_64_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8 IMM0:r:b
VPALIGNR             | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x0F V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256     UIMM8()    | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8 IMM0:r:b
VPALIGNR             | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x0F V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256     UIMM8()  ESIZE_64_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8 IMM0:r:b
VPALIGNR             | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x0F V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512     UIMM8()    | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8 IMM0:r:b
VPALIGNR             | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x0F V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512     UIMM8()  ESIZE_64_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8 IMM0:r:b
VPANDD               | LOGICAL        | AVX512EVEX     | AVX512F_128    | EVV 0xDB V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0              | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
VPANDD               | LOGICAL        | AVX512EVEX     | AVX512F_128    | EVV 0xDB V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPANDD               | LOGICAL        | AVX512EVEX     | AVX512F_256    | EVV 0xDB V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0              | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
VPANDD               | LOGICAL        | AVX512EVEX     | AVX512F_256    | EVV 0xDB V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPANDD               | LOGICAL        | AVX512EVEX     | AVX512F_512    | EVV 0xDB V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0              | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
VPANDD               | LOGICAL        | AVX512EVEX     | AVX512F_512    | EVV 0xDB V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
VPANDND              | LOGICAL        | AVX512EVEX     | AVX512F_128    | EVV 0xDF V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0              | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
VPANDND              | LOGICAL        | AVX512EVEX     | AVX512F_128    | EVV 0xDF V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPANDND              | LOGICAL        | AVX512EVEX     | AVX512F_256    | EVV 0xDF V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0              | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
VPANDND              | LOGICAL        | AVX512EVEX     | AVX512F_256    | EVV 0xDF V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPANDND              | LOGICAL        | AVX512EVEX     | AVX512F_512    | EVV 0xDF V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0              | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
VPANDND              | LOGICAL        | AVX512EVEX     | AVX512F_512    | EVV 0xDF V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
VPANDNQ              | LOGICAL        | AVX512EVEX     | AVX512F_128    | EVV 0xDF V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1              | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
VPANDNQ              | LOGICAL        | AVX512EVEX     | AVX512F_128    | EVV 0xDF V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPANDNQ              | LOGICAL        | AVX512EVEX     | AVX512F_256    | EVV 0xDF V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1              | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
VPANDNQ              | LOGICAL        | AVX512EVEX     | AVX512F_256    | EVV 0xDF V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPANDNQ              | LOGICAL        | AVX512EVEX     | AVX512F_512    | EVV 0xDF V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1              | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
VPANDNQ              | LOGICAL        | AVX512EVEX     | AVX512F_512    | EVV 0xDF V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
VPANDQ               | LOGICAL        | AVX512EVEX     | AVX512F_128    | EVV 0xDB V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1              | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
VPANDQ               | LOGICAL        | AVX512EVEX     | AVX512F_128    | EVV 0xDB V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPANDQ               | LOGICAL        | AVX512EVEX     | AVX512F_256    | EVV 0xDB V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1              | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
VPANDQ               | LOGICAL        | AVX512EVEX     | AVX512F_256    | EVV 0xDB V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPANDQ               | LOGICAL        | AVX512EVEX     | AVX512F_512    | EVV 0xDB V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1              | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
VPANDQ               | LOGICAL        | AVX512EVEX     | AVX512F_512    | EVV 0xDB V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
VPAVGB               | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xE0 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                  | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
VPAVGB               | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xE0 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
VPAVGB               | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xE0 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                  | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
VPAVGB               | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xE0 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
VPAVGB               | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xE0 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                  | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
VPAVGB               | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xE0 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
VPAVGW               | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xE3 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                  | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
VPAVGW               | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xE3 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
VPAVGW               | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xE3 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                  | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
VPAVGW               | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xE3 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
VPAVGW               | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xE3 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                  | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
VPAVGW               | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xE3 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
VPBLENDMB            | BLEND          | AVX512EVEX     | AVX512BW_128   | EVV 0x66 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
VPBLENDMB            | BLEND          | AVX512EVEX     | AVX512BW_128   | EVV 0x66 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0    ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
VPBLENDMB            | BLEND          | AVX512EVEX     | AVX512BW_256   | EVV 0x66 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
VPBLENDMB            | BLEND          | AVX512EVEX     | AVX512BW_256   | EVV 0x66 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0    ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
VPBLENDMB            | BLEND          | AVX512EVEX     | AVX512BW_512   | EVV 0x66 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
VPBLENDMB            | BLEND          | AVX512EVEX     | AVX512BW_512   | EVV 0x66 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0    ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
VPBLENDMD            | BLEND          | AVX512EVEX     | AVX512F_128    | EVV 0x64 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
VPBLENDMD            | BLEND          | AVX512EVEX     | AVX512F_128    | EVV 0x64 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPBLENDMD            | BLEND          | AVX512EVEX     | AVX512F_256    | EVV 0x64 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
VPBLENDMD            | BLEND          | AVX512EVEX     | AVX512F_256    | EVV 0x64 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPBLENDMD            | BLEND          | AVX512EVEX     | AVX512F_512    | EVV 0x64 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
VPBLENDMD            | BLEND          | AVX512EVEX     | AVX512F_512    | EVV 0x64 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
VPBLENDMQ            | BLEND          | AVX512EVEX     | AVX512F_128    | EVV 0x64 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
VPBLENDMQ            | BLEND          | AVX512EVEX     | AVX512F_128    | EVV 0x64 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPBLENDMQ            | BLEND          | AVX512EVEX     | AVX512F_256    | EVV 0x64 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
VPBLENDMQ            | BLEND          | AVX512EVEX     | AVX512F_256    | EVV 0x64 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPBLENDMQ            | BLEND          | AVX512EVEX     | AVX512F_512    | EVV 0x64 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
VPBLENDMQ            | BLEND          | AVX512EVEX     | AVX512F_512    | EVV 0x64 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
VPBLENDMW            | BLEND          | AVX512EVEX     | AVX512BW_128   | EVV 0x66 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
VPBLENDMW            | BLEND          | AVX512EVEX     | AVX512BW_128   | EVV 0x66 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
VPBLENDMW            | BLEND          | AVX512EVEX     | AVX512BW_256   | EVV 0x66 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
VPBLENDMW            | BLEND          | AVX512EVEX     | AVX512BW_256   | EVV 0x66 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
VPBLENDMW            | BLEND          | AVX512EVEX     | AVX512BW_512   | EVV 0x66 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
VPBLENDMW            | BLEND          | AVX512EVEX     | AVX512BW_512   | EVV 0x66 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
VPBROADCASTB         | BROADCAST      | AVX512EVEX     | AVX512BW_128   | EVV 0x78 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u8 EMX_BROADCAST_1TO16_8
VPBROADCASTB         | BROADCAST      | AVX512EVEX     | AVX512BW_128   | EVV 0x78 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_8_BITS() NELEM_TUPLE1_BYTE() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:b:u8 EMX_BROADCAST_1TO16_8
VPBROADCASTB         | BROADCAST      | AVX512EVEX     | AVX512BW_128   | EVV 0x7A V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=GPR32_B():r:d:u8 EMX_BROADCAST_1TO16_8
VPBROADCASTB         | BROADCAST      | AVX512EVEX     | AVX512BW_256   | EVV 0x78 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u8 EMX_BROADCAST_1TO32_8
VPBROADCASTB         | BROADCAST      | AVX512EVEX     | AVX512BW_256   | EVV 0x78 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_8_BITS() NELEM_TUPLE1_BYTE() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:b:u8 EMX_BROADCAST_1TO32_8
VPBROADCASTB         | BROADCAST      | AVX512EVEX     | AVX512BW_256   | EVV 0x7A V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=GPR32_B():r:d:u8 EMX_BROADCAST_1TO32_8
VPBROADCASTB         | BROADCAST      | AVX512EVEX     | AVX512BW_512   | EVV 0x78 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u8 EMX_BROADCAST_1TO64_8
VPBROADCASTB         | BROADCAST      | AVX512EVEX     | AVX512BW_512   | EVV 0x78 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_8_BITS() NELEM_TUPLE1_BYTE() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:b:u8 EMX_BROADCAST_1TO64_8
VPBROADCASTB         | BROADCAST      | AVX512EVEX     | AVX512BW_512   | EVV 0x7A V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=GPR32_B():r:d:u8 EMX_BROADCAST_1TO64_8
VPBROADCASTD         | BROADCAST      | AVX512EVEX     | AVX512F_128    | EVV 0x58 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_TUPLE1() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:d:u32 EMX_BROADCAST_1TO4_32
VPBROADCASTD         | BROADCAST      | AVX512EVEX     | AVX512F_128    | EVV 0x58 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32 EMX_BROADCAST_1TO4_32
VPBROADCASTD         | BROADCAST      | AVX512EVEX     | AVX512F_128    | EVV 0x7C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  not64  NOEVSR | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=GPR32_B():r:d:u32 EMX_BROADCAST_1TO4_32
VPBROADCASTD         | BROADCAST      | AVX512EVEX     | AVX512F_128    | EVV 0x7C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  mode64 W0  NOEVSR | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=GPR32_B():r:d:u32 EMX_BROADCAST_1TO4_32
VPBROADCASTD         | BROADCAST      | AVX512EVEX     | AVX512F_256    | EVV 0x58 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_TUPLE1() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:d:u32 EMX_BROADCAST_1TO8_32
VPBROADCASTD         | BROADCAST      | AVX512EVEX     | AVX512F_256    | EVV 0x58 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32 EMX_BROADCAST_1TO8_32
VPBROADCASTD         | BROADCAST      | AVX512EVEX     | AVX512F_256    | EVV 0x7C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  not64  NOEVSR | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=GPR32_B():r:d:u32 EMX_BROADCAST_1TO8_32
VPBROADCASTD         | BROADCAST      | AVX512EVEX     | AVX512F_256    | EVV 0x7C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  mode64 W0  NOEVSR | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=GPR32_B():r:d:u32 EMX_BROADCAST_1TO8_32
VPBROADCASTD         | BROADCAST      | AVX512EVEX     | AVX512F_512    | EVV 0x58 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_TUPLE1() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:d:u32 EMX_BROADCAST_1TO16_32
VPBROADCASTD         | BROADCAST      | AVX512EVEX     | AVX512F_512    | EVV 0x58 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32 EMX_BROADCAST_1TO16_32
VPBROADCASTD         | BROADCAST      | AVX512EVEX     | AVX512F_512    | EVV 0x7C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  not64  NOEVSR | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=GPR32_B():r:d:u32 EMX_BROADCAST_1TO16_32
VPBROADCASTD         | BROADCAST      | AVX512EVEX     | AVX512F_512    | EVV 0x7C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  mode64 W0  NOEVSR | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=GPR32_B():r:d:u32 EMX_BROADCAST_1TO16_32
VPBROADCASTMB2Q      | BROADCAST      | AVX512EVEX     | AVX512CD_128   | EVV 0x2A VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR  ZEROING=0 MASK=0 | REG0=XMM_R3():w:dq:u64 REG1=MASK_B():r:mskw:u64 EMX_BROADCAST_1TO2_8
VPBROADCASTMB2Q      | BROADCAST      | AVX512EVEX     | AVX512CD_256   | EVV 0x2A VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR  ZEROING=0 MASK=0 | REG0=YMM_R3():w:qq:u64 REG1=MASK_B():r:mskw:u64 EMX_BROADCAST_1TO4_8
VPBROADCASTMB2Q      | BROADCAST      | AVX512EVEX     | AVX512CD_512   | EVV 0x2A VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR  ZEROING=0 MASK=0 | REG0=ZMM_R3():w:zu64 REG1=MASK_B():r:mskw:u64 EMX_BROADCAST_1TO8_8
VPBROADCASTMW2D      | BROADCAST      | AVX512EVEX     | AVX512CD_128   | EVV 0x3A VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR  ZEROING=0 MASK=0 | REG0=XMM_R3():w:dq:u32 REG1=MASK_B():r:mskw:u32 EMX_BROADCAST_1TO4_16
VPBROADCASTMW2D      | BROADCAST      | AVX512EVEX     | AVX512CD_256   | EVV 0x3A VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR  ZEROING=0 MASK=0 | REG0=YMM_R3():w:qq:u32 REG1=MASK_B():r:mskw:u32 EMX_BROADCAST_1TO8_16
VPBROADCASTMW2D      | BROADCAST      | AVX512EVEX     | AVX512CD_512   | EVV 0x3A VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR  ZEROING=0 MASK=0 | REG0=ZMM_R3():w:zu32 REG1=MASK_B():r:mskw:u32 EMX_BROADCAST_1TO16_16
VPBROADCASTQ         | BROADCAST      | AVX512EVEX     | AVX512F_128    | EVV 0x59 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_TUPLE1() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:u64 EMX_BROADCAST_1TO2_64
VPBROADCASTQ         | BROADCAST      | AVX512EVEX     | AVX512F_128    | EVV 0x59 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR    | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u64 EMX_BROADCAST_1TO2_64
VPBROADCASTQ         | BROADCAST      | AVX512EVEX     | AVX512F_128    | EVV 0x7C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  mode64  NOEVSR | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=GPR64_B():r:q:u64 EMX_BROADCAST_1TO2_64
VPBROADCASTQ         | BROADCAST      | AVX512EVEX     | AVX512F_256    | EVV 0x59 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_TUPLE1() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:u64 EMX_BROADCAST_1TO4_64
VPBROADCASTQ         | BROADCAST      | AVX512EVEX     | AVX512F_256    | EVV 0x59 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR    | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u64 EMX_BROADCAST_1TO4_64
VPBROADCASTQ         | BROADCAST      | AVX512EVEX     | AVX512F_256    | EVV 0x7C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  mode64  NOEVSR | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=GPR64_B():r:q:u64 EMX_BROADCAST_1TO4_64
VPBROADCASTQ         | BROADCAST      | AVX512EVEX     | AVX512F_512    | EVV 0x59 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_TUPLE1() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:u64 EMX_BROADCAST_1TO8_64
VPBROADCASTQ         | BROADCAST      | AVX512EVEX     | AVX512F_512    | EVV 0x59 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR    | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u64 EMX_BROADCAST_1TO8_64
VPBROADCASTQ         | BROADCAST      | AVX512EVEX     | AVX512F_512    | EVV 0x7C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  mode64  W1  NOEVSR | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=GPR64_B():r:q:u64 EMX_BROADCAST_1TO8_64
VPBROADCASTW         | BROADCAST      | AVX512EVEX     | AVX512BW_128   | EVV 0x79 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u16 EMX_BROADCAST_1TO8_16
VPBROADCASTW         | BROADCAST      | AVX512EVEX     | AVX512BW_128   | EVV 0x79 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_16_BITS() NELEM_TUPLE1_WORD() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:wrd:u16 EMX_BROADCAST_1TO8_16
VPBROADCASTW         | BROADCAST      | AVX512EVEX     | AVX512BW_128   | EVV 0x7B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=GPR32_B():r:d:u16 EMX_BROADCAST_1TO8_16
VPBROADCASTW         | BROADCAST      | AVX512EVEX     | AVX512BW_256   | EVV 0x79 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u16 EMX_BROADCAST_1TO16_16
VPBROADCASTW         | BROADCAST      | AVX512EVEX     | AVX512BW_256   | EVV 0x79 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_16_BITS() NELEM_TUPLE1_WORD() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:wrd:u16 EMX_BROADCAST_1TO16_16
VPBROADCASTW         | BROADCAST      | AVX512EVEX     | AVX512BW_256   | EVV 0x7B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=GPR32_B():r:d:u16 EMX_BROADCAST_1TO16_16
VPBROADCASTW         | BROADCAST      | AVX512EVEX     | AVX512BW_512   | EVV 0x79 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u16 EMX_BROADCAST_1TO32_16
VPBROADCASTW         | BROADCAST      | AVX512EVEX     | AVX512BW_512   | EVV 0x79 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_16_BITS() NELEM_TUPLE1_WORD() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:wrd:u16 EMX_BROADCAST_1TO32_16
VPBROADCASTW         | BROADCAST      | AVX512EVEX     | AVX512BW_512   | EVV 0x7B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=GPR32_B():r:d:u16 EMX_BROADCAST_1TO32_16
VPCLMULQDQ           | VPCLMULQDQ     | AVX512EVEX     | AVX512_VPCLMULQDQ_128 | EVV 0x44 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128      ZEROING=0 MASK=0 UIMM8() | REG0=XMM_R3():w:dq:u128 REG1=XMM_N3():r:dq:u64 REG2=XMM_B3():r:dq:u64 IMM0:r:b
VPCLMULQDQ           | VPCLMULQDQ     | AVX512EVEX     | AVX512_VPCLMULQDQ_128 | EVV 0x44 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ZEROING=0 MASK=0 UIMM8()  ESIZE_64_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u128 REG1=XMM_N3():r:dq:u64 MEM0:r:dq:u64 IMM0:r:b
VPCLMULQDQ           | VPCLMULQDQ     | AVX512EVEX     | AVX512_VPCLMULQDQ_256 | EVV 0x44 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256      ZEROING=0 MASK=0 UIMM8() | REG0=YMM_R3():w:qq:u128 REG1=YMM_N3():r:qq:u64 REG2=YMM_B3():r:qq:u64 IMM0:r:b
VPCLMULQDQ           | VPCLMULQDQ     | AVX512EVEX     | AVX512_VPCLMULQDQ_256 | EVV 0x44 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ZEROING=0 MASK=0 UIMM8()  ESIZE_64_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u128 REG1=YMM_N3():r:qq:u64 MEM0:r:qq:u64 IMM0:r:b
VPCLMULQDQ           | VPCLMULQDQ     | AVX512EVEX     | AVX512_VPCLMULQDQ_512 | EVV 0x44 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512      ZEROING=0 MASK=0 UIMM8() | REG0=ZMM_R3():w:zu128 REG1=ZMM_N3():r:zu64 REG2=ZMM_B3():r:zu64 IMM0:r:b
VPCLMULQDQ           | VPCLMULQDQ     | AVX512EVEX     | AVX512_VPCLMULQDQ_512 | EVV 0x44 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ZEROING=0 MASK=0 UIMM8()  ESIZE_64_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu128 REG1=ZMM_N3():r:zu64 MEM0:r:zd:u64 IMM0:r:b
VPCLMULQDQ           | VPCLMULQDQ     | VPCLMULQDQ     | VPCLMULQDQ     | VV1 0x44 V66 V0F3A MOD[0b11] MOD=3  REG[rrr] RM[nnn]  VL256     UIMM8()          | REG0=YMM_R():w:qq:u128 REG1=YMM_N():r:qq:u64 REG2=YMM_B():r:qq:u64 IMM0:r:b
VPCLMULQDQ           | VPCLMULQDQ     | VPCLMULQDQ     | VPCLMULQDQ     | VV1 0x44 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256     UIMM8()   | REG0=YMM_R():w:qq:u128 REG1=YMM_N():r:qq:u64 MEM0:r:qq:u64 IMM0:r:b
VPCMOV               | XOP            | XOP            | XOP            | XOPV 0xA2 VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()  | REG0=XMM_R():w:dq:i1 REG1=XMM_N():r:dq:i1 MEM0:r:dq:i1 REG2=XMM_SE():r:dq:i1
VPCMOV               | XOP            | XOP            | XOP            | XOPV 0xA2 VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()         | REG0=XMM_R():w:dq:i1 REG1=XMM_N():r:dq:i1 REG2=XMM_B():r:dq:i1 REG3=XMM_SE():r:dq:i1
VPCMOV               | XOP            | XOP            | XOP            | XOPV 0xA2 VNP W1 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()  | REG0=XMM_R():w:dq:i1 REG1=XMM_N():r:dq:i1 REG2=XMM_SE():r:dq:i1 MEM0:r:dq:i1
VPCMOV               | XOP            | XOP            | XOP            | XOPV 0xA2 VNP W1 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()         | REG0=XMM_R():w:dq:i1 REG1=XMM_N():r:dq:i1 REG2=XMM_SE():r:dq:i1 REG3=XMM_B():r:dq:i1
VPCMOV               | XOP            | XOP            | XOP            | XOPV 0xA2 VNP W0 VL256  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()  | REG0=YMM_R():w:qq:i1 REG1=YMM_N():r:qq:i1 MEM0:r:qq:i1 REG2=YMM_SE():r:qq:i1
VPCMOV               | XOP            | XOP            | XOP            | XOPV 0xA2 VNP W0 VL256  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()         | REG0=YMM_R():w:qq:i1 REG1=YMM_N():r:qq:i1 REG2=YMM_B():r:qq:i1 REG3=YMM_SE():r:qq:i1
VPCMOV               | XOP            | XOP            | XOP            | XOPV 0xA2 VNP W1 VL256  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()  | REG0=YMM_R():w:qq:i1 REG1=YMM_N():r:qq:i1 REG2=YMM_SE():r:qq:i1 MEM0:r:qq:i1
VPCMOV               | XOP            | XOP            | XOP            | XOPV 0xA2 VNP W1 VL256  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()         | REG0=YMM_R():w:qq:i1 REG1=YMM_N():r:qq:i1 REG2=YMM_SE():r:qq:i1 REG3=YMM_B():r:qq:i1
VPCMPB               | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x3F V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:i8 REG3=XMM_B3():r:dq:i8 IMM0:r:b
VPCMPB               | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x3F V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0    ZEROING=0 UIMM8()  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:i8 MEM0:r:dq:i8 IMM0:r:b
VPCMPB               | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x3F V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:i8 REG3=YMM_B3():r:qq:i8 IMM0:r:b
VPCMPB               | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x3F V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0    ZEROING=0 UIMM8()  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:i8 MEM0:r:qq:i8 IMM0:r:b
VPCMPB               | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x3F V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zi8 REG3=ZMM_B3():r:zi8 IMM0:r:b
VPCMPB               | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x3F V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0    ZEROING=0 UIMM8()  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zi8 MEM0:r:zd:i8 IMM0:r:b
VPCMPD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x1F V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:i32 REG3=XMM_B3():r:dq:i32 IMM0:r:b
VPCMPD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x1F V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:i32 MEM0:r:vv:i32:TXT=BCASTSTR IMM0:r:b
VPCMPD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x1F V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:i32 REG3=YMM_B3():r:qq:i32 IMM0:r:b
VPCMPD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x1F V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:i32 MEM0:r:vv:i32:TXT=BCASTSTR IMM0:r:b
VPCMPD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x1F V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zi32 REG3=ZMM_B3():r:zi32 IMM0:r:b
VPCMPD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x1F V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zi32 MEM0:r:vv:i32:TXT=BCASTSTR IMM0:r:b
VPCMPEQB             | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x74 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128      ZEROING=0   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
VPCMPEQB             | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x74 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ZEROING=0  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
VPCMPEQB             | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x74 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256      ZEROING=0   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
VPCMPEQB             | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x74 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ZEROING=0  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
VPCMPEQB             | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x74 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512      ZEROING=0   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
VPCMPEQB             | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x74 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ZEROING=0  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
VPCMPEQD             | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x76 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
VPCMPEQD             | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x76 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ZEROING=0  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPCMPEQD             | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x76 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
VPCMPEQD             | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x76 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ZEROING=0  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPCMPEQD             | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x76 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
VPCMPEQD             | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x76 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ZEROING=0  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
VPCMPEQQ             | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x29 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
VPCMPEQQ             | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x29 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ZEROING=0  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPCMPEQQ             | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x29 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
VPCMPEQQ             | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x29 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ZEROING=0  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPCMPEQQ             | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x29 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
VPCMPEQQ             | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x29 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ZEROING=0  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
VPCMPEQW             | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x75 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128      ZEROING=0   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
VPCMPEQW             | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x75 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ZEROING=0  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
VPCMPEQW             | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x75 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256      ZEROING=0   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
VPCMPEQW             | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x75 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ZEROING=0  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
VPCMPEQW             | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x75 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512      ZEROING=0   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
VPCMPEQW             | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x75 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ZEROING=0  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
VPCMPGTB             | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x64 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128      ZEROING=0   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
VPCMPGTB             | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x64 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ZEROING=0  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
VPCMPGTB             | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x64 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256      ZEROING=0   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
VPCMPGTB             | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x64 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ZEROING=0  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
VPCMPGTB             | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x64 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512      ZEROING=0   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
VPCMPGTB             | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x64 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ZEROING=0  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
VPCMPGTD             | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x66 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:i32 REG3=XMM_B3():r:dq:i32
VPCMPGTD             | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x66 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ZEROING=0  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:i32 MEM0:r:vv:i32:TXT=BCASTSTR
VPCMPGTD             | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x66 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:i32 REG3=YMM_B3():r:qq:i32
VPCMPGTD             | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x66 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ZEROING=0  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:i32 MEM0:r:vv:i32:TXT=BCASTSTR
VPCMPGTD             | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x66 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zi32 REG3=ZMM_B3():r:zi32
VPCMPGTD             | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x66 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ZEROING=0  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zi32 MEM0:r:vv:i32:TXT=BCASTSTR
VPCMPGTQ             | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x37 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:i64 REG3=XMM_B3():r:dq:i64
VPCMPGTQ             | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x37 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ZEROING=0  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:i64 MEM0:r:vv:i64:TXT=BCASTSTR
VPCMPGTQ             | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x37 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:i64 REG3=YMM_B3():r:qq:i64
VPCMPGTQ             | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x37 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ZEROING=0  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:i64 MEM0:r:vv:i64:TXT=BCASTSTR
VPCMPGTQ             | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x37 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zi64 REG3=ZMM_B3():r:zi64
VPCMPGTQ             | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x37 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ZEROING=0  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zi64 MEM0:r:vv:i64:TXT=BCASTSTR
VPCMPGTW             | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x65 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128      ZEROING=0   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
VPCMPGTW             | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x65 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ZEROING=0  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
VPCMPGTW             | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x65 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256      ZEROING=0   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
VPCMPGTW             | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x65 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ZEROING=0  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
VPCMPGTW             | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x65 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512      ZEROING=0   | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
VPCMPGTW             | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x65 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ZEROING=0  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
VPCMPQ               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x1F V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:i64 REG3=XMM_B3():r:dq:i64 IMM0:r:b
VPCMPQ               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x1F V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:i64 MEM0:r:vv:i64:TXT=BCASTSTR IMM0:r:b
VPCMPQ               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x1F V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:i64 REG3=YMM_B3():r:qq:i64 IMM0:r:b
VPCMPQ               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x1F V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:i64 MEM0:r:vv:i64:TXT=BCASTSTR IMM0:r:b
VPCMPQ               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x1F V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zi64 REG3=ZMM_B3():r:zi64 IMM0:r:b
VPCMPQ               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x1F V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zi64 MEM0:r:vv:i64:TXT=BCASTSTR IMM0:r:b
VPCMPUB              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x3E V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8 IMM0:r:b
VPCMPUB              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x3E V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0    ZEROING=0 UIMM8()  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8 IMM0:r:b
VPCMPUB              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x3E V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8 IMM0:r:b
VPCMPUB              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x3E V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0    ZEROING=0 UIMM8()  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8 IMM0:r:b
VPCMPUB              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x3E V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8 IMM0:r:b
VPCMPUB              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x3E V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0    ZEROING=0 UIMM8()  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8 IMM0:r:b
VPCMPUD              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x1E V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32 IMM0:r:b
VPCMPUD              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x1E V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
VPCMPUD              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x1E V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32 IMM0:r:b
VPCMPUD              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x1E V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
VPCMPUD              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x1E V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32 IMM0:r:b
VPCMPUD              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x1E V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ZEROING=0 UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
VPCMPUQ              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x1E V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64 IMM0:r:b
VPCMPUQ              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x1E V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VPCMPUQ              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x1E V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64 IMM0:r:b
VPCMPUQ              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x1E V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VPCMPUQ              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x1E V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64 IMM0:r:b
VPCMPUQ              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x1E V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ZEROING=0 UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VPCMPUW              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x3E V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16 IMM0:r:b
VPCMPUW              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x3E V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1    ZEROING=0 UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16 IMM0:r:b
VPCMPUW              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x3E V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16 IMM0:r:b
VPCMPUW              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x3E V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1    ZEROING=0 UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16 IMM0:r:b
VPCMPUW              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x3E V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16 IMM0:r:b
VPCMPUW              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x3E V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1    ZEROING=0 UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16 IMM0:r:b
VPCMPW               | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x3F V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:i16 REG3=XMM_B3():r:dq:i16 IMM0:r:b
VPCMPW               | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x3F V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1    ZEROING=0 UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:i16 MEM0:r:dq:i16 IMM0:r:b
VPCMPW               | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x3F V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:i16 REG3=YMM_B3():r:qq:i16 IMM0:r:b
VPCMPW               | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x3F V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1    ZEROING=0 UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:i16 MEM0:r:qq:i16 IMM0:r:b
VPCMPW               | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x3F V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1    ZEROING=0 UIMM8() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zi16 REG3=ZMM_B3():r:zi16 IMM0:r:b
VPCMPW               | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x3F V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1    ZEROING=0 UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zi16 MEM0:r:zd:i16 IMM0:r:b
VPCOMB               | XOP            | XOP            | XOP            | XOPV 0xCC VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()    | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 MEM0:r:dq:i8 IMM0:r:b:u8
VPCOMB               | XOP            | XOP            | XOP            | XOPV 0xCC VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()           | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 REG2=XMM_B():r:dq:i8 IMM0:r:b:u8
VPCOMD               | XOP            | XOP            | XOP            | XOPV 0xCE VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()    | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32 IMM0:r:b:u8
VPCOMD               | XOP            | XOP            | XOP            | XOPV 0xCE VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()           | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32 IMM0:r:b:u8
VPCOMPRESSB          | COMPRESS       | AVX512EVEX     | AVX512_VBMI2_128 | EVV 0x63 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_GSCAT() | MEM0:w:dq:u8 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u8
VPCOMPRESSB          | COMPRESS       | AVX512EVEX     | AVX512_VBMI2_128 | EVV 0x63 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_B3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u8
VPCOMPRESSB          | COMPRESS       | AVX512EVEX     | AVX512_VBMI2_256 | EVV 0x63 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_GSCAT() | MEM0:w:qq:u8 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u8
VPCOMPRESSB          | COMPRESS       | AVX512EVEX     | AVX512_VBMI2_256 | EVV 0x63 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=YMM_B3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u8
VPCOMPRESSB          | COMPRESS       | AVX512EVEX     | AVX512_VBMI2_512 | EVV 0x63 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_GSCAT() | MEM0:w:zd:u8 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu8
VPCOMPRESSB          | COMPRESS       | AVX512EVEX     | AVX512_VBMI2_512 | EVV 0x63 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=ZMM_B3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu8
VPCOMPRESSD          | COMPRESS       | AVX512EVEX     | AVX512F_128    | EVV 0x8B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:w:dq:u32 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u32
VPCOMPRESSD          | COMPRESS       | AVX512EVEX     | AVX512F_128    | EVV 0x8B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_B3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u32
VPCOMPRESSD          | COMPRESS       | AVX512EVEX     | AVX512F_256    | EVV 0x8B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:w:qq:u32 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u32
VPCOMPRESSD          | COMPRESS       | AVX512EVEX     | AVX512F_256    | EVV 0x8B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=YMM_B3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u32
VPCOMPRESSD          | COMPRESS       | AVX512EVEX     | AVX512F_512    | EVV 0x8B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:w:zd:u32 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu32
VPCOMPRESSD          | COMPRESS       | AVX512EVEX     | AVX512F_512    | EVV 0x8B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=ZMM_B3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu32
VPCOMPRESSQ          | COMPRESS       | AVX512EVEX     | AVX512F_128    | EVV 0x8B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:w:dq:u64 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u64
VPCOMPRESSQ          | COMPRESS       | AVX512EVEX     | AVX512F_128    | EVV 0x8B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR    | REG0=XMM_B3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u64
VPCOMPRESSQ          | COMPRESS       | AVX512EVEX     | AVX512F_256    | EVV 0x8B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:w:qq:u64 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u64
VPCOMPRESSQ          | COMPRESS       | AVX512EVEX     | AVX512F_256    | EVV 0x8B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR    | REG0=YMM_B3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u64
VPCOMPRESSQ          | COMPRESS       | AVX512EVEX     | AVX512F_512    | EVV 0x8B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:w:zd:u64 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu64
VPCOMPRESSQ          | COMPRESS       | AVX512EVEX     | AVX512F_512    | EVV 0x8B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR    | REG0=ZMM_B3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu64
VPCOMPRESSW          | COMPRESS       | AVX512EVEX     | AVX512_VBMI2_128 | EVV 0x63 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_GSCAT() | MEM0:w:dq:u16 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u16
VPCOMPRESSW          | COMPRESS       | AVX512EVEX     | AVX512_VBMI2_128 | EVV 0x63 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR    | REG0=XMM_B3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u16
VPCOMPRESSW          | COMPRESS       | AVX512EVEX     | AVX512_VBMI2_256 | EVV 0x63 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_GSCAT() | MEM0:w:qq:u16 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u16
VPCOMPRESSW          | COMPRESS       | AVX512EVEX     | AVX512_VBMI2_256 | EVV 0x63 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR    | REG0=YMM_B3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u16
VPCOMPRESSW          | COMPRESS       | AVX512EVEX     | AVX512_VBMI2_512 | EVV 0x63 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_GSCAT() | MEM0:w:zd:u16 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu16
VPCOMPRESSW          | COMPRESS       | AVX512EVEX     | AVX512_VBMI2_512 | EVV 0x63 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR    | REG0=ZMM_B3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu16
VPCOMQ               | XOP            | XOP            | XOP            | XOPV 0xCF VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()    | REG0=XMM_R():w:dq:i64 REG1=XMM_N():r:dq:i64 MEM0:r:dq:i64 IMM0:r:b:u8
VPCOMQ               | XOP            | XOP            | XOP            | XOPV 0xCF VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()           | REG0=XMM_R():w:dq:i64 REG1=XMM_N():r:dq:i64 REG2=XMM_B():r:dq:i64 IMM0:r:b:u8
VPCOMUB              | XOP            | XOP            | XOP            | XOPV 0xEC VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()    | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 MEM0:r:dq:u8 IMM0:r:b:u8
VPCOMUB              | XOP            | XOP            | XOP            | XOPV 0xEC VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()           | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:u8 IMM0:r:b:u8
VPCOMUD              | XOP            | XOP            | XOP            | XOPV 0xEE VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()    | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 MEM0:r:dq:u32 IMM0:r:b:u8
VPCOMUD              | XOP            | XOP            | XOP            | XOPV 0xEE VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()           | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 REG2=XMM_B():r:dq:u32 IMM0:r:b:u8
VPCOMUQ              | XOP            | XOP            | XOP            | XOPV 0xEF VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()    | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 MEM0:r:dq:u64 IMM0:r:b:u8
VPCOMUQ              | XOP            | XOP            | XOP            | XOPV 0xEF VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()           | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 REG2=XMM_B():r:dq:u64 IMM0:r:b:u8
VPCOMUW              | XOP            | XOP            | XOP            | XOPV 0xED VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()    | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 MEM0:r:dq:u16 IMM0:r:b:u8
VPCOMUW              | XOP            | XOP            | XOP            | XOPV 0xED VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()           | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 REG2=XMM_B():r:dq:u16 IMM0:r:b:u8
VPCOMW               | XOP            | XOP            | XOP            | XOPV 0xCD VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8()    | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16 IMM0:r:b:u8
VPCOMW               | XOP            | XOP            | XOP            | XOPV 0xCD VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()           | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16 IMM0:r:b:u8
VPCONFLICTD          | CONFLICT       | AVX512EVEX     | AVX512CD_128   | EVV 0xC4 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32
VPCONFLICTD          | CONFLICT       | AVX512EVEX     | AVX512CD_128   | EVV 0xC4 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR
VPCONFLICTD          | CONFLICT       | AVX512EVEX     | AVX512CD_256   | EVV 0xC4 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u32
VPCONFLICTD          | CONFLICT       | AVX512EVEX     | AVX512CD_256   | EVV 0xC4 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR
VPCONFLICTD          | CONFLICT       | AVX512EVEX     | AVX512CD_512   | EVV 0xC4 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu32
VPCONFLICTD          | CONFLICT       | AVX512EVEX     | AVX512CD_512   | EVV 0xC4 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR
VPCONFLICTQ          | CONFLICT       | AVX512EVEX     | AVX512CD_128   | EVV 0xC4 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR    | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u64
VPCONFLICTQ          | CONFLICT       | AVX512EVEX     | AVX512CD_128   | EVV 0xC4 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR
VPCONFLICTQ          | CONFLICT       | AVX512EVEX     | AVX512CD_256   | EVV 0xC4 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR    | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u64
VPCONFLICTQ          | CONFLICT       | AVX512EVEX     | AVX512CD_256   | EVV 0xC4 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR
VPCONFLICTQ          | CONFLICT       | AVX512EVEX     | AVX512CD_512   | EVV 0xC4 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR    | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu64
VPCONFLICTQ          | CONFLICT       | AVX512EVEX     | AVX512CD_512   | EVV 0xC4 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR
VPDPBUSD             | AVX512         | AVX512EVEX     | AVX512_VNNI_128 | EVV 0x50 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():rw:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u32
VPDPBUSD             | AVX512         | AVX512EVEX     | AVX512_VNNI_128 | EVV 0x50 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:vv:u32:TXT=BCASTSTR
VPDPBUSD             | AVX512         | AVX512EVEX     | AVX512_VNNI_256 | EVV 0x50 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():rw:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u32
VPDPBUSD             | AVX512         | AVX512EVEX     | AVX512_VNNI_256 | EVV 0x50 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:vv:u32:TXT=BCASTSTR
VPDPBUSD             | AVX512         | AVX512EVEX     | AVX512_VNNI_512 | EVV 0x50 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():rw:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu32
VPDPBUSD             | AVX512         | AVX512EVEX     | AVX512_VNNI_512 | EVV 0x50 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:vv:u32:TXT=BCASTSTR
VPDPBUSDS            | AVX512         | AVX512EVEX     | AVX512_VNNI_128 | EVV 0x51 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():rw:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u32
VPDPBUSDS            | AVX512         | AVX512EVEX     | AVX512_VNNI_128 | EVV 0x51 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:vv:u32:TXT=BCASTSTR
VPDPBUSDS            | AVX512         | AVX512EVEX     | AVX512_VNNI_256 | EVV 0x51 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():rw:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u32
VPDPBUSDS            | AVX512         | AVX512EVEX     | AVX512_VNNI_256 | EVV 0x51 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:vv:u32:TXT=BCASTSTR
VPDPBUSDS            | AVX512         | AVX512EVEX     | AVX512_VNNI_512 | EVV 0x51 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():rw:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu32
VPDPBUSDS            | AVX512         | AVX512EVEX     | AVX512_VNNI_512 | EVV 0x51 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:vv:u32:TXT=BCASTSTR
VPDPWSSD             | AVX512         | AVX512EVEX     | AVX512_VNNI_128 | EVV 0x52 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():rw:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 REG3=XMM_B3():r:dq:u32
VPDPWSSD             | AVX512         | AVX512EVEX     | AVX512_VNNI_128 | EVV 0x52 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 MEM0:r:vv:u32:TXT=BCASTSTR
VPDPWSSD             | AVX512         | AVX512EVEX     | AVX512_VNNI_256 | EVV 0x52 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():rw:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 REG3=YMM_B3():r:qq:u32
VPDPWSSD             | AVX512         | AVX512EVEX     | AVX512_VNNI_256 | EVV 0x52 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 MEM0:r:vv:u32:TXT=BCASTSTR
VPDPWSSD             | AVX512         | AVX512EVEX     | AVX512_VNNI_512 | EVV 0x52 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():rw:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 REG3=ZMM_B3():r:zu32
VPDPWSSD             | AVX512         | AVX512EVEX     | AVX512_VNNI_512 | EVV 0x52 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 MEM0:r:vv:u32:TXT=BCASTSTR
VPDPWSSDS            | AVX512         | AVX512EVEX     | AVX512_VNNI_128 | EVV 0x53 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():rw:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 REG3=XMM_B3():r:dq:u32
VPDPWSSDS            | AVX512         | AVX512EVEX     | AVX512_VNNI_128 | EVV 0x53 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 MEM0:r:vv:u32:TXT=BCASTSTR
VPDPWSSDS            | AVX512         | AVX512EVEX     | AVX512_VNNI_256 | EVV 0x53 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():rw:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 REG3=YMM_B3():r:qq:u32
VPDPWSSDS            | AVX512         | AVX512EVEX     | AVX512_VNNI_256 | EVV 0x53 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 MEM0:r:vv:u32:TXT=BCASTSTR
VPDPWSSDS            | AVX512         | AVX512EVEX     | AVX512_VNNI_512 | EVV 0x53 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():rw:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 REG3=ZMM_B3():r:zu32
VPDPWSSDS            | AVX512         | AVX512EVEX     | AVX512_VNNI_512 | EVV 0x53 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 MEM0:r:vv:u32:TXT=BCASTSTR
VPERMB               | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_128 | EVV 0x8D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
VPERMB               | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_128 | EVV 0x8D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0    ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
VPERMB               | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_256 | EVV 0x8D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
VPERMB               | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_256 | EVV 0x8D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0    ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
VPERMB               | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_512 | EVV 0x8D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
VPERMB               | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_512 | EVV 0x8D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0    ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
VPERMD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x36 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
VPERMD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x36 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPERMD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x36 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
VPERMD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x36 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
VPERMI2B             | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_128 | EVV 0x75 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():rw:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
VPERMI2B             | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_128 | EVV 0x75 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0    ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():rw:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
VPERMI2B             | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_256 | EVV 0x75 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():rw:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
VPERMI2B             | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_256 | EVV 0x75 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0    ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():rw:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
VPERMI2B             | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_512 | EVV 0x75 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():rw:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
VPERMI2B             | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_512 | EVV 0x75 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0    ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():rw:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
VPERMI2D             | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x76 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():rw:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
VPERMI2D             | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x76 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPERMI2D             | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x76 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():rw:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
VPERMI2D             | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x76 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPERMI2D             | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x76 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():rw:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
VPERMI2D             | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x76 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
VPERMI2PD            | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x77 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VPERMI2PD            | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x77 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VPERMI2PD            | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x77 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
VPERMI2PD            | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x77 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VPERMI2PD            | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x77 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VPERMI2PD            | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x77 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
VPERMI2PS            | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x77 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VPERMI2PS            | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x77 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VPERMI2PS            | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x77 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
VPERMI2PS            | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x77 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VPERMI2PS            | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x77 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VPERMI2PS            | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x77 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
VPERMI2Q             | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x76 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():rw:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
VPERMI2Q             | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x76 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPERMI2Q             | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x76 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():rw:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
VPERMI2Q             | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x76 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPERMI2Q             | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x76 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():rw:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
VPERMI2Q             | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x76 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
VPERMI2W             | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x75 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():rw:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
VPERMI2W             | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x75 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():rw:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
VPERMI2W             | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x75 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():rw:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
VPERMI2W             | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x75 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():rw:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
VPERMI2W             | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x75 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():rw:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
VPERMI2W             | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x75 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():rw:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
VPERMIL2PD           | XOP            | XOP            | XOP            | VV1 0x49 VL128 V66 V0F3A W0  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64 REG2=XMM_SE():r:dq:f64 IMM0:r:b
VPERMIL2PD           | XOP            | XOP            | XOP            | VV1 0x49 VL128 V66 V0F3A W0  MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64 REG3=XMM_SE():r:dq:f64 IMM0:r:b
VPERMIL2PD           | XOP            | XOP            | XOP            | VV1 0x49 VL256 V66 V0F3A W0   MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()  | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64 REG2=YMM_SE():r:qq:f64 IMM0:r:b
VPERMIL2PD           | XOP            | XOP            | XOP            | VV1 0x49 VL256 V66 V0F3A W0   MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()         | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64 REG3=YMM_SE():r:qq:f64 IMM0:r:b
VPERMIL2PD           | XOP            | XOP            | XOP            | VV1 0x49 VL128 V66 V0F3A W1  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_SE():r:dq:f64 MEM0:r:dq:f64  IMM0:r:b
VPERMIL2PD           | XOP            | XOP            | XOP            | VV1 0x49 VL128 V66 V0F3A W1  MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=XMM_R():w:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_SE():r:dq:f64 REG3=XMM_B():r:dq:f64  IMM0:r:b
VPERMIL2PD           | XOP            | XOP            | XOP            | VV1 0x49 VL256 V66 V0F3A W1   MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()  | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64  REG2=YMM_SE():r:qq:f64  MEM0:r:qq:f64 IMM0:r:b
VPERMIL2PD           | XOP            | XOP            | XOP            | VV1 0x49 VL256 V66 V0F3A W1   MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()         | REG0=YMM_R():w:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_SE():r:qq:f64 REG3=YMM_B():r:qq:f64  IMM0:r:b
VPERMIL2PS           | XOP            | XOP            | XOP            | VV1 0x48 VL128 V66 V0F3A W0  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32 REG2=XMM_SE():r:dq:f32 IMM0:r:b
VPERMIL2PS           | XOP            | XOP            | XOP            | VV1 0x48 VL128 V66 V0F3A W0  MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32 REG3=XMM_SE():r:dq:f32 IMM0:r:b
VPERMIL2PS           | XOP            | XOP            | XOP            | VV1 0x48 VL256 V66 V0F3A W0   MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()  | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32 REG2=YMM_SE():r:qq:f32 IMM0:r:b
VPERMIL2PS           | XOP            | XOP            | XOP            | VV1 0x48 VL256 V66 V0F3A W0   MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()         | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32 REG3=YMM_SE():r:qq:f32 IMM0:r:b
VPERMIL2PS           | XOP            | XOP            | XOP            | VV1 0x48 VL128 V66 V0F3A W1  MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()   | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_SE():r:dq:f32 MEM0:r:dq:f32  IMM0:r:b
VPERMIL2PS           | XOP            | XOP            | XOP            | VV1 0x48 VL128 V66 V0F3A W1  MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()          | REG0=XMM_R():w:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_SE():r:dq:f32 REG3=XMM_B():r:dq:f32  IMM0:r:b
VPERMIL2PS           | XOP            | XOP            | XOP            | VV1 0x48 VL256 V66 V0F3A W1   MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()  | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32  REG2=YMM_SE():r:qq:f32  MEM0:r:qq:f32 IMM0:r:b
VPERMIL2PS           | XOP            | XOP            | XOP            | VV1 0x48 VL256 V66 V0F3A W1   MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()         | REG0=YMM_R():w:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_SE():r:qq:f32 REG3=YMM_B():r:qq:f32  IMM0:r:b
VPERMILPD            | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x05 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR UIMM8() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64 IMM0:r:b
VPERMILPD            | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x05 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
VPERMILPD            | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x0D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VPERMILPD            | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x0D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VPERMILPD            | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x05 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR UIMM8() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64 IMM0:r:b
VPERMILPD            | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x05 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
VPERMILPD            | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x0D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
VPERMILPD            | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x0D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VPERMILPD            | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x05 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR UIMM8() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64 IMM0:r:b
VPERMILPD            | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x05 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
VPERMILPD            | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x0D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VPERMILPD            | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x0D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
VPERMILPS            | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x04 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR UIMM8() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32 IMM0:r:b
VPERMILPS            | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x04 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
VPERMILPS            | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x0C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VPERMILPS            | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x0C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VPERMILPS            | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x04 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR UIMM8() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32 IMM0:r:b
VPERMILPS            | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x04 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
VPERMILPS            | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x0C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
VPERMILPS            | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x0C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VPERMILPS            | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x04 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR UIMM8() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32 IMM0:r:b
VPERMILPS            | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x04 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
VPERMILPS            | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x0C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VPERMILPS            | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x0C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
VPERMPD              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x01 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR UIMM8() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64 IMM0:r:b
VPERMPD              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x01 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
VPERMPD              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x16 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
VPERMPD              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x16 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VPERMPD              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x01 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR UIMM8() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64 IMM0:r:b
VPERMPD              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x01 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
VPERMPD              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x16 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VPERMPD              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x16 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
VPERMPS              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x16 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
VPERMPS              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x16 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VPERMPS              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x16 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VPERMPS              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x16 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
VPERMQ               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x00 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR UIMM8() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u64 IMM0:r:b
VPERMQ               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x00 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VPERMQ               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x36 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
VPERMQ               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x36 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPERMQ               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x00 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR UIMM8() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu64 IMM0:r:b
VPERMQ               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x00 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VPERMQ               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x36 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
VPERMQ               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x36 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
VPERMT2B             | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_128 | EVV 0x7D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():rw:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
VPERMT2B             | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_128 | EVV 0x7D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0    ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():rw:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
VPERMT2B             | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_256 | EVV 0x7D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():rw:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
VPERMT2B             | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_256 | EVV 0x7D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0    ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():rw:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
VPERMT2B             | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_512 | EVV 0x7D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():rw:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
VPERMT2B             | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_512 | EVV 0x7D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0    ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():rw:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
VPERMT2D             | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x7E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():rw:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
VPERMT2D             | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x7E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPERMT2D             | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x7E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():rw:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
VPERMT2D             | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x7E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPERMT2D             | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x7E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():rw:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
VPERMT2D             | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x7E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
VPERMT2PD            | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x7F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VPERMT2PD            | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x7F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VPERMT2PD            | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x7F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
VPERMT2PD            | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x7F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VPERMT2PD            | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x7F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VPERMT2PD            | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x7F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
VPERMT2PS            | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x7F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VPERMT2PS            | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x7F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VPERMT2PS            | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x7F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
VPERMT2PS            | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x7F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VPERMT2PS            | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x7F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VPERMT2PS            | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x7F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
VPERMT2Q             | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x7E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():rw:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
VPERMT2Q             | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x7E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPERMT2Q             | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x7E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():rw:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
VPERMT2Q             | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x7E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPERMT2Q             | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x7E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():rw:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
VPERMT2Q             | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x7E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
VPERMT2W             | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x7D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():rw:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
VPERMT2W             | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x7D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():rw:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
VPERMT2W             | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x7D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():rw:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
VPERMT2W             | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x7D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():rw:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
VPERMT2W             | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x7D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():rw:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
VPERMT2W             | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x7D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():rw:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
VPERMW               | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x8D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
VPERMW               | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x8D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
VPERMW               | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x8D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
VPERMW               | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x8D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
VPERMW               | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x8D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
VPERMW               | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x8D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
VPEXPANDB            | EXPAND         | AVX512EVEX     | AVX512_VBMI2_128 | EVV 0x62 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_8_BITS() NELEM_GSCAT() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u8
VPEXPANDB            | EXPAND         | AVX512EVEX     | AVX512_VBMI2_128 | EVV 0x62 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u8
VPEXPANDB            | EXPAND         | AVX512EVEX     | AVX512_VBMI2_256 | EVV 0x62 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_8_BITS() NELEM_GSCAT() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u8
VPEXPANDB            | EXPAND         | AVX512EVEX     | AVX512_VBMI2_256 | EVV 0x62 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u8
VPEXPANDB            | EXPAND         | AVX512EVEX     | AVX512_VBMI2_512 | EVV 0x62 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_8_BITS() NELEM_GSCAT() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:u8
VPEXPANDB            | EXPAND         | AVX512EVEX     | AVX512_VBMI2_512 | EVV 0x62 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu8
VPEXPANDD            | EXPAND         | AVX512EVEX     | AVX512F_128    | EVV 0x89 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_GSCAT() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u32
VPEXPANDD            | EXPAND         | AVX512EVEX     | AVX512F_128    | EVV 0x89 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32
VPEXPANDD            | EXPAND         | AVX512EVEX     | AVX512F_256    | EVV 0x89 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_GSCAT() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u32
VPEXPANDD            | EXPAND         | AVX512EVEX     | AVX512F_256    | EVV 0x89 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u32
VPEXPANDD            | EXPAND         | AVX512EVEX     | AVX512F_512    | EVV 0x89 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_GSCAT() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:u32
VPEXPANDD            | EXPAND         | AVX512EVEX     | AVX512F_512    | EVV 0x89 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu32
VPEXPANDQ            | EXPAND         | AVX512EVEX     | AVX512F_128    | EVV 0x89 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_GSCAT() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u64
VPEXPANDQ            | EXPAND         | AVX512EVEX     | AVX512F_128    | EVV 0x89 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR    | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u64
VPEXPANDQ            | EXPAND         | AVX512EVEX     | AVX512F_256    | EVV 0x89 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_GSCAT() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u64
VPEXPANDQ            | EXPAND         | AVX512EVEX     | AVX512F_256    | EVV 0x89 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR    | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u64
VPEXPANDQ            | EXPAND         | AVX512EVEX     | AVX512F_512    | EVV 0x89 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_GSCAT() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:u64
VPEXPANDQ            | EXPAND         | AVX512EVEX     | AVX512F_512    | EVV 0x89 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR    | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu64
VPEXPANDW            | EXPAND         | AVX512EVEX     | AVX512_VBMI2_128 | EVV 0x62 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ESIZE_16_BITS() NELEM_GSCAT() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u16
VPEXPANDW            | EXPAND         | AVX512EVEX     | AVX512_VBMI2_128 | EVV 0x62 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR    | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u16
VPEXPANDW            | EXPAND         | AVX512EVEX     | AVX512_VBMI2_256 | EVV 0x62 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ESIZE_16_BITS() NELEM_GSCAT() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u16
VPEXPANDW            | EXPAND         | AVX512EVEX     | AVX512_VBMI2_256 | EVV 0x62 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR    | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u16
VPEXPANDW            | EXPAND         | AVX512EVEX     | AVX512_VBMI2_512 | EVV 0x62 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ESIZE_16_BITS() NELEM_GSCAT() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:u16
VPEXPANDW            | EXPAND         | AVX512EVEX     | AVX512_VBMI2_512 | EVV 0x62 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR    | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu16
VPEXTRB              | AVX512         | AVX512EVEX     | AVX512BW_128N  | EVV 0x14 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR  ZEROING=0 MASK=0 UIMM8() | REG0=GPR32_B():w:d:u8 REG1=XMM_R3():r:dq:u8 IMM0:r:b
VPEXTRB              | AVX512         | AVX512EVEX     | AVX512BW_128N  | EVV 0x14 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128    NOEVSR  ZEROING=0 MASK=0 UIMM8()  ESIZE_8_BITS() NELEM_GPR_WRITER_STORE_BYTE() | MEM0:w:b:u8 REG0=XMM_R3():r:dq:u8 IMM0:r:b
VPEXTRD              | AVX512         | AVX512EVEX     | AVX512DQ_128N  | EVV 0x16 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  not64  NOEVSR  ZEROING=0 MASK=0 UIMM8() | REG0=GPR32_B():w:d:u32 REG1=XMM_R3():r:dq:u32 IMM0:r:b
VPEXTRD              | AVX512         | AVX512EVEX     | AVX512DQ_128N  | EVV 0x16 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  mode64 W0  NOEVSR  ZEROING=0 MASK=0 UIMM8() | REG0=GPR32_B():w:d:u32 REG1=XMM_R3():r:dq:u32 IMM0:r:b
VPEXTRD              | AVX512         | AVX512EVEX     | AVX512DQ_128N  | EVV 0x16 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  not64  NOEVSR  ZEROING=0 MASK=0 UIMM8()  ESIZE_32_BITS() NELEM_GPR_WRITER_STORE() | MEM0:w:d:u32 REG0=XMM_R3():r:dq:u32 IMM0:r:b
VPEXTRD              | AVX512         | AVX512EVEX     | AVX512DQ_128N  | EVV 0x16 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  mode64 W0  NOEVSR  ZEROING=0 MASK=0 UIMM8()  ESIZE_32_BITS() NELEM_GPR_WRITER_STORE() | MEM0:w:d:u32 REG0=XMM_R3():r:dq:u32 IMM0:r:b
VPEXTRQ              | AVX512         | AVX512EVEX     | AVX512DQ_128N  | EVV 0x16 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  mode64  NOEVSR  ZEROING=0 MASK=0 UIMM8() | REG0=GPR64_B():w:q:u64 REG1=XMM_R3():r:dq:u64 IMM0:r:b
VPEXTRQ              | AVX512         | AVX512EVEX     | AVX512DQ_128N  | EVV 0x16 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  mode64  NOEVSR  ZEROING=0 MASK=0 UIMM8()  ESIZE_64_BITS() NELEM_GPR_WRITER_STORE() | MEM0:w:q:u64 REG0=XMM_R3():r:dq:u64 IMM0:r:b
VPEXTRW              | AVX512         | AVX512EVEX     | AVX512BW_128N  | EVV 0x15 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR  ZEROING=0 MASK=0 UIMM8() | REG0=GPR32_B():w:d:u16 REG1=XMM_R3():r:dq:u16 IMM0:r:b
VPEXTRW              | AVX512         | AVX512EVEX     | AVX512BW_128N  | EVV 0x15 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128    NOEVSR  ZEROING=0 MASK=0 UIMM8()  ESIZE_16_BITS() NELEM_GPR_WRITER_STORE_WORD() | MEM0:w:wrd:u16 REG0=XMM_R3():r:dq:u16 IMM0:r:b
VPEXTRW_C5           | AVX512         | AVX512EVEX     | AVX512BW_128N  | EVV 0xC5 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR  ZEROING=0 MASK=0 UIMM8() not64 | REG0=GPR32_R():w:d:u16 REG1=XMM_B3():r:dq:u16 IMM0:r:b
VPEXTRW_C5           | AVX512         | AVX512EVEX     | AVX512BW_128N  | EVV 0xC5 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR  ZEROING=0 MASK=0 UIMM8() mode64 EVEXRR_ONE | REG0=GPR32_R():w:d:u16 REG1=XMM_B3():r:dq:u16 IMM0:r:b
VPGATHERDD           | GATHER         | AVX512EVEX     | AVX512F_128    | EVV 0x90 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL128  W0 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | REG0=XMM_R3():w:dq:u32 REG1=MASKNOT0():rw:mskw MEM0:r:d:u32
VPGATHERDD           | GATHER         | AVX512EVEX     | AVX512F_256    | EVV 0x90 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL256  W0 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | REG0=YMM_R3():w:qq:u32 REG1=MASKNOT0():rw:mskw MEM0:r:d:u32
VPGATHERDD           | GATHER         | AVX512EVEX     | AVX512F_512    | EVV 0x90 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL512  W0 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | REG0=ZMM_R3():w:zu32 REG1=MASKNOT0():rw:mskw MEM0:r:d:u32
VPGATHERDD           | KNC            | KNCE           | KNCE           | KVV 0x90 V66 V0F38  W0 NO_SPARSE_EVSR MOD[mm] MOD!=3 REG[rrr] RM[0b100] KNC_VMODRM() UPCONVERT_INT32_LOAD() | REG0=ZMM_R3():rw:zd REG1=MASK1():rw:mskw  MEM0:r:zv:TXT=NT:TXT=CONVERT NELEM=1:SUPP
VPGATHERDQ           | GATHER         | AVX512EVEX     | AVX512F_128    | EVV 0x90 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL128  W1 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | REG0=XMM_R3():w:dq:u64 REG1=MASKNOT0():rw:mskw MEM0:r:q:u64
VPGATHERDQ           | GATHER         | AVX512EVEX     | AVX512F_256    | EVV 0x90 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL256  W1 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | REG0=YMM_R3():w:qq:u64 REG1=MASKNOT0():rw:mskw MEM0:r:q:u64
VPGATHERDQ           | GATHER         | AVX512EVEX     | AVX512F_512    | EVV 0x90 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL512  W1 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | REG0=ZMM_R3():w:zu64 REG1=MASKNOT0():rw:mskw MEM0:r:q:u64
VPGATHERDQ           | KNC            | KNCE           | KNCE           | KVV 0x90 V66 V0F38  W1 NO_SPARSE_EVSR MOD[mm] MOD!=3 REG[rrr] RM[0b100] KNC_VMODRM() UPCONVERT_INT64_LOAD() | REG0=ZMM_R3():rw:zq REG1=MASK1():rw:mskw  MEM0:r:zv:TXT=NT:TXT=CONVERT NELEM=1:SUPP
VPGATHERQD           | GATHER         | AVX512EVEX     | AVX512F_128    | EVV 0x91 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL128  W0 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | REG0=XMM_R3():w:dq:u32 REG1=MASKNOT0():rw:mskw MEM0:r:d:u32
VPGATHERQD           | GATHER         | AVX512EVEX     | AVX512F_256    | EVV 0x91 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL256  W0 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | REG0=XMM_R3():w:dq:u32 REG1=MASKNOT0():rw:mskw MEM0:r:d:u32
VPGATHERQD           | GATHER         | AVX512EVEX     | AVX512F_512    | EVV 0x91 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL512  W0 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | REG0=YMM_R3():w:qq:u32 REG1=MASKNOT0():rw:mskw MEM0:r:d:u32
VPGATHERQQ           | GATHER         | AVX512EVEX     | AVX512F_128    | EVV 0x91 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL128  W1 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | REG0=XMM_R3():w:dq:u64 REG1=MASKNOT0():rw:mskw MEM0:r:q:u64
VPGATHERQQ           | GATHER         | AVX512EVEX     | AVX512F_256    | EVV 0x91 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL256  W1 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | REG0=YMM_R3():w:qq:u64 REG1=MASKNOT0():rw:mskw MEM0:r:q:u64
VPGATHERQQ           | GATHER         | AVX512EVEX     | AVX512F_512    | EVV 0x91 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL512  W1 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | REG0=ZMM_R3():w:zu64 REG1=MASKNOT0():rw:mskw MEM0:r:q:u64
VPHADDBD             | XOP            | XOP            | XOP            | XOPV 0xC2 VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()       | REG0=XMM_R():w:dq:i32 MEM0:r:dq:i8
VPHADDBD             | XOP            | XOP            | XOP            | XOPV 0xC2 VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]              | REG0=XMM_R():w:dq:i32 REG1=XMM_B():r:dq:i8
VPHADDBQ             | XOP            | XOP            | XOP            | XOPV 0xC3 VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()       | REG0=XMM_R():w:dq:i64 MEM0:r:dq:i8
VPHADDBQ             | XOP            | XOP            | XOP            | XOPV 0xC3 VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]              | REG0=XMM_R():w:dq:i64 REG1=XMM_B():r:dq:i8
VPHADDBW             | XOP            | XOP            | XOP            | XOPV 0xC1 VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()       | REG0=XMM_R():w:dq:i8 MEM0:r:dq:i8
VPHADDBW             | XOP            | XOP            | XOP            | XOPV 0xC1 VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]              | REG0=XMM_R():w:dq:i8 REG1=XMM_B():r:dq:i8
VPHADDDQ             | XOP            | XOP            | XOP            | XOPV 0xCB VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()       | REG0=XMM_R():w:dq:i64 MEM0:r:dq:i32
VPHADDDQ             | XOP            | XOP            | XOP            | XOPV 0xCB VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]              | REG0=XMM_R():w:dq:i64 REG1=XMM_B():r:dq:i32
VPHADDUBD            | XOP            | XOP            | XOP            | XOPV 0xD2 VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()       | REG0=XMM_R():w:dq:u32 MEM0:r:dq:u8
VPHADDUBD            | XOP            | XOP            | XOP            | XOPV 0xD2 VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]              | REG0=XMM_R():w:dq:u32 REG1=XMM_B():r:dq:u8
VPHADDUBQ            | XOP            | XOP            | XOP            | XOPV 0xD3 VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()       | REG0=XMM_R():w:dq:u64 MEM0:r:dq:u8
VPHADDUBQ            | XOP            | XOP            | XOP            | XOPV 0xD3 VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]              | REG0=XMM_R():w:dq:u64 REG1=XMM_B():r:dq:u8
VPHADDUBW            | XOP            | XOP            | XOP            | XOPV 0xD1 VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()       | REG0=XMM_R():w:dq:u16 MEM0:r:dq:u8
VPHADDUBW            | XOP            | XOP            | XOP            | XOPV 0xD1 VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]              | REG0=XMM_R():w:dq:u16 REG1=XMM_B():r:dq:u8
VPHADDUDQ            | XOP            | XOP            | XOP            | XOPV 0xDB VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()       | REG0=XMM_R():w:dq:u64 MEM0:r:dq:u32
VPHADDUDQ            | XOP            | XOP            | XOP            | XOPV 0xDB VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]              | REG0=XMM_R():w:dq:u64 REG1=XMM_B():r:dq:u32
VPHADDUWD            | XOP            | XOP            | XOP            | XOPV 0xD6 VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()       | REG0=XMM_R():w:dq:u32 MEM0:r:dq:u16
VPHADDUWD            | XOP            | XOP            | XOP            | XOPV 0xD6 VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]              | REG0=XMM_R():w:dq:u32 REG1=XMM_B():r:dq:u16
VPHADDUWQ            | XOP            | XOP            | XOP            | XOPV 0xD7 VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()       | REG0=XMM_R():w:dq:u64 MEM0:r:dq:u16
VPHADDUWQ            | XOP            | XOP            | XOP            | XOPV 0xD7 VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]              | REG0=XMM_R():w:dq:u64 REG1=XMM_B():r:dq:u16
VPHADDWD             | XOP            | XOP            | XOP            | XOPV 0xC6 VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()       | REG0=XMM_R():w:dq:i32 MEM0:r:dq:i16
VPHADDWD             | XOP            | XOP            | XOP            | XOPV 0xC6 VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]              | REG0=XMM_R():w:dq:i32 REG1=XMM_B():r:dq:i16
VPHADDWQ             | XOP            | XOP            | XOP            | XOPV 0xC7 VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()       | REG0=XMM_R():w:dq:i64 MEM0:r:dq:i16
VPHADDWQ             | XOP            | XOP            | XOP            | XOPV 0xC7 VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]              | REG0=XMM_R():w:dq:i64 REG1=XMM_B():r:dq:i16
VPHSUBBW             | XOP            | XOP            | XOP            | XOPV 0xE1 VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()       | REG0=XMM_R():w:dq:i16 MEM0:r:dq:i8
VPHSUBBW             | XOP            | XOP            | XOP            | XOPV 0xE1 VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]              | REG0=XMM_R():w:dq:i16 REG1=XMM_B():r:dq:i8
VPHSUBDQ             | XOP            | XOP            | XOP            | XOPV 0xE3 VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()       | REG0=XMM_R():w:dq:i64 MEM0:r:dq:i32
VPHSUBDQ             | XOP            | XOP            | XOP            | XOPV 0xE3 VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]              | REG0=XMM_R():w:dq:i64 REG1=XMM_B():r:dq:i32
VPHSUBWD             | XOP            | XOP            | XOP            | XOPV 0xE2 VNP W0 VL128 NOVSR XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()       | REG0=XMM_R():w:dq:i32 MEM0:r:dq:i16
VPHSUBWD             | XOP            | XOP            | XOP            | XOPV 0xE2 VNP W0 VL128 NOVSR XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]              | REG0=XMM_R():w:dq:i32 REG1=XMM_B():r:dq:i16
VPINSRB              | AVX512         | AVX512EVEX     | AVX512BW_128N  | EVV 0x20 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128      ZEROING=0 MASK=0 UIMM8() | REG0=XMM_R3():w:dq:u8 REG1=XMM_N3():r:dq:u8 REG2=GPR32_B():r:d:u8 IMM0:r:b
VPINSRB              | AVX512         | AVX512EVEX     | AVX512BW_128N  | EVV 0x20 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ZEROING=0 MASK=0 UIMM8()  ESIZE_8_BITS() NELEM_GPR_READER_BYTE() | REG0=XMM_R3():w:dq:u8 REG1=XMM_N3():r:dq:u8 MEM0:r:b:u8 IMM0:r:b
VPINSRD              | AVX512         | AVX512EVEX     | AVX512DQ_128N  | EVV 0x22 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  not64    ZEROING=0 MASK=0 UIMM8() | REG0=XMM_R3():w:dq:u32 REG1=XMM_N3():r:dq:u32 REG2=GPR32_B():r:d:u32 IMM0:r:b
VPINSRD              | AVX512         | AVX512EVEX     | AVX512DQ_128N  | EVV 0x22 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  mode64 W0    ZEROING=0 MASK=0 UIMM8() | REG0=XMM_R3():w:dq:u32 REG1=XMM_N3():r:dq:u32 REG2=GPR32_B():r:d:u32 IMM0:r:b
VPINSRD              | AVX512         | AVX512EVEX     | AVX512DQ_128N  | EVV 0x22 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  not64    ZEROING=0 MASK=0 UIMM8()  ESIZE_32_BITS() NELEM_GPR_READER() | REG0=XMM_R3():w:dq:u32 REG1=XMM_N3():r:dq:u32 MEM0:r:d:u32 IMM0:r:b
VPINSRD              | AVX512         | AVX512EVEX     | AVX512DQ_128N  | EVV 0x22 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  mode64 W0    ZEROING=0 MASK=0 UIMM8()  ESIZE_32_BITS() NELEM_GPR_READER() | REG0=XMM_R3():w:dq:u32 REG1=XMM_N3():r:dq:u32 MEM0:r:d:u32 IMM0:r:b
VPINSRQ              | AVX512         | AVX512EVEX     | AVX512DQ_128N  | EVV 0x22 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  mode64    ZEROING=0 MASK=0 UIMM8() | REG0=XMM_R3():w:dq:u64 REG1=XMM_N3():r:dq:u64 REG2=GPR64_B():r:q:u64 IMM0:r:b
VPINSRQ              | AVX512         | AVX512EVEX     | AVX512DQ_128N  | EVV 0x22 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  mode64    ZEROING=0 MASK=0 UIMM8()  ESIZE_64_BITS() NELEM_GPR_READER() | REG0=XMM_R3():w:dq:u64 REG1=XMM_N3():r:dq:u64 MEM0:r:q:u64 IMM0:r:b
VPINSRW              | AVX512         | AVX512EVEX     | AVX512BW_128N  | EVV 0xC4 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128      ZEROING=0 MASK=0 UIMM8() | REG0=XMM_R3():w:dq:u16 REG1=XMM_N3():r:dq:u16 REG2=GPR32_B():r:d:u16 IMM0:r:b
VPINSRW              | AVX512         | AVX512EVEX     | AVX512BW_128N  | EVV 0xC4 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ZEROING=0 MASK=0 UIMM8()  ESIZE_16_BITS() NELEM_GPR_READER_WORD() | REG0=XMM_R3():w:dq:u16 REG1=XMM_N3():r:dq:u16 MEM0:r:wrd:u16 IMM0:r:b
VPLZCNTD             | CONFLICT       | AVX512EVEX     | AVX512CD_128   | EVV 0x44 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32
VPLZCNTD             | CONFLICT       | AVX512EVEX     | AVX512CD_128   | EVV 0x44 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR
VPLZCNTD             | CONFLICT       | AVX512EVEX     | AVX512CD_256   | EVV 0x44 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u32
VPLZCNTD             | CONFLICT       | AVX512EVEX     | AVX512CD_256   | EVV 0x44 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR
VPLZCNTD             | CONFLICT       | AVX512EVEX     | AVX512CD_512   | EVV 0x44 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu32
VPLZCNTD             | CONFLICT       | AVX512EVEX     | AVX512CD_512   | EVV 0x44 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR
VPLZCNTQ             | CONFLICT       | AVX512EVEX     | AVX512CD_128   | EVV 0x44 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR    | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u64
VPLZCNTQ             | CONFLICT       | AVX512EVEX     | AVX512CD_128   | EVV 0x44 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR
VPLZCNTQ             | CONFLICT       | AVX512EVEX     | AVX512CD_256   | EVV 0x44 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR    | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u64
VPLZCNTQ             | CONFLICT       | AVX512EVEX     | AVX512CD_256   | EVV 0x44 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR
VPLZCNTQ             | CONFLICT       | AVX512EVEX     | AVX512CD_512   | EVV 0x44 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR    | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu64
VPLZCNTQ             | CONFLICT       | AVX512EVEX     | AVX512CD_512   | EVV 0x44 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR
VPMACSDD             | XOP            | XOP            | XOP            | XOPV 0x9E VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()  | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32 REG2=XMM_SE():r:dq:i32
VPMACSDD             | XOP            | XOP            | XOP            | XOPV 0x9E VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()         | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32 REG3=XMM_SE():r:dq:i32
VPMACSDQH            | XOP            | XOP            | XOP            | XOPV 0x9F VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()  | REG0=XMM_R():w:dq:i64 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32 REG2=XMM_SE():r:dq:i64
VPMACSDQH            | XOP            | XOP            | XOP            | XOPV 0x9F VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()         | REG0=XMM_R():w:dq:i64 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32 REG3=XMM_SE():r:dq:i64
VPMACSDQL            | XOP            | XOP            | XOP            | XOPV 0x97 VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()  | REG0=XMM_R():w:dq:i64 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32 REG2=XMM_SE():r:dq:i64
VPMACSDQL            | XOP            | XOP            | XOP            | XOPV 0x97 VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()         | REG0=XMM_R():w:dq:i64 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32 REG3=XMM_SE():r:dq:i64
VPMACSSDD            | XOP            | XOP            | XOP            | XOPV 0x8E VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()  | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32 REG2=XMM_SE():r:dq:i32
VPMACSSDD            | XOP            | XOP            | XOP            | XOPV 0x8E VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()         | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32 REG3=XMM_SE():r:dq:i32
VPMACSSDQH           | XOP            | XOP            | XOP            | XOPV 0x8F VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()  | REG0=XMM_R():w:dq:i64 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32 REG2=XMM_SE():r:dq:i64
VPMACSSDQH           | XOP            | XOP            | XOP            | XOPV 0x8F VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()         | REG0=XMM_R():w:dq:i64 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32 REG3=XMM_SE():r:dq:i64
VPMACSSDQL           | XOP            | XOP            | XOP            | XOPV 0x87 VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()  | REG0=XMM_R():w:dq:i64 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32 REG2=XMM_SE():r:dq:i64
VPMACSSDQL           | XOP            | XOP            | XOP            | XOPV 0x87 VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()         | REG0=XMM_R():w:dq:i64 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32 REG3=XMM_SE():r:dq:i64
VPMACSSWD            | XOP            | XOP            | XOP            | XOPV 0x86 VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()  | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16 REG2=XMM_SE():r:dq:i32
VPMACSSWD            | XOP            | XOP            | XOP            | XOPV 0x86 VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()         | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16 REG3=XMM_SE():r:dq:i32
VPMACSSWW            | XOP            | XOP            | XOP            | XOPV 0x85 VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()  | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16 REG2=XMM_SE():r:dq:i16
VPMACSSWW            | XOP            | XOP            | XOP            | XOPV 0x85 VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()         | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16 REG3=XMM_SE():r:dq:i16
VPMACSWD             | XOP            | XOP            | XOP            | XOPV 0x96 VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()  | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16 REG2=XMM_SE():r:dq:i32
VPMACSWD             | XOP            | XOP            | XOP            | XOPV 0x96 VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()         | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16 REG3=XMM_SE():r:dq:i32
VPMACSWW             | XOP            | XOP            | XOP            | XOPV 0x95 VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()  | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16 REG2=XMM_SE():r:dq:i16
VPMACSWW             | XOP            | XOP            | XOP            | XOPV 0x95 VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()         | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16 REG3=XMM_SE():r:dq:i16
VPMADCSSWD           | XOP            | XOP            | XOP            | XOPV 0xA6 VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()  | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16 REG2=XMM_SE():r:dq:i32
VPMADCSSWD           | XOP            | XOP            | XOP            | XOPV 0xA6 VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()         | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16 REG3=XMM_SE():r:dq:i32
VPMADCSWD            | XOP            | XOP            | XOP            | XOPV 0xB6 VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()  | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16 REG2=XMM_SE():r:dq:i32
VPMADCSWD            | XOP            | XOP            | XOP            | XOPV 0xB6 VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()         | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16 REG3=XMM_SE():r:dq:i32
VPMADD52HUQ          | IFMA           | AVX512EVEX     | AVX512_IFMA_128 | EVV 0xB5 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():rw:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
VPMADD52HUQ          | IFMA           | AVX512EVEX     | AVX512_IFMA_128 | EVV 0xB5 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPMADD52HUQ          | IFMA           | AVX512EVEX     | AVX512_IFMA_256 | EVV 0xB5 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():rw:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
VPMADD52HUQ          | IFMA           | AVX512EVEX     | AVX512_IFMA_256 | EVV 0xB5 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPMADD52HUQ          | IFMA           | AVX512EVEX     | AVX512_IFMA_512 | EVV 0xB5 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():rw:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
VPMADD52HUQ          | IFMA           | AVX512EVEX     | AVX512_IFMA_512 | EVV 0xB5 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
VPMADD52LUQ          | IFMA           | AVX512EVEX     | AVX512_IFMA_128 | EVV 0xB4 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():rw:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
VPMADD52LUQ          | IFMA           | AVX512EVEX     | AVX512_IFMA_128 | EVV 0xB4 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPMADD52LUQ          | IFMA           | AVX512EVEX     | AVX512_IFMA_256 | EVV 0xB4 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():rw:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
VPMADD52LUQ          | IFMA           | AVX512EVEX     | AVX512_IFMA_256 | EVV 0xB4 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPMADD52LUQ          | IFMA           | AVX512EVEX     | AVX512_IFMA_512 | EVV 0xB4 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():rw:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
VPMADD52LUQ          | IFMA           | AVX512EVEX     | AVX512_IFMA_512 | EVV 0xB4 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
VPMADDUBSW           | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x04 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 REG3=XMM_B3():r:dq:i16
VPMADDUBSW           | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x04 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 MEM0:r:dq:i16
VPMADDUBSW           | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x04 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 REG3=YMM_B3():r:qq:i16
VPMADDUBSW           | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x04 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 MEM0:r:qq:i16
VPMADDUBSW           | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x04 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 REG3=ZMM_B3():r:zi16
VPMADDUBSW           | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x04 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 MEM0:r:zd:i16
VPMADDWD             | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xF5 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                  | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 REG3=XMM_B3():r:dq:i16
VPMADDWD             | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xF5 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 MEM0:r:dq:i16
VPMADDWD             | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xF5 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                  | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 REG3=YMM_B3():r:qq:i16
VPMADDWD             | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xF5 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 MEM0:r:qq:i16
VPMADDWD             | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xF5 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                  | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 REG3=ZMM_B3():r:zi16
VPMADDWD             | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xF5 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 MEM0:r:zd:i16
VPMAXSB              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x3C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                | REG0=XMM_R3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i8 REG3=XMM_B3():r:dq:i8
VPMAXSB              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x3C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i8 MEM0:r:dq:i8
VPMAXSB              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x3C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                | REG0=YMM_R3():w:qq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i8 REG3=YMM_B3():r:qq:i8
VPMAXSB              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x3C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i8 MEM0:r:qq:i8
VPMAXSB              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x3C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                | REG0=ZMM_R3():w:zi8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi8 REG3=ZMM_B3():r:zi8
VPMAXSB              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x3C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zi8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi8 MEM0:r:zd:i8
VPMAXSD              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x3D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i32 REG3=XMM_B3():r:dq:i32
VPMAXSD              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x3D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i32 MEM0:r:vv:i32:TXT=BCASTSTR
VPMAXSD              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x3D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i32 REG3=YMM_B3():r:qq:i32
VPMAXSD              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x3D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i32 MEM0:r:vv:i32:TXT=BCASTSTR
VPMAXSD              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x3D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi32 REG3=ZMM_B3():r:zi32
VPMAXSD              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x3D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi32 MEM0:r:vv:i32:TXT=BCASTSTR
VPMAXSQ              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x3D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i64 REG3=XMM_B3():r:dq:i64
VPMAXSQ              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x3D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i64 MEM0:r:vv:i64:TXT=BCASTSTR
VPMAXSQ              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x3D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i64 REG3=YMM_B3():r:qq:i64
VPMAXSQ              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x3D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i64 MEM0:r:vv:i64:TXT=BCASTSTR
VPMAXSQ              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x3D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi64 REG3=ZMM_B3():r:zi64
VPMAXSQ              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x3D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi64 MEM0:r:vv:i64:TXT=BCASTSTR
VPMAXSW              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xEE V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                  | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 REG3=XMM_B3():r:dq:i16
VPMAXSW              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xEE V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 MEM0:r:dq:i16
VPMAXSW              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xEE V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                  | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 REG3=YMM_B3():r:qq:i16
VPMAXSW              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xEE V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 MEM0:r:qq:i16
VPMAXSW              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xEE V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                  | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 REG3=ZMM_B3():r:zi16
VPMAXSW              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xEE V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 MEM0:r:zd:i16
VPMAXUB              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xDE V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                  | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
VPMAXUB              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xDE V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
VPMAXUB              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xDE V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                  | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
VPMAXUB              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xDE V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
VPMAXUB              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xDE V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                  | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
VPMAXUB              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xDE V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
VPMAXUD              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x3F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
VPMAXUD              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x3F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPMAXUD              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x3F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
VPMAXUD              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x3F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPMAXUD              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x3F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
VPMAXUD              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x3F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
VPMAXUQ              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x3F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
VPMAXUQ              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x3F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPMAXUQ              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x3F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
VPMAXUQ              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x3F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPMAXUQ              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x3F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
VPMAXUQ              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x3F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
VPMAXUW              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x3E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
VPMAXUW              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x3E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
VPMAXUW              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x3E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
VPMAXUW              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x3E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
VPMAXUW              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x3E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
VPMAXUW              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x3E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
VPMINSB              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x38 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                | REG0=XMM_R3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i8 REG3=XMM_B3():r:dq:i8
VPMINSB              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x38 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i8 MEM0:r:dq:i8
VPMINSB              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x38 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                | REG0=YMM_R3():w:qq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i8 REG3=YMM_B3():r:qq:i8
VPMINSB              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x38 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i8 MEM0:r:qq:i8
VPMINSB              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x38 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                | REG0=ZMM_R3():w:zi8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi8 REG3=ZMM_B3():r:zi8
VPMINSB              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x38 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zi8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi8 MEM0:r:zd:i8
VPMINSD              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x39 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i32 REG3=XMM_B3():r:dq:i32
VPMINSD              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x39 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i32 MEM0:r:vv:i32:TXT=BCASTSTR
VPMINSD              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x39 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i32 REG3=YMM_B3():r:qq:i32
VPMINSD              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x39 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i32 MEM0:r:vv:i32:TXT=BCASTSTR
VPMINSD              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x39 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi32 REG3=ZMM_B3():r:zi32
VPMINSD              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x39 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi32 MEM0:r:vv:i32:TXT=BCASTSTR
VPMINSQ              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x39 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i64 REG3=XMM_B3():r:dq:i64
VPMINSQ              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x39 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i64 MEM0:r:vv:i64:TXT=BCASTSTR
VPMINSQ              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x39 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i64 REG3=YMM_B3():r:qq:i64
VPMINSQ              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x39 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i64 MEM0:r:vv:i64:TXT=BCASTSTR
VPMINSQ              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x39 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi64 REG3=ZMM_B3():r:zi64
VPMINSQ              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x39 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi64 MEM0:r:vv:i64:TXT=BCASTSTR
VPMINSW              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xEA V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                  | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 REG3=XMM_B3():r:dq:i16
VPMINSW              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xEA V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 MEM0:r:dq:i16
VPMINSW              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xEA V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                  | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 REG3=YMM_B3():r:qq:i16
VPMINSW              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xEA V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 MEM0:r:qq:i16
VPMINSW              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xEA V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                  | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 REG3=ZMM_B3():r:zi16
VPMINSW              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xEA V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 MEM0:r:zd:i16
VPMINUB              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xDA V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                  | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
VPMINUB              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xDA V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
VPMINUB              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xDA V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                  | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
VPMINUB              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xDA V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
VPMINUB              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xDA V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                  | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
VPMINUB              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xDA V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
VPMINUD              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x3B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
VPMINUD              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x3B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPMINUD              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x3B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
VPMINUD              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x3B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPMINUD              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x3B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
VPMINUD              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x3B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
VPMINUQ              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x3B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
VPMINUQ              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x3B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPMINUQ              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x3B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
VPMINUQ              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x3B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPMINUQ              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x3B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
VPMINUQ              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x3B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
VPMINUW              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x3A V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
VPMINUW              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x3A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
VPMINUW              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x3A V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
VPMINUW              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x3A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
VPMINUW              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x3A V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
VPMINUW              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x3A V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
VPMOVB2M             | DATAXFER       | AVX512EVEX     | AVX512BW_128   | EVV 0x29 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR  ZEROING=0 MASK=0 | REG0=MASK_R():w:mskw REG1=XMM_B3():r:dq:u8
VPMOVB2M             | DATAXFER       | AVX512EVEX     | AVX512BW_256   | EVV 0x29 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR  ZEROING=0 MASK=0 | REG0=MASK_R():w:mskw REG1=YMM_B3():r:qq:u8
VPMOVB2M             | DATAXFER       | AVX512EVEX     | AVX512BW_512   | EVV 0x29 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR  ZEROING=0 MASK=0 | REG0=MASK_R():w:mskw REG1=ZMM_B3():r:zu8
VPMOVD2M             | DATAXFER       | AVX512EVEX     | AVX512DQ_128   | EVV 0x39 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR  ZEROING=0 MASK=0 | REG0=MASK_R():w:mskw REG1=XMM_B3():r:dq:u32
VPMOVD2M             | DATAXFER       | AVX512EVEX     | AVX512DQ_256   | EVV 0x39 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR  ZEROING=0 MASK=0 | REG0=MASK_R():w:mskw REG1=YMM_B3():r:qq:u32
VPMOVD2M             | DATAXFER       | AVX512EVEX     | AVX512DQ_512   | EVV 0x39 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR  ZEROING=0 MASK=0 | REG0=MASK_R():w:mskw REG1=ZMM_B3():r:zu32
VPMOVDB              | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x31 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_B3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u32
VPMOVDB              | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x31 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_QUARTERMEM() | MEM0:w:d:u8 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u32
VPMOVDB              | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x31 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=XMM_B3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u32
VPMOVDB              | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x31 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_QUARTERMEM() | MEM0:w:q:u8 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u32
VPMOVDB              | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x31 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=XMM_B3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu32
VPMOVDB              | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x31 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_QUARTERMEM() | MEM0:w:dq:u8 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu32
VPMOVDW              | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x33 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_B3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u32
VPMOVDW              | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x33 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_HALFMEM() | MEM0:w:q:u16 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u32
VPMOVDW              | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x33 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=XMM_B3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u32
VPMOVDW              | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x33 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_HALFMEM() | MEM0:w:dq:u16 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u32
VPMOVDW              | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x33 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=YMM_B3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu32
VPMOVDW              | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x33 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_HALFMEM() | MEM0:w:qq:u16 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu32
VPMOVM2B             | DATAXFER       | AVX512EVEX     | AVX512BW_128   | EVV 0x28 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR  ZEROING=0 MASK=0 | REG0=XMM_R3():w:dq:u8 REG1=MASK_B():r:mskw
VPMOVM2B             | DATAXFER       | AVX512EVEX     | AVX512BW_256   | EVV 0x28 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR  ZEROING=0 MASK=0 | REG0=YMM_R3():w:qq:u8 REG1=MASK_B():r:mskw
VPMOVM2B             | DATAXFER       | AVX512EVEX     | AVX512BW_512   | EVV 0x28 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR  ZEROING=0 MASK=0 | REG0=ZMM_R3():w:zu8 REG1=MASK_B():r:mskw
VPMOVM2D             | DATAXFER       | AVX512EVEX     | AVX512DQ_128   | EVV 0x38 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR  ZEROING=0 MASK=0 | REG0=XMM_R3():w:dq:u32 REG1=MASK_B():r:mskw
VPMOVM2D             | DATAXFER       | AVX512EVEX     | AVX512DQ_256   | EVV 0x38 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR  ZEROING=0 MASK=0 | REG0=YMM_R3():w:qq:u32 REG1=MASK_B():r:mskw
VPMOVM2D             | DATAXFER       | AVX512EVEX     | AVX512DQ_512   | EVV 0x38 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR  ZEROING=0 MASK=0 | REG0=ZMM_R3():w:zu32 REG1=MASK_B():r:mskw
VPMOVM2Q             | DATAXFER       | AVX512EVEX     | AVX512DQ_128   | EVV 0x38 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR  ZEROING=0 MASK=0 | REG0=XMM_R3():w:dq:u64 REG1=MASK_B():r:mskw
VPMOVM2Q             | DATAXFER       | AVX512EVEX     | AVX512DQ_256   | EVV 0x38 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR  ZEROING=0 MASK=0 | REG0=YMM_R3():w:qq:u64 REG1=MASK_B():r:mskw
VPMOVM2Q             | DATAXFER       | AVX512EVEX     | AVX512DQ_512   | EVV 0x38 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR  ZEROING=0 MASK=0 | REG0=ZMM_R3():w:zu64 REG1=MASK_B():r:mskw
VPMOVM2W             | DATAXFER       | AVX512EVEX     | AVX512BW_128   | EVV 0x28 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR  ZEROING=0 MASK=0 | REG0=XMM_R3():w:dq:u16 REG1=MASK_B():r:mskw
VPMOVM2W             | DATAXFER       | AVX512EVEX     | AVX512BW_256   | EVV 0x28 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR  ZEROING=0 MASK=0 | REG0=YMM_R3():w:qq:u16 REG1=MASK_B():r:mskw
VPMOVM2W             | DATAXFER       | AVX512EVEX     | AVX512BW_512   | EVV 0x28 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR  ZEROING=0 MASK=0 | REG0=ZMM_R3():w:zu16 REG1=MASK_B():r:mskw
VPMOVQ2M             | DATAXFER       | AVX512EVEX     | AVX512DQ_128   | EVV 0x39 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR  ZEROING=0 MASK=0 | REG0=MASK_R():w:mskw REG1=XMM_B3():r:dq:u64
VPMOVQ2M             | DATAXFER       | AVX512EVEX     | AVX512DQ_256   | EVV 0x39 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR  ZEROING=0 MASK=0 | REG0=MASK_R():w:mskw REG1=YMM_B3():r:qq:u64
VPMOVQ2M             | DATAXFER       | AVX512EVEX     | AVX512DQ_512   | EVV 0x39 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR  ZEROING=0 MASK=0 | REG0=MASK_R():w:mskw REG1=ZMM_B3():r:zu64
VPMOVQB              | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x32 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_B3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u64
VPMOVQB              | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x32 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_EIGHTHMEM() | MEM0:w:wrd:u8 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u64
VPMOVQB              | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x32 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=XMM_B3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u64
VPMOVQB              | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x32 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_EIGHTHMEM() | MEM0:w:d:u8 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u64
VPMOVQB              | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x32 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=XMM_B3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu64
VPMOVQB              | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x32 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_EIGHTHMEM() | MEM0:w:q:u8 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu64
VPMOVQD              | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x35 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_B3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u64
VPMOVQD              | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x35 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_HALFMEM() | MEM0:w:q:u32 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u64
VPMOVQD              | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x35 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=XMM_B3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u64
VPMOVQD              | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x35 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_HALFMEM() | MEM0:w:dq:u32 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u64
VPMOVQD              | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x35 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=YMM_B3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu64
VPMOVQD              | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x35 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_HALFMEM() | MEM0:w:qq:u32 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu64
VPMOVQW              | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x34 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_B3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u64
VPMOVQW              | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x34 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_QUARTERMEM() | MEM0:w:d:u16 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u64
VPMOVQW              | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x34 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=XMM_B3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u64
VPMOVQW              | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x34 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_QUARTERMEM() | MEM0:w:q:u16 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u64
VPMOVQW              | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x34 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=XMM_B3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu64
VPMOVQW              | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x34 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_QUARTERMEM() | MEM0:w:dq:u16 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu64
VPMOVSDB             | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x21 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_B3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:i32
VPMOVSDB             | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x21 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_QUARTERMEM() | MEM0:w:d:i8 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:i32
VPMOVSDB             | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x21 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=XMM_B3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:i32
VPMOVSDB             | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x21 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_QUARTERMEM() | MEM0:w:q:i8 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:i32
VPMOVSDB             | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x21 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=XMM_B3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zi32
VPMOVSDB             | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x21 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_QUARTERMEM() | MEM0:w:dq:i8 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zi32
VPMOVSDW             | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x23 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_B3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:i32
VPMOVSDW             | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x23 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_HALFMEM() | MEM0:w:q:i16 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:i32
VPMOVSDW             | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x23 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=XMM_B3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:i32
VPMOVSDW             | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x23 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_HALFMEM() | MEM0:w:dq:i16 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:i32
VPMOVSDW             | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x23 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=YMM_B3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zi32
VPMOVSDW             | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x23 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_HALFMEM() | MEM0:w:qq:i16 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zi32
VPMOVSQB             | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x22 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_B3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:i64
VPMOVSQB             | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x22 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_EIGHTHMEM() | MEM0:w:wrd:i8 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:i64
VPMOVSQB             | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x22 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=XMM_B3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:i64
VPMOVSQB             | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x22 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_EIGHTHMEM() | MEM0:w:d:i8 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:i64
VPMOVSQB             | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x22 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=XMM_B3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zi64
VPMOVSQB             | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x22 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_EIGHTHMEM() | MEM0:w:q:i8 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zi64
VPMOVSQD             | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x25 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_B3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:i64
VPMOVSQD             | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x25 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_HALFMEM() | MEM0:w:q:i32 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:i64
VPMOVSQD             | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x25 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=XMM_B3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:i64
VPMOVSQD             | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x25 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_HALFMEM() | MEM0:w:dq:i32 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:i64
VPMOVSQD             | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x25 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=YMM_B3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zi64
VPMOVSQD             | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x25 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_HALFMEM() | MEM0:w:qq:i32 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zi64
VPMOVSQW             | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x24 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_B3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:i64
VPMOVSQW             | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x24 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_QUARTERMEM() | MEM0:w:d:i16 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:i64
VPMOVSQW             | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x24 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=XMM_B3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:i64
VPMOVSQW             | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x24 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_QUARTERMEM() | MEM0:w:q:i16 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:i64
VPMOVSQW             | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x24 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=XMM_B3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zi64
VPMOVSQW             | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x24 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_QUARTERMEM() | MEM0:w:dq:i16 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zi64
VPMOVSWB             | DATAXFER       | AVX512EVEX     | AVX512BW_128   | EVV 0x20 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_B3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:i16
VPMOVSWB             | DATAXFER       | AVX512EVEX     | AVX512BW_128   | EVV 0x20 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_HALFMEM() | MEM0:w:q:i8 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:i16
VPMOVSWB             | DATAXFER       | AVX512EVEX     | AVX512BW_256   | EVV 0x20 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=XMM_B3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:i16
VPMOVSWB             | DATAXFER       | AVX512EVEX     | AVX512BW_256   | EVV 0x20 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_HALFMEM() | MEM0:w:dq:i8 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:i16
VPMOVSWB             | DATAXFER       | AVX512EVEX     | AVX512BW_512   | EVV 0x20 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=YMM_B3():w:qq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zi16
VPMOVSWB             | DATAXFER       | AVX512EVEX     | AVX512BW_512   | EVV 0x20 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_HALFMEM() | MEM0:w:qq:i8 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zi16
VPMOVSXBD            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x21 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR      | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i8
VPMOVSXBD            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x21 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128    NOEVSR  ESIZE_8_BITS() NELEM_QUARTERMEM() | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:d:i8
VPMOVSXBD            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x21 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256    NOEVSR      | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i8
VPMOVSXBD            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x21 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256    NOEVSR  ESIZE_8_BITS() NELEM_QUARTERMEM() | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:i8
VPMOVSXBD            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x21 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512    NOEVSR      | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i8
VPMOVSXBD            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x21 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512    NOEVSR  ESIZE_8_BITS() NELEM_QUARTERMEM() | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:i8
VPMOVSXBQ            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x22 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR      | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i8
VPMOVSXBQ            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x22 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128    NOEVSR  ESIZE_8_BITS() NELEM_EIGHTHMEM() | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:wrd:i8
VPMOVSXBQ            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x22 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256    NOEVSR      | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i8
VPMOVSXBQ            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x22 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256    NOEVSR  ESIZE_8_BITS() NELEM_EIGHTHMEM() | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:d:i8
VPMOVSXBQ            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x22 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512    NOEVSR      | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i8
VPMOVSXBQ            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x22 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512    NOEVSR  ESIZE_8_BITS() NELEM_EIGHTHMEM() | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:i8
VPMOVSXBW            | DATAXFER       | AVX512EVEX     | AVX512BW_128   | EVV 0x20 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR      | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i8
VPMOVSXBW            | DATAXFER       | AVX512EVEX     | AVX512BW_128   | EVV 0x20 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128    NOEVSR  ESIZE_8_BITS() NELEM_HALFMEM() | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:i8
VPMOVSXBW            | DATAXFER       | AVX512EVEX     | AVX512BW_256   | EVV 0x20 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256    NOEVSR      | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i8
VPMOVSXBW            | DATAXFER       | AVX512EVEX     | AVX512BW_256   | EVV 0x20 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256    NOEVSR  ESIZE_8_BITS() NELEM_HALFMEM() | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:i8
VPMOVSXBW            | DATAXFER       | AVX512EVEX     | AVX512BW_512   | EVV 0x20 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512    NOEVSR      | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:i8
VPMOVSXBW            | DATAXFER       | AVX512EVEX     | AVX512BW_512   | EVV 0x20 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512    NOEVSR  ESIZE_8_BITS() NELEM_HALFMEM() | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:i8
VPMOVSXDQ            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x25 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i32
VPMOVSXDQ            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x25 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALFMEM() | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:i32
VPMOVSXDQ            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x25 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i32
VPMOVSXDQ            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x25 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALFMEM() | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:i32
VPMOVSXDQ            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x25 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:i32
VPMOVSXDQ            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x25 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALFMEM() | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:i32
VPMOVSXWD            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x23 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR      | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i16
VPMOVSXWD            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x23 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128    NOEVSR  ESIZE_16_BITS() NELEM_HALFMEM() | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:i16
VPMOVSXWD            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x23 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256    NOEVSR      | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i16
VPMOVSXWD            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x23 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256    NOEVSR  ESIZE_16_BITS() NELEM_HALFMEM() | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:i16
VPMOVSXWD            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x23 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512    NOEVSR      | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:i16
VPMOVSXWD            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x23 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512    NOEVSR  ESIZE_16_BITS() NELEM_HALFMEM() | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:i16
VPMOVSXWQ            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x24 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR      | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i16
VPMOVSXWQ            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x24 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128    NOEVSR  ESIZE_16_BITS() NELEM_QUARTERMEM() | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:d:i16
VPMOVSXWQ            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x24 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256    NOEVSR      | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i16
VPMOVSXWQ            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x24 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256    NOEVSR  ESIZE_16_BITS() NELEM_QUARTERMEM() | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:i16
VPMOVSXWQ            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x24 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512    NOEVSR      | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i16
VPMOVSXWQ            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x24 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512    NOEVSR  ESIZE_16_BITS() NELEM_QUARTERMEM() | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:i16
VPMOVUSDB            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x11 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_B3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u32
VPMOVUSDB            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x11 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_QUARTERMEM() | MEM0:w:d:u8 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u32
VPMOVUSDB            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x11 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=XMM_B3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u32
VPMOVUSDB            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x11 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_QUARTERMEM() | MEM0:w:q:u8 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u32
VPMOVUSDB            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x11 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=XMM_B3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu32
VPMOVUSDB            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x11 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_QUARTERMEM() | MEM0:w:dq:u8 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu32
VPMOVUSDW            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x13 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_B3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u32
VPMOVUSDW            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x13 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_HALFMEM() | MEM0:w:q:u16 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u32
VPMOVUSDW            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x13 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=XMM_B3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u32
VPMOVUSDW            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x13 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_HALFMEM() | MEM0:w:dq:u16 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u32
VPMOVUSDW            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x13 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=YMM_B3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu32
VPMOVUSDW            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x13 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_HALFMEM() | MEM0:w:qq:u16 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu32
VPMOVUSQB            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x12 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_B3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u64
VPMOVUSQB            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x12 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_EIGHTHMEM() | MEM0:w:wrd:u8 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u64
VPMOVUSQB            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x12 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=XMM_B3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u64
VPMOVUSQB            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x12 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_EIGHTHMEM() | MEM0:w:d:u8 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u64
VPMOVUSQB            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x12 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=XMM_B3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu64
VPMOVUSQB            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x12 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_EIGHTHMEM() | MEM0:w:q:u8 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu64
VPMOVUSQD            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x15 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_B3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u64
VPMOVUSQD            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x15 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_HALFMEM() | MEM0:w:q:u32 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u64
VPMOVUSQD            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x15 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=XMM_B3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u64
VPMOVUSQD            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x15 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_HALFMEM() | MEM0:w:dq:u32 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u64
VPMOVUSQD            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x15 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=YMM_B3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu64
VPMOVUSQD            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x15 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_32_BITS() NELEM_HALFMEM() | MEM0:w:qq:u32 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu64
VPMOVUSQW            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x14 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_B3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u64
VPMOVUSQW            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x14 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_QUARTERMEM() | MEM0:w:d:u16 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u64
VPMOVUSQW            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x14 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=XMM_B3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u64
VPMOVUSQW            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x14 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_QUARTERMEM() | MEM0:w:q:u16 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u64
VPMOVUSQW            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x14 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=XMM_B3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu64
VPMOVUSQW            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x14 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_16_BITS() NELEM_QUARTERMEM() | MEM0:w:dq:u16 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu64
VPMOVUSWB            | DATAXFER       | AVX512EVEX     | AVX512BW_128   | EVV 0x10 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_B3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u16
VPMOVUSWB            | DATAXFER       | AVX512EVEX     | AVX512BW_128   | EVV 0x10 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_HALFMEM() | MEM0:w:q:u8 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u16
VPMOVUSWB            | DATAXFER       | AVX512EVEX     | AVX512BW_256   | EVV 0x10 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=XMM_B3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u16
VPMOVUSWB            | DATAXFER       | AVX512EVEX     | AVX512BW_256   | EVV 0x10 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_HALFMEM() | MEM0:w:dq:u8 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u16
VPMOVUSWB            | DATAXFER       | AVX512EVEX     | AVX512BW_512   | EVV 0x10 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=YMM_B3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu16
VPMOVUSWB            | DATAXFER       | AVX512EVEX     | AVX512BW_512   | EVV 0x10 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_HALFMEM() | MEM0:w:qq:u8 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu16
VPMOVW2M             | DATAXFER       | AVX512EVEX     | AVX512BW_128   | EVV 0x29 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR  ZEROING=0 MASK=0 | REG0=MASK_R():w:mskw REG1=XMM_B3():r:dq:u16
VPMOVW2M             | DATAXFER       | AVX512EVEX     | AVX512BW_256   | EVV 0x29 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR  ZEROING=0 MASK=0 | REG0=MASK_R():w:mskw REG1=YMM_B3():r:qq:u16
VPMOVW2M             | DATAXFER       | AVX512EVEX     | AVX512BW_512   | EVV 0x29 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR  ZEROING=0 MASK=0 | REG0=MASK_R():w:mskw REG1=ZMM_B3():r:zu16
VPMOVWB              | DATAXFER       | AVX512EVEX     | AVX512BW_128   | EVV 0x30 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_B3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_R3():r:dq:u16
VPMOVWB              | DATAXFER       | AVX512EVEX     | AVX512BW_128   | EVV 0x30 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_HALFMEM() | MEM0:w:q:u8 REG0=MASK1():r:mskw REG1=XMM_R3():r:dq:u16
VPMOVWB              | DATAXFER       | AVX512EVEX     | AVX512BW_256   | EVV 0x30 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=XMM_B3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_R3():r:qq:u16
VPMOVWB              | DATAXFER       | AVX512EVEX     | AVX512BW_256   | EVV 0x30 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_HALFMEM() | MEM0:w:dq:u8 REG0=MASK1():r:mskw REG1=YMM_R3():r:qq:u16
VPMOVWB              | DATAXFER       | AVX512EVEX     | AVX512BW_512   | EVV 0x30 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=YMM_B3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_R3():r:zu16
VPMOVWB              | DATAXFER       | AVX512EVEX     | AVX512BW_512   | EVV 0x30 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ZEROING=0  ESIZE_8_BITS() NELEM_HALFMEM() | MEM0:w:qq:u8 REG0=MASK1():r:mskw REG1=ZMM_R3():r:zu16
VPMOVZXBD            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x31 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR      | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i8
VPMOVZXBD            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x31 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128    NOEVSR  ESIZE_8_BITS() NELEM_QUARTERMEM() | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:d:i8
VPMOVZXBD            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x31 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256    NOEVSR      | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i8
VPMOVZXBD            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x31 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256    NOEVSR  ESIZE_8_BITS() NELEM_QUARTERMEM() | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:i8
VPMOVZXBD            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x31 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512    NOEVSR      | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i8
VPMOVZXBD            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x31 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512    NOEVSR  ESIZE_8_BITS() NELEM_QUARTERMEM() | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:i8
VPMOVZXBQ            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x32 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR      | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i8
VPMOVZXBQ            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x32 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128    NOEVSR  ESIZE_8_BITS() NELEM_EIGHTHMEM() | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:wrd:i8
VPMOVZXBQ            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x32 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256    NOEVSR      | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i8
VPMOVZXBQ            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x32 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256    NOEVSR  ESIZE_8_BITS() NELEM_EIGHTHMEM() | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:d:i8
VPMOVZXBQ            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x32 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512    NOEVSR      | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i8
VPMOVZXBQ            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x32 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512    NOEVSR  ESIZE_8_BITS() NELEM_EIGHTHMEM() | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:i8
VPMOVZXBW            | DATAXFER       | AVX512EVEX     | AVX512BW_128   | EVV 0x30 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR      | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i8
VPMOVZXBW            | DATAXFER       | AVX512EVEX     | AVX512BW_128   | EVV 0x30 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128    NOEVSR  ESIZE_8_BITS() NELEM_HALFMEM() | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:i8
VPMOVZXBW            | DATAXFER       | AVX512EVEX     | AVX512BW_256   | EVV 0x30 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256    NOEVSR      | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i8
VPMOVZXBW            | DATAXFER       | AVX512EVEX     | AVX512BW_256   | EVV 0x30 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256    NOEVSR  ESIZE_8_BITS() NELEM_HALFMEM() | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:i8
VPMOVZXBW            | DATAXFER       | AVX512EVEX     | AVX512BW_512   | EVV 0x30 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512    NOEVSR      | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:i8
VPMOVZXBW            | DATAXFER       | AVX512EVEX     | AVX512BW_512   | EVV 0x30 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512    NOEVSR  ESIZE_8_BITS() NELEM_HALFMEM() | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:i8
VPMOVZXDQ            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x35 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i32
VPMOVZXDQ            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x35 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALFMEM() | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:i32
VPMOVZXDQ            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x35 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i32
VPMOVZXDQ            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x35 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALFMEM() | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:i32
VPMOVZXDQ            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x35 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:i32
VPMOVZXDQ            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x35 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_HALFMEM() | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:i32
VPMOVZXWD            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x33 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR      | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i16
VPMOVZXWD            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x33 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128    NOEVSR  ESIZE_16_BITS() NELEM_HALFMEM() | REG0=XMM_R3():w:dq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:i16
VPMOVZXWD            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x33 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256    NOEVSR      | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i16
VPMOVZXWD            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x33 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256    NOEVSR  ESIZE_16_BITS() NELEM_HALFMEM() | REG0=YMM_R3():w:qq:i32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:i16
VPMOVZXWD            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x33 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512    NOEVSR      | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:i16
VPMOVZXWD            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x33 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512    NOEVSR  ESIZE_16_BITS() NELEM_HALFMEM() | REG0=ZMM_R3():w:zi32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:i16
VPMOVZXWQ            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x34 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR      | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i16
VPMOVZXWQ            | DATAXFER       | AVX512EVEX     | AVX512F_128    | EVV 0x34 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128    NOEVSR  ESIZE_16_BITS() NELEM_QUARTERMEM() | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:d:i16
VPMOVZXWQ            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x34 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256    NOEVSR      | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i16
VPMOVZXWQ            | DATAXFER       | AVX512EVEX     | AVX512F_256    | EVV 0x34 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256    NOEVSR  ESIZE_16_BITS() NELEM_QUARTERMEM() | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:q:i16
VPMOVZXWQ            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x34 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512    NOEVSR      | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:i16
VPMOVZXWQ            | DATAXFER       | AVX512EVEX     | AVX512F_512    | EVV 0x34 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512    NOEVSR  ESIZE_16_BITS() NELEM_QUARTERMEM() | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:i16
VPMULDQ              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x28 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i64 REG3=XMM_B3():r:dq:i64
VPMULDQ              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x28 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i64 MEM0:r:vv:i64:TXT=BCASTSTR
VPMULDQ              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x28 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i64 REG3=YMM_B3():r:qq:i64
VPMULDQ              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x28 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:i64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i64 MEM0:r:vv:i64:TXT=BCASTSTR
VPMULDQ              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x28 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi64 REG3=ZMM_B3():r:zi64
VPMULDQ              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x28 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zi64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi64 MEM0:r:vv:i64:TXT=BCASTSTR
VPMULHRSW            | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x0B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 REG3=XMM_B3():r:dq:i16
VPMULHRSW            | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x0B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 MEM0:r:dq:i16
VPMULHRSW            | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x0B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 REG3=YMM_B3():r:qq:i16
VPMULHRSW            | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x0B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 MEM0:r:qq:i16
VPMULHRSW            | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x0B V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 REG3=ZMM_B3():r:zi16
VPMULHRSW            | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x0B V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 MEM0:r:zd:i16
VPMULHUW             | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xE4 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                  | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
VPMULHUW             | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xE4 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
VPMULHUW             | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xE4 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                  | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
VPMULHUW             | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xE4 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
VPMULHUW             | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xE4 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                  | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
VPMULHUW             | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xE4 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
VPMULHW              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xE5 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                  | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
VPMULHW              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xE5 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
VPMULHW              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xE5 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                  | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
VPMULHW              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xE5 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
VPMULHW              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xE5 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                  | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
VPMULHW              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xE5 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
VPMULLD              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x40 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
VPMULLD              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x40 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPMULLD              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x40 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
VPMULLD              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x40 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPMULLD              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x40 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
VPMULLD              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x40 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
VPMULLQ              | AVX512         | AVX512EVEX     | AVX512DQ_128   | EVV 0x40 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
VPMULLQ              | AVX512         | AVX512EVEX     | AVX512DQ_128   | EVV 0x40 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPMULLQ              | AVX512         | AVX512EVEX     | AVX512DQ_256   | EVV 0x40 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
VPMULLQ              | AVX512         | AVX512EVEX     | AVX512DQ_256   | EVV 0x40 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPMULLQ              | AVX512         | AVX512EVEX     | AVX512DQ_512   | EVV 0x40 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
VPMULLQ              | AVX512         | AVX512EVEX     | AVX512DQ_512   | EVV 0x40 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
VPMULLW              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xD5 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                  | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
VPMULLW              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xD5 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
VPMULLW              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xD5 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                  | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
VPMULLW              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xD5 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
VPMULLW              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xD5 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                  | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
VPMULLW              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xD5 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
VPMULTISHIFTQB       | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_128 | EVV 0x83 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u64
VPMULTISHIFTQB       | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_128 | EVV 0x83 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:vv:u64:TXT=BCASTSTR
VPMULTISHIFTQB       | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_256 | EVV 0x83 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u64
VPMULTISHIFTQB       | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_256 | EVV 0x83 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:vv:u64:TXT=BCASTSTR
VPMULTISHIFTQB       | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_512 | EVV 0x83 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu64
VPMULTISHIFTQB       | AVX512_VBMI    | AVX512EVEX     | AVX512_VBMI_512 | EVV 0x83 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:vv:u64:TXT=BCASTSTR
VPMULUDQ             | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0xF4 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1              | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
VPMULUDQ             | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0xF4 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPMULUDQ             | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0xF4 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1              | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
VPMULUDQ             | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0xF4 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPMULUDQ             | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0xF4 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1              | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
VPMULUDQ             | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0xF4 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
VPOPCNTB             | AVX512         | AVX512EVEX     | AVX512_BITALG_128 | EVV 0x54 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u8
VPOPCNTB             | AVX512         | AVX512EVEX     | AVX512_BITALG_128 | EVV 0x54 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0  NOEVSR  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u8
VPOPCNTB             | AVX512         | AVX512EVEX     | AVX512_BITALG_256 | EVV 0x54 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u8
VPOPCNTB             | AVX512         | AVX512EVEX     | AVX512_BITALG_256 | EVV 0x54 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0  NOEVSR  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u8
VPOPCNTB             | AVX512_BITALG  | AVX512EVEX     | AVX512_BITALG_512 | EVV 0x54 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu8
VPOPCNTB             | AVX512_BITALG  | AVX512EVEX     | AVX512_BITALG_512 | EVV 0x54 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0  NOEVSR  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:u8
VPOPCNTD             | AVX512         | AVX512EVEX     | AVX512_VPOPCNTDQ_128 | EVV 0x55 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32
VPOPCNTD             | AVX512         | AVX512EVEX     | AVX512_VPOPCNTDQ_128 | EVV 0x55 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR
VPOPCNTD             | AVX512         | AVX512EVEX     | AVX512_VPOPCNTDQ_256 | EVV 0x55 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u32
VPOPCNTD             | AVX512         | AVX512EVEX     | AVX512_VPOPCNTDQ_256 | EVV 0x55 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR
VPOPCNTD             | AVX512         | AVX512EVEX     | AVX512_VPOPCNTDQ_512 | EVV 0x55 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu32
VPOPCNTD             | AVX512         | AVX512EVEX     | AVX512_VPOPCNTDQ_512 | EVV 0x55 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR
VPOPCNTQ             | AVX512         | AVX512EVEX     | AVX512_VPOPCNTDQ_128 | EVV 0x55 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR    | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u64
VPOPCNTQ             | AVX512         | AVX512EVEX     | AVX512_VPOPCNTDQ_128 | EVV 0x55 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR
VPOPCNTQ             | AVX512         | AVX512EVEX     | AVX512_VPOPCNTDQ_256 | EVV 0x55 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR    | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u64
VPOPCNTQ             | AVX512         | AVX512EVEX     | AVX512_VPOPCNTDQ_256 | EVV 0x55 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR
VPOPCNTQ             | AVX512         | AVX512EVEX     | AVX512_VPOPCNTDQ_512 | EVV 0x55 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR    | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu64
VPOPCNTQ             | AVX512         | AVX512EVEX     | AVX512_VPOPCNTDQ_512 | EVV 0x55 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR
VPOPCNTW             | AVX512         | AVX512EVEX     | AVX512_BITALG_128 | EVV 0x54 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR    | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u16
VPOPCNTW             | AVX512         | AVX512EVEX     | AVX512_BITALG_128 | EVV 0x54 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1  NOEVSR  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u16
VPOPCNTW             | AVX512         | AVX512EVEX     | AVX512_BITALG_256 | EVV 0x54 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR    | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u16
VPOPCNTW             | AVX512         | AVX512EVEX     | AVX512_BITALG_256 | EVV 0x54 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1  NOEVSR  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u16
VPOPCNTW             | AVX512_BITALG  | AVX512EVEX     | AVX512_BITALG_512 | EVV 0x54 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR    | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu16
VPOPCNTW             | AVX512_BITALG  | AVX512EVEX     | AVX512_BITALG_512 | EVV 0x54 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1  NOEVSR  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:u16
VPORD                | LOGICAL        | AVX512EVEX     | AVX512F_128    | EVV 0xEB V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0              | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
VPORD                | LOGICAL        | AVX512EVEX     | AVX512F_128    | EVV 0xEB V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPORD                | LOGICAL        | AVX512EVEX     | AVX512F_256    | EVV 0xEB V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0              | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
VPORD                | LOGICAL        | AVX512EVEX     | AVX512F_256    | EVV 0xEB V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPORD                | LOGICAL        | AVX512EVEX     | AVX512F_512    | EVV 0xEB V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0              | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
VPORD                | LOGICAL        | AVX512EVEX     | AVX512F_512    | EVV 0xEB V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
VPORQ                | LOGICAL        | AVX512EVEX     | AVX512F_128    | EVV 0xEB V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1              | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
VPORQ                | LOGICAL        | AVX512EVEX     | AVX512F_128    | EVV 0xEB V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPORQ                | LOGICAL        | AVX512EVEX     | AVX512F_256    | EVV 0xEB V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1              | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
VPORQ                | LOGICAL        | AVX512EVEX     | AVX512F_256    | EVV 0xEB V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPORQ                | LOGICAL        | AVX512EVEX     | AVX512F_512    | EVV 0xEB V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1              | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
VPORQ                | LOGICAL        | AVX512EVEX     | AVX512F_512    | EVV 0xEB V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
VPPERM               | XOP            | XOP            | XOP            | XOPV 0xA3 VNP W0 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()  | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16 REG2=XMM_SE():r:dq:i16
VPPERM               | XOP            | XOP            | XOP            | XOPV 0xA3 VNP W0 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()         | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16 REG3=XMM_SE():r:dq:i16
VPPERM               | XOP            | XOP            | XOP            | XOPV 0xA3 VNP W1 VL128  XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() SE_IMM8()  | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_SE():r:dq:i16 MEM0:r:dq:i16
VPPERM               | XOP            | XOP            | XOP            | XOPV 0xA3 VNP W1 VL128  XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] SE_IMM8()         | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_SE():r:dq:i16 REG3=XMM_B():r:dq:i16
VPREFETCH0           | PREFETCH       | KNC            | KNCV           | VV1 0x18  VL128 VNP V0F  NOVSR MOD[mm] MOD!=3 REG[0b001] RM[nnn]  MODRM()        | MEM0:r:mprefetch
VPREFETCH0_EVEX      | PREFETCH       | KNCE           | KNCE           | KVV 0x18  VL512 VNP V0F  NOEVSR MOD[mm] MOD!=3 REG[0b001] RM[nnn]  MODRM() NOSWIZD() | MEM0:r:mprefetch
VPREFETCH1           | PREFETCH       | KNC            | KNCV           | VV1 0x18  VL128 VNP V0F  NOVSR MOD[mm] MOD!=3 REG[0b010] RM[nnn]  MODRM()        | MEM0:r:mprefetch
VPREFETCH1_EVEX      | PREFETCH       | KNCE           | KNCE           | KVV 0x18  VL512 VNP V0F  NOEVSR MOD[mm] MOD!=3 REG[0b010] RM[nnn]  MODRM() NOSWIZD() | MEM0:r:mprefetch
VPREFETCH2           | PREFETCH       | KNC            | KNCV           | VV1 0x18  VL128 VNP V0F  NOVSR MOD[mm] MOD!=3 REG[0b011] RM[nnn]  MODRM()        | MEM0:r:mprefetch
VPREFETCH2_EVEX      | PREFETCH       | KNCE           | KNCE           | KVV 0x18  VL512 VNP V0F  NOEVSR MOD[mm] MOD!=3 REG[0b011] RM[nnn]  MODRM() NOSWIZD() | MEM0:r:mprefetch
VPREFETCHE0          | PREFETCH       | KNC            | KNCV           | VV1 0x18  VL128 VNP V0F  NOVSR MOD[mm] MOD!=3 REG[0b101] RM[nnn]  MODRM()        | MEM0:r:mprefetch
VPREFETCHE0_EVEX     | PREFETCH       | KNCE           | KNCE           | KVV 0x18  VL512 VNP V0F  NOEVSR MOD[mm] MOD!=3 REG[0b101] RM[nnn]  MODRM() NOSWIZD() | MEM0:r:mprefetch
VPREFETCHE1          | PREFETCH       | KNC            | KNCV           | VV1 0x18  VL128 VNP V0F  NOVSR MOD[mm] MOD!=3 REG[0b110] RM[nnn]  MODRM()        | MEM0:r:mprefetch
VPREFETCHE1_EVEX     | PREFETCH       | KNCE           | KNCE           | KVV 0x18  VL512 VNP V0F  NOEVSR MOD[mm] MOD!=3 REG[0b110] RM[nnn]  MODRM() NOSWIZD() | MEM0:r:mprefetch
VPREFETCHE2          | PREFETCH       | KNC            | KNCV           | VV1 0x18  VL128 VNP V0F  NOVSR MOD[mm] MOD!=3 REG[0b111] RM[nnn]  MODRM()        | MEM0:r:mprefetch
VPREFETCHE2_EVEX     | PREFETCH       | KNCE           | KNCE           | KVV 0x18  VL512 VNP V0F  NOEVSR MOD[mm] MOD!=3 REG[0b111] RM[nnn]  MODRM() NOSWIZD() | MEM0:r:mprefetch
VPREFETCHENTA        | PREFETCH       | KNC            | KNCV           | VV1 0x18  VL128 VNP V0F  NOVSR MOD[mm] MOD!=3 REG[0b100] RM[nnn]  MODRM()        | MEM0:r:mprefetch
VPREFETCHENTA_EVEX   | PREFETCH       | KNCE           | KNCE           | KVV 0x18  VL512 VNP V0F  NOEVSR MOD[mm] MOD!=3 REG[0b100] RM[nnn]  MODRM() NOSWIZD() | MEM0:r:mprefetch
VPREFETCHNTA         | PREFETCH       | KNC            | KNCV           | VV1 0x18  VL128 VNP V0F  NOVSR MOD[mm] MOD!=3 REG[0b000] RM[nnn]  MODRM()        | MEM0:r:mprefetch
VPREFETCHNTA_EVEX    | PREFETCH       | KNCE           | KNCE           | KVV 0x18  VL512 VNP V0F  NOEVSR MOD[mm] MOD!=3 REG[0b000] RM[nnn]  MODRM() NOSWIZD() | MEM0:r:mprefetch
VPROLD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b001] RM[nnn]  VL128  W0   UIMM8()  | REG0=XMM_N3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32 IMM0:r:b
VPROLD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b001] RM[nnn]  MODRM()  VL128  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_N3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
VPROLD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b001] RM[nnn]  VL256  W0   UIMM8()  | REG0=YMM_N3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u32 IMM0:r:b
VPROLD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b001] RM[nnn]  MODRM()  VL256  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_N3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
VPROLD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b001] RM[nnn]  VL512  W0   UIMM8()  | REG0=ZMM_N3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu32 IMM0:r:b
VPROLD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b001] RM[nnn]  MODRM()  VL512  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_N3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
VPROLQ               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b001] RM[nnn]  VL128  W1   UIMM8()  | REG0=XMM_N3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u64 IMM0:r:b
VPROLQ               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b001] RM[nnn]  MODRM()  VL128  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_N3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VPROLQ               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b001] RM[nnn]  VL256  W1   UIMM8()  | REG0=YMM_N3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u64 IMM0:r:b
VPROLQ               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b001] RM[nnn]  MODRM()  VL256  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_N3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VPROLQ               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b001] RM[nnn]  VL512  W1   UIMM8()  | REG0=ZMM_N3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu64 IMM0:r:b
VPROLQ               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b001] RM[nnn]  MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_N3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VPROLVD              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x15 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
VPROLVD              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x15 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPROLVD              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x15 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
VPROLVD              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x15 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPROLVD              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x15 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
VPROLVD              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x15 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
VPROLVQ              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x15 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
VPROLVQ              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x15 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPROLVQ              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x15 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
VPROLVQ              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x15 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPROLVQ              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x15 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
VPROLVQ              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x15 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
VPRORD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b000] RM[nnn]  VL128  W0   UIMM8()  | REG0=XMM_N3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32 IMM0:r:b
VPRORD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b000] RM[nnn]  MODRM()  VL128  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_N3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
VPRORD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b000] RM[nnn]  VL256  W0   UIMM8()  | REG0=YMM_N3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u32 IMM0:r:b
VPRORD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b000] RM[nnn]  MODRM()  VL256  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_N3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
VPRORD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b000] RM[nnn]  VL512  W0   UIMM8()  | REG0=ZMM_N3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu32 IMM0:r:b
VPRORD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b000] RM[nnn]  MODRM()  VL512  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_N3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
VPRORQ               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b000] RM[nnn]  VL128  W1   UIMM8()  | REG0=XMM_N3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u64 IMM0:r:b
VPRORQ               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b000] RM[nnn]  MODRM()  VL128  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_N3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VPRORQ               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b000] RM[nnn]  VL256  W1   UIMM8()  | REG0=YMM_N3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u64 IMM0:r:b
VPRORQ               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b000] RM[nnn]  MODRM()  VL256  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_N3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VPRORQ               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b000] RM[nnn]  VL512  W1   UIMM8()  | REG0=ZMM_N3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu64 IMM0:r:b
VPRORQ               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b000] RM[nnn]  MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_N3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VPRORVD              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x14 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
VPRORVD              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x14 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPRORVD              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x14 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
VPRORVD              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x14 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPRORVD              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x14 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
VPRORVD              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x14 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
VPRORVQ              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x14 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
VPRORVQ              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x14 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPRORVQ              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x14 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
VPRORVQ              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x14 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPRORVQ              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x14 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
VPRORVQ              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x14 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
VPROTB               | XOP            | XOP            | XOP            | XOPV 0xC0 VNP W0 VL128 NOVSR XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() | REG0=XMM_R():w:dq:u8 MEM0:r:dq:u8 IMM0:r:b:u8
VPROTB               | XOP            | XOP            | XOP            | XOPV 0xC0 VNP W0 VL128 NOVSR XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()      | REG0=XMM_R():w:dq:u8 REG1=XMM_B():r:dq:u8 IMM0:r:b:u8
VPROTB               | XOP            | XOP            | XOP            | XOPV 0x90 VNP W0 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()            | REG0=XMM_R():w:dq:u8 MEM0:r:dq:u8 REG1=XMM_N():r:dq:u8
VPROTB               | XOP            | XOP            | XOP            | XOPV 0x90 VNP W0 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                   | REG0=XMM_R():w:dq:u8 REG1=XMM_B():r:dq:u8 REG2=XMM_N():r:dq:u8
VPROTB               | XOP            | XOP            | XOP            | XOPV 0x90 VNP W1 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()            | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 MEM0:r:dq:u8
VPROTB               | XOP            | XOP            | XOP            | XOPV 0x90 VNP W1 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                   | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:u8
VPROTD               | XOP            | XOP            | XOP            | XOPV 0xC2 VNP W0 VL128 NOVSR XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() | REG0=XMM_R():w:dq:u32 MEM0:r:dq:u32 IMM0:r:b:u8
VPROTD               | XOP            | XOP            | XOP            | XOPV 0xC2 VNP W0 VL128 NOVSR XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()      | REG0=XMM_R():w:dq:u32 REG1=XMM_B():r:dq:u32 IMM0:r:b:u8
VPROTD               | XOP            | XOP            | XOP            | XOPV 0x92 VNP W0 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()            | REG0=XMM_R():w:dq:u32 MEM0:r:dq:u32 REG1=XMM_N():r:dq:u32
VPROTD               | XOP            | XOP            | XOP            | XOPV 0x92 VNP W0 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                   | REG0=XMM_R():w:dq:u32 REG1=XMM_B():r:dq:u32 REG2=XMM_N():r:dq:u32
VPROTD               | XOP            | XOP            | XOP            | XOPV 0x92 VNP W1 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()            | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 MEM0:r:dq:u32
VPROTD               | XOP            | XOP            | XOP            | XOPV 0x92 VNP W1 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                   | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 REG2=XMM_B():r:dq:u32
VPROTQ               | XOP            | XOP            | XOP            | XOPV 0xC3 VNP W0 VL128 NOVSR XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() | REG0=XMM_R():w:dq:u64 MEM0:r:dq:u64 IMM0:r:b:u8
VPROTQ               | XOP            | XOP            | XOP            | XOPV 0xC3 VNP W0 VL128 NOVSR XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()      | REG0=XMM_R():w:dq:u64 REG1=XMM_B():r:dq:u64 IMM0:r:b:u8
VPROTQ               | XOP            | XOP            | XOP            | XOPV 0x93 VNP W0 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()            | REG0=XMM_R():w:dq:u64 MEM0:r:dq:u64 REG1=XMM_N():r:dq:u64
VPROTQ               | XOP            | XOP            | XOP            | XOPV 0x93 VNP W0 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                   | REG0=XMM_R():w:dq:u64 REG1=XMM_B():r:dq:u64 REG2=XMM_N():r:dq:u64
VPROTQ               | XOP            | XOP            | XOP            | XOPV 0x93 VNP W1 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()            | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 MEM0:r:dq:u64
VPROTQ               | XOP            | XOP            | XOP            | XOPV 0x93 VNP W1 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                   | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 REG2=XMM_B():r:dq:u64
VPROTW               | XOP            | XOP            | XOP            | XOPV 0xC1 VNP W0 VL128 NOVSR XMAP8 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() | REG0=XMM_R():w:dq:u16 MEM0:r:dq:u16 IMM0:r:b:u8
VPROTW               | XOP            | XOP            | XOP            | XOPV 0xC1 VNP W0 VL128 NOVSR XMAP8 MOD[0b11] MOD=3 REG[rrr] RM[nnn] UIMM8()      | REG0=XMM_R():w:dq:u16 REG1=XMM_B():r:dq:u16 IMM0:r:b:u8
VPROTW               | XOP            | XOP            | XOP            | XOPV 0x91 VNP W0 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()            | REG0=XMM_R():w:dq:u16 MEM0:r:dq:u16 REG1=XMM_N():r:dq:u16
VPROTW               | XOP            | XOP            | XOP            | XOPV 0x91 VNP W0 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                   | REG0=XMM_R():w:dq:u16 REG1=XMM_B():r:dq:u16 REG2=XMM_N():r:dq:u16
VPROTW               | XOP            | XOP            | XOP            | XOPV 0x91 VNP W1 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()            | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 MEM0:r:dq:u16
VPROTW               | XOP            | XOP            | XOP            | XOPV 0x91 VNP W1 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                   | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 REG2=XMM_B():r:dq:u16
VPSADBW              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xF6 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128      ZEROING=0 MASK=0 | REG0=XMM_R3():w:dq:u16 REG1=XMM_N3():r:dq:u8 REG2=XMM_B3():r:dq:u8
VPSADBW              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xF6 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ZEROING=0 MASK=0  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=XMM_N3():r:dq:u8 MEM0:r:dq:u8
VPSADBW              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xF6 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256      ZEROING=0 MASK=0 | REG0=YMM_R3():w:qq:u16 REG1=YMM_N3():r:qq:u8 REG2=YMM_B3():r:qq:u8
VPSADBW              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xF6 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ZEROING=0 MASK=0  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=YMM_N3():r:qq:u8 MEM0:r:qq:u8
VPSADBW              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xF6 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512      ZEROING=0 MASK=0 | REG0=ZMM_R3():w:zu16 REG1=ZMM_N3():r:zu8 REG2=ZMM_B3():r:zu8
VPSADBW              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xF6 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ZEROING=0 MASK=0  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=ZMM_N3():r:zu8 MEM0:r:zd:u8
VPSCATTERDD          | KNC            | KNCE           | KNCE           | KVV 0xA0 V66 V0F38  W0 NO_SPARSE_EVSR MOD[mm] MOD!=3 REG[rrr] RM[0b100] KNC_VMODRM() DNCONVERT_INT32() | MEM0:w:zv:TXT=NT:TXT=CONVERT  REG1=MASK1():rw:mskw   REG0=ZMM_R3():r:zd NELEM=1:SUPP
VPSCATTERDD          | SCATTER        | AVX512EVEX     | AVX512F_128    | EVV 0xA0 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL128  W0 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:w:d:u32 REG0=MASKNOT0():rw:mskw REG1=XMM_R3():r:dq:u32
VPSCATTERDD          | SCATTER        | AVX512EVEX     | AVX512F_256    | EVV 0xA0 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL256  W0 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:w:d:u32 REG0=MASKNOT0():rw:mskw REG1=YMM_R3():r:qq:u32
VPSCATTERDD          | SCATTER        | AVX512EVEX     | AVX512F_512    | EVV 0xA0 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL512  W0 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:w:d:u32 REG0=MASKNOT0():rw:mskw REG1=ZMM_R3():r:zu32
VPSCATTERDQ          | KNC            | KNCE           | KNCE           | KVV 0xA0 V66 V0F38  W1 NO_SPARSE_EVSR MOD[mm] MOD!=3 REG[rrr] RM[0b100] KNC_VMODRM() DNCONVERT_INT64() | MEM0:w:zv:TXT=NT:TXT=CONVERT  REG1=MASK1():rw:mskw   REG0=ZMM_R3():r:zq NELEM=1:SUPP
VPSCATTERDQ          | SCATTER        | AVX512EVEX     | AVX512F_128    | EVV 0xA0 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL128  W1 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:w:q:u64 REG0=MASKNOT0():rw:mskw REG1=XMM_R3():r:dq:u64
VPSCATTERDQ          | SCATTER        | AVX512EVEX     | AVX512F_256    | EVV 0xA0 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL256  W1 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:w:q:u64 REG0=MASKNOT0():rw:mskw REG1=YMM_R3():r:qq:u64
VPSCATTERDQ          | SCATTER        | AVX512EVEX     | AVX512F_512    | EVV 0xA0 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL512  W1 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:w:q:u64 REG0=MASKNOT0():rw:mskw REG1=ZMM_R3():r:zu64
VPSCATTERQD          | SCATTER        | AVX512EVEX     | AVX512F_128    | EVV 0xA1 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL128  W0 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:w:d:u32 REG0=MASKNOT0():rw:mskw REG1=XMM_R3():r:dq:u32
VPSCATTERQD          | SCATTER        | AVX512EVEX     | AVX512F_256    | EVV 0xA1 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL256  W0 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:w:d:u32 REG0=MASKNOT0():rw:mskw REG1=XMM_R3():r:dq:u32
VPSCATTERQD          | SCATTER        | AVX512EVEX     | AVX512F_512    | EVV 0xA1 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL512  W0 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:w:d:u32 REG0=MASKNOT0():rw:mskw REG1=YMM_R3():r:qq:u32
VPSCATTERQQ          | SCATTER        | AVX512EVEX     | AVX512F_128    | EVV 0xA1 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL128  W1 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:w:q:u64 REG0=MASKNOT0():rw:mskw REG1=XMM_R3():r:dq:u64
VPSCATTERQQ          | SCATTER        | AVX512EVEX     | AVX512F_256    | EVV 0xA1 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL256  W1 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:w:q:u64 REG0=MASKNOT0():rw:mskw REG1=YMM_R3():r:qq:u64
VPSCATTERQQ          | SCATTER        | AVX512EVEX     | AVX512F_512    | EVV 0xA1 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL512  W1 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:w:q:u64 REG0=MASKNOT0():rw:mskw REG1=ZMM_R3():r:zu64
VPSHAB               | XOP            | XOP            | XOP            | XOPV 0x98 VNP W0 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()            | REG0=XMM_R():w:dq:i8 MEM0:r:dq:i8 REG1=XMM_N():r:dq:i8
VPSHAB               | XOP            | XOP            | XOP            | XOPV 0x98 VNP W0 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                   | REG0=XMM_R():w:dq:i8 REG1=XMM_B():r:dq:i8 REG2=XMM_N():r:dq:i8
VPSHAB               | XOP            | XOP            | XOP            | XOPV 0x98 VNP W1 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()            | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 MEM0:r:dq:i8
VPSHAB               | XOP            | XOP            | XOP            | XOPV 0x98 VNP W1 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                   | REG0=XMM_R():w:dq:i8 REG1=XMM_N():r:dq:i8 REG2=XMM_B():r:dq:i8
VPSHAD               | XOP            | XOP            | XOP            | XOPV 0x9A VNP W0 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()            | REG0=XMM_R():w:dq:i32 MEM0:r:dq:i32 REG1=XMM_N():r:dq:i32
VPSHAD               | XOP            | XOP            | XOP            | XOPV 0x9A VNP W0 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                   | REG0=XMM_R():w:dq:i32 REG1=XMM_B():r:dq:i32 REG2=XMM_N():r:dq:i32
VPSHAD               | XOP            | XOP            | XOP            | XOPV 0x9A VNP W1 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()            | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 MEM0:r:dq:i32
VPSHAD               | XOP            | XOP            | XOP            | XOPV 0x9A VNP W1 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                   | REG0=XMM_R():w:dq:i32 REG1=XMM_N():r:dq:i32 REG2=XMM_B():r:dq:i32
VPSHAQ               | XOP            | XOP            | XOP            | XOPV 0x9B VNP W0 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()            | REG0=XMM_R():w:dq:i64 MEM0:r:dq:i64 REG1=XMM_N():r:dq:i64
VPSHAQ               | XOP            | XOP            | XOP            | XOPV 0x9B VNP W0 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                   | REG0=XMM_R():w:dq:i64 REG1=XMM_B():r:dq:i64 REG2=XMM_N():r:dq:i64
VPSHAQ               | XOP            | XOP            | XOP            | XOPV 0x9B VNP W1 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()            | REG0=XMM_R():w:dq:i64 REG1=XMM_N():r:dq:i64 MEM0:r:dq:i64
VPSHAQ               | XOP            | XOP            | XOP            | XOPV 0x9B VNP W1 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                   | REG0=XMM_R():w:dq:i64 REG1=XMM_N():r:dq:i64 REG2=XMM_B():r:dq:i64
VPSHAW               | XOP            | XOP            | XOP            | XOPV 0x99 VNP W0 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()            | REG0=XMM_R():w:dq:i16 MEM0:r:dq:i16 REG1=XMM_N():r:dq:i16
VPSHAW               | XOP            | XOP            | XOP            | XOPV 0x99 VNP W0 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                   | REG0=XMM_R():w:dq:i16 REG1=XMM_B():r:dq:i16 REG2=XMM_N():r:dq:i16
VPSHAW               | XOP            | XOP            | XOP            | XOPV 0x99 VNP W1 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()            | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 MEM0:r:dq:i16
VPSHAW               | XOP            | XOP            | XOP            | XOPV 0x99 VNP W1 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                   | REG0=XMM_R():w:dq:i16 REG1=XMM_N():r:dq:i16 REG2=XMM_B():r:dq:i16
VPSHLB               | XOP            | XOP            | XOP            | XOPV 0x94 VNP W0 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()            | REG0=XMM_R():w:dq:u8 MEM0:r:dq:u8 REG1=XMM_N():r:dq:u8
VPSHLB               | XOP            | XOP            | XOP            | XOPV 0x94 VNP W0 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                   | REG0=XMM_R():w:dq:u8 REG1=XMM_B():r:dq:u8 REG2=XMM_N():r:dq:u8
VPSHLB               | XOP            | XOP            | XOP            | XOPV 0x94 VNP W1 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()            | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 MEM0:r:dq:u8
VPSHLB               | XOP            | XOP            | XOP            | XOPV 0x94 VNP W1 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                   | REG0=XMM_R():w:dq:u8 REG1=XMM_N():r:dq:u8 REG2=XMM_B():r:dq:u8
VPSHLD               | XOP            | XOP            | XOP            | XOPV 0x96 VNP W0 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()            | REG0=XMM_R():w:dq:u32 MEM0:r:dq:u32 REG1=XMM_N():r:dq:u32
VPSHLD               | XOP            | XOP            | XOP            | XOPV 0x96 VNP W0 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                   | REG0=XMM_R():w:dq:u32 REG1=XMM_B():r:dq:u32 REG2=XMM_N():r:dq:u32
VPSHLD               | XOP            | XOP            | XOP            | XOPV 0x96 VNP W1 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()            | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 MEM0:r:dq:u32
VPSHLD               | XOP            | XOP            | XOP            | XOPV 0x96 VNP W1 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                   | REG0=XMM_R():w:dq:u32 REG1=XMM_N():r:dq:u32 REG2=XMM_B():r:dq:u32
VPSHLDD              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | EVV 0x71 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0   UIMM8()  | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32 IMM0:r:b
VPSHLDD              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | EVV 0x71 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
VPSHLDD              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | EVV 0x71 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0   UIMM8()  | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32 IMM0:r:b
VPSHLDD              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | EVV 0x71 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
VPSHLDD              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | EVV 0x71 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0   UIMM8()  | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32 IMM0:r:b
VPSHLDD              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | EVV 0x71 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
VPSHLDQ              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | EVV 0x71 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1   UIMM8()  | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64 IMM0:r:b
VPSHLDQ              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | EVV 0x71 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VPSHLDQ              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | EVV 0x71 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1   UIMM8()  | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64 IMM0:r:b
VPSHLDQ              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | EVV 0x71 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VPSHLDQ              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | EVV 0x71 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1   UIMM8()  | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64 IMM0:r:b
VPSHLDQ              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | EVV 0x71 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VPSHLDVD             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | EVV 0x71 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():rw:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
VPSHLDVD             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | EVV 0x71 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPSHLDVD             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | EVV 0x71 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():rw:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
VPSHLDVD             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | EVV 0x71 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPSHLDVD             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | EVV 0x71 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():rw:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
VPSHLDVD             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | EVV 0x71 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
VPSHLDVQ             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | EVV 0x71 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():rw:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
VPSHLDVQ             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | EVV 0x71 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPSHLDVQ             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | EVV 0x71 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():rw:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
VPSHLDVQ             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | EVV 0x71 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPSHLDVQ             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | EVV 0x71 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():rw:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
VPSHLDVQ             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | EVV 0x71 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
VPSHLDVW             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | EVV 0x70 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():rw:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
VPSHLDVW             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | EVV 0x70 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():rw:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
VPSHLDVW             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | EVV 0x70 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():rw:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
VPSHLDVW             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | EVV 0x70 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():rw:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
VPSHLDVW             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | EVV 0x70 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():rw:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
VPSHLDVW             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | EVV 0x70 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():rw:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
VPSHLDW              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | EVV 0x70 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1   UIMM8()  | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16 IMM0:r:b
VPSHLDW              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | EVV 0x70 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1   UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16 IMM0:r:b
VPSHLDW              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | EVV 0x70 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1   UIMM8()  | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16 IMM0:r:b
VPSHLDW              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | EVV 0x70 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1   UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16 IMM0:r:b
VPSHLDW              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | EVV 0x70 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1   UIMM8()  | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16 IMM0:r:b
VPSHLDW              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | EVV 0x70 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1   UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16 IMM0:r:b
VPSHLQ               | XOP            | XOP            | XOP            | XOPV 0x97 VNP W0 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()            | REG0=XMM_R():w:dq:u64 MEM0:r:dq:u64 REG1=XMM_N():r:dq:u64
VPSHLQ               | XOP            | XOP            | XOP            | XOPV 0x97 VNP W0 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                   | REG0=XMM_R():w:dq:u64 REG1=XMM_B():r:dq:u64 REG2=XMM_N():r:dq:u64
VPSHLQ               | XOP            | XOP            | XOP            | XOPV 0x97 VNP W1 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()            | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 MEM0:r:dq:u64
VPSHLQ               | XOP            | XOP            | XOP            | XOPV 0x97 VNP W1 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                   | REG0=XMM_R():w:dq:u64 REG1=XMM_N():r:dq:u64 REG2=XMM_B():r:dq:u64
VPSHLW               | XOP            | XOP            | XOP            | XOPV 0x95 VNP W0 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()            | REG0=XMM_R():w:dq:u16 MEM0:r:dq:u16 REG1=XMM_N():r:dq:u16
VPSHLW               | XOP            | XOP            | XOP            | XOPV 0x95 VNP W0 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                   | REG0=XMM_R():w:dq:u16 REG1=XMM_B():r:dq:u16 REG2=XMM_N():r:dq:u16
VPSHLW               | XOP            | XOP            | XOP            | XOPV 0x95 VNP W1 VL128  XMAP9 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()            | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 MEM0:r:dq:u16
VPSHLW               | XOP            | XOP            | XOP            | XOPV 0x95 VNP W1 VL128  XMAP9 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                   | REG0=XMM_R():w:dq:u16 REG1=XMM_N():r:dq:u16 REG2=XMM_B():r:dq:u16
VPSHRDD              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | EVV 0x73 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0   UIMM8()  | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32 IMM0:r:b
VPSHRDD              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | EVV 0x73 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
VPSHRDD              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | EVV 0x73 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0   UIMM8()  | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32 IMM0:r:b
VPSHRDD              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | EVV 0x73 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
VPSHRDD              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | EVV 0x73 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0   UIMM8()  | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32 IMM0:r:b
VPSHRDD              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | EVV 0x73 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
VPSHRDQ              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | EVV 0x73 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1   UIMM8()  | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64 IMM0:r:b
VPSHRDQ              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | EVV 0x73 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VPSHRDQ              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | EVV 0x73 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1   UIMM8()  | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64 IMM0:r:b
VPSHRDQ              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | EVV 0x73 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VPSHRDQ              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | EVV 0x73 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1   UIMM8()  | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64 IMM0:r:b
VPSHRDQ              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | EVV 0x73 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VPSHRDVD             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | EVV 0x73 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():rw:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
VPSHRDVD             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | EVV 0x73 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPSHRDVD             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | EVV 0x73 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():rw:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
VPSHRDVD             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | EVV 0x73 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPSHRDVD             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | EVV 0x73 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():rw:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
VPSHRDVD             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | EVV 0x73 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
VPSHRDVQ             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | EVV 0x73 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():rw:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
VPSHRDVQ             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | EVV 0x73 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPSHRDVQ             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | EVV 0x73 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():rw:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
VPSHRDVQ             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | EVV 0x73 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPSHRDVQ             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | EVV 0x73 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():rw:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
VPSHRDVQ             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | EVV 0x73 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
VPSHRDVW             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | EVV 0x72 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():rw:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
VPSHRDVW             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | EVV 0x72 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():rw:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
VPSHRDVW             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | EVV 0x72 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():rw:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
VPSHRDVW             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | EVV 0x72 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():rw:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
VPSHRDVW             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | EVV 0x72 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():rw:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
VPSHRDVW             | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | EVV 0x72 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():rw:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
VPSHRDW              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | EVV 0x72 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1   UIMM8()  | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16 IMM0:r:b
VPSHRDW              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_128 | EVV 0x72 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1   UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16 IMM0:r:b
VPSHRDW              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | EVV 0x72 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1   UIMM8()  | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16 IMM0:r:b
VPSHRDW              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_256 | EVV 0x72 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1   UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16 IMM0:r:b
VPSHRDW              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | EVV 0x72 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1   UIMM8()  | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16 IMM0:r:b
VPSHRDW              | VBMI2          | AVX512EVEX     | AVX512_VBMI2_512 | EVV 0x72 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1   UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16 IMM0:r:b
VPSHUFB              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x00 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
VPSHUFB              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x00 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
VPSHUFB              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x00 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
VPSHUFB              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x00 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
VPSHUFB              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x00 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
VPSHUFB              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x00 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
VPSHUFBITQMB         | AVX512         | AVX512EVEX     | AVX512_BITALG_128 | EVV 0x8F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u8
VPSHUFBITQMB         | AVX512         | AVX512EVEX     | AVX512_BITALG_128 | EVV 0x8F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0    ZEROING=0  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u64 MEM0:r:dq:u8
VPSHUFBITQMB         | AVX512         | AVX512EVEX     | AVX512_BITALG_256 | EVV 0x8F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u8
VPSHUFBITQMB         | AVX512         | AVX512EVEX     | AVX512_BITALG_256 | EVV 0x8F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0    ZEROING=0  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u64 MEM0:r:qq:u8
VPSHUFBITQMB         | AVX512_BITALG  | AVX512EVEX     | AVX512_BITALG_512 | EVV 0x8F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu8
VPSHUFBITQMB         | AVX512_BITALG  | AVX512EVEX     | AVX512_BITALG_512 | EVV 0x8F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0    ZEROING=0  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu64 MEM0:r:zd:u8
VPSHUFD              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x70 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR UIMM8() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32 IMM0:r:b
VPSHUFD              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x70 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
VPSHUFD              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x70 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR UIMM8() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u32 IMM0:r:b
VPSHUFD              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x70 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
VPSHUFD              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x70 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR UIMM8() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu32 IMM0:r:b
VPSHUFD              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x70 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
VPSHUFHW             | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x70 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR UIMM8() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u16 IMM0:r:b
VPSHUFHW             | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x70 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128    NOEVSR UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u16 IMM0:r:b
VPSHUFHW             | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x70 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256    NOEVSR UIMM8() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u16 IMM0:r:b
VPSHUFHW             | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x70 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256    NOEVSR UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u16 IMM0:r:b
VPSHUFHW             | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x70 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512    NOEVSR UIMM8() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu16 IMM0:r:b
VPSHUFHW             | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x70 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512    NOEVSR UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:u16 IMM0:r:b
VPSHUFLW             | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x70 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128    NOEVSR UIMM8() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u16 IMM0:r:b
VPSHUFLW             | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x70 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128    NOEVSR UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u16 IMM0:r:b
VPSHUFLW             | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x70 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256    NOEVSR UIMM8() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u16 IMM0:r:b
VPSHUFLW             | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x70 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256    NOEVSR UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u16 IMM0:r:b
VPSHUFLW             | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x70 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512    NOEVSR UIMM8() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu16 IMM0:r:b
VPSHUFLW             | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x70 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512    NOEVSR UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:u16 IMM0:r:b
VPSLLD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0xF2 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0              | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
VPSLLD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0xF2 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_MEM128() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:dq:u32
VPSLLD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b110] RM[nnn]  VL128  W0   UIMM8()  | REG0=XMM_N3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32 IMM0:r:b
VPSLLD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b110] RM[nnn]  MODRM()  VL128  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_N3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
VPSLLD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0xF2 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0              | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=XMM_B3():r:dq:u32
VPSLLD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0xF2 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_MEM128() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:dq:u32
VPSLLD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b110] RM[nnn]  VL256  W0   UIMM8()  | REG0=YMM_N3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u32 IMM0:r:b
VPSLLD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b110] RM[nnn]  MODRM()  VL256  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_N3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
VPSLLD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0xF2 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0              | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=XMM_B3():r:dq:u32
VPSLLD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0xF2 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_MEM128() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:dq:u32
VPSLLD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b110] RM[nnn]  VL512  W0   UIMM8()  | REG0=ZMM_N3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu32 IMM0:r:b
VPSLLD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b110] RM[nnn]  MODRM()  VL512  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_N3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
VPSLLDQ              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x73 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b111] RM[nnn]  VL128      ZEROING=0 MASK=0 UIMM8() | REG0=XMM_N3():w:dq:u8 REG1=XMM_B3():r:dq:u8 IMM0:r:b
VPSLLDQ              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x73 V66 V0F MOD[mm] MOD!=3 REG[0b111] RM[nnn] BCRC=0 MODRM()  VL128      ZEROING=0 MASK=0 UIMM8()  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_N3():w:dq:u8 MEM0:r:dq:u8 IMM0:r:b
VPSLLDQ              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x73 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b111] RM[nnn]  VL256      ZEROING=0 MASK=0 UIMM8() | REG0=YMM_N3():w:qq:u8 REG1=YMM_B3():r:qq:u8 IMM0:r:b
VPSLLDQ              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x73 V66 V0F MOD[mm] MOD!=3 REG[0b111] RM[nnn] BCRC=0 MODRM()  VL256      ZEROING=0 MASK=0 UIMM8()  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_N3():w:qq:u8 MEM0:r:qq:u8 IMM0:r:b
VPSLLDQ              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x73 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b111] RM[nnn]  VL512      ZEROING=0 MASK=0 UIMM8() | REG0=ZMM_N3():w:zu8 REG1=ZMM_B3():r:zu8 IMM0:r:b
VPSLLDQ              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x73 V66 V0F MOD[mm] MOD!=3 REG[0b111] RM[nnn] BCRC=0 MODRM()  VL512      ZEROING=0 MASK=0 UIMM8()  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_N3():w:zu8 MEM0:r:zd:u8 IMM0:r:b
VPSLLQ               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0xF3 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1              | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
VPSLLQ               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0xF3 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_MEM128() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:dq:u64
VPSLLQ               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x73 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b110] RM[nnn]  VL128  W1   UIMM8()  | REG0=XMM_N3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u64 IMM0:r:b
VPSLLQ               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x73 V66 V0F MOD[mm] MOD!=3 REG[0b110] RM[nnn]  MODRM()  VL128  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_N3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VPSLLQ               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0xF3 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1              | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=XMM_B3():r:dq:u64
VPSLLQ               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0xF3 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_MEM128() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:dq:u64
VPSLLQ               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x73 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b110] RM[nnn]  VL256  W1   UIMM8()  | REG0=YMM_N3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u64 IMM0:r:b
VPSLLQ               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x73 V66 V0F MOD[mm] MOD!=3 REG[0b110] RM[nnn]  MODRM()  VL256  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_N3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VPSLLQ               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0xF3 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1              | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=XMM_B3():r:dq:u64
VPSLLQ               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0xF3 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_MEM128() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:dq:u64
VPSLLQ               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x73 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b110] RM[nnn]  VL512  W1   UIMM8()  | REG0=ZMM_N3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu64 IMM0:r:b
VPSLLQ               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x73 V66 V0F MOD[mm] MOD!=3 REG[0b110] RM[nnn]  MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_N3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VPSLLVD              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x47 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
VPSLLVD              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x47 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPSLLVD              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x47 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
VPSLLVD              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x47 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPSLLVD              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x47 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
VPSLLVD              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x47 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
VPSLLVQ              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x47 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
VPSLLVQ              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x47 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPSLLVQ              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x47 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
VPSLLVQ              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x47 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPSLLVQ              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x47 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
VPSLLVQ              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x47 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
VPSLLVW              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x12 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
VPSLLVW              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x12 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
VPSLLVW              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x12 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
VPSLLVW              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x12 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
VPSLLVW              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x12 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
VPSLLVW              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x12 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
VPSLLW               | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xF1 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                  | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
VPSLLW               | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xF1 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_MEM128() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
VPSLLW               | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x71 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b110] RM[nnn]  VL128     UIMM8()    | REG0=XMM_N3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u16 IMM0:r:b
VPSLLW               | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x71 V66 V0F MOD[mm] MOD!=3 REG[0b110] RM[nnn] BCRC=0 MODRM()  VL128     UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_N3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u16 IMM0:r:b
VPSLLW               | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xF1 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                  | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=XMM_B3():r:dq:u16
VPSLLW               | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xF1 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_MEM128() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:dq:u16
VPSLLW               | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x71 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b110] RM[nnn]  VL256     UIMM8()    | REG0=YMM_N3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u16 IMM0:r:b
VPSLLW               | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x71 V66 V0F MOD[mm] MOD!=3 REG[0b110] RM[nnn] BCRC=0 MODRM()  VL256     UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_N3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u16 IMM0:r:b
VPSLLW               | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xF1 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                  | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=XMM_B3():r:dq:u16
VPSLLW               | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xF1 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_MEM128() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:dq:u16
VPSLLW               | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x71 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b110] RM[nnn]  VL512     UIMM8()    | REG0=ZMM_N3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu16 IMM0:r:b
VPSLLW               | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x71 V66 V0F MOD[mm] MOD!=3 REG[0b110] RM[nnn] BCRC=0 MODRM()  VL512     UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_N3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:u16 IMM0:r:b
VPSRAD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0xE2 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0              | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
VPSRAD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0xE2 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_MEM128() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:dq:u32
VPSRAD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b100] RM[nnn]  VL128  W0   UIMM8()  | REG0=XMM_N3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32 IMM0:r:b
VPSRAD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b100] RM[nnn]  MODRM()  VL128  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_N3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
VPSRAD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0xE2 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0              | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=XMM_B3():r:dq:u32
VPSRAD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0xE2 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_MEM128() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:dq:u32
VPSRAD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b100] RM[nnn]  VL256  W0   UIMM8()  | REG0=YMM_N3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u32 IMM0:r:b
VPSRAD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b100] RM[nnn]  MODRM()  VL256  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_N3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
VPSRAD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0xE2 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0              | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=XMM_B3():r:dq:u32
VPSRAD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0xE2 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_MEM128() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:dq:u32
VPSRAD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b100] RM[nnn]  VL512  W0   UIMM8()  | REG0=ZMM_N3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu32 IMM0:r:b
VPSRAD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b100] RM[nnn]  MODRM()  VL512  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_N3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
VPSRAQ               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0xE2 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1              | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
VPSRAQ               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0xE2 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_MEM128() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:dq:u64
VPSRAQ               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b100] RM[nnn]  VL128  W1   UIMM8()  | REG0=XMM_N3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u64 IMM0:r:b
VPSRAQ               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b100] RM[nnn]  MODRM()  VL128  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_N3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VPSRAQ               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0xE2 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1              | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=XMM_B3():r:dq:u64
VPSRAQ               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0xE2 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_MEM128() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:dq:u64
VPSRAQ               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b100] RM[nnn]  VL256  W1   UIMM8()  | REG0=YMM_N3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u64 IMM0:r:b
VPSRAQ               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b100] RM[nnn]  MODRM()  VL256  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_N3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VPSRAQ               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0xE2 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1              | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=XMM_B3():r:dq:u64
VPSRAQ               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0xE2 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_MEM128() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:dq:u64
VPSRAQ               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b100] RM[nnn]  VL512  W1   UIMM8()  | REG0=ZMM_N3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu64 IMM0:r:b
VPSRAQ               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b100] RM[nnn]  MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_N3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VPSRAVD              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x46 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
VPSRAVD              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x46 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPSRAVD              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x46 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
VPSRAVD              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x46 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPSRAVD              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x46 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
VPSRAVD              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x46 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
VPSRAVQ              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x46 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
VPSRAVQ              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x46 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPSRAVQ              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x46 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
VPSRAVQ              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x46 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPSRAVQ              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x46 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
VPSRAVQ              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x46 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
VPSRAVW              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x11 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
VPSRAVW              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x11 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
VPSRAVW              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x11 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
VPSRAVW              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x11 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
VPSRAVW              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x11 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
VPSRAVW              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x11 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
VPSRAW               | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xE1 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                  | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
VPSRAW               | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xE1 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_MEM128() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
VPSRAW               | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x71 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b100] RM[nnn]  VL128     UIMM8()    | REG0=XMM_N3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u16 IMM0:r:b
VPSRAW               | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x71 V66 V0F MOD[mm] MOD!=3 REG[0b100] RM[nnn] BCRC=0 MODRM()  VL128     UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_N3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u16 IMM0:r:b
VPSRAW               | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xE1 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                  | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=XMM_B3():r:dq:u16
VPSRAW               | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xE1 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_MEM128() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:dq:u16
VPSRAW               | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x71 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b100] RM[nnn]  VL256     UIMM8()    | REG0=YMM_N3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u16 IMM0:r:b
VPSRAW               | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x71 V66 V0F MOD[mm] MOD!=3 REG[0b100] RM[nnn] BCRC=0 MODRM()  VL256     UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_N3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u16 IMM0:r:b
VPSRAW               | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xE1 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                  | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=XMM_B3():r:dq:u16
VPSRAW               | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xE1 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_MEM128() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:dq:u16
VPSRAW               | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x71 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b100] RM[nnn]  VL512     UIMM8()    | REG0=ZMM_N3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu16 IMM0:r:b
VPSRAW               | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x71 V66 V0F MOD[mm] MOD!=3 REG[0b100] RM[nnn] BCRC=0 MODRM()  VL512     UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_N3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:u16 IMM0:r:b
VPSRLD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0xD2 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0              | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
VPSRLD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0xD2 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_MEM128() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:dq:u32
VPSRLD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b010] RM[nnn]  VL128  W0   UIMM8()  | REG0=XMM_N3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u32 IMM0:r:b
VPSRLD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b010] RM[nnn]  MODRM()  VL128  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_N3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
VPSRLD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0xD2 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0              | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=XMM_B3():r:dq:u32
VPSRLD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0xD2 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_MEM128() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:dq:u32
VPSRLD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b010] RM[nnn]  VL256  W0   UIMM8()  | REG0=YMM_N3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u32 IMM0:r:b
VPSRLD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b010] RM[nnn]  MODRM()  VL256  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_N3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
VPSRLD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0xD2 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0              | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=XMM_B3():r:dq:u32
VPSRLD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0xD2 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_MEM128() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:dq:u32
VPSRLD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x72 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b010] RM[nnn]  VL512  W0   UIMM8()  | REG0=ZMM_N3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu32 IMM0:r:b
VPSRLD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x72 V66 V0F MOD[mm] MOD!=3 REG[0b010] RM[nnn]  MODRM()  VL512  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_N3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
VPSRLDQ              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x73 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b011] RM[nnn]  VL128      ZEROING=0 MASK=0 UIMM8() | REG0=XMM_N3():w:dq:u8 REG1=XMM_B3():r:dq:u8 IMM0:r:b
VPSRLDQ              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x73 V66 V0F MOD[mm] MOD!=3 REG[0b011] RM[nnn] BCRC=0 MODRM()  VL128      ZEROING=0 MASK=0 UIMM8()  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_N3():w:dq:u8 MEM0:r:dq:u8 IMM0:r:b
VPSRLDQ              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x73 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b011] RM[nnn]  VL256      ZEROING=0 MASK=0 UIMM8() | REG0=YMM_N3():w:qq:u8 REG1=YMM_B3():r:qq:u8 IMM0:r:b
VPSRLDQ              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x73 V66 V0F MOD[mm] MOD!=3 REG[0b011] RM[nnn] BCRC=0 MODRM()  VL256      ZEROING=0 MASK=0 UIMM8()  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_N3():w:qq:u8 MEM0:r:qq:u8 IMM0:r:b
VPSRLDQ              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x73 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b011] RM[nnn]  VL512      ZEROING=0 MASK=0 UIMM8() | REG0=ZMM_N3():w:zu8 REG1=ZMM_B3():r:zu8 IMM0:r:b
VPSRLDQ              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x73 V66 V0F MOD[mm] MOD!=3 REG[0b011] RM[nnn] BCRC=0 MODRM()  VL512      ZEROING=0 MASK=0 UIMM8()  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_N3():w:zu8 MEM0:r:zd:u8 IMM0:r:b
VPSRLQ               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0xD3 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1              | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
VPSRLQ               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0xD3 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_MEM128() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:dq:u64
VPSRLQ               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x73 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b010] RM[nnn]  VL128  W1   UIMM8()  | REG0=XMM_N3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u64 IMM0:r:b
VPSRLQ               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x73 V66 V0F MOD[mm] MOD!=3 REG[0b010] RM[nnn]  MODRM()  VL128  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_N3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VPSRLQ               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0xD3 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1              | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=XMM_B3():r:dq:u64
VPSRLQ               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0xD3 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_MEM128() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:dq:u64
VPSRLQ               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x73 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b010] RM[nnn]  VL256  W1   UIMM8()  | REG0=YMM_N3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u64 IMM0:r:b
VPSRLQ               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x73 V66 V0F MOD[mm] MOD!=3 REG[0b010] RM[nnn]  MODRM()  VL256  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_N3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VPSRLQ               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0xD3 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1              | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=XMM_B3():r:dq:u64
VPSRLQ               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0xD3 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_MEM128() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:dq:u64
VPSRLQ               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x73 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b010] RM[nnn]  VL512  W1   UIMM8()  | REG0=ZMM_N3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu64 IMM0:r:b
VPSRLQ               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x73 V66 V0F MOD[mm] MOD!=3 REG[0b010] RM[nnn]  MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_N3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VPSRLVD              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x45 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
VPSRLVD              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x45 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPSRLVD              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x45 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
VPSRLVD              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x45 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPSRLVD              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x45 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
VPSRLVD              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x45 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
VPSRLVQ              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x45 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
VPSRLVQ              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x45 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPSRLVQ              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x45 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
VPSRLVQ              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x45 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPSRLVQ              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x45 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
VPSRLVQ              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x45 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
VPSRLVW              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x10 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
VPSRLVW              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x10 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
VPSRLVW              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x10 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
VPSRLVW              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x10 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
VPSRLVW              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x10 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
VPSRLVW              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x10 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1    ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
VPSRLW               | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xD1 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                  | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
VPSRLW               | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xD1 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_MEM128() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
VPSRLW               | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x71 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b010] RM[nnn]  VL128     UIMM8()    | REG0=XMM_N3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:u16 IMM0:r:b
VPSRLW               | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x71 V66 V0F MOD[mm] MOD!=3 REG[0b010] RM[nnn] BCRC=0 MODRM()  VL128     UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_N3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:dq:u16 IMM0:r:b
VPSRLW               | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xD1 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                  | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=XMM_B3():r:dq:u16
VPSRLW               | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xD1 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_MEM128() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:dq:u16
VPSRLW               | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x71 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b010] RM[nnn]  VL256     UIMM8()    | REG0=YMM_N3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:u16 IMM0:r:b
VPSRLW               | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x71 V66 V0F MOD[mm] MOD!=3 REG[0b010] RM[nnn] BCRC=0 MODRM()  VL256     UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_N3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:qq:u16 IMM0:r:b
VPSRLW               | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xD1 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                  | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=XMM_B3():r:dq:u16
VPSRLW               | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xD1 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_MEM128() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:dq:u16
VPSRLW               | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x71 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[0b010] RM[nnn]  VL512     UIMM8()    | REG0=ZMM_N3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zu16 IMM0:r:b
VPSRLW               | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x71 V66 V0F MOD[mm] MOD!=3 REG[0b010] RM[nnn] BCRC=0 MODRM()  VL512     UIMM8()  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_N3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:zd:u16 IMM0:r:b
VPSUBB               | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xF8 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                  | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
VPSUBB               | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xF8 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
VPSUBB               | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xF8 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                  | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
VPSUBB               | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xF8 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
VPSUBB               | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xF8 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                  | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
VPSUBB               | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xF8 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
VPSUBD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0xFA V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0              | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
VPSUBD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0xFA V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPSUBD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0xFA V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0              | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
VPSUBD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0xFA V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPSUBD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0xFA V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0              | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
VPSUBD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0xFA V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
VPSUBQ               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0xFB V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1              | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
VPSUBQ               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0xFB V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPSUBQ               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0xFB V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1              | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
VPSUBQ               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0xFB V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPSUBQ               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0xFB V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1              | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
VPSUBQ               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0xFB V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
VPSUBSB              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xE8 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                  | REG0=XMM_R3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i8 REG3=XMM_B3():r:dq:i8
VPSUBSB              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xE8 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i8 MEM0:r:dq:i8
VPSUBSB              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xE8 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                  | REG0=YMM_R3():w:qq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i8 REG3=YMM_B3():r:qq:i8
VPSUBSB              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xE8 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:i8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i8 MEM0:r:qq:i8
VPSUBSB              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xE8 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                  | REG0=ZMM_R3():w:zi8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi8 REG3=ZMM_B3():r:zi8
VPSUBSB              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xE8 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zi8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi8 MEM0:r:zd:i8
VPSUBSW              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xE9 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                  | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 REG3=XMM_B3():r:dq:i16
VPSUBSW              | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xE9 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:i16 MEM0:r:dq:i16
VPSUBSW              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xE9 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                  | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 REG3=YMM_B3():r:qq:i16
VPSUBSW              | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xE9 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:i16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:i16 MEM0:r:qq:i16
VPSUBSW              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xE9 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                  | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 REG3=ZMM_B3():r:zi16
VPSUBSW              | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xE9 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zi16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zi16 MEM0:r:zd:i16
VPSUBUSB             | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xD8 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                  | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
VPSUBUSB             | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xD8 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
VPSUBUSB             | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xD8 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                  | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
VPSUBUSB             | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xD8 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
VPSUBUSB             | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xD8 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                  | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
VPSUBUSB             | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xD8 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
VPSUBUSW             | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xD9 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                  | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
VPSUBUSW             | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xD9 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
VPSUBUSW             | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xD9 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                  | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
VPSUBUSW             | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xD9 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
VPSUBUSW             | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xD9 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                  | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
VPSUBUSW             | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xD9 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
VPSUBW               | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xF9 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                  | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
VPSUBW               | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0xF9 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
VPSUBW               | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xF9 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                  | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
VPSUBW               | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0xF9 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
VPSUBW               | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xF9 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                  | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
VPSUBW               | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0xF9 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
VPTERNLOGD           | LOGICAL        | AVX512EVEX     | AVX512F_128    | EVV 0x25 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0   UIMM8()  | REG0=XMM_R3():rw:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32 IMM0:r:b
VPTERNLOGD           | LOGICAL        | AVX512EVEX     | AVX512F_128    | EVV 0x25 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
VPTERNLOGD           | LOGICAL        | AVX512EVEX     | AVX512F_256    | EVV 0x25 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0   UIMM8()  | REG0=YMM_R3():rw:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32 IMM0:r:b
VPTERNLOGD           | LOGICAL        | AVX512EVEX     | AVX512F_256    | EVV 0x25 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
VPTERNLOGD           | LOGICAL        | AVX512EVEX     | AVX512F_512    | EVV 0x25 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0   UIMM8()  | REG0=ZMM_R3():rw:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32 IMM0:r:b
VPTERNLOGD           | LOGICAL        | AVX512EVEX     | AVX512F_512    | EVV 0x25 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
VPTERNLOGQ           | LOGICAL        | AVX512EVEX     | AVX512F_128    | EVV 0x25 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1   UIMM8()  | REG0=XMM_R3():rw:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64 IMM0:r:b
VPTERNLOGQ           | LOGICAL        | AVX512EVEX     | AVX512F_128    | EVV 0x25 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():rw:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VPTERNLOGQ           | LOGICAL        | AVX512EVEX     | AVX512F_256    | EVV 0x25 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1   UIMM8()  | REG0=YMM_R3():rw:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64 IMM0:r:b
VPTERNLOGQ           | LOGICAL        | AVX512EVEX     | AVX512F_256    | EVV 0x25 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():rw:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VPTERNLOGQ           | LOGICAL        | AVX512EVEX     | AVX512F_512    | EVV 0x25 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1   UIMM8()  | REG0=ZMM_R3():rw:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64 IMM0:r:b
VPTERNLOGQ           | LOGICAL        | AVX512EVEX     | AVX512F_512    | EVV 0x25 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():rw:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VPTESTMB             | LOGICAL        | AVX512EVEX     | AVX512BW_128   | EVV 0x26 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
VPTESTMB             | LOGICAL        | AVX512EVEX     | AVX512BW_128   | EVV 0x26 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0    ZEROING=0  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
VPTESTMB             | LOGICAL        | AVX512EVEX     | AVX512BW_256   | EVV 0x26 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
VPTESTMB             | LOGICAL        | AVX512EVEX     | AVX512BW_256   | EVV 0x26 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0    ZEROING=0  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
VPTESTMB             | LOGICAL        | AVX512EVEX     | AVX512BW_512   | EVV 0x26 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
VPTESTMB             | LOGICAL        | AVX512EVEX     | AVX512BW_512   | EVV 0x26 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0    ZEROING=0  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
VPTESTMD             | LOGICAL        | AVX512EVEX     | AVX512F_128    | EVV 0x27 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
VPTESTMD             | LOGICAL        | AVX512EVEX     | AVX512F_128    | EVV 0x27 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ZEROING=0  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPTESTMD             | LOGICAL        | AVX512EVEX     | AVX512F_256    | EVV 0x27 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
VPTESTMD             | LOGICAL        | AVX512EVEX     | AVX512F_256    | EVV 0x27 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ZEROING=0  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPTESTMD             | LOGICAL        | AVX512EVEX     | AVX512F_512    | EVV 0x27 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
VPTESTMD             | LOGICAL        | AVX512EVEX     | AVX512F_512    | EVV 0x27 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ZEROING=0  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
VPTESTMQ             | LOGICAL        | AVX512EVEX     | AVX512F_128    | EVV 0x27 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
VPTESTMQ             | LOGICAL        | AVX512EVEX     | AVX512F_128    | EVV 0x27 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ZEROING=0  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPTESTMQ             | LOGICAL        | AVX512EVEX     | AVX512F_256    | EVV 0x27 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
VPTESTMQ             | LOGICAL        | AVX512EVEX     | AVX512F_256    | EVV 0x27 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ZEROING=0  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPTESTMQ             | LOGICAL        | AVX512EVEX     | AVX512F_512    | EVV 0x27 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
VPTESTMQ             | LOGICAL        | AVX512EVEX     | AVX512F_512    | EVV 0x27 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ZEROING=0  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
VPTESTMW             | LOGICAL        | AVX512EVEX     | AVX512BW_128   | EVV 0x26 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
VPTESTMW             | LOGICAL        | AVX512EVEX     | AVX512BW_128   | EVV 0x26 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1    ZEROING=0  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
VPTESTMW             | LOGICAL        | AVX512EVEX     | AVX512BW_256   | EVV 0x26 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
VPTESTMW             | LOGICAL        | AVX512EVEX     | AVX512BW_256   | EVV 0x26 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1    ZEROING=0  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
VPTESTMW             | LOGICAL        | AVX512EVEX     | AVX512BW_512   | EVV 0x26 V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
VPTESTMW             | LOGICAL        | AVX512EVEX     | AVX512BW_512   | EVV 0x26 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1    ZEROING=0  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
VPTESTNMB            | LOGICAL        | AVX512EVEX     | AVX512BW_128   | EVV 0x26 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
VPTESTNMB            | LOGICAL        | AVX512EVEX     | AVX512BW_128   | EVV 0x26 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W0    ZEROING=0  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
VPTESTNMB            | LOGICAL        | AVX512EVEX     | AVX512BW_256   | EVV 0x26 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
VPTESTNMB            | LOGICAL        | AVX512EVEX     | AVX512BW_256   | EVV 0x26 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W0    ZEROING=0  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
VPTESTNMB            | LOGICAL        | AVX512EVEX     | AVX512BW_512   | EVV 0x26 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
VPTESTNMB            | LOGICAL        | AVX512EVEX     | AVX512BW_512   | EVV 0x26 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W0    ZEROING=0  ESIZE_8_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
VPTESTNMD            | LOGICAL        | AVX512EVEX     | AVX512F_128    | EVV 0x27 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
VPTESTNMD            | LOGICAL        | AVX512EVEX     | AVX512F_128    | EVV 0x27 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ZEROING=0  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPTESTNMD            | LOGICAL        | AVX512EVEX     | AVX512F_256    | EVV 0x27 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
VPTESTNMD            | LOGICAL        | AVX512EVEX     | AVX512F_256    | EVV 0x27 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ZEROING=0  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPTESTNMD            | LOGICAL        | AVX512EVEX     | AVX512F_512    | EVV 0x27 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
VPTESTNMD            | LOGICAL        | AVX512EVEX     | AVX512F_512    | EVV 0x27 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ZEROING=0  ESIZE_32_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
VPTESTNMQ            | LOGICAL        | AVX512EVEX     | AVX512F_128    | EVV 0x27 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
VPTESTNMQ            | LOGICAL        | AVX512EVEX     | AVX512F_128    | EVV 0x27 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ZEROING=0  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPTESTNMQ            | LOGICAL        | AVX512EVEX     | AVX512F_256    | EVV 0x27 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
VPTESTNMQ            | LOGICAL        | AVX512EVEX     | AVX512F_256    | EVV 0x27 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ZEROING=0  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPTESTNMQ            | LOGICAL        | AVX512EVEX     | AVX512F_512    | EVV 0x27 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
VPTESTNMQ            | LOGICAL        | AVX512EVEX     | AVX512F_512    | EVV 0x27 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ZEROING=0  ESIZE_64_BITS() NELEM_FULL() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
VPTESTNMW            | LOGICAL        | AVX512EVEX     | AVX512BW_128   | EVV 0x26 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
VPTESTNMW            | LOGICAL        | AVX512EVEX     | AVX512BW_128   | EVV 0x26 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128  W1    ZEROING=0  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
VPTESTNMW            | LOGICAL        | AVX512EVEX     | AVX512BW_256   | EVV 0x26 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
VPTESTNMW            | LOGICAL        | AVX512EVEX     | AVX512BW_256   | EVV 0x26 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256  W1    ZEROING=0  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
VPTESTNMW            | LOGICAL        | AVX512EVEX     | AVX512BW_512   | EVV 0x26 VF3 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1    ZEROING=0 | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
VPTESTNMW            | LOGICAL        | AVX512EVEX     | AVX512BW_512   | EVV 0x26 VF3 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512  W1    ZEROING=0  ESIZE_16_BITS() NELEM_FULLMEM() | REG0=MASK_R():w:mskw REG1=MASK1():r:mskw REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
VPUNPCKHBW           | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x68 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                  | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
VPUNPCKHBW           | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x68 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
VPUNPCKHBW           | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x68 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                  | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
VPUNPCKHBW           | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x68 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
VPUNPCKHBW           | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x68 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                  | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
VPUNPCKHBW           | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x68 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
VPUNPCKHDQ           | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x6A V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0              | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
VPUNPCKHDQ           | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x6A V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPUNPCKHDQ           | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x6A V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0              | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
VPUNPCKHDQ           | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x6A V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPUNPCKHDQ           | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x6A V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0              | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
VPUNPCKHDQ           | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x6A V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
VPUNPCKHQDQ          | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x6D V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1              | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
VPUNPCKHQDQ          | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x6D V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPUNPCKHQDQ          | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x6D V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1              | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
VPUNPCKHQDQ          | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x6D V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPUNPCKHQDQ          | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x6D V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1              | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
VPUNPCKHQDQ          | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x6D V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
VPUNPCKHWD           | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x69 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                  | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
VPUNPCKHWD           | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x69 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
VPUNPCKHWD           | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x69 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                  | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
VPUNPCKHWD           | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x69 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
VPUNPCKHWD           | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x69 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                  | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
VPUNPCKHWD           | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x69 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
VPUNPCKLBW           | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x60 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                  | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 REG3=XMM_B3():r:dq:u8
VPUNPCKLBW           | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x60 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u8 MEM0:r:dq:u8
VPUNPCKLBW           | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x60 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                  | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 REG3=YMM_B3():r:qq:u8
VPUNPCKLBW           | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x60 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u8 MEM0:r:qq:u8
VPUNPCKLBW           | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x60 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                  | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 REG3=ZMM_B3():r:zu8
VPUNPCKLBW           | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x60 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_8_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu8 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu8 MEM0:r:zd:u8
VPUNPCKLDQ           | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x62 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0              | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
VPUNPCKLDQ           | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x62 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPUNPCKLDQ           | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x62 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0              | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
VPUNPCKLDQ           | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x62 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPUNPCKLDQ           | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x62 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0              | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
VPUNPCKLDQ           | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x62 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
VPUNPCKLQDQ          | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x6C V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1              | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
VPUNPCKLQDQ          | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x6C V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPUNPCKLQDQ          | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x6C V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1              | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
VPUNPCKLQDQ          | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x6C V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPUNPCKLQDQ          | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x6C V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1              | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
VPUNPCKLQDQ          | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x6C V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
VPUNPCKLWD           | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x61 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128                  | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 REG3=XMM_B3():r:dq:u16
VPUNPCKLWD           | AVX512         | AVX512EVEX     | AVX512BW_128   | EVV 0x61 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL128      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=XMM_R3():w:dq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u16 MEM0:r:dq:u16
VPUNPCKLWD           | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x61 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256                  | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 REG3=YMM_B3():r:qq:u16
VPUNPCKLWD           | AVX512         | AVX512EVEX     | AVX512BW_256   | EVV 0x61 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL256      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=YMM_R3():w:qq:u16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u16 MEM0:r:qq:u16
VPUNPCKLWD           | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x61 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512                  | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 REG3=ZMM_B3():r:zu16
VPUNPCKLWD           | AVX512         | AVX512EVEX     | AVX512BW_512   | EVV 0x61 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  VL512      ESIZE_16_BITS() NELEM_FULLMEM() | REG0=ZMM_R3():w:zu16 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu16 MEM0:r:zd:u16
VPXORD               | LOGICAL        | AVX512EVEX     | AVX512F_128    | EVV 0xEF V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0              | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
VPXORD               | LOGICAL        | AVX512EVEX     | AVX512F_128    | EVV 0xEF V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPXORD               | LOGICAL        | AVX512EVEX     | AVX512F_256    | EVV 0xEF V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0              | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
VPXORD               | LOGICAL        | AVX512EVEX     | AVX512F_256    | EVV 0xEF V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VPXORD               | LOGICAL        | AVX512EVEX     | AVX512F_512    | EVV 0xEF V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0              | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
VPXORD               | LOGICAL        | AVX512EVEX     | AVX512F_512    | EVV 0xEF V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
VPXORQ               | LOGICAL        | AVX512EVEX     | AVX512F_128    | EVV 0xEF V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1              | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
VPXORQ               | LOGICAL        | AVX512EVEX     | AVX512F_128    | EVV 0xEF V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPXORQ               | LOGICAL        | AVX512EVEX     | AVX512F_256    | EVV 0xEF V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1              | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
VPXORQ               | LOGICAL        | AVX512EVEX     | AVX512F_256    | EVV 0xEF V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VPXORQ               | LOGICAL        | AVX512EVEX     | AVX512F_512    | EVV 0xEF V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1              | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
VPXORQ               | LOGICAL        | AVX512EVEX     | AVX512F_512    | EVV 0xEF V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
VRANGEPD             | AVX512         | AVX512EVEX     | AVX512DQ_128   | EVV 0x50 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1   UIMM8()  | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64 IMM0:r:b
VRANGEPD             | AVX512         | AVX512EVEX     | AVX512DQ_128   | EVV 0x50 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
VRANGEPD             | AVX512         | AVX512EVEX     | AVX512DQ_256   | EVV 0x50 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1   UIMM8()  | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64 IMM0:r:b
VRANGEPD             | AVX512         | AVX512EVEX     | AVX512DQ_256   | EVV 0x50 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
VRANGEPD             | AVX512         | AVX512EVEX     | AVX512DQ_512   | EVV 0x50 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1   UIMM8()  | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64 IMM0:r:b
VRANGEPD             | AVX512         | AVX512EVEX     | AVX512DQ_512   | EVV 0x50 V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W1   UIMM8() | REG0=ZMM_R3():w:zf64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64 IMM0:r:b
VRANGEPD             | AVX512         | AVX512EVEX     | AVX512DQ_512   | EVV 0x50 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
VRANGEPS             | AVX512         | AVX512EVEX     | AVX512DQ_128   | EVV 0x50 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0   UIMM8()  | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32 IMM0:r:b
VRANGEPS             | AVX512         | AVX512EVEX     | AVX512DQ_128   | EVV 0x50 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
VRANGEPS             | AVX512         | AVX512EVEX     | AVX512DQ_256   | EVV 0x50 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0   UIMM8()  | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32 IMM0:r:b
VRANGEPS             | AVX512         | AVX512EVEX     | AVX512DQ_256   | EVV 0x50 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
VRANGEPS             | AVX512         | AVX512EVEX     | AVX512DQ_512   | EVV 0x50 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0   UIMM8()  | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32 IMM0:r:b
VRANGEPS             | AVX512         | AVX512EVEX     | AVX512DQ_512   | EVV 0x50 V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0   UIMM8() | REG0=ZMM_R3():w:zf32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32 IMM0:r:b
VRANGEPS             | AVX512         | AVX512EVEX     | AVX512DQ_512   | EVV 0x50 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
VRANGESD             | AVX512         | AVX512EVEX     | AVX512DQ_SCALAR | EVV 0x51 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1   UIMM8()         | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64 IMM0:r:b
VRANGESD             | AVX512         | AVX512EVEX     | AVX512DQ_SCALAR | EVV 0x51 V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W1   UIMM8() | REG0=XMM_R3():w:dq:f64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64 IMM0:r:b
VRANGESD             | AVX512         | AVX512EVEX     | AVX512DQ_SCALAR | EVV 0x51 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1   UIMM8()  ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64 IMM0:r:b
VRANGESS             | AVX512         | AVX512EVEX     | AVX512DQ_SCALAR | EVV 0x51 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0   UIMM8()         | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32 IMM0:r:b
VRANGESS             | AVX512         | AVX512EVEX     | AVX512DQ_SCALAR | EVV 0x51 V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W0   UIMM8() | REG0=XMM_R3():w:dq:f32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32 IMM0:r:b
VRANGESS             | AVX512         | AVX512EVEX     | AVX512DQ_SCALAR | EVV 0x51 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0   UIMM8()  ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32 IMM0:r:b
VRCP14PD             | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x4C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR    | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64
VRCP14PD             | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x4C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VRCP14PD             | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x4C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR    | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64
VRCP14PD             | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x4C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VRCP14PD             | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x4C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR    | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
VRCP14PD             | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x4C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VRCP14PS             | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x4C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
VRCP14PS             | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x4C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VRCP14PS             | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x4C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
VRCP14PS             | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x4C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VRCP14PS             | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x4C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
VRCP14PS             | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x4C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VRCP14SD             | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x4D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                   | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VRCP14SD             | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x4D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
VRCP14SS             | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x4D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                   | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VRCP14SS             | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x4D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
VRCP23PS             | KNC            | KNCE           | KNCE           | KVV 0xCA V0F38 V66  W0  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() NOSWIZF32() | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  MEM0:r:zd:TXT=NT
VRCP23PS             | KNC            | KNCE           | KNCE           | KVV 0xCA V0F38 V66  W0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] NR=1 KNC_SAE()  | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw REG2=ZMM_B3():r:zd:TXT=SAEC
VRCP23PS             | KNC            | KNCE           | KNCE           | KVV 0xCA V0F38 V66  W0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] NR=0 SWIZ=0     | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw REG2=ZMM_B3():r:zd
VRCP28PD             | AVX512         | AVX512EVEX     | AVX512ER_512   | EVV 0xCA V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR    | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
VRCP28PD             | AVX512         | AVX512EVEX     | AVX512ER_512   | EVV 0xCA V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W1  NOEVSR | REG0=ZMM_R3():w:zf64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
VRCP28PD             | AVX512         | AVX512EVEX     | AVX512ER_512   | EVV 0xCA V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VRCP28PS             | AVX512         | AVX512EVEX     | AVX512ER_512   | EVV 0xCA V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
VRCP28PS             | AVX512         | AVX512EVEX     | AVX512ER_512   | EVV 0xCA V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0  NOEVSR | REG0=ZMM_R3():w:zf32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
VRCP28PS             | AVX512         | AVX512EVEX     | AVX512ER_512   | EVV 0xCA V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VRCP28SD             | AVX512         | AVX512EVEX     | AVX512ER_SCALAR | EVV 0xCB V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                   | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VRCP28SD             | AVX512         | AVX512EVEX     | AVX512ER_SCALAR | EVV 0xCB V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W1 | REG0=XMM_R3():w:dq:f64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VRCP28SD             | AVX512         | AVX512EVEX     | AVX512ER_SCALAR | EVV 0xCB V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
VRCP28SS             | AVX512         | AVX512EVEX     | AVX512ER_SCALAR | EVV 0xCB V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                   | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VRCP28SS             | AVX512         | AVX512EVEX     | AVX512ER_SCALAR | EVV 0xCB V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W0 | REG0=XMM_R3():w:dq:f32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VRCP28SS             | AVX512         | AVX512EVEX     | AVX512ER_SCALAR | EVV 0xCB V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
VREDUCEPD            | AVX512         | AVX512EVEX     | AVX512DQ_128   | EVV 0x56 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR UIMM8() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64 IMM0:r:b
VREDUCEPD            | AVX512         | AVX512EVEX     | AVX512DQ_128   | EVV 0x56 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
VREDUCEPD            | AVX512         | AVX512EVEX     | AVX512DQ_256   | EVV 0x56 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR UIMM8() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64 IMM0:r:b
VREDUCEPD            | AVX512         | AVX512EVEX     | AVX512DQ_256   | EVV 0x56 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
VREDUCEPD            | AVX512         | AVX512EVEX     | AVX512DQ_512   | EVV 0x56 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR UIMM8() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64 IMM0:r:b
VREDUCEPD            | AVX512         | AVX512EVEX     | AVX512DQ_512   | EVV 0x56 V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W1  NOEVSR UIMM8() | REG0=ZMM_R3():w:zf64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64 IMM0:r:b
VREDUCEPD            | AVX512         | AVX512EVEX     | AVX512DQ_512   | EVV 0x56 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
VREDUCEPS            | AVX512         | AVX512EVEX     | AVX512DQ_128   | EVV 0x56 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR UIMM8() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32 IMM0:r:b
VREDUCEPS            | AVX512         | AVX512EVEX     | AVX512DQ_128   | EVV 0x56 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
VREDUCEPS            | AVX512         | AVX512EVEX     | AVX512DQ_256   | EVV 0x56 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR UIMM8() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32 IMM0:r:b
VREDUCEPS            | AVX512         | AVX512EVEX     | AVX512DQ_256   | EVV 0x56 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
VREDUCEPS            | AVX512         | AVX512EVEX     | AVX512DQ_512   | EVV 0x56 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR UIMM8() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32 IMM0:r:b
VREDUCEPS            | AVX512         | AVX512EVEX     | AVX512DQ_512   | EVV 0x56 V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0  NOEVSR UIMM8() | REG0=ZMM_R3():w:zf32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32 IMM0:r:b
VREDUCEPS            | AVX512         | AVX512EVEX     | AVX512DQ_512   | EVV 0x56 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
VREDUCESD            | AVX512         | AVX512EVEX     | AVX512DQ_SCALAR | EVV 0x57 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1   UIMM8()         | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64 IMM0:r:b
VREDUCESD            | AVX512         | AVX512EVEX     | AVX512DQ_SCALAR | EVV 0x57 V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W1   UIMM8() | REG0=XMM_R3():w:dq:f64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64 IMM0:r:b
VREDUCESD            | AVX512         | AVX512EVEX     | AVX512DQ_SCALAR | EVV 0x57 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1   UIMM8()  ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64 IMM0:r:b
VREDUCESS            | AVX512         | AVX512EVEX     | AVX512DQ_SCALAR | EVV 0x57 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0   UIMM8()         | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32 IMM0:r:b
VREDUCESS            | AVX512         | AVX512EVEX     | AVX512DQ_SCALAR | EVV 0x57 V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W0   UIMM8() | REG0=XMM_R3():w:dq:f32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32 IMM0:r:b
VREDUCESS            | AVX512         | AVX512EVEX     | AVX512DQ_SCALAR | EVV 0x57 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0   UIMM8()  ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32 IMM0:r:b
VRNDFXPNTPS          | KNC            | KNCE           | KNCE           | KVV 0x52 V0F3A V66  W0  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() UIMM8() UPCONVERT_FLT32() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw  MEM0:r:zv:TXT=CONVERT:TXT=NT IMM0:r:b
VRNDFXPNTPS          | KNC            | KNCE           | KNCE           | KVV 0x52 V0F3A V66  W0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] UIMM8() NR=1 KNC_SAE() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw  REG2=ZMM_B3():r:zf32:TXT=SAEC  IMM0:r:b
VRNDFXPNTPS          | KNC            | KNCE           | KNCE           | KVV 0x52 V0F3A V66  W0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] UIMM8() NR=0 REG_SWIZZLE32() | REG0=ZMM_R3():rw:zf32 REG1=MASK1():r:mskw  REG2=ZMM_B3():r:zf32:TXT=REGSWIZ IMM0:r:b
VRNDSCALEPD          | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x09 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR UIMM8() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64 IMM0:r:b
VRNDSCALEPD          | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x09 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
VRNDSCALEPD          | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x09 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR UIMM8() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64 IMM0:r:b
VRNDSCALEPD          | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x09 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
VRNDSCALEPD          | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x09 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR UIMM8() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64 IMM0:r:b
VRNDSCALEPD          | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x09 V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W1  NOEVSR UIMM8() | REG0=ZMM_R3():w:zf64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64 IMM0:r:b
VRNDSCALEPD          | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x09 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
VRNDSCALEPS          | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x08 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR UIMM8() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32 IMM0:r:b
VRNDSCALEPS          | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x08 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
VRNDSCALEPS          | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x08 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR UIMM8() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32 IMM0:r:b
VRNDSCALEPS          | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x08 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
VRNDSCALEPS          | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x08 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR UIMM8() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32 IMM0:r:b
VRNDSCALEPS          | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x08 V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0  NOEVSR UIMM8() | REG0=ZMM_R3():w:zf32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32 IMM0:r:b
VRNDSCALEPS          | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x08 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
VRNDSCALESD          | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x0B V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1   UIMM8()         | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64 IMM0:r:b
VRNDSCALESD          | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x0B V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W1   UIMM8() | REG0=XMM_R3():w:dq:f64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64 IMM0:r:b
VRNDSCALESD          | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x0B V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1   UIMM8()  ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64 IMM0:r:b
VRNDSCALESS          | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x0A V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0   UIMM8()         | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32 IMM0:r:b
VRNDSCALESS          | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x0A V66 V0F3A MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W0   UIMM8() | REG0=XMM_R3():w:dq:f32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32 IMM0:r:b
VRNDSCALESS          | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x0A V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0   UIMM8()  ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32 IMM0:r:b
VRSQRT14PD           | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x4E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR    | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64
VRSQRT14PD           | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x4E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VRSQRT14PD           | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x4E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR    | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64
VRSQRT14PD           | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x4E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VRSQRT14PD           | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x4E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR    | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
VRSQRT14PD           | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x4E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VRSQRT14PS           | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x4E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR    | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
VRSQRT14PS           | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x4E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VRSQRT14PS           | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x4E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR    | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
VRSQRT14PS           | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x4E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VRSQRT14PS           | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x4E V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
VRSQRT14PS           | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x4E V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VRSQRT14SD           | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x4F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                   | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VRSQRT14SD           | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x4F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
VRSQRT14SS           | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x4F V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                   | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VRSQRT14SS           | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x4F V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
VRSQRT23PS           | KNC            | KNCE           | KNCE           | KVV 0xCB V0F38 V66  W0  NOEVSR MOD[mm]  MOD!=3 REG[rrr] RM[nnn] MODRM() NOSWIZF32() | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw  MEM0:r:zd:TXT=NT
VRSQRT23PS           | KNC            | KNCE           | KNCE           | KVV 0xCB V0F38 V66  W0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] NR=1 KNC_SAE()  | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw REG2=ZMM_B3():r:zd:TXT=SAEC
VRSQRT23PS           | KNC            | KNCE           | KNCE           | KVV 0xCB V0F38 V66  W0  NOEVSR MOD[0b11]  MOD=3 REG[rrr] RM[nnn] NR=0 SWIZ=0     | REG0=ZMM_R3():rw:zd REG1=MASK1():r:mskw REG2=ZMM_B3():r:zd
VRSQRT28PD           | AVX512         | AVX512EVEX     | AVX512ER_512   | EVV 0xCC V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR    | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
VRSQRT28PD           | AVX512         | AVX512EVEX     | AVX512ER_512   | EVV 0xCC V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W1  NOEVSR | REG0=ZMM_R3():w:zf64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
VRSQRT28PD           | AVX512         | AVX512EVEX     | AVX512ER_512   | EVV 0xCC V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VRSQRT28PS           | AVX512         | AVX512EVEX     | AVX512ER_512   | EVV 0xCC V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR    | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
VRSQRT28PS           | AVX512         | AVX512EVEX     | AVX512ER_512   | EVV 0xCC V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() SAE()  W0  NOEVSR | REG0=ZMM_R3():w:zf32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
VRSQRT28PS           | AVX512         | AVX512EVEX     | AVX512ER_512   | EVV 0xCC V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VRSQRT28SD           | AVX512         | AVX512EVEX     | AVX512ER_SCALAR | EVV 0xCD V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                   | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VRSQRT28SD           | AVX512         | AVX512EVEX     | AVX512ER_SCALAR | EVV 0xCD V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W1 | REG0=XMM_R3():w:dq:f64:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VRSQRT28SD           | AVX512         | AVX512EVEX     | AVX512ER_SCALAR | EVV 0xCD V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
VRSQRT28SS           | AVX512         | AVX512EVEX     | AVX512ER_SCALAR | EVV 0xCD V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                   | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VRSQRT28SS           | AVX512         | AVX512EVEX     | AVX512ER_SCALAR | EVV 0xCD V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W0 | REG0=XMM_R3():w:dq:f32:TXT=SAESTR REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VRSQRT28SS           | AVX512         | AVX512EVEX     | AVX512ER_SCALAR | EVV 0xCD V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
VSCALEFPD            | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x2C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1            | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VSCALEFPD            | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x2C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VSCALEFPD            | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x2C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1            | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
VSCALEFPD            | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x2C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VSCALEFPD            | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x2C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1            | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VSCALEFPD            | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x2C V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1 | REG0=ZMM_R3():w:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VSCALEFPD            | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x2C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
VSCALEFPS            | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x2C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0            | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VSCALEFPS            | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x2C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VSCALEFPS            | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x2C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0            | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
VSCALEFPS            | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x2C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VSCALEFPS            | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x2C V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0            | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VSCALEFPS            | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x2C V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0 | REG0=ZMM_R3():w:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VSCALEFPS            | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x2C V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
VSCALEFSD            | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                   | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VSCALEFSD            | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2D V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1 | REG0=XMM_R3():w:dq:f64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VSCALEFSD            | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
VSCALEFSS            | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2D V66 V0F38 MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                   | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VSCALEFSS            | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2D V66 V0F38 MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W0 | REG0=XMM_R3():w:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VSCALEFSS            | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2D V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
VSCATTERDPD          | KNC            | KNCE           | KNCE           | KVV 0xA2 V66 V0F38  W1 NO_SPARSE_EVSR MOD[mm] MOD!=3 REG[rrr] RM[0b100] KNC_VMODRM() DNCONVERT_FLT64() | MEM0:w:zv:TXT=NT:TXT=CONVERT  REG1=MASK1():rw:mskw   REG0=ZMM_R3():r:zf64 NELEM=1:SUPP
VSCATTERDPD          | SCATTER        | AVX512EVEX     | AVX512F_128    | EVV 0xA2 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL128  W1 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:w:q:f64 REG0=MASKNOT0():rw:mskw REG1=XMM_R3():r:dq:f64
VSCATTERDPD          | SCATTER        | AVX512EVEX     | AVX512F_256    | EVV 0xA2 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL256  W1 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:w:q:f64 REG0=MASKNOT0():rw:mskw REG1=YMM_R3():r:qq:f64
VSCATTERDPD          | SCATTER        | AVX512EVEX     | AVX512F_512    | EVV 0xA2 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL512  W1 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:w:q:f64 REG0=MASKNOT0():rw:mskw REG1=ZMM_R3():r:zf64
VSCATTERDPS          | KNC            | KNCE           | KNCE           | KVV 0xA2 V66 V0F38  W0 NO_SPARSE_EVSR MOD[mm] MOD!=3 REG[rrr] RM[0b100] KNC_VMODRM() DNCONVERT_FLT32() | MEM0:w:zv:TXT=NT:TXT=CONVERT  REG1=MASK1():rw:mskw   REG0=ZMM_R3():r:zf32 NELEM=1:SUPP
VSCATTERDPS          | SCATTER        | AVX512EVEX     | AVX512F_128    | EVV 0xA2 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL128  W0 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:w:d:f32 REG0=MASKNOT0():rw:mskw REG1=XMM_R3():r:dq:f32
VSCATTERDPS          | SCATTER        | AVX512EVEX     | AVX512F_256    | EVV 0xA2 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL256  W0 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:w:d:f32 REG0=MASKNOT0():rw:mskw REG1=YMM_R3():r:qq:f32
VSCATTERDPS          | SCATTER        | AVX512EVEX     | AVX512F_512    | EVV 0xA2 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL512  W0 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:w:d:f32 REG0=MASKNOT0():rw:mskw REG1=ZMM_R3():r:zf32
VSCATTERPF0DPD       | SCATTER        | AVX512EVEX     | AVX512PF_512   | EVV 0xC6 V66 V0F38 MOD[mm] MOD!=3 REG[0b101] RM[nnn] BCRC=0   VL512  W1 RM=4 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:r:q:f64 REG0=MASKNOT0():rw:mskw
VSCATTERPF0DPS       | SCATTER        | AVX512EVEX     | AVX512PF_512   | EVV 0xC6 V66 V0F38 MOD[mm] MOD!=3 REG[0b101] RM[nnn] BCRC=0   VL512  W0 RM=4 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:r:d:f32 REG0=MASKNOT0():rw:mskw
VSCATTERPF0HINTDPD   | PREFETCH       | KNCE           | KNC_PF_HINT    | KVV 0xC6 V0F38 V66  REXW=1 NO_SPARSE_EVSR MOD[mm] MOD!=3 REG[0b100] RM[0b100] KNC_VMODRM() UPCONVERT_FLT32_LOAD() | MEM0:w:zv:TXT=NT REG0=MASK1():rw:mskw
VSCATTERPF0HINTDPS   | PREFETCH       | KNCE           | KNC_PF_HINT    | KVV 0xC6 V0F38 V66  REXW=0 NO_SPARSE_EVSR MOD[mm] MOD!=3 REG[0b100] RM[0b100] KNC_VMODRM() UPCONVERT_FLT32_LOAD() | MEM0:w:zv:TXT=NT REG0=MASK1():rw:mskw
VSCATTERPF0QPD       | SCATTER        | AVX512EVEX     | AVX512PF_512   | EVV 0xC7 V66 V0F38 MOD[mm] MOD!=3 REG[0b101] RM[nnn] BCRC=0   VL512  W1 RM=4 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:r:q:f64 REG0=MASKNOT0():rw:mskw
VSCATTERPF0QPS       | SCATTER        | AVX512EVEX     | AVX512PF_512   | EVV 0xC7 V66 V0F38 MOD[mm] MOD!=3 REG[0b101] RM[nnn] BCRC=0   VL512  W0 RM=4 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:r:d:f32 REG0=MASKNOT0():rw:mskw
VSCATTERPF1DPD       | SCATTER        | AVX512EVEX     | AVX512PF_512   | EVV 0xC6 V66 V0F38 MOD[mm] MOD!=3 REG[0b110] RM[nnn] BCRC=0   VL512  W1 RM=4 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:r:q:f64 REG0=MASKNOT0():rw:mskw
VSCATTERPF1DPS       | SCATTER        | AVX512EVEX     | AVX512PF_512   | EVV 0xC6 V66 V0F38 MOD[mm] MOD!=3 REG[0b110] RM[nnn] BCRC=0   VL512  W0 RM=4 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:r:d:f32 REG0=MASKNOT0():rw:mskw
VSCATTERPF1QPD       | SCATTER        | AVX512EVEX     | AVX512PF_512   | EVV 0xC7 V66 V0F38 MOD[mm] MOD!=3 REG[0b110] RM[nnn] BCRC=0   VL512  W1 RM=4 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:r:q:f64 REG0=MASKNOT0():rw:mskw
VSCATTERPF1QPS       | SCATTER        | AVX512EVEX     | AVX512PF_512   | EVV 0xC7 V66 V0F38 MOD[mm] MOD!=3 REG[0b110] RM[nnn] BCRC=0   VL512  W0 RM=4 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:r:d:f32 REG0=MASKNOT0():rw:mskw
VSCATTERQPD          | SCATTER        | AVX512EVEX     | AVX512F_128    | EVV 0xA3 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL128  W1 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:w:q:f64 REG0=MASKNOT0():rw:mskw REG1=XMM_R3():r:dq:f64
VSCATTERQPD          | SCATTER        | AVX512EVEX     | AVX512F_256    | EVV 0xA3 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL256  W1 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:w:q:f64 REG0=MASKNOT0():rw:mskw REG1=YMM_R3():r:qq:f64
VSCATTERQPD          | SCATTER        | AVX512EVEX     | AVX512F_512    | EVV 0xA3 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL512  W1 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_64_BITS() NELEM_GSCAT() | MEM0:w:q:f64 REG0=MASKNOT0():rw:mskw REG1=ZMM_R3():r:zf64
VSCATTERQPS          | SCATTER        | AVX512EVEX     | AVX512F_128    | EVV 0xA3 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL128  W0 UISA_VMODRM_XMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:w:d:f32 REG0=MASKNOT0():rw:mskw REG1=XMM_R3():r:dq:f32
VSCATTERQPS          | SCATTER        | AVX512EVEX     | AVX512F_256    | EVV 0xA3 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL256  W0 UISA_VMODRM_YMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:w:d:f32 REG0=MASKNOT0():rw:mskw REG1=XMM_R3():r:dq:f32
VSCATTERQPS          | SCATTER        | AVX512EVEX     | AVX512F_512    | EVV 0xA3 V66 V0F38 MOD[mm] MOD!=3 REG[rrr] RM[0b100] RM=4 BCRC=0   VL512  W0 UISA_VMODRM_ZMM() eanot16  NOVSR  ZEROING=0  ESIZE_32_BITS() NELEM_GSCAT() | MEM0:w:d:f32 REG0=MASKNOT0():rw:mskw REG1=YMM_R3():r:qq:f32
VSHUFF32X4           | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x23 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0   UIMM8()  | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32 IMM0:r:b
VSHUFF32X4           | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x23 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
VSHUFF32X4           | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x23 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0   UIMM8()  | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32 IMM0:r:b
VSHUFF32X4           | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x23 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
VSHUFF64X2           | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x23 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1   UIMM8()  | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64 IMM0:r:b
VSHUFF64X2           | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x23 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
VSHUFF64X2           | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x23 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1   UIMM8()  | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64 IMM0:r:b
VSHUFF64X2           | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x23 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
VSHUFI32X4           | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x43 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0   UIMM8()  | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32 IMM0:r:b
VSHUFI32X4           | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x43 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
VSHUFI32X4           | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x43 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0   UIMM8()  | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32 IMM0:r:b
VSHUFI32X4           | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x43 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR IMM0:r:b
VSHUFI64X2           | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x43 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1   UIMM8()  | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64 IMM0:r:b
VSHUFI64X2           | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x43 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VSHUFI64X2           | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x43 V66 V0F3A MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1   UIMM8()  | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64 IMM0:r:b
VSHUFI64X2           | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x43 V66 V0F3A MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR IMM0:r:b
VSHUFPD              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0xC6 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1   UIMM8()    | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64 IMM0:r:b
VSHUFPD              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0xC6 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
VSHUFPD              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0xC6 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1   UIMM8()    | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64 IMM0:r:b
VSHUFPD              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0xC6 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
VSHUFPD              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0xC6 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1   UIMM8()    | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64 IMM0:r:b
VSHUFPD              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0xC6 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1   UIMM8()  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR IMM0:r:b
VSHUFPS              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0xC6 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0   UIMM8()    | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32 IMM0:r:b
VSHUFPS              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0xC6 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
VSHUFPS              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0xC6 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0   UIMM8()    | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32 IMM0:r:b
VSHUFPS              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0xC6 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
VSHUFPS              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0xC6 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0   UIMM8()    | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32 IMM0:r:b
VSHUFPS              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0xC6 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0   UIMM8()  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR IMM0:r:b
VSQRTPD              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x51 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1  NOEVSR      | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f64
VSQRTPD              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x51 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VSQRTPD              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x51 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1  NOEVSR      | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f64
VSQRTPD              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x51 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VSQRTPD              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x51 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1  NOEVSR      | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
VSQRTPD              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x51 V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1  NOEVSR | REG0=ZMM_R3():w:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf64
VSQRTPD              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x51 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1  NOEVSR  ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f64:TXT=BCASTSTR
VSQRTPS              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x51 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0  NOEVSR      | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_B3():r:dq:f32
VSQRTPS              | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x51 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VSQRTPS              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x51 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0  NOEVSR      | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_B3():r:qq:f32
VSQRTPS              | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x51 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VSQRTPS              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x51 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0  NOEVSR      | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
VSQRTPS              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x51 VNP V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0  NOEVSR | REG0=ZMM_R3():w:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_B3():r:zf32
VSQRTPS              | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x51 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0  NOEVSR  ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR MEM0:r:vv:f32:TXT=BCASTSTR
VSQRTSD              | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x51 VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                     | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VSQRTSD              | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x51 VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1 | REG0=XMM_R3():w:dq:f64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VSQRTSD              | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x51 VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
VSQRTSS              | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x51 VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                     | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VSQRTSS              | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x51 VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W0 | REG0=XMM_R3():w:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VSQRTSS              | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x51 VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
VSUBPD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x5C V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1              | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VSUBPD               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x5C V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VSUBPD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x5C V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1              | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
VSUBPD               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x5C V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VSUBPD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x5C V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1              | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VSUBPD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x5C V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W1 | REG0=ZMM_R3():w:zf64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VSUBPD               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x5C V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
VSUBPS               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x5C VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0              | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VSUBPS               | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x5C VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VSUBPS               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x5C VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0              | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
VSUBPS               | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x5C VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VSUBPS               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x5C VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0              | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VSUBPS               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x5C VNP V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN512() AVX512_ROUND()  W0 | REG0=ZMM_R3():w:zf32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VSUBPS               | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x5C VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
VSUBSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x5C VF2 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1                     | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VSUBSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x5C VF2 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W1 | REG0=XMM_R3():w:dq:f64:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VSUBSD               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x5C VF2 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1    ESIZE_64_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:q:f64
VSUBSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x5C VF3 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0                     | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VSUBSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x5C VF3 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() AVX512_ROUND()  W0 | REG0=XMM_R3():w:dq:f32:TXT=ROUNDC REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VSUBSS               | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x5C VF3 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0    ESIZE_32_BITS() NELEM_SCALAR() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:d:f32
VUCOMISD             | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2E V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W1  NOEVSR  ZEROING=0 MASK=0  FIX_ROUND_LEN128() | REG0=XMM_R3():r:dq:f64 REG1=XMM_B3():r:dq:f64
VUCOMISD             | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2E V66 V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W1  NOEVSR  ZEROING=0 MASK=0 | REG0=XMM_R3():r:dq:f64:TXT=SAESTR REG1=XMM_B3():r:dq:f64
VUCOMISD             | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2E V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W1  NOEVSR  ZEROING=0 MASK=0  ESIZE_64_BITS() NELEM_SCALAR() FIX_ROUND_LEN128() | REG0=XMM_R3():r:dq:f64 MEM0:r:q:f64
VUCOMISS             | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2E VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  W0  NOEVSR  ZEROING=0 MASK=0  FIX_ROUND_LEN128() | REG0=XMM_R3():r:dq:f32 REG1=XMM_B3():r:dq:f32
VUCOMISS             | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2E VNP V0F MOD[0b11] MOD=3 BCRC=1 REG[rrr] RM[nnn] FIX_ROUND_LEN128() SAE()  W0  NOEVSR  ZEROING=0 MASK=0 | REG0=XMM_R3():r:dq:f32:TXT=SAESTR REG1=XMM_B3():r:dq:f32
VUCOMISS             | AVX512         | AVX512EVEX     | AVX512F_SCALAR | EVV 0x2E VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn] BCRC=0 MODRM()  W0  NOEVSR  ZEROING=0 MASK=0  ESIZE_32_BITS() NELEM_SCALAR() FIX_ROUND_LEN128() | REG0=XMM_R3():r:dq:f32 MEM0:r:d:f32
VUNPCKHPD            | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x15 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1              | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VUNPCKHPD            | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x15 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VUNPCKHPD            | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x15 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1              | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
VUNPCKHPD            | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x15 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VUNPCKHPD            | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x15 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1              | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VUNPCKHPD            | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x15 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
VUNPCKHPS            | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x15 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0              | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VUNPCKHPS            | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x15 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VUNPCKHPS            | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x15 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0              | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
VUNPCKHPS            | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x15 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VUNPCKHPS            | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x15 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0              | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VUNPCKHPS            | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x15 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
VUNPCKLPD            | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x14 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1              | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 REG3=XMM_B3():r:dq:f64
VUNPCKLPD            | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x14 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VUNPCKLPD            | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x14 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1              | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 REG3=YMM_B3():r:qq:f64
VUNPCKLPD            | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x14 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f64 MEM0:r:vv:f64:TXT=BCASTSTR
VUNPCKLPD            | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x14 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1              | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 REG3=ZMM_B3():r:zf64
VUNPCKLPD            | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x14 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf64 MEM0:r:vv:f64:TXT=BCASTSTR
VUNPCKLPS            | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x14 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0              | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 REG3=XMM_B3():r:dq:f32
VUNPCKLPS            | AVX512         | AVX512EVEX     | AVX512F_128    | EVV 0x14 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VUNPCKLPS            | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x14 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0              | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 REG3=YMM_B3():r:qq:f32
VUNPCKLPS            | AVX512         | AVX512EVEX     | AVX512F_256    | EVV 0x14 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:f32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:f32 MEM0:r:vv:f32:TXT=BCASTSTR
VUNPCKLPS            | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x14 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0              | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 REG3=ZMM_B3():r:zf32
VUNPCKLPS            | AVX512         | AVX512EVEX     | AVX512F_512    | EVV 0x14 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zf32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zf32 MEM0:r:vv:f32:TXT=BCASTSTR
VXORPD               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_128   | EVV 0x57 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W1              | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 REG3=XMM_B3():r:dq:u64
VXORPD               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_128   | EVV 0x57 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VXORPD               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_256   | EVV 0x57 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W1              | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 REG3=YMM_B3():r:qq:u64
VXORPD               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_256   | EVV 0x57 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u64 MEM0:r:vv:u64:TXT=BCASTSTR
VXORPD               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_512   | EVV 0x57 V66 V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W1              | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 REG3=ZMM_B3():r:zu64
VXORPD               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_512   | EVV 0x57 V66 V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W1    ESIZE_64_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu64 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu64 MEM0:r:vv:u64:TXT=BCASTSTR
VXORPS               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_128   | EVV 0x57 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL128  W0              | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 REG3=XMM_B3():r:dq:u32
VXORPS               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_128   | EVV 0x57 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL128  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=XMM_R3():w:dq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=XMM_N3():r:dq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VXORPS               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_256   | EVV 0x57 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL256  W0              | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 REG3=YMM_B3():r:qq:u32
VXORPS               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_256   | EVV 0x57 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL256  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=YMM_R3():w:qq:u32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=YMM_N3():r:qq:u32 MEM0:r:vv:u32:TXT=BCASTSTR
VXORPS               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_512   | EVV 0x57 VNP V0F MOD[0b11] MOD=3 BCRC=0 REG[rrr] RM[nnn]  VL512  W0              | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 REG3=ZMM_B3():r:zu32
VXORPS               | LOGICAL_FP     | AVX512EVEX     | AVX512DQ_512   | EVV 0x57 VNP V0F MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  VL512  W0    ESIZE_32_BITS() NELEM_FULL() | REG0=ZMM_R3():w:zu32 REG1=MASK1():r:mskw:TXT=ZEROSTR REG2=ZMM_N3():r:zu32 MEM0:r:vv:u32:TXT=BCASTSTR
WBINVD               | SYSTEM         | BASE           | I486REAL       | 0x0F 0x09                                                                        | 
WBINVD               | SYSTEM         | BASE           | I486REAL       | 0x0F 0x09 WBNOINVD=0                                                             | 
WBINVD               | SYSTEM         | BASE           | I486REAL       | 0x0F 0x09 WBNOINVD=1 REP!=3                                                      | 
WBNOINVD             | SYSTEM         | WBNOINVD       | WBNOINVD       | 0x0F 0x09 WBNOINVD=1 f3_refining_prefix                                          | 
WRMSR                | SYSTEM         | BASE           | PENTIUMREAL    | 0x0F 0x30                                                                        | REG0=XED_REG_EAX:r:SUPP REG1=XED_REG_EDX:r:SUPP REG2=XED_REG_ECX:r:SUPP REG3=XED_REG_MSRS:w:SUPP
WRPKRU               | PKU            | PKU            | PKU            | 0x0F 0x01 MOD[0b11] MOD=3 REG[0b101] RM[0b111]  no_refining_prefix               | REG0=XED_REG_EDX:r:SUPP REG1=XED_REG_EAX:r:SUPP REG2=XED_REG_ECX:r:SUPP
WRSSD                | CET            | CET            | CET            | 0x0F 0x38 0xF6 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  no_refining_prefix    W0 | MEM0:w:d:u32 REG0=GPR32_R():r:d:u32
WRSSQ                | CET            | CET            | CET            | 0x0F 0x38 0xF6 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  no_refining_prefix    W1  mode64 | MEM0:w:q:u64 REG0=GPR64_R():r:q:u64
WRUSSD               | CET            | CET            | CET            | 0x0F 0x38 0xF5 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  osz_refining_prefix    W0 | MEM0:w:d:u32 REG0=GPR32_R():r:d:u32
WRUSSQ               | CET            | CET            | CET            | 0x0F 0x38 0xF5 MOD[mm] MOD!=3 REG[rrr] RM[nnn]  MODRM()  osz_refining_prefix    W1  mode64 | MEM0:w:q:u64 REG0=GPR64_R():r:q:u64
XADD                 | SEMAPHORE      | BASE           | I486REAL       | 0x0F 0xC0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                  | MEM0:rw:b REG0=GPR8_R():rw
XADD                 | SEMAPHORE      | BASE           | I486REAL       | 0x0F 0xC0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPR8_B():rw REG1=GPR8_R():rw
XADD                 | SEMAPHORE      | BASE           | I486REAL       | 0x0F 0xC1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                  | MEM0:rw:v REG0=GPRv_R():rw
XADD                 | SEMAPHORE      | BASE           | I486REAL       | 0x0F 0xC1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                       | REG0=GPRv_B():rw REG1=GPRv_R():rw
XADD_LOCK            | SEMAPHORE      | BASE           | I486REAL       | 0x0F 0xC0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                    | MEM0:rw:b REG0=GPR8_R():rw
XADD_LOCK            | SEMAPHORE      | BASE           | I486REAL       | 0x0F 0xC1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                    | MEM0:rw:v REG0=GPRv_R():rw
XCHG                 | DATAXFER       | BASE           | I86            | 0x86 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                         | MEM0:rw:b REG0=GPR8_R():rw
XCHG                 | DATAXFER       | BASE           | I86            | 0x86 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                       | MEM0:rw:b REG0=GPR8_R():rw
XCHG                 | DATAXFER       | BASE           | I86            | 0x86 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPR8_B():rw REG1=GPR8_R():rw
XCHG                 | DATAXFER       | BASE           | I86            | 0x87 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                         | MEM0:rw:v REG0=GPRv_R():rw
XCHG                 | DATAXFER       | BASE           | I86            | 0x87 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                       | MEM0:rw:v REG0=GPRv_R():rw
XCHG                 | DATAXFER       | BASE           | I86            | 0x87 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPRv_B():rw REG1=GPRv_R():rw
XCHG                 | DATAXFER       | BASE           | I86            | 0b1001_0 SRM[rrr] SRM!=0                                                         | REG0=GPRv_SB():rw REG1=OrAX():rw:IMPL
XCHG                 | DATAXFER       | BASE           | I86            | 0b1001_0 SRM[rrr] SRM=0 not_refining_f3 rexb_prefix                              | REG0=GPRv_SB():rw REG1=OrAX():rw:IMPL
XLAT                 | MISC           | BASE           | I86            | 0xD7 OVERRIDE_SEG0()                                                             | MEM0:r:SUPP:b BASE0=ArBX():r:SUPP  INDEX=XED_REG_AL:r:SUPP  REG0=XED_REG_AL:w:SUPP SEG0=FINAL_DSEG():r:SUPP SCALE=1:r:SUPP
XOR                  | LOGICAL        | BASE           | I86            | 0x80 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM() UIMM8() nolock_prefix             | MEM0:rw:b IMM0:r:b
XOR                  | LOGICAL        | BASE           | I86            | 0x80 MOD[0b11] MOD=3 REG[0b110] RM[nnn] UIMM8()                                  | REG0=GPR8_B():rw IMM0:r:b
XOR                  | LOGICAL        | BASE           | I86            | 0x81 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM() SIMMz() nolock_prefix             | MEM0:rw:v IMM0:r:z
XOR                  | LOGICAL        | BASE           | I86            | 0x81 MOD[0b11] MOD=3 REG[0b110] RM[nnn] SIMMz()                                  | REG0=GPRv_B():rw IMM0:r:z
XOR                  | LOGICAL        | BASE           | I86            | 0x82 MOD[mm] MOD!=3 REG[0b110] RM[nnn] not64 MODRM() UIMM8() nolock_prefix       | MEM0:rw:b IMM0:r:b
XOR                  | LOGICAL        | BASE           | I86            | 0x82 MOD[0b11] MOD=3 REG[0b110] RM[nnn] not64 UIMM8()                            | REG0=GPR8_B():rw IMM0:r:b
XOR                  | LOGICAL        | BASE           | I86            | 0x83 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM() SIMM8() nolock_prefix             | MEM0:rw:v IMM0:r:b:i8
XOR                  | LOGICAL        | BASE           | I86            | 0x83 MOD[0b11] MOD=3 REG[0b110] RM[nnn] SIMM8()                                  | REG0=GPRv_B():rw IMM0:r:b:i8
XOR                  | LOGICAL        | BASE           | I86            | 0x30 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                       | MEM0:rw:b REG0=GPR8_R():r
XOR                  | LOGICAL        | BASE           | I86            | 0x30 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPR8_B():rw REG1=GPR8_R():r
XOR                  | LOGICAL        | BASE           | I86            | 0x31 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() nolock_prefix                       | MEM0:rw:v REG0=GPRv_R():r
XOR                  | LOGICAL        | BASE           | I86            | 0x31 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPRv_B():rw REG1=GPRv_R():r
XOR                  | LOGICAL        | BASE           | I86            | 0x32 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPR8_R():rw REG1=GPR8_B():r
XOR                  | LOGICAL        | BASE           | I86            | 0x32 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=GPR8_R():rw MEM0:r:b
XOR                  | LOGICAL        | BASE           | I86            | 0x33 MOD[0b11] MOD=3 REG[rrr] RM[nnn]                                            | REG0=GPRv_R():rw REG1=GPRv_B():r
XOR                  | LOGICAL        | BASE           | I86            | 0x33 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()                                     | REG0=GPRv_R():rw MEM0:r:v
XOR                  | LOGICAL        | BASE           | I86            | 0x34 UIMM8()                                                                     | REG0=XED_REG_AL:rw:IMPL IMM0:r:b
XOR                  | LOGICAL        | BASE           | I86            | 0x35 SIMMz()                                                                     | REG0=OrAX():rw:IMPL IMM0:r:z
XOR_LOCK             | LOGICAL        | BASE           | I86            | 0x80 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM() UIMM8() lock_prefix               | MEM0:rw:b IMM0:r:b
XOR_LOCK             | LOGICAL        | BASE           | I86            | 0x81 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM() SIMMz() lock_prefix               | MEM0:rw:v IMM0:r:z
XOR_LOCK             | LOGICAL        | BASE           | I86            | 0x82 MOD[mm] MOD!=3 REG[0b110] RM[nnn] not64 MODRM() UIMM8() lock_prefix         | MEM0:rw:b IMM0:r:b
XOR_LOCK             | LOGICAL        | BASE           | I86            | 0x83 MOD[mm] MOD!=3 REG[0b110] RM[nnn] MODRM() SIMM8() lock_prefix               | MEM0:rw:v IMM0:r:b:i8
XOR_LOCK             | LOGICAL        | BASE           | I86            | 0x30 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                         | MEM0:rw:b REG0=GPR8_R():r
XOR_LOCK             | LOGICAL        | BASE           | I86            | 0x31 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM() lock_prefix                         | MEM0:rw:v REG0=GPRv_R():r
XRESLDTRK            | TSX_LDTRK      | TSX_LDTRK      | TSX_LDTRK      | 0x0F 0x01 MOD[0b11] MOD=3  REG[0b101] RM[0b001]  f2_refining_prefix              | 
XSUSLDTRK            | TSX_LDTRK      | TSX_LDTRK      | TSX_LDTRK      | 0x0F 0x01 MOD[0b11] MOD=3  REG[0b101] RM[0b000]  f2_refining_prefix              | 
