# PMOVZX — Packed Move with Zero Extend
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
| Opcode/Instruction                                           | Op / En | 64/32 bit Mode Support | CPUID Feature Flag | Description                                                                                                                      |
| 66 0f 38 30 /r PMOVZXBW xmm1, xmm2/m64                       | A       | V/V                    | SSE4_1             | Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 16-bit integers in xmm1.                          |
| 66 0f 38 31 /r PMOVZXBD xmm1, xmm2/m32                       | A       | V/V                    | SSE4_1             | Zero extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm1.                          |
| 66 0f 38 32 /r PMOVZXBQ xmm1, xmm2/m16                       | A       | V/V                    | SSE4_1             | Zero extend 2 packed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed 64-bit integers in xmm1.                          |
| 66 0f 38 33 /r PMOVZXWD xmm1, xmm2/m64                       | A       | V/V                    | SSE4_1             | Zero extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 32-bit integers in xmm1.                         |
| 66 0f 38 34 /r PMOVZXWQ xmm1, xmm2/m32                       | A       | V/V                    | SSE4_1             | Zero extend 2 packed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed 64-bit integers in xmm1.                         |
| 66 0f 38 35 /r PMOVZXDQ xmm1, xmm2/m64                       | A       | V/V                    | SSE4_1             | Zero extend 2 packed 32-bit integers in the low 8 bytes of xmm2/m64 to 2 packed 64-bit integers in xmm1.                         |
| VEX.128.66.0F38.WIG 30 /r VPMOVZXBW xmm1, xmm2/m64           | A       | V/V                    | AVX                | Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 16-bit integers in xmm1.                          |
| VEX.128.66.0F38.WIG 31 /r VPMOVZXBD xmm1, xmm2/m32           | A       | V/V                    | AVX                | Zero extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm1.                          |
| VEX.128.66.0F38.WIG 32 /r VPMOVZXBQ xmm1, xmm2/m16           | A       | V/V                    | AVX                | Zero extend 2 packed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed 64-bit integers in xmm1.                          |
| VEX.128.66.0F38.WIG 33 /r VPMOVZXWD xmm1, xmm2/m64           | A       | V/V                    | AVX                | Zero extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 32-bit integers in xmm1.                         |
| VEX.128.66.0F38.WIG 34 /r VPMOVZXWQ xmm1, xmm2/m32           | A       | V/V                    | AVX                | Zero extend 2 packed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed 64-bit integers in xmm1.                         |
| VEX.128.66.0F 38.WIG 35 /r VPMOVZXDQ xmm1, xmm2/m64          | A       | V/V                    | AVX                | Zero extend 2 packed 32-bit integers in the low 8 bytes of xmm2/m64 to 2 packed 64-bit integers in xmm1.                         |
| VEX.256.66.0F38.WIG 30 /r VPMOVZXBW ymm1, xmm2/m128          | A       | V/V                    | AVX2               | Zero extend 16 packed 8-bit integers in xmm2/m128 to 16 packed 16-bit integers in ymm1.                                          |
| VEX.256.66.0F38.WIG 31 /r VPMOVZXBD ymm1, xmm2/m64           | A       | V/V                    | AVX2               | Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 32-bit integers in ymm1.                          |
| VEX.256.66.0F38.WIG 32 /r VPMOVZXBQ ymm1, xmm2/m32           | A       | V/V                    | AVX2               | Zero extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 64-bit integers in ymm1.                          |
| VEX.256.66.0F38.WIG 33 /r VPMOVZXWD ymm1, xmm2/m128          | A       | V/V                    | AVX2               | Zero extend 8 packed 16-bit integers xmm2/m128 to 8 packed 32-bit integers in ymm1.                                              |
| VEX.256.66.0F38.WIG 34 /r VPMOVZXWQ ymm1, xmm2/m64           | A       | V/V                    | AVX2               | Zero extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 64-bit integers in xmm1.                         |
| VEX.256.66.0F38.WIG 35 /r VPMOVZXDQ ymm1, xmm2/m128          | A       | V/V                    | AVX2               | Zero extend 4 packed 32-bit integers in xmm2/m128 to 4 packed 64-bit integers in ymm1.                                           |
| EVEX.128.66.0F38 30.WIG /r VPMOVZXBW xmm1 {k1}{z}, xmm2/m64  | B       | V/V                    | AVX512VL AVX512BW  | Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 16-bit integers in xmm1.                          |
| EVEX.256.66.0F38.WIG 30 /r VPMOVZXBW ymm1 {k1}{z}, xmm2/m128 | B       | V/V                    | AVX512VL AVX512BW  | Zero extend 16 packed 8-bit integers in xmm2/m128 to 16 packed 16-bit integers in ymm1.                                          |
| EVEX.512.66.0F38.WIG 30 /r VPMOVZXBW zmm1 {k1}{z}, ymm2/m256 | B       | V/V                    | AVX512BW           | Zero extend 32 packed 8-bit integers in ymm2/m256 to 32 packed 16-bit integers in zmm1.                                          |
| EVEX.128.66.0F38.WIG 31 /r VPMOVZXBD xmm1 {k1}{z}, xmm2/m32  | C       | V/V                    | AVX512VL AVX512F   | Zero extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm1 subject to writemask k1.  |
| EVEX.256.66.0F38.WIG 31 /r VPMOVZXBD ymm1 {k1}{z}, xmm2/m64  | C       | V/V                    | AVX512VL AVX512F   | Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 32-bit integers in ymm1 subject to writemask k1.  |
| EVEX.512.66.0F38.WIG 31 /r VPMOVZXBD zmm1 {k1}{z}, xmm2/m128 | C       | V/V                    | AVX512F            | Zero extend 16 packed 8-bit integers in xmm2/m128 to 16 packed 32-bit integers in zmm1 subject to writemask k1.                  |
| EVEX.128.66.0F38.WIG 32 /r VPMOVZXBQ xmm1 {k1}{z}, xmm2/m16  | D       | V/V                    | AVX512VL AVX512F   | Zero extend 2 packed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed 64-bit integers in xmm1 subject to writemask k1.  |
| EVEX.256.66.0F38.WIG 32 /r VPMOVZXBQ ymm1 {k1}{z}, xmm2/m32  | D       | V/V                    | AVX512VL AVX512F   | Zero extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 64-bit integers in ymm1 subject to writemask k1.  |
| EVEX.512.66.0F38.WIG 32 /r VPMOVZXBQ zmm1 {k1}{z}, xmm2/m64  | D       | V/V                    | AVX512F            | Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 64-bit integers in zmm1 subject to writemask k1.  |
| EVEX.128.66.0F38.WIG 33 /r VPMOVZXWD xmm1 {k1}{z}, xmm2/m64  | B       | V/V                    | AVX512VL AVX512F   | Zero extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 32-bit integers in xmm1 subject to writemask k1. |
| EVEX.256.66.0F38.WIG 33 /r VPMOVZXWD ymm1 {k1}{z}, xmm2/m128 | B       | V/V                    | AVX512VL AVX512F   | Zero extend 8 packed 16-bit integers in xmm2/m128 to 8 packed 32-bit integers in zmm1 subject to writemask k1.                   |
| EVEX.512.66.0F38.WIG 33 /r VPMOVZXWD zmm1 {k1}{z}, ymm2/m256 | B       | V/V                    | AVX512F            | Zero extend 16 packed 16-bit integers in ymm2/m256 to 16 packed 32-bit integers in zmm1 subject to writemask k1.                 |
| EVEX.128.66.0F38.WIG 34 /r VPMOVZXWQ xmm1 {k1}{z}, xmm2/m32  | C       | V/V                    | AVX512VL AVX512F   | Zero extend 2 packed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed 64-bit integers in xmm1 subject to writemask k1. |
| EVEX.256.66.0F38.WIG 34 /r VPMOVZXWQ ymm1 {k1}{z}, xmm2/m64  | C       | V/V                    | AVX512VL AVX512F   | Zero extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 64-bit integers in ymm1 subject to writemask k1. |
| EVEX.512.66.0F38.WIG 34 /r VPMOVZXWQ zmm1 {k1}{z}, xmm2/m128 | C       | V/V                    | AVX512F            | Zero extend 8 packed 16-bit integers in xmm2/m128 to 8 packed 64-bit integers in zmm1 subject to writemask k1.                   |
| EVEX.128.66.0F38.W0 35 /r VPMOVZXDQ xmm1 {k1}{z}, xmm2/m64   | B       | V/V                    | AVX512VL AVX512F   | Zero extend 2 packed 32-bit integers in the low 8 bytes of xmm2/m64 to 2 packed 64-bit integers in zmm1 using writemask k1.      |
| EVEX.256.66.0F38.W0 35 /r VPMOVZXDQ ymm1 {k1}{z}, xmm2/m128  | B       | V/V                    | AVX512VL AVX512F   | Zero extend 4 packed 32-bit integers in xmm2/m128 to 4 packed 64-bit integers in zmm1 using writemask k1.                        |
| EVEX.512.66.0F38.W0 35 /r VPMOVZXDQ zmm1 {k1}{z}, ymm2/m256  | B       | V/V                    | AVX512F            | Zero extend 8 packed 32-bit integers in ymm2/m256 to 8 packed 64-bit integers in zmm1 using writemask k1.                        |


# Operation
----------------------------------------------------------------------------------
Packed_Zero_Extend_BYTE_to_WORD(DEST, SRC) ¶
DEST[15:0] ←ZeroExtend(SRC[7:0]);
DEST[31:16] ←ZeroExtend(SRC[15:8]);
DEST[47:32] ←ZeroExtend(SRC[23:16]);
DEST[63:48] ←ZeroExtend(SRC[31:24]);
DEST[79:64] ←ZeroExtend(SRC[39:32]);
DEST[95:80] ←ZeroExtend(SRC[47:40]);
DEST[111:96] ←ZeroExtend(SRC[55:48]);
DEST[127:112] ←ZeroExtend(SRC[63:56]);

Packed_Zero_Extend_BYTE_to_DWORD(DEST, SRC) ¶
----------------------------------------------------------------------------------
DEST[31:0] ←ZeroExtend(SRC[7:0]);
DEST[63:32] ←ZeroExtend(SRC[15:8]);
DEST[95:64] ←ZeroExtend(SRC[23:16]);
DEST[127:96] ←ZeroExtend(SRC[31:24]);

Packed_Zero_Extend_BYTE_to_QWORD(DEST, SRC) ¶
----------------------------------------------------------------------------------
DEST[63:0] ←ZeroExtend(SRC[7:0]);
DEST[127:64] ←ZeroExtend(SRC[15:8]);

Packed_Zero_Extend_WORD_to_DWORD(DEST, SRC) ¶
----------------------------------------------------------------------------------
DEST[31:0] ←ZeroExtend(SRC[15:0]);
DEST[63:32] ←ZeroExtend(SRC[31:16]);
DEST[95:64] ←ZeroExtend(SRC[47:32]);
DEST[127:96] ←ZeroExtend(SRC[63:48]);

Packed_Zero_Extend_WORD_to_QWORD(DEST, SRC) ¶
----------------------------------------------------------------------------------
DEST[63:0] ←ZeroExtend(SRC[15:0]);
DEST[127:64] ←ZeroExtend(SRC[31:16]);

Packed_Zero_Extend_DWORD_to_QWORD(DEST, SRC) ¶
----------------------------------------------------------------------------------
DEST[63:0] ←ZeroExtend(SRC[31:0]);
DEST[127:64] ←ZeroExtend(SRC[63:32]);